{
  "fetched_at": "2025-12-06T00:24:57.862669",
  "total_papers": 100,
  "papers": [
    {
      "id": "2512.05117v1",
      "title": "The Universal Weight Subspace Hypothesis",
      "authors": [
        "Prakhar Kaushik",
        "Shravan Chaudhari",
        "Ankit Vaidya",
        "Rama Chellappa",
        "Alan Yuille"
      ],
      "abstract": "We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05117v1",
        "pdf": "https://arxiv.org/pdf/2512.05117v1"
      },
      "arxiv_id": "2512.05117v1",
      "comment": "37 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05115v1",
      "title": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
      "authors": [
        "Tianqi Liu",
        "Zhaoxi Chen",
        "Zihao Huang",
        "Shaocong Xu",
        "Saining Zhang",
        "Chongjie Ye",
        "Bohan Li",
        "Zhiguo Cao",
        "Wei Li",
        "Hao Zhao",
        "Ziwei Liu"
      ],
      "abstract": "Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05115v1",
        "pdf": "https://arxiv.org/pdf/2512.05115v1"
      },
      "arxiv_id": "2512.05115v1",
      "comment": "Project Page: https://lightx-ai.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.05116v1",
      "title": "Value Gradient Guidance for Flow Matching Alignment",
      "authors": [
        "Zhen Liu",
        "Tim Z. Xiao",
        "Carles Domingo-Enrich",
        "Weiyang Liu",
        "Dinghuai Zhang"
      ],
      "abstract": "While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05116v1",
        "pdf": "https://arxiv.org/pdf/2512.05116v1"
      },
      "arxiv_id": "2512.05116v1",
      "comment": "Accepted at NeurIPS 2025; 26 pages, 20 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05114v1",
      "title": "Deep infant brain segmentation from multi-contrast MRI",
      "authors": [
        "Malte Hoffmann",
        "Lilla Zöllei",
        "Adrian V. Dalca"
      ],
      "abstract": "Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures. However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints. Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts. This has led to specialized segmentation models that are often limited to specific image types or narrow age groups, or that are fragile for more variable images such as those acquired clinically. We address this method fragmentation with BabySeg, a deep learning brain segmentation framework for infants and young children that supports diverse MRI protocols, including repeat scans and image types unavailable during training. Our approach builds on recent domain randomization techniques, which synthesize training images far beyond realistic bounds to promote dataset shift invariance. We also describe a mechanism that enables models to flexibly pool and interact features from any number of input scans. We demonstrate state-of-the-art performance that matches or exceeds the accuracy of several existing methods for various age cohorts and input configurations using a single model, in a fraction of the runtime required by many existing tools.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05114v1",
        "pdf": "https://arxiv.org/pdf/2512.05114v1"
      },
      "arxiv_id": "2512.05114v1",
      "comment": "8 pages, 8 figures, 1 table, website at https://w3id.org/babyseg, presented at the 2025 IEEE Asilomar Conference on Signals, Systems, and Computers",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05113v1",
      "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
      "authors": [
        "Hao-Jen Chien",
        "Yi-Chuan Huang",
        "Chung-Ho Wu",
        "Wei-Lun Chao",
        "Yu-Lun Liu"
      ],
      "abstract": "Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05113v1",
        "pdf": "https://arxiv.org/pdf/2512.05113v1"
      },
      "arxiv_id": "2512.05113v1",
      "comment": "WACV 2025. Project page: https://chien90190.github.io/splannequin/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.05112v1",
      "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
      "authors": [
        "Dongzhi Jiang",
        "Renrui Zhang",
        "Haodong Li",
        "Zhuofan Zong",
        "Ziyu Guo",
        "Jun He",
        "Claire Guo",
        "Junyan Ye",
        "Rongyao Fang",
        "Weijia Li",
        "Rui Liu",
        "Hongsheng Li"
      ],
      "abstract": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05112v1",
        "pdf": "https://arxiv.org/pdf/2512.05112v1"
      },
      "arxiv_id": "2512.05112v1",
      "comment": "Project Page: https://github.com/CaraJ7/DraCo",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.05111v1",
      "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
      "authors": [
        "Shengyuan Ding",
        "Xinyu Fang",
        "Ziyu Liu",
        "Yuhang Zang",
        "Yuhang Cao",
        "Xiangyu Zhao",
        "Haodong Duan",
        "Xiaoyi Dong",
        "Jianze Liang",
        "Bin Wang",
        "Conghui He",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "abstract": "Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05111v1",
        "pdf": "https://arxiv.org/pdf/2512.05111v1"
      },
      "arxiv_id": "2512.05111v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05110v1",
      "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
      "authors": [
        "Rundong Luo",
        "Noah Snavely",
        "Wei-Chiu Ma"
      ],
      "abstract": "We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05110v1",
        "pdf": "https://arxiv.org/pdf/2512.05110v1"
      },
      "arxiv_id": "2512.05110v1",
      "comment": "Project page: https://red-fairy.github.io/ShadowDraw/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.05105v1",
      "title": "Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning",
      "authors": [
        "Purbesh Mitra",
        "Sennur Ulukus"
      ],
      "abstract": "Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \\textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05105v1",
        "pdf": "https://arxiv.org/pdf/2512.05105v1"
      },
      "arxiv_id": "2512.05105v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05106v1",
      "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
      "authors": [
        "Yu Zeng",
        "Charles Ochoa",
        "Mingyuan Zhou",
        "Vishal M. Patel",
        "Vitor Guizilini",
        "Rowan McAllister"
      ],
      "abstract": "Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion φ-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. φ-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, φ-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, φ-PD improves CARLA-to-Waymo planner performance by 50\\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our \\href{https://yuzeng-at-tri.github.io/ppd-page/}{project page}.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05106v1",
        "pdf": "https://arxiv.org/pdf/2512.05106v1"
      },
      "arxiv_id": "2512.05106v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05104v1",
      "title": "EvoIR: Towards All-in-One Image Restoration via Evolutionary Frequency Modulation",
      "authors": [
        "Jiaqi Ma",
        "Shengkai Hu",
        "Jun Wan",
        "Jiaxing Huang",
        "Lefei Zhang",
        "Salman Khan"
      ],
      "abstract": "All-in-One Image Restoration (AiOIR) tasks often involve diverse degradation that require robust and versatile strategies. However, most existing approaches typically lack explicit frequency modeling and rely on fixed or heuristic optimization schedules, which limit the generalization across heterogeneous degradation. To address these limitations, we propose EvoIR, an AiOIR-specific framework that introduces evolutionary frequency modulation for dynamic and adaptive image restoration. Specifically, EvoIR employs the Frequency-Modulated Module (FMM) that decomposes features into high- and low-frequency branches in an explicit manner and adaptively modulates them to enhance both structural fidelity and fine-grained details. Central to EvoIR, an Evolutionary Optimization Strategy (EOS) iteratively adjusts frequency-aware objectives through a population-based evolutionary process, dynamically balancing structural accuracy and perceptual fidelity. Its evolutionary guidance further mitigates gradient conflicts across degradation and accelerates convergence. By synergizing FMM and EOS, EvoIR yields greater improvements than using either component alone, underscoring their complementary roles. Extensive experiments on multiple benchmarks demonstrate that EvoIR outperforms state-of-the-art AiOIR methods.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05104v1",
        "pdf": "https://arxiv.org/pdf/2512.05104v1"
      },
      "arxiv_id": "2512.05104v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05103v1",
      "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
      "authors": [
        "Xiaochuang Han",
        "Youssef Emad",
        "Melissa Hall",
        "John Nguyen",
        "Karthik Padthe",
        "Liam Robbins",
        "Amir Bar",
        "Delong Chen",
        "Michal Drozdzal",
        "Maha Elbayad",
        "Yushi Hu",
        "Shang-Wen Li",
        "Sreya Dutta Roy",
        "Jakob Verbeek",
        "XuDong Wang",
        "Marjan Ghazvininejad",
        "Luke Zettlemoyer",
        "Emily Dinan"
      ],
      "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05103v1",
        "pdf": "https://arxiv.org/pdf/2512.05103v1"
      },
      "arxiv_id": "2512.05103v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05100v1",
      "title": "Structured Document Translation via Format Reinforcement Learning",
      "authors": [
        "Haiyue Song",
        "Johannes Eschbach-Dymanus",
        "Hour Kaing",
        "Sumire Honda",
        "Hideki Tanaka",
        "Bianka Buschbeck",
        "Masao Utiyama"
      ],
      "abstract": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05100v1",
        "pdf": "https://arxiv.org/pdf/2512.05100v1"
      },
      "arxiv_id": "2512.05100v1",
      "comment": "IJCNLP-AACL 2025 Main (Oral)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05098v1",
      "title": "SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards",
      "authors": [
        "Yuan Gao",
        "Jin Song"
      ],
      "abstract": "In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05098v1",
        "pdf": "https://arxiv.org/pdf/2512.05098v1"
      },
      "arxiv_id": "2512.05098v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05094v1",
      "title": "From Generated Human Videos to Physically Plausible Robot Trajectories",
      "authors": [
        "James Ni",
        "Zekai Wang",
        "Wei Lin",
        "Amir Bar",
        "Yann LeCun",
        "Trevor Darrell",
        "Jitendra Malik",
        "Roei Herzig"
      ],
      "abstract": "Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05094v1",
        "pdf": "https://arxiv.org/pdf/2512.05094v1"
      },
      "arxiv_id": "2512.05094v1",
      "comment": "For project website, see https://genmimic.github.io",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.05092v1",
      "title": "Foundations of Diffusion Models in General State Spaces: A Self-Contained Introduction",
      "authors": [
        "Vincent Pauline",
        "Tobias Höppe",
        "Kirill Neklyudov",
        "Alexander Tong",
        "Stefan Bauer",
        "Andrea Dittadi"
      ],
      "abstract": "Although diffusion models now occupy a central place in generative modeling, introductory treatments commonly assume Euclidean data and seldom clarify their connection to discrete-state analogues. This article is a self-contained primer on diffusion over general state spaces, unifying continuous domains and discrete/categorical structures under one lens. We develop the discrete-time view (forward noising via Markov kernels and learned reverse dynamics) alongside its continuous-time limits -- stochastic differential equations (SDEs) in $\\mathbb{R}^d$ and continuous-time Markov chains (CTMCs) on finite alphabets -- and derive the associated Fokker--Planck and master equations. A common variational treatment yields the ELBO that underpins standard training losses. We make explicit how forward corruption choices -- Gaussian processes in continuous spaces and structured categorical transition kernels (uniform, masking/absorbing and more) in discrete spaces -- shape reverse dynamics and the ELBO. The presentation is layered for three audiences: newcomers seeking a self-contained intuitive introduction; diffusion practitioners wanting a global theoretical synthesis; and continuous-diffusion experts looking for an analogy-first path into discrete diffusion. The result is a unified roadmap to modern diffusion methodology across continuous domains and discrete sequences, highlighting a compact set of reusable proofs, identities, and core theoretical principles.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05092v1",
        "pdf": "https://arxiv.org/pdf/2512.05092v1"
      },
      "arxiv_id": "2512.05092v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05091v1",
      "title": "Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark",
      "authors": [
        "Haobo Yuan",
        "Yueyi Sun",
        "Yanwei Li",
        "Tao Zhang",
        "Xueqing Deng",
        "Henghui Ding",
        "Lu Qi",
        "Anran Wang",
        "Xiangtai Li",
        "Ming-Hsuan Yang"
      ],
      "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering. However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result. This contrasts with human intelligence, which naturally operates through a chain of visual reasoning. To address this limitation, we introduce the Visual Reasoning Tracer (VRT) task, which requires models to not only localize the target object but also explicitly predict the intermediate objects that form the reasoning path. To advance research in this area, we contribute: (1) VRT-Bench, a human-annotated benchmark for evaluating visual reasoning; (2) a new metric for assessing the quality of reasoning traces; and (3) VRT-80k, a large-scale dataset for reasoning model training. Our experiments reveal that while existing models often produce the correct final output, they struggle to ground their intermediate reasoning. In contrast, models trained on VRT-80k achieve substantial improvements in tracing the reasoning path.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05091v1",
        "pdf": "https://arxiv.org/pdf/2512.05091v1"
      },
      "arxiv_id": "2512.05091v1",
      "comment": "Technical Report; Project Page: https://harboryuan.github.io/visual-reasoning-tracer",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.05089v1",
      "title": "The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception",
      "authors": [
        "Eduardo Di Santi"
      ],
      "abstract": "Real-world physical processes do not generate arbitrary variability: their signals concentrate on compact and low-variability subsets of functional space. This geometric structure enables rapid generalization from a few examples in both biological and artificial systems.\n  This work develops a deterministic functional-topological framework in which the set of valid realizations of a physical phenomenon forms a compact perceptual manifold with stable invariants and a finite Hausdorff radius. We show that the boundaries of this manifold can be discovered in a fully self-supervised manner through Monte Carlo sampling, even when the governing equations of the system are unknown.\n  We provide theoretical guarantees, practical estimators of knowledge boundaries, and empirical validations across three domains: electromechanical railway point machines, electrochemical battery discharge curves, and physiological ECG signals.\n  Our results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction, explaining why biological learners and self-supervised AI models can generalize from limited observations.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05089v1",
        "pdf": "https://arxiv.org/pdf/2512.05089v1"
      },
      "arxiv_id": "2512.05089v1",
      "comment": "35 pages, 6 figures. This preprint develops a deterministic functional-topological framework showing that physical systems generate compact perceptual manifolds with finite radius. We provide theory, Monte-Carlo estimators, and validation across PM, battery, and ECG domains, unifying biological perception and self-supervised AI",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05084v1",
      "title": "Gradient Descent with Provably Tuned Learning-rate Schedules",
      "authors": [
        "Dravyansh Sharma"
      ],
      "abstract": "Gradient-based iterative optimization methods are the workhorse of modern machine learning. They crucially rely on careful tuning of parameters like learning rate and momentum. However, one typically sets them using heuristic approaches without formal near-optimality guarantees. Recent work by Gupta and Roughgarden studies how to learn a good step-size in gradient descent. However, like most of the literature with theoretical guarantees for gradient-based optimization, their results rely on strong assumptions on the function class including convexity and smoothness which do not hold in typical applications. In this work, we develop novel analytical tools for provably tuning hyperparameters in gradient-based algorithms that apply to non-convex and non-smooth functions. We obtain matching sample complexity bounds for learning the step-size in gradient descent shown for smooth, convex functions in prior work (up to logarithmic factors) but for a much broader class of functions. Our analysis applies to gradient descent on neural networks with commonly used activation functions (including ReLU, sigmoid and tanh). We extend our framework to tuning multiple hyperparameters, including tuning the learning rate schedule, simultaneously tuning momentum and step-size, and pre-training the initialization vector. Our approach can be used to bound the sample complexity for minimizing both the validation loss as well as the number of gradient descent iterations.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05084v1",
        "pdf": "https://arxiv.org/pdf/2512.05084v1"
      },
      "arxiv_id": "2512.05084v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05081v1",
      "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
      "authors": [
        "Jung Yi",
        "Wooseok Jang",
        "Paul Hyunbin Cho",
        "Jisu Nam",
        "Heeji Yoon",
        "Seungryong Kim"
      ],
      "abstract": "Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05081v1",
        "pdf": "https://arxiv.org/pdf/2512.05081v1"
      },
      "arxiv_id": "2512.05081v1",
      "comment": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.05080v1",
      "title": "OMTRA: A Multi-Task Generative Model for Structure-Based Drug Design",
      "authors": [
        "Ian Dunn",
        "Liv Toft",
        "Tyler Katz",
        "Juhi Gupta",
        "Riya Shah",
        "Ramith Hettiarachchi",
        "David R. Koes"
      ],
      "abstract": "Structure-based drug design (SBDD) focuses on designing small-molecule ligands that bind to specific protein pockets. Computational methods are integral in modern SBDD workflows and often make use of virtual screening methods via docking or pharmacophore search. Modern generative modeling approaches have focused on improving novel ligand discovery by enabling de novo design. In this work, we recognize that these tasks share a common structure and can therefore be represented as different instantiations of a consistent generative modeling framework. We propose a unified approach in OMTRA, a multi-modal flow matching model that flexibly performs many tasks relevant to SBDD, including some with no analogue in conventional workflows. Additionally, we curate a dataset of 500M 3D molecular conformers, complementing protein-ligand data and expanding the chemical diversity available for training. OMTRA obtains state of the art performance on pocket-conditioned de novo design and docking; however, the effects of large-scale pretraining and multi-task training are modest. All code, trained models, and dataset for reproducing this work are available at https://github.com/gnina/OMTRA",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05080v1",
        "pdf": "https://arxiv.org/pdf/2512.05080v1"
      },
      "arxiv_id": "2512.05080v1",
      "comment": "Presented at the Machine Learning for Structural Biology Workshop, 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05079v1",
      "title": "Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints",
      "authors": [
        "Minghan Zhu",
        "Zhiyi Wang",
        "Qihang Sun",
        "Maani Ghaffari",
        "Michael Posa"
      ],
      "abstract": "Object geometry is key information for robot manipulation. Yet, object reconstruction is a challenging task because cameras only capture partial observations of objects, especially when occlusion occurs. In this paper, we leverage two extra sources of information to reduce the ambiguity of vision signals. First, generative models learn priors of the shapes of commonly seen objects, allowing us to make reasonable guesses of the unseen part of geometry. Second, contact information, which can be obtained from videos and physical interactions, provides sparse constraints on the boundary of the geometry. We combine the two sources of information through contact-guided 3D generation. The guidance formulation is inspired by drag-based editing in generative models. Experiments on synthetic and real-world data show that our approach improves the reconstruction compared to pure 3D generation and contact-based optimization.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05079v1",
        "pdf": "https://arxiv.org/pdf/2512.05079v1"
      },
      "arxiv_id": "2512.05079v1",
      "comment": "Project page: https://contactgen3d.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.05076v1",
      "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
      "authors": [
        "Yiming Wang",
        "Qihang Zhang",
        "Shengqu Cai",
        "Tong Wu",
        "Jan Ackermann",
        "Zhengfei Kuang",
        "Yang Zheng",
        "Frano Rajič",
        "Siyu Tang",
        "Gordon Wetzstein"
      ],
      "abstract": "Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05076v1",
        "pdf": "https://arxiv.org/pdf/2512.05076v1"
      },
      "arxiv_id": "2512.05076v1",
      "comment": "Project Page: https://19reborn.github.io/Bullet4D/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.05073v1",
      "title": "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?",
      "authors": [
        "Shashwat Shankar",
        "Subhranshu Pandey",
        "Innocent Dengkhw Mochahari",
        "Bhabesh Mali",
        "Animesh Basak Chowdhury",
        "Sukanta Bhattacharjee",
        "Chandan Karfa"
      ],
      "abstract": "Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05073v1",
        "pdf": "https://arxiv.org/pdf/2512.05073v1"
      },
      "arxiv_id": "2512.05073v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05070v1",
      "title": "Control Consistency Losses for Diffusion Bridges",
      "authors": [
        "Samuel Howard",
        "Nikolas Nüsken",
        "Jakiw Pidstrigach"
      ],
      "abstract": "Simulating the conditioned dynamics of diffusion processes, given their initial and terminal states, is an important but challenging problem in the sciences. The difficulty is particularly pronounced for rare events, for which the unconditioned dynamics rarely reach the terminal state. In this work, we leverage a self-consistency property of the conditioned dynamics to learn the diffusion bridge in an iterative online manner, and demonstrate promising empirical results in a range of settings.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05070v1",
        "pdf": "https://arxiv.org/pdf/2512.05070v1"
      },
      "arxiv_id": "2512.05070v1",
      "comment": "Frontiers in Probabilistic Inference: Sampling Meets Learning Workshop at NeurIPS 2025 (Oral)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05069v1",
      "title": "Hybrid Quantum-Classical Autoencoders for Unsupervised Network Intrusion Detection",
      "authors": [
        "Mohammad Arif Rasyidi",
        "Omar Alhussein",
        "Sami Muhaidat",
        "Ernesto Damiani"
      ],
      "abstract": "Unsupervised anomaly-based intrusion detection requires models that can generalize to attack patterns not observed during training. This work presents the first large-scale evaluation of hybrid quantum-classical (HQC) autoencoders for this task. We construct a unified experimental framework that iterates over key quantum design choices, including quantum-layer placement, measurement approach, variational and non-variational formulations, and latent-space regularization. Experiments across three benchmark NIDS datasets show that HQC autoencoders can match or exceed classical performance in their best configurations, although they exhibit higher sensitivity to architectural decisions. Under zero-day evaluation, well-configured HQC models provide stronger and more stable generalization than classical and supervised baselines. Simulated gate-noise experiments reveal early performance degradation, indicating the need for noise-aware HQC designs. These results provide the first data-driven characterization of HQC autoencoder behavior for network intrusion detection and outline key factors that govern their practical viability. All experiment code and configurations are available at https://github.com/arasyi/hqcae-network-intrusion-detection.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG",
        "cs.CR",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05069v1",
        "pdf": "https://arxiv.org/pdf/2512.05069v1"
      },
      "arxiv_id": "2512.05069v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05066v1",
      "title": "Multi-LLM Collaboration for Medication Recommendation",
      "authors": [
        "Huascar Sanchez",
        "Briland Hitaj",
        "Jules Bergmann",
        "Linda Briesemeister"
      ],
      "abstract": "As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05066v1",
        "pdf": "https://arxiv.org/pdf/2512.05066v1"
      },
      "arxiv_id": "2512.05066v1",
      "comment": "8 pages, 5 figures, 1 table",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05060v1",
      "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
      "authors": [
        "Xianfeng Wu",
        "Yajing Bai",
        "Minghan Li",
        "Xianzu Wu",
        "Xueqi Zhao",
        "Zhongyuan Lai",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "abstract": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05060v1",
        "pdf": "https://arxiv.org/pdf/2512.05060v1"
      },
      "arxiv_id": "2512.05060v1",
      "comment": "Code: https://github.com/hustvl/4DLangVGGT, Webpage: https://hustvl.github.io/4DLangVGGT",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.05058v1",
      "title": "Meta-Learning for Quantum Optimization via Quantum Sequence Model",
      "authors": [
        "Yu-Cheng Lin",
        "Yu-Chao Hsu",
        "Samuel Yen-Chi Chen"
      ],
      "abstract": "The Quantum Approximate Optimization Algorithm (QAOA) is a leading approach for solving combinatorial optimization problems on near-term quantum processors. However, finding good variational parameters remains a significant challenge due to the non-convex energy landscape, often resulting in slow convergence and poor solution quality. In this work, we propose a quantum meta-learning framework that trains advanced quantum sequence models to generate effective parameter initialization policies. We investigate four classical or quantum sequence models, including the Quantum Kernel-based Long Short-Term Memory (QK-LSTM), as learned optimizers in a \"learning to learn\" paradigm. Our numerical experiments on the Max-Cut problem demonstrate that the QK-LSTM optimizer achieves superior performance, obtaining the highest approximation ratios and exhibiting the fastest convergence rate across all tested problem sizes (n=10 to 13). Crucially, the QK-LSTM model achieves perfect parameter transferability by synthesizing a single, fixed set of near-optimal parameters, leading to a remarkable sustained acceleration of convergence even when generalizing to larger problems. This capability, enabled by the compact and expressive power of the quantum kernel architecture, underscores its effectiveness. The QK-LSTM, with only 43 trainable parameters, substantially outperforms the classical LSTM (56 parameters) and other quantum sequence models, establishing a robust pathway toward highly efficient parameter initialization for variational quantum algorithms in the NISQ era.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05058v1",
        "pdf": "https://arxiv.org/pdf/2512.05058v1"
      },
      "arxiv_id": "2512.05058v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05049v1",
      "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
      "authors": [
        "Yu-Chao Hsu",
        "Jiun-Cheng Jiang",
        "Chun-Hua Lin",
        "Kuo-Chung Peng",
        "Nan-Yow Chen",
        "Samuel Yen-Chi Chen",
        "En-Jui Kuo",
        "Hsi-Sheng Goan"
      ],
      "abstract": "Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05049v1",
        "pdf": "https://arxiv.org/pdf/2512.05049v1"
      },
      "arxiv_id": "2512.05049v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05044v1",
      "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
      "authors": [
        "Yanran Zhang",
        "Ziyi Wang",
        "Wenzhao Zheng",
        "Zheng Zhu",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05044v1",
        "pdf": "https://arxiv.org/pdf/2512.05044v1"
      },
      "arxiv_id": "2512.05044v1",
      "comment": "18 Pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05039v1",
      "title": "Semantic-Guided Two-Stage GAN for Face Inpainting with Hybrid Perceptual Encoding",
      "authors": [
        "Abhigyan Bhattacharya",
        "Hiranmoy Roy"
      ],
      "abstract": "Facial Image inpainting aim is to restore the missing or corrupted regions in face images while preserving identity, structural consistency and photorealistic image quality, a task specifically created for photo restoration. Though there are recent lot of advances in deep generative models, existing methods face problems with large irregular masks, often producing blurry textures on the edges of the masked region, semantic inconsistencies, or unconvincing facial structures due to direct pixel level synthesis approach and limited exploitation of facial priors. In this paper we propose a novel architecture, which address these above challenges through semantic-guided hierarchical synthesis. Our approach starts with a method that organizes and synthesizes information based on meaning, followed by refining the texture. This process gives clear insights into the facial structure before we move on to creating detailed images. In the first stage, we blend two techniques: one that focuses on local features with CNNs and global features with Vision Transformers. This helped us create clear and detailed semantic layouts. In the second stage, we use a Multi-Modal Texture Generator to refine these layouts by pulling in information from different scales, ensuring everything looks cohesive and consistent. The architecture naturally handles arbitrary mask configurations through dynamic attention without maskspecific training. Experiment on two datasets CelebA-HQ and FFHQ shows that our model outperforms other state-of-the-art methods, showing improvements in metrics like LPIPS, PSNR, and SSIM. It produces visually striking results with better semantic preservation, in challenging large-area inpainting situations.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05039v1",
        "pdf": "https://arxiv.org/pdf/2512.05039v1"
      },
      "arxiv_id": "2512.05039v1",
      "comment": "Submitted for review CVPR-2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05038v1",
      "title": "SuperActivators: Only the Tail of the Distribution Contains Reliable Concept Signals",
      "authors": [
        "Cassandra Goldberg",
        "Chaehyeon Kim",
        "Adam Stein",
        "Eric Wong"
      ],
      "abstract": "Concept vectors aim to enhance model interpretability by linking internal representations with human-understandable semantics, but their utility is often limited by noisy and inconsistent activations. In this work, we uncover a clear pattern within the noise, which we term the SuperActivator Mechanism: while in-concept and out-of-concept activations overlap considerably, the token activations in the extreme high tail of the in-concept distribution provide a reliable signal of concept presence. We demonstrate the generality of this mechanism by showing that SuperActivator tokens consistently outperform standard vector-based and prompting concept detection approaches, achieving up to a 14% higher F1 score across image and text modalities, model architectures, model layers, and concept extraction techniques. Finally, we leverage SuperActivator tokens to improve feature attributions for concepts.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05038v1",
        "pdf": "https://arxiv.org/pdf/2512.05038v1"
      },
      "arxiv_id": "2512.05038v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05033v1",
      "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation",
      "authors": [
        "Monishwaran Maheswaran",
        "Rishabh Tiwari",
        "Yuezhou Hu",
        "Kerem Dilmen",
        "Coleman Hooper",
        "Haocheng Xi",
        "Nicholas Lee",
        "Mehrdad Farajtabar",
        "Michael W. Mahoney",
        "Kurt Keutzer",
        "Amir Gholami"
      ],
      "abstract": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to $\\sim2\\times$ at matched accuracy.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05033v1",
        "pdf": "https://arxiv.org/pdf/2512.05033v1"
      },
      "arxiv_id": "2512.05033v1",
      "comment": "22 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05030v1",
      "title": "Dual-Path Region-Guided Attention Network for Ground Reaction Force and Moment Regression",
      "authors": [
        "Xuan Li",
        "Samuel Bello"
      ],
      "abstract": "Accurate estimation of three-dimensional ground reaction forces and moments (GRFs/GRMs) is crucial for both biomechanics research and clinical rehabilitation evaluation. In this study, we focus on insole-based GRF/GRM estimation and further validate our approach on a public walking dataset. We propose a Dual-Path Region-Guided Attention Network that integrates anatomy-inspired spatial priors and temporal priors into a region-level attention mechanism, while a complementary path captures context from the full sensor field. The two paths are trained jointly and their outputs are combined to produce the final GRF/GRM predictions. Conclusions: Our model outperforms strong baseline models, including CNN and CNN-LSTM architectures on two datasets, achieving the lowest six-component average NRMSE of 5.78% on the insole dataset and 1.42% for the vertical ground reaction force on the public dataset. This demonstrates robust performance for ground reaction force and moment estimation.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05030v1",
        "pdf": "https://arxiv.org/pdf/2512.05030v1"
      },
      "arxiv_id": "2512.05030v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05025v1",
      "title": "RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation",
      "authors": [
        "Nicolas Houdré",
        "Diego Marcos",
        "Hugo Riffaud de Turckheim",
        "Dino Ienco",
        "Laurent Wendling",
        "Camille Kurtz",
        "Sylvain Lobry"
      ],
      "abstract": "Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05025v1",
        "pdf": "https://arxiv.org/pdf/2512.05025v1"
      },
      "arxiv_id": "2512.05025v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05024v1",
      "title": "Model-Free Assessment of Simulator Fidelity via Quantile Curves",
      "authors": [
        "Garud Iyengar",
        "Yu-Shiou Willy Lin",
        "Kaizheng Wang"
      ],
      "abstract": "Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems. We propose a computationally tractable method to estimate the quantile function of the discrepancy between the simulated and ground-truth outcome distributions. Our approach focuses on output uncertainty and treats the simulator as a black box, imposing no modeling assumptions on its internals, and hence applies broadly across many parameter families, from Bernoulli and multinomial models to continuous, vector-valued settings. The resulting quantile curve supports confidence interval construction for unseen scenarios, risk-aware summaries of sim-to-real discrepancy (e.g., VaR/CVaR), and comparison of simulators' performance. We demonstrate our methodology in an application assessing LLM simulation fidelity on the WorldValueBench dataset spanning four LLMs.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ME",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05024v1",
        "pdf": "https://arxiv.org/pdf/2512.05024v1"
      },
      "arxiv_id": "2512.05024v1",
      "comment": "33 pages, 11 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05021v1",
      "title": "HTR-ConvText: Leveraging Convolution and Textual Information for Handwritten Text Recognition",
      "authors": [
        "Pham Thach Thanh Truc",
        "Dang Hoai Nam",
        "Huynh Tong Dang Khoa",
        "Vo Nguyen Le Duy"
      ],
      "abstract": "Handwritten Text Recognition remains challenging due to the limited data, high writing style variance, and scripts with complex diacritics. Existing approaches, though partially address these issues, often struggle to generalize without massive synthetic data. To address these challenges, we propose HTR-ConvText, a model designed to capture fine-grained, stroke-level local features while preserving global contextual dependencies. In the feature extraction stage, we integrate a residual Convolutional Neural Network backbone with a MobileViT with Positional Encoding block. This enables the model to both capture structural patterns and learn subtle writing details. We then introduce the ConvText encoder, a hybrid architecture combining global context and local features within a hierarchical structure that reduces sequence length for improved efficiency. Additionally, an auxiliary module injects textual context to mitigate the weakness of Connectionist Temporal Classification. Evaluations on IAM, READ2016, LAM and HANDS-VNOnDB demonstrate that our approach achieves improved performance and better generalization compared to existing methods, especially in scenarios with limited training samples and high handwriting diversity.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05021v1",
        "pdf": "https://arxiv.org/pdf/2512.05021v1"
      },
      "arxiv_id": "2512.05021v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05016v1",
      "title": "Generative Neural Video Compression via Video Diffusion Prior",
      "authors": [
        "Qi Mao",
        "Hao Cheng",
        "Tinghan Yang",
        "Libiao Jin",
        "Siwei Ma"
      ],
      "abstract": "We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05016v1",
        "pdf": "https://arxiv.org/pdf/2512.05016v1"
      },
      "arxiv_id": "2512.05016v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05013v1",
      "title": "Detecting Perspective Shifts in Multi-agent Systems",
      "authors": [
        "Eric Bridgeford",
        "Hayden Helm"
      ],
      "abstract": "Generative models augmented with external tools and update mechanisms (or \\textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.AI",
        "cs.MA",
        "stat.ME"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05013v1",
        "pdf": "https://arxiv.org/pdf/2512.05013v1"
      },
      "arxiv_id": "2512.05013v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05006v1",
      "title": "Self-Supervised Learning for Transparent Object Depth Completion Using Depth from Non-Transparent Objects",
      "authors": [
        "Xianghui Fan",
        "Zhaoyu Chen",
        "Mengyang Pan",
        "Anping Deng",
        "Hang Yang"
      ],
      "abstract": "The perception of transparent objects is one of the well-known challenges in computer vision. Conventional depth sensors have difficulty in sensing the depth of transparent objects due to refraction and reflection of light. Previous research has typically train a neural network to complete the depth acquired by the sensor, and this method can quickly and accurately acquire accurate depth maps of transparent objects. However, previous training relies on a large amount of annotation data for supervision, and the labeling of depth maps is costly. To tackle this challenge, we propose a new self-supervised method for training depth completion networks. Our method simulates the depth deficits of transparent objects within non-transparent regions and utilizes the original depth map as ground truth for supervision. Experiments demonstrate that our method achieves performance comparable to supervised approach, and pre-training with our method can improve the model performance when the training samples are small.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05006v1",
        "pdf": "https://arxiv.org/pdf/2512.05006v1"
      },
      "arxiv_id": "2512.05006v1",
      "comment": "conference",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.05000v1",
      "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
      "authors": [
        "Daniyar Zakarin",
        "Thiemo Wandel",
        "Anton Obukhov",
        "Dengxin Dai"
      ],
      "abstract": "We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.05000v1",
        "pdf": "https://arxiv.org/pdf/2512.05000v1"
      },
      "arxiv_id": "2512.05000v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04996v1",
      "title": "A dynamic memory assignment strategy for dilation-based ICP algorithm on embedded GPUs",
      "authors": [
        "Qiong Chang",
        "Weimin Wang",
        "Junpei Zhong",
        "Jun Miyazaki"
      ],
      "abstract": "This paper proposes a memory-efficient optimization strategy for the high-performance point cloud registration algorithm VANICP, enabling lightweight execution on embedded GPUs with constrained hardware resources. VANICP is a recently published acceleration framework that significantly improves the computational efficiency of point-cloud-based applications. By transforming the global nearest neighbor search into a localized process through a dilation-based information propagation mechanism, VANICP greatly reduces the computational complexity of the NNS. However, its original implementation demands a considerable amount of memory, which restricts its deployment in resource-constrained environments such as embedded systems. To address this issue, we propose a GPU-oriented dynamic memory assignment strategy that optimizes the memory usage of the dilation operation. Furthermore, based on this strategy, we construct an enhanced version of the VANICP framework that achieves over 97% reduction in memory consumption while preserving the original performance. Source code is published on: https://github.com/changqiong/VANICP4Em.git.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04996v1",
        "pdf": "https://arxiv.org/pdf/2512.04996v1"
      },
      "arxiv_id": "2512.04996v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04992v1",
      "title": "Evolutionary Architecture Search through Grammar-Based Sequence Alignment",
      "authors": [
        "Adri Gómez Martín",
        "Felix Möller",
        "Steven McDonagh",
        "Monica Abella",
        "Manuel Desco",
        "Elliot J. Crowley",
        "Aaron Klein",
        "Linus Ericsson"
      ],
      "abstract": "Neural architecture search (NAS) in expressive search spaces is a computationally hard problem, but it also holds the potential to automatically discover completely novel and performant architectures. To achieve this we need effective search algorithms that can identify powerful components and reuse them in new candidate architectures. In this paper, we introduce two adapted variants of the Smith-Waterman algorithm for local sequence alignment and use them to compute the edit distance in a grammar-based evolutionary architecture search. These algorithms enable us to efficiently calculate a distance metric for neural architectures and to generate a set of hybrid offspring from two parent models. This facilitates the deployment of crossover-based search heuristics, allows us to perform a thorough analysis on the architectural loss landscape, and track population diversity during search. We highlight how our method vastly improves computational complexity over previous work and enables us to efficiently compute shortest paths between architectures. When instantiating the crossover in evolutionary searches, we achieve competitive results, outperforming competing methods. Future work can build upon this new tool, discovering novel components that can be used more broadly across neural architecture design, and broadening its applications beyond NAS.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04992v1",
        "pdf": "https://arxiv.org/pdf/2512.04992v1"
      },
      "arxiv_id": "2512.04992v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04988v1",
      "title": "Strategic Self-Improvement for Competitive Agents in AI Labour Markets",
      "authors": [
        "Christopher Chiu",
        "Simpson Zhang",
        "Mihaela van der Schaar"
      ],
      "abstract": "As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: \\textbf{metacognition} (accurate self-assessment of skills), \\textbf{competitive awareness} (modeling rivals and market dynamics), and \\textbf{long-horizon strategic planning}. We illustrate our framework through a tractable simulated gig economy where agentic Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our simulations illustrate how LLM agents explicitly prompted with reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulations reproduce classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04988v1",
        "pdf": "https://arxiv.org/pdf/2512.04988v1"
      },
      "arxiv_id": "2512.04988v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04985v1",
      "title": "Towards a unified framework for guided diffusion models",
      "authors": [
        "Yuchen Jiao",
        "Yuxin Chen",
        "Gen Li"
      ],
      "abstract": "Guided or controlled data generation with diffusion models\\blfootnote{Partial preliminary results of this work appeared in International Conference on Machine Learning 2025 \\citep{li2025provable}.} has become a cornerstone of modern generative modeling. Despite substantial advances in diffusion model theory, the theoretical understanding of guided diffusion samplers remains severely limited. We make progress by developing a unified algorithmic and theoretical framework that accommodates both diffusion guidance and reward-guided diffusion. Aimed at fine-tuning diffusion models to improve certain rewards, we propose injecting a reward guidance term -- constructed from the difference between the original and reward-reweighted scores -- into the backward diffusion process, and rigorously quantify the resulting reward improvement over the unguided counterpart. As a key application, our framework shows that classifier-free guidance (CFG) decreases the expected reciprocal of the classifier probability, providing the first theoretical characterization of the specific performance metric that CFG improves for general target distributions. When applied to reward-guided diffusion, our framework yields a new sampler that is easy-to-train and requires no full diffusion trajectories during training. Numerical experiments further corroborate our theoretical findings.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04985v1",
        "pdf": "https://arxiv.org/pdf/2512.04985v1"
      },
      "arxiv_id": "2512.04985v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04981v1",
      "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
      "authors": [
        "NaHyeon Park",
        "Namin An",
        "Kunhee Kim",
        "Soyeon Yoon",
        "Jiahao Huo",
        "Hyunjung Shim"
      ],
      "abstract": "Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04981v1",
        "pdf": "https://arxiv.org/pdf/2512.04981v1"
      },
      "arxiv_id": "2512.04981v1",
      "comment": "Project page: https://fairpro-t2i.github.io",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.04980v1",
      "title": "Learning Causality for Longitudinal Data",
      "authors": [
        "Mouad EL Bouchattaoui"
      ],
      "abstract": "This thesis develops methods for causal inference and causal representation learning (CRL) in high-dimensional, time-varying data.\n  The first contribution introduces the Causal Dynamic Variational Autoencoder (CDVAE), a model for estimating Individual Treatment Effects (ITEs) by capturing unobserved heterogeneity in treatment response driven by latent risk factors that affect only outcomes. CDVAE comes with theoretical guarantees on valid latent adjustment and generalization bounds for ITE error. Experiments on synthetic and real datasets show that CDVAE outperforms baselines, and that state-of-the-art models greatly improve when augmented with its latent substitutes, approaching oracle performance without access to true adjustment variables.\n  The second contribution proposes an efficient framework for long-term counterfactual regression based on RNNs enhanced with Contrastive Predictive Coding (CPC) and InfoMax. It captures long-range dependencies under time-varying confounding while avoiding the computational cost of transformers, achieving state-of-the-art results and introducing CPC into causal inference.\n  The third contribution advances CRL by addressing how latent causes manifest in observed variables. We introduce a model-agnostic interpretability layer based on the geometry of the decoder Jacobian. A sparse self-expression prior induces modular, possibly overlapping groups of observed features aligned with shared latent influences. We provide recovery guarantees in both disjoint and overlapping settings and show that meaningful latent-to-observed structure can be recovered without anchor features or single-parent assumptions. Scalable Jacobian-based regularization techniques are also developed.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "stat.ML",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04980v1",
        "pdf": "https://arxiv.org/pdf/2512.04980v1"
      },
      "arxiv_id": "2512.04980v1",
      "comment": "PhD thesis manuscript",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04974v1",
      "title": "Efficient Generative Transformer Operators For Million-Point PDEs",
      "authors": [
        "Armand Kassaï Koupaï",
        "Lise Le Boudec",
        "Patrick Gallinari"
      ],
      "abstract": "We introduce ECHO, a transformer-operator framework for generating million-point PDE trajectories. While existing neural operators (NOs) have shown promise for solving partial differential equations, they remain limited in practice due to poor scalability on dense grids, error accumulation during dynamic unrolling, and task-specific design. ECHO addresses these challenges through three key innovations. (i) It employs a hierarchical convolutional encode-decode architecture that achieves a 100 $\\times$ spatio-temporal compression while preserving fidelity on mesh points. (ii) It incorporates a training and adaptation strategy that enables high-resolution PDE solution generation from sparse input grids. (iii) It adopts a generative modeling paradigm that learns complete trajectory segments, mitigating long-horizon error drift. The training strategy decouples representation learning from downstream task supervision, allowing the model to tackle multiple tasks such as trajectory generation, forward and inverse problems, and interpolation. The generative model further supports both conditional and unconditional generation. We demonstrate state-of-the-art performance on million-point simulations across diverse PDE systems featuring complex geometries, high-frequency dynamics, and long-term horizons.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04974v1",
        "pdf": "https://arxiv.org/pdf/2512.04974v1"
      },
      "arxiv_id": "2512.04974v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04970v1",
      "title": "Stable Single-Pixel Contrastive Learning for Semantic and Geometric Tasks",
      "authors": [
        "Leonid Pogorelyuk",
        "Niels Bracher",
        "Aaron Verkleeren",
        "Lars Kühmichel",
        "Stefan T. Radev"
      ],
      "abstract": "We pilot a family of stable contrastive losses for learning pixel-level representations that jointly capture semantic and geometric information. Our approach maps each pixel of an image to an overcomplete descriptor that is both view-invariant and semantically meaningful. It enables precise point-correspondence across images without requiring momentum-based teacher-student training. Two experiments in synthetic 2D and 3D environments demonstrate the properties of our loss and the resulting overcomplete representations.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04970v1",
        "pdf": "https://arxiv.org/pdf/2512.04970v1"
      },
      "arxiv_id": "2512.04970v1",
      "comment": "UniReps Workshop 2025, 12 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04969v1",
      "title": "Rethinking the Use of Vision Transformers for AI-Generated Image Detection",
      "authors": [
        "NaHyeon Park",
        "Kunhee Kim",
        "Junsuk Choe",
        "Hyunjung Shim"
      ],
      "abstract": "Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04969v1",
        "pdf": "https://arxiv.org/pdf/2512.04969v1"
      },
      "arxiv_id": "2512.04969v1",
      "comment": "Code: https://github.com/nahyeonkaty/mold",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.04967v1",
      "title": "Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis",
      "authors": [
        "Jasmaine Khale",
        "Ravi Prakash Srivastava"
      ],
      "abstract": "Automated retinal disease diagnosis is vital given the rising prevalence of conditions such as diabetic retinopathy and macular degeneration. Conventional deep learning approaches require large annotated datasets, which are costly and often imbalanced across disease categories, limiting their reliability in practice. Few-shot learning (FSL) addresses this challenge by enabling models to generalize from only a few labeled samples per class. In this study,we propose a balanced few-shot episodic learning framework tailored to the Retinal Fundus Multi-Disease Image Dataset (RFMiD). Focusing on the ten most represented classes, which still show substantial imbalance between majority diseases (e.g., Diabetic Retinopathy, Macular Hole) and minority ones (e.g., Optic Disc Edema, Branch Retinal Vein Occlusion), our method integrates three key components: (i) balanced episodic sampling, ensuring equal participation of all classes in each 5-way 5-shot episode; (ii) targeted augmentation, including Contrast Limited Adaptive Histogram Equalization (CLAHE) and color/geometry transformations, to improve minority-class di- versity; and (iii) a ResNet-50 encoder pretrained on ImageNet, selected for its superior ability to capture fine-grained retinal features. Prototypes are computed in the embedding space and classification is performed with cosine similarity for improved stability. Trained on 100 episodes and evaluated on 1,000 test episodes, our framework achieves substantial accuracy gains and reduces bias toward majority classes, with notable improvements for underrepresented diseases. These results demonstrate that dataset-aware few-shot pipelines, combined with balanced sampling and CLAHE-enhanced preprocessing, can deliver more robust and clinically fair retinal disease diagnosis under data-constrained conditions.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04967v1",
        "pdf": "https://arxiv.org/pdf/2512.04967v1"
      },
      "arxiv_id": "2512.04967v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04966v1",
      "title": "Environment-Aware Channel Inference via Cross-Modal Flow: From Multimodal Sensing to Wireless Channels",
      "authors": [
        "Guangming Liang",
        "Mingjie Yang",
        "Dongzhu Liu",
        "Paul Henderson",
        "Lajos Hanzo"
      ],
      "abstract": "Accurate channel state information (CSI) underpins reliable and efficient wireless communication. However, acquiring CSI via pilot estimation incurs substantial overhead, especially in massive multiple-input multiple-output (MIMO) systems operating in high-Doppler environments. By leveraging the growing availability of environmental sensing data, this treatise investigates pilot-free channel inference that estimates complete CSI directly from multimodal observations, including camera images, LiDAR point clouds, and GPS coordinates. In contrast to prior studies that rely on predefined channel models, we develop a data-driven framework that formulates the sensing-to-channel mapping as a cross-modal flow matching problem. The framework fuses multimodal features into a latent distribution within the channel domain, and learns a velocity field that continuously transforms the latent distribution toward the channel distribution. To make this formulation tractable and efficient, we reformulate the problem as an equivalent conditional flow matching objective and incorporate a modality alignment loss, while adopting low-latency inference mechanisms to enable real-time CSI estimation. In experiments, we build a procedural data generator based on Sionna and Blender to support realistic modeling of sensing scenes and wireless propagation. System-level evaluations demonstrate significant improvements over pilot- and sensing-based benchmarks in both channel estimation accuracy and spectral efficiency for the downstream beamforming task.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.IT",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.IT",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04966v1",
        "pdf": "https://arxiv.org/pdf/2512.04966v1"
      },
      "arxiv_id": "2512.04966v1",
      "comment": "13 pages, 13 figures, 40 references, submitted to IEEE for possible publication",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04963v1",
      "title": "GeoPE:A Unified Geometric Positional Embedding for Structured Tensors",
      "authors": [
        "Yupu Yao",
        "Bowen Yang"
      ],
      "abstract": "Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04963v1",
        "pdf": "https://arxiv.org/pdf/2512.04963v1"
      },
      "arxiv_id": "2512.04963v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04958v1",
      "title": "Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning",
      "authors": [
        "Roberto Cipollone",
        "Luca Iocchi",
        "Matteo Leonetti"
      ],
      "abstract": "The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04958v1",
        "pdf": "https://arxiv.org/pdf/2512.04958v1"
      },
      "arxiv_id": "2512.04958v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04957v1",
      "title": "LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics",
      "authors": [
        "Weiye Shi",
        "Zhaowei Zhang",
        "Shaoheng Yan",
        "Yaodong Yang"
      ],
      "abstract": "Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04957v1",
        "pdf": "https://arxiv.org/pdf/2512.04957v1"
      },
      "arxiv_id": "2512.04957v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04954v1",
      "title": "Amortized Inference of Multi-Modal Posteriors using Likelihood-Weighted Normalizing Flows",
      "authors": [
        "Rajneil Baruah"
      ],
      "abstract": "We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG",
        "hep-ex",
        "hep-ph",
        "physics.comp-ph",
        "physics.data-an"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04954v1",
        "pdf": "https://arxiv.org/pdf/2512.04954v1"
      },
      "arxiv_id": "2512.04954v1",
      "comment": "14 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04952v1",
      "title": "FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization",
      "authors": [
        "Yicheng Liu",
        "Shiduo Zhang",
        "Zibin Dong",
        "Baijun Ye",
        "Tianyuan Yuan",
        "Xiaopeng Yu",
        "Linqi Yin",
        "Chenhao Lu",
        "Junhao Shi",
        "Luca Jiang-Tao Yu",
        "Liangtao Zheng",
        "Tao Jiang",
        "Jingjing Gong",
        "Xipeng Qiu",
        "Hang Zhao"
      ],
      "abstract": "Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04952v1",
        "pdf": "https://arxiv.org/pdf/2512.04952v1"
      },
      "arxiv_id": "2512.04952v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04949v1",
      "title": "CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent",
      "authors": [
        "Leyang Shen",
        "Yang Zhang",
        "Chun Kai Ling",
        "Xiaoyan Zhao",
        "Tat-Seng Chua"
      ],
      "abstract": "Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for multi-step agents. CARL achieves focused training through providing action-level optimization signals for high-criticality actions while excluding low-criticality actions from model update. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency during training and inference across diverse evaluation settings.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04949v1",
        "pdf": "https://arxiv.org/pdf/2512.04949v1"
      },
      "arxiv_id": "2512.04949v1",
      "comment": "10 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04943v1",
      "title": "Towards Adaptive Fusion of Multimodal Deep Networks for Human Action Recognition",
      "authors": [
        "Novanto Yudistira"
      ],
      "abstract": "This study introduces a pioneering methodology for human action recognition by harnessing deep neural network techniques and adaptive fusion strategies across multiple modalities, including RGB, optical flows, audio, and depth information. Employing gating mechanisms for multimodal fusion, we aim to surpass limitations inherent in traditional unimodal recognition methods while exploring novel possibilities for diverse applications. Through an exhaustive investigation of gating mechanisms and adaptive weighting-based fusion architectures, our methodology enables the selective integration of relevant information from various modalities, thereby bolstering both accuracy and robustness in action recognition tasks. We meticulously examine various gated fusion strategies to pinpoint the most effective approach for multimodal action recognition, showcasing its superiority over conventional unimodal methods. Gating mechanisms facilitate the extraction of pivotal features, resulting in a more holistic representation of actions and substantial enhancements in recognition performance. Our evaluations across human action recognition, violence action detection, and multiple self-supervised learning tasks on benchmark datasets demonstrate promising advancements in accuracy. The significance of this research lies in its potential to revolutionize action recognition systems across diverse fields. The fusion of multimodal information promises sophisticated applications in surveillance and human-computer interaction, especially in contexts related to active assisted living.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04943v1",
        "pdf": "https://arxiv.org/pdf/2512.04943v1"
      },
      "arxiv_id": "2512.04943v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04939v1",
      "title": "LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging",
      "authors": [
        "Zhijian Shu",
        "Cheng Lin",
        "Tao Xie",
        "Wei Yin",
        "Ben Li",
        "Zhiyuan Pu",
        "Weize Li",
        "Yao Yao",
        "Xun Cao",
        "Xiaoyang Guo",
        "Xiao-Xiao Long"
      ],
      "abstract": "3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04939v1",
        "pdf": "https://arxiv.org/pdf/2512.04939v1"
      },
      "arxiv_id": "2512.04939v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04938v1",
      "title": "Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases",
      "authors": [
        "Raquel Norel",
        "Michele Merler",
        "Pavitra Modi"
      ],
      "abstract": "Patients with rare neurological diseases report cognitive symptoms -\"brain fog\"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived \"Proficiency in Verbal Discourse\" correlates with blood phenylalanine (p = -0.50, p < 0.005) but not standard cognitive tests (all |r| < 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04938v1",
        "pdf": "https://arxiv.org/pdf/2512.04938v1"
      },
      "arxiv_id": "2512.04938v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04927v1",
      "title": "Virtually Unrolling the Herculaneum Papyri by Diffeomorphic Spiral Fitting",
      "authors": [
        "Paul Henderson"
      ],
      "abstract": "The Herculaneum Papyri are a collection of rolled papyrus documents that were charred and buried by the famous eruption of Mount Vesuvius. They promise to contain a wealth of previously unseen Greek and Latin texts, but are extremely fragile and thus most cannot be unrolled physically. A solution to access these texts is virtual unrolling, where the papyrus surface is digitally traced out in a CT scan of the scroll, to create a flattened representation. This tracing is very laborious to do manually in gigavoxel-sized scans, so automated approaches are desirable. We present the first top-down method that automatically fits a surface model to a CT scan of a severely damaged scroll. We take a novel approach that globally fits an explicit parametric model of the deformed scroll to existing neural network predictions of where the rolled papyrus likely passes. Our method guarantees the resulting surface is a single continuous 2D sheet, even passing through regions where the surface is not detectable in the CT scan. We conduct comprehensive experiments on high-resolution CT scans of two scrolls, showing that our approach successfully unrolls large regions, and exceeds the performance of the only existing automated unrolling method suitable for this data.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04927v1",
        "pdf": "https://arxiv.org/pdf/2512.04927v1"
      },
      "arxiv_id": "2512.04927v1",
      "comment": "Accepted at WACV 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04926v1",
      "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
      "authors": [
        "Yueming Pan",
        "Ruoyu Feng",
        "Qi Dai",
        "Yuqi Wang",
        "Wenfeng Lin",
        "Mingyu Guo",
        "Chong Luo",
        "Nanning Zheng"
      ],
      "abstract": "Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04926v1",
        "pdf": "https://arxiv.org/pdf/2512.04926v1"
      },
      "arxiv_id": "2512.04926v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04923v1",
      "title": "Algorithmic Thinking Theory",
      "authors": [
        "MohammadHossein Bateni",
        "Vincent Cohen-Addad",
        "Yuzhou Gu",
        "Silvio Lattanzi",
        "Simon Meierhans",
        "Christopher Mohri"
      ],
      "abstract": "Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle.\n  We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04923v1",
        "pdf": "https://arxiv.org/pdf/2512.04923v1"
      },
      "arxiv_id": "2512.04923v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04921v1",
      "title": "The AI Consumer Index (ACE)",
      "authors": [
        "Julien Benchek",
        "Rohit Shetty",
        "Benjamin Hunsberger",
        "Ajay Arun",
        "Zach Richards",
        "Brendan Foody",
        "Osvald Nitski",
        "Bertie Vidgen"
      ],
      "abstract": "We introduce the first version of the AI Consumer Index (ACE), a benchmark for assessing whether frontier AI models can perform high-value consumer tasks. ACE contains a hidden heldout set of 400 test cases, split across four consumer activities: shopping, food, gaming, and DIY. We are also open sourcing 80 cases as a devset with a CC-BY license. For the ACE leaderboard we evaluated 10 frontier models (with websearch turned on) using a novel grading methodology that dynamically checks whether relevant parts of the response are grounded in the retrieved web sources. GPT 5 (Thinking = High) is the top-performing model, scoring 56.1%, followed by o3 Pro (Thinking = On) (55.2%) and GPT 5.1 (Thinking = High) (55.1%). Models differ across domains, and in Shopping the top model scores under 50%. For some requests (such as giving the correct price or providing working links), models are highly prone to hallucination. Overall, ACE shows a substantial gap between the performance of even the best models and consumers' AI needs.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04921v1",
        "pdf": "https://arxiv.org/pdf/2512.04921v1"
      },
      "arxiv_id": "2512.04921v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04918v1",
      "title": "Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty",
      "authors": [
        "Kailiang Liu",
        "Ying Chen",
        "Ralf Borndörfer",
        "Thorsten Koch"
      ],
      "abstract": "Intraday surgical scheduling is a multi-objective decision problem under uncertainty-balancing elective throughput, urgent and emergency demand, delays, sequence-dependent setups, and overtime. We formulate the problem as a cooperative Markov game and propose a multi-agent reinforcement learning (MARL) framework in which each operating room (OR) is an agent trained with centralized training and decentralized execution. All agents share a policy trained via Proximal Policy Optimization (PPO), which maps rich system states to actions, while a within-epoch sequential assignment protocol constructs conflict-free joint schedules across ORs. A mixed-integer pre-schedule provides reference starting times for electives; we impose type-specific quadratic delay penalties relative to these references and a terminal overtime penalty, yielding a single reward that captures throughput, timeliness, and staff workload. In simulations reflecting a realistic hospital mix (six ORs, eight surgery types, random urgent and emergency arrivals), the learned policy outperforms six rule-based heuristics across seven metrics and three evaluation subsets, and, relative to an ex post MIP oracle, quantifies optimality gaps. Policy analytics reveal interpretable behavior-prioritizing emergencies, batching similar cases to reduce setups, and deferring lower-value electives. We also derive a suboptimality bound for the sequential decomposition under simplifying assumptions. We discuss limitations-including OR homogeneity and the omission of explicit staffing constraints-and outline extensions. Overall, the approach offers a practical, interpretable, and tunable data-driven complement to optimization for real-time OR scheduling.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04918v1",
        "pdf": "https://arxiv.org/pdf/2512.04918v1"
      },
      "arxiv_id": "2512.04918v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04912v1",
      "title": "A result relating convex n-widths to covering numbers with some applications to neural networks",
      "authors": [
        "Jonathan Baxter",
        "Peter Bartlett"
      ],
      "abstract": "In general, approximating classes of functions defined over high-dimensional input spaces by linear combinations of a fixed set of basis functions or ``features'' is known to be hard. Typically, the worst-case error of the best basis set decays only as fast as $Θ\\(n^{-1/d}\\)$, where $n$ is the number of basis functions and $d$ is the input dimension. However, there are many examples of high-dimensional pattern recognition problems (such as face recognition) where linear combinations of small sets of features do solve the problem well. Hence these function classes do not suffer from the ``curse of dimensionality'' associated with more general classes. It is natural then, to look for characterizations of high-dimensional function classes that nevertheless are approximated well by linear combinations of small sets of features. In this paper we give a general result relating the error of approximation of a function class to the covering number of its ``convex core''. For one-hidden-layer neural networks, covering numbers of the class of functions computed by a single hidden node upper bound the covering numbers of the convex core. Hence, using standard results we obtain upper bounds on the approximation rate of neural network classes.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04912v1",
        "pdf": "https://arxiv.org/pdf/2512.04912v1"
      },
      "arxiv_id": "2512.04912v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04910v1",
      "title": "Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming",
      "authors": [
        "Fang Li"
      ],
      "abstract": "This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04910v1",
        "pdf": "https://arxiv.org/pdf/2512.04910v1"
      },
      "arxiv_id": "2512.04910v1",
      "comment": "Accepted by the 43rd IEEE International Conference on Computer Design (ICCD 2025)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04904v1",
      "title": "ReflexFlow: Rethinking Learning Objective for Exposure Bias Alleviation in Flow Matching",
      "authors": [
        "Guanbo Huang",
        "Jingjia Mao",
        "Fanding Huang",
        "Fengkai Liu",
        "Xiangyang Luo",
        "Yaoyuan Liang",
        "Jiasheng Lu",
        "Xiaoe Wang",
        "Pei Liu",
        "Ruiliu Fu",
        "Shao-Lun Huang"
      ],
      "abstract": "Despite tremendous recent progress, Flow Matching methods still suffer from exposure bias due to discrepancies in training and inference. This paper investigates the root causes of exposure bias in Flow Matching, including: (1) the model lacks generalization to biased inputs during training, and (2) insufficient low-frequency content captured during early denoising, leading to accumulated bias. Based on these insights, we propose ReflexFlow, a simple and effective reflexive refinement of the Flow Matching learning objective that dynamically corrects exposure bias. ReflexFlow consists of two components: (1) Anti-Drift Rectification (ADR), which reflexively adjusts prediction targets for biased inputs utilizing a redesigned loss under training-time scheduled sampling; and (2) Frequency Compensation (FC), which reflects on missing low-frequency components and compensates them by reweighting the loss using exposure bias. ReflexFlow is model-agnostic, compatible with all Flow Matching frameworks, and improves generation quality across datasets. Experiments on CIFAR-10, CelebA-64, and ImageNet-256 show that ReflexFlow outperforms prior approaches in mitigating exposure bias, achieving a 35.65% reduction in FID on CelebA-64.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04904v1",
        "pdf": "https://arxiv.org/pdf/2512.04904v1"
      },
      "arxiv_id": "2512.04904v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04895v1",
      "title": "Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems",
      "authors": [
        "M Zeeshan",
        "Saud Satti"
      ],
      "abstract": "Multimodal Artificial Intelligence (AI) systems, particularly Vision-Language Models (VLMs), have become integral to critical applications ranging from autonomous decision-making to automated document processing. As these systems scale, they rely heavily on preprocessing pipelines to handle diverse inputs efficiently. However, this dependency on standard preprocessing operations, specifically image downscaling, creates a significant yet often overlooked security vulnerability. While intended for computational optimization, scaling algorithms can be exploited to conceal malicious visual prompts that are invisible to human observers but become active semantic instructions once processed by the model. Current adversarial strategies remain largely static, failing to account for the dynamic nature of modern agentic workflows. To address this gap, we propose Chameleon, a novel, adaptive adversarial framework designed to expose and exploit scaling vulnerabilities in production VLMs. Unlike traditional static attacks, Chameleon employs an iterative, agent-based optimization mechanism that dynamically refines image perturbations based on the target model's real-time feedback. This allows the framework to craft highly robust adversarial examples that survive standard downscaling operations to hijack downstream execution. We evaluate Chameleon against Gemini 2.5 Flash model. Our experiments demonstrate that Chameleon achieves an Attack Success Rate (ASR) of 84.5% across varying scaling factors, significantly outperforming static baseline attacks which average only 32.1%. Furthermore, we show that these attacks effectively compromise agentic pipelines, reducing decision-making accuracy by over 45% in multi-step tasks. Finally, we discuss the implications of these vulnerabilities and propose multi-scale consistency checks as a necessary defense mechanism.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04895v1",
        "pdf": "https://arxiv.org/pdf/2512.04895v1"
      },
      "arxiv_id": "2512.04895v1",
      "comment": "5 pages, 2 figures, IEEE Transactions on Dependable and Secure Computing",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04890v1",
      "title": "Equivariant Symmetry-Aware Head Pose Estimation for Fetal MRI",
      "authors": [
        "Ramya Muthukrishnan",
        "Borjan Gagoski",
        "Aryn Lee",
        "P. Ellen Grant",
        "Elfar Adalsteinsson",
        "Polina Golland",
        "Benjamin Billot"
      ],
      "abstract": "We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04890v1",
        "pdf": "https://arxiv.org/pdf/2512.04890v1"
      },
      "arxiv_id": "2512.04890v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04888v1",
      "title": "You Only Train Once (YOTO): A Retraining-Free Object Detection Framework",
      "authors": [
        "Priyanto Hidayatullah",
        "Nurjannah Syakrani",
        "Yudi Widhiyasana",
        "Muhammad Rizqi Sholahuddin",
        "Refdinal Tubagus",
        "Zahri Al Adzani Hidayat",
        "Hanri Fajar Ramadhan",
        "Dafa Alfarizki Pratama",
        "Farhan Muhammad Yasin"
      ],
      "abstract": "Object detection constitutes the primary task within the domain of computer vision. It is utilized in numerous domains. Nonetheless, object detection continues to encounter the issue of catastrophic forgetting. The model must be retrained whenever new products are introduced, utilizing not only the new products dataset but also the entirety of the previous dataset. The outcome is obvious: increasing model training expenses and significant time consumption. In numerous sectors, particularly retail checkout, the frequent introduction of new products presents a great challenge. This study introduces You Only Train Once (YOTO), a methodology designed to address the issue of catastrophic forgetting by integrating YOLO11n for object localization with DeIT and Proxy Anchor Loss for feature extraction and metric learning. For classification, we utilize cosine similarity between the embedding features of the target product and those in the Qdrant vector database. In a case study conducted in a retail store with 140 products, the experimental results demonstrate that our proposed framework achieves encouraging accuracy, whether for detecting new or existing products. Furthermore, without retraining, the training duration difference is significant. We achieve almost 3 times the training time efficiency compared to classical object detection approaches. This efficiency escalates as additional new products are added to the product database. The average inference time is 580 ms per image containing multiple products, on an edge device, validating the proposed framework's feasibility for practical use.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04888v1",
        "pdf": "https://arxiv.org/pdf/2512.04888v1"
      },
      "arxiv_id": "2512.04888v1",
      "comment": "under review in the Elsevier Engineering Journal",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04883v1",
      "title": "SDG-Track: A Heterogeneous Observer-Follower Framework for High-Resolution UAV Tracking on Embedded Platforms",
      "authors": [
        "Jiawen Wen",
        "Yu Hu",
        "Suixuan Qiu",
        "Jinshan Huang",
        "Xiaowen Chu"
      ],
      "abstract": "Real-time tracking of small unmanned aerial vehicles (UAVs) on edge devices faces a fundamental resolution-speed conflict. Downsampling high-resolution imagery to standard detector input sizes causes small target features to collapse below detectable thresholds. Yet processing native 1080p frames on resource-constrained platforms yields insufficient throughput for smooth gimbal control. We propose SDG-Track, a Sparse Detection-Guided Tracker that adopts an Observer-Follower architecture to reconcile this conflict. The Observer stream runs a high-capacity detector at low frequency on the GPU to provide accurate position anchors from 1920x1080 frames. The Follower stream performs high-frequency trajectory interpolation via ROI-constrained sparse optical flow on the CPU. To handle tracking failures from occlusion or model drift caused by spectrally similar distractors, we introduce Dual-Space Recovery, a training-free re-acquisition mechanism combining color histogram matching with geometric consistency constraints. Experiments on a ground-to-air tracking station demonstrate that SDG-Track achieves 35.1 FPS system throughput while retaining 97.2\\% of the frame-by-frame detection precision. The system successfully tracks agile FPV drones under real-world operational conditions on an NVIDIA Jetson Orin Nano. Our paper code is publicly available at https://github.com/Jeffry-wen/SDG-Track",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04883v1",
        "pdf": "https://arxiv.org/pdf/2512.04883v1"
      },
      "arxiv_id": "2512.04883v1",
      "comment": "https://github.com/Jeffry-wen/SDG-Track",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.04875v1",
      "title": "SP-Det: Self-Prompted Dual-Text Fusion for Generalized Multi-Label Lesion Detection",
      "authors": [
        "Qing Xu",
        "Yanqian Wang",
        "Xiangjian Hea",
        "Yue Li",
        "Yixuan Zhang",
        "Rong Qu",
        "Wenting Duan",
        "Zhen Chen"
      ],
      "abstract": "Automated lesion detection in chest X-rays has demonstrated significant potential for improving clinical diagnosis by precisely localizing pathological abnormalities. While recent promptable detection frameworks have achieved remarkable accuracy in target localization, existing methods typically rely on manual annotations as prompts, which are labor-intensive and impractical for clinical applications. To address this limitation, we propose SP-Det, a novel self-prompted detection framework that automatically generates rich textual context to guide multi-label lesion detection without requiring expert annotations. Specifically, we introduce an expert-free dual-text prompt generator (DTPG) that leverages two complementary textual modalities: semantic context prompts that capture global pathological patterns and disease beacon prompts that focus on disease-specific manifestations. Moreover, we devise a bidirectional feature enhancer (BFE) that synergistically integrates comprehensive diagnostic context with disease-specific embeddings to significantly improve feature representation and detection accuracy. Extensive experiments on two chest X-ray datasets with diverse thoracic disease categories demonstrate that our SP-Det framework outperforms state-of-the-art detection methods while completely eliminating the dependency on expert-annotated prompts compared to existing promptable architectures.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04875v1",
        "pdf": "https://arxiv.org/pdf/2512.04875v1"
      },
      "arxiv_id": "2512.04875v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04874v1",
      "title": "Shorting Dynamics and Structured Kernel Regularization",
      "authors": [
        "James Tian"
      ],
      "abstract": "This paper develops a nonlinear operator dynamic that progressively removes the influence of a prescribed feature subspace while retaining maximal structure elsewhere. The induced sequence of positive operators is monotone, admits an exact residual decomposition, and converges to the classical shorted operator. Transporting this dynamic to reproducing kernel Hilbert spaces yields a corresponding family of kernels that converges to the largest kernel dominated by the original one and annihilating the given subspace. In the finite-sample setting, the associated Gram operators inherit a structured residual decomposition that leads to a canonical form of kernel ridge regression and a principled way to enforce nuisance invariance. This gives a unified operator-analytic approach to invariant kernel construction and structured regularization in data analysis.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "math.FA",
        "cs.LG"
      ],
      "primary_category": "math.FA",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04874v1",
        "pdf": "https://arxiv.org/pdf/2512.04874v1"
      },
      "arxiv_id": "2512.04874v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04871v1",
      "title": "STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions",
      "authors": [
        "Junjie Fan",
        "Hongye Zhao",
        "Linduo Wei",
        "Jiayu Rao",
        "Guijia Li",
        "Jiaxin Yuan",
        "Wenqi Xu",
        "Yong Qi"
      ],
      "abstract": "Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04871v1",
        "pdf": "https://arxiv.org/pdf/2512.04871v1"
      },
      "arxiv_id": "2512.04871v1",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04869v1",
      "title": "Developing a General Personal Tutor for Education",
      "authors": [
        "Jaan Aru",
        "Kristjan-Julius Laak"
      ],
      "abstract": "The vision of a universal AI tutor has remained elusive, despite decades of effort. Could LLMs be the game-changer? We overview novel issues arising from developing a nationwide AI tutor. We highlight the practical questions that point to specific gaps in our scientific understanding of the learning process.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CY",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.CY",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04869v1",
        "pdf": "https://arxiv.org/pdf/2512.04869v1"
      },
      "arxiv_id": "2512.04869v1",
      "comment": "",
      "journal_ref": "Trends in Cognitive Sciences, 29 (11),957-960 (2025)",
      "has_code": false
    },
    {
      "id": "2512.04868v1",
      "title": "SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs",
      "authors": [
        "Hao Wang",
        "Jialun Zhong",
        "Changcheng Wang",
        "Zhujun Nie",
        "Zheng Li",
        "Shunyu Yao",
        "Yanzeng Li",
        "Xinchi Li"
      ],
      "abstract": "Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully executable S-expression. This decomposition not only simplifies logical form generation but also significantly enhances structural fidelity and linking efficiency. Crucially, SEAL incorporates a self-evolving mechanism that integrates local and global memory with a reflection module, enabling continuous adaptation from dialog history and execution feedback without explicit retraining. Extensive experiments on the SPICE benchmark demonstrate that SEAL achieves state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks. The results validate notable gains in both structural accuracy and computational efficiency, underscoring the framework's capacity for robust and scalable conversational reasoning.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04868v1",
        "pdf": "https://arxiv.org/pdf/2512.04868v1"
      },
      "arxiv_id": "2512.04868v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04865v1",
      "title": "Series of quasi-uniform scatterings with fast search, root systems and neural network classifications",
      "authors": [
        "Igor V. Netay"
      ],
      "abstract": "In this paper we describe an approach to construct large extendable collections of vectors in predefined spaces of given dimensions. These collections are useful for neural network latent space configuration and training. For classification problem with large or unknown number of classes this allows to construct classifiers without classification layer and extend the number of classes without retraining of network from the very beginning. The construction allows to create large well-spaced vector collections in spaces of minimal possible dimension. If the number of classes is known or approximately predictable, one can choose sufficient enough vector collection size. If one needs to significantly extend the number of classes, one can extend the collection in the same latent space, or to incorporate the collection into collection of higher dimensions with same spacing between vectors. Also, regular symmetric structure of constructed vector collections can significantly simplify problems of search for nearest cluster centers or embeddings in the latent space. Construction of vector collections is based on combinatorics and geometry of semi-simple Lie groups irreducible representations with highest weight.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "math.AG",
        "cs.LG",
        "math.RT"
      ],
      "primary_category": "math.AG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04865v1",
        "pdf": "https://arxiv.org/pdf/2512.04865v1"
      },
      "arxiv_id": "2512.04865v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04864v1",
      "title": "Are Your Agents Upward Deceivers?",
      "authors": [
        "Dadi Guo",
        "Qingyu Liu",
        "Dongrui Liu",
        "Qihan Ren",
        "Shuai Shao",
        "Tianyi Qiu",
        "Haoran Li",
        "Yi R. Fung",
        "Zhongjie Ba",
        "Juntao Dai",
        "Jiaming Ji",
        "Zhikai Chen",
        "Jialing Tao",
        "Yaodong Yang",
        "Jing Shao",
        "Xia Hu"
      ],
      "abstract": "Large Language Model (LLM)-based agents are increasingly used as autonomous subordinates that carry out tasks for users. This raises the question of whether they may also engage in deception, similar to how individuals in human organizations lie to superiors to create a good image or avoid punishment. We observe and define agentic upward deception, a phenomenon in which an agent facing environmental constraints conceals its failure and performs actions that were not requested without reporting. To assess its prevalence, we construct a benchmark of 200 tasks covering five task types and eight realistic scenarios in a constrained environment, such as broken tools or mismatched information sources. Evaluations of 11 popular LLMs reveal that these agents typically exhibit action-based deceptive behaviors, such as guessing results, performing unsupported simulations, substituting unavailable information sources, and fabricating local files. We further test prompt-based mitigation and find only limited reductions, suggesting that it is difficult to eliminate and highlighting the need for stronger mitigation strategies to ensure the safety of LLM-based agents.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04864v1",
        "pdf": "https://arxiv.org/pdf/2512.04864v1"
      },
      "arxiv_id": "2512.04864v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04862v1",
      "title": "Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing",
      "authors": [
        "Maria-Paola Forte",
        "Nikos Athanasiou",
        "Giulia Ballardini",
        "Jan Ulrich Bartels",
        "Katherine J. Kuchenbecker",
        "Michael J. Black"
      ],
      "abstract": "Capturing accurate 3D human pose in the wild would provide valuable data for training pose estimation and motion generation methods. While video-based estimation approaches have become increasingly accurate, they often fail in common scenarios involving self-contact, such as a hand touching the face. In contrast, wearable bioimpedance sensing can cheaply and unobtrusively measure ground-truth skin-to-skin contact. Consequently, we propose a novel framework that combines visual pose estimators with bioimpedance sensing to capture the 3D pose of people by taking self-contact into account. Our method, BioTUCH, initializes the pose using an off-the-shelf estimator and introduces contact-aware pose optimization during measured self-contact: reprojection error and deviations from the input estimate are minimized while enforcing vertex proximity constraints. We validate our approach using a new dataset of synchronized RGB video, bioimpedance measurements, and 3D motion capture. Testing with three input pose estimators, we demonstrate an average of 11.7% improvement in reconstruction accuracy. We also present a miniature wearable bioimpedance sensor that enables efficient large-scale collection of contact-aware training data for improving pose estimation and generation using BioTUCH. Code and data are available at biotuch.is.tue.mpg.de",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04862v1",
        "pdf": "https://arxiv.org/pdf/2512.04862v1"
      },
      "arxiv_id": "2512.04862v1",
      "comment": "* Equal contribution. Minor figure corrections compared to the ICCV 2025 version",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04857v1",
      "title": "Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens",
      "authors": [
        "Ziran Qin",
        "Youru Lv",
        "Mingbao Lin",
        "Zeren Zhang",
        "Chanfan Gan",
        "Tieyuan Chen",
        "Weiyao Lin"
      ],
      "abstract": "Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \\textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04857v1",
        "pdf": "https://arxiv.org/pdf/2512.04857v1"
      },
      "arxiv_id": "2512.04857v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04854v1",
      "title": "From Task Executors to Research Partners: Evaluating AI Co-Pilots Through Workflow Integration in Biomedical Research",
      "authors": [
        "Lukas Weidener",
        "Marko Brkić",
        "Chiara Bacci",
        "Mihailo Jovanović",
        "Emre Ulgac",
        "Alex Dobrin",
        "Johannes Weniger",
        "Martin Vlas",
        "Ritvik Singh",
        "Aakaash Meduri"
      ],
      "abstract": "Artificial intelligence systems are increasingly deployed in biomedical research. However, current evaluation frameworks may inadequately assess their effectiveness as research collaborators. This rapid review examines benchmarking practices for AI systems in preclinical biomedical research. Three major databases and two preprint servers were searched from January 1, 2018 to October 31, 2025, identifying 14 benchmarks that assess AI capabilities in literature understanding, experimental design, and hypothesis generation. The results revealed that all current benchmarks assess isolated component capabilities, including data analysis quality, hypothesis validity, and experimental protocol design. However, authentic research collaboration requires integrated workflows spanning multiple sessions, with contextual memory, adaptive dialogue, and constraint propagation. This gap implies that systems excelling on component benchmarks may fail as practical research co-pilots. A process-oriented evaluation framework is proposed that addresses four critical dimensions absent from current benchmarks: dialogue quality, workflow orchestration, session continuity, and researcher experience. These dimensions are essential for evaluating AI systems as research co-pilots rather than as isolated task executors.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04854v1",
        "pdf": "https://arxiv.org/pdf/2512.04854v1"
      },
      "arxiv_id": "2512.04854v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04847v1",
      "title": "Language Models as Semantic Teachers: Post-Training Alignment for Medical Audio Understanding",
      "authors": [
        "Tsai-Ning Wang",
        "Lin-Lin Chen",
        "Neil Zeghidour",
        "Aaqib Saeed"
      ],
      "abstract": "Pre-trained audio models excel at detecting acoustic patterns in auscultation sounds but often fail to grasp their clinical significance, limiting their use and performance in diagnostic tasks. To bridge this gap, we introduce AcuLa (Audio-Clinical Understanding via Language Alignment), a lightweight post-training framework that instills semantic understanding into any audio encoder by aligning it with a medical language model, which acts as a \"semantic teacher.\" To enable alignment at scale, we construct a large-scale dataset by leveraging off-the-shelf large language models to translate the rich, structured metadata accompanying existing audio recordings into coherent clinical reports. Our alignment strategy combines a representation-level contrastive objective with a self-supervised modeling, ensuring that the model learns clinical semantics while preserving fine-grained temporal cues. AcuLa achieves state-of-the-art results across 18 diverse cardio-respiratory tasks from 10 different datasets, improving the mean AUROC on classification benchmarks from 0.68 to 0.79 and, on the most challenging COVID-19 cough detection task, boosting the AUROC from 0.55 to 0.89. Our work demonstrates that this audio-language alignment transforms purely acoustic models into clinically-aware diagnostic tools, establishing a novel paradigm for enhancing physiological understanding in audio-based health monitoring.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04847v1",
        "pdf": "https://arxiv.org/pdf/2512.04847v1"
      },
      "arxiv_id": "2512.04847v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04844v1",
      "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
      "authors": [
        "Atsuki Yamaguchi",
        "Terufumi Morishita",
        "Aline Villavicencio",
        "Nikolaos Aletras"
      ],
      "abstract": "Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04844v1",
        "pdf": "https://arxiv.org/pdf/2512.04844v1"
      },
      "arxiv_id": "2512.04844v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04843v1",
      "title": "From Symptoms to Systems: An Expert-Guided Approach to Understanding Risks of Generative AI for Eating Disorders",
      "authors": [
        "Amy Winecoff",
        "Kevin Klyman"
      ],
      "abstract": "Generative AI systems may pose serious risks to individuals vulnerable to eating disorders. Existing safeguards tend to overlook subtle but clinically significant cues, leaving many risks unaddressed. To better understand the nature of these risks, we conducted semi-structured interviews with 15 clinicians, researchers, and advocates with expertise in eating disorders. Using abductive qualitative analysis, we developed an expert-guided taxonomy of generative AI risks across seven categories: (1) providing generalized health advice; (2) encouraging disordered behaviors; (3) supporting symptom concealment; (4) creating thinspiration; (5) reinforcing negative self-beliefs; (6) promoting excessive focus on the body; and (7) perpetuating narrow views about eating disorders. Our results demonstrate how certain user interactions with generative AI systems intersect with clinical features of eating disorders in ways that may intensify risk. We discuss implications of our work, including approaches for risk assessment, safeguard design, and participatory evaluation practices with domain experts.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04843v1",
        "pdf": "https://arxiv.org/pdf/2512.04843v1"
      },
      "arxiv_id": "2512.04843v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04841v1",
      "title": "SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security",
      "authors": [
        "Wei Zhao",
        "Zhe Li",
        "Jun Sun"
      ],
      "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses.\n  In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\\% detection accuracy across multiple threat types.\n  By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at https://github.com/Amadeuszhao/SOK_Casuality.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04841v1",
        "pdf": "https://arxiv.org/pdf/2512.04841v1"
      },
      "arxiv_id": "2512.04841v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04837v1",
      "title": "A Sanity Check for Multi-In-Domain Face Forgery Detection in the Real World",
      "authors": [
        "Jikang Cheng",
        "Renye Yan",
        "Zhiyuan Yan",
        "Yaozhong Gan",
        "Xueyi Zhang",
        "Zhongyuan Wang",
        "Wei Peng",
        "Ling Liang"
      ],
      "abstract": "Existing methods for deepfake detection aim to develop generalizable detectors. Although \"generalizable\" is the ultimate target once and for all, with limited training forgeries and domains, it appears idealistic to expect generalization that covers entirely unseen variations, especially given the diversity of real-world deepfakes. Therefore, introducing large-scale multi-domain data for training can be feasible and important for real-world applications. However, within such a multi-domain scenario, the differences between multiple domains, rather than the subtle real/fake distinctions, dominate the feature space. As a result, despite detectors being able to relatively separate real and fake within each domain (i.e., high AUC), they struggle with single-image real/fake judgments in domain-unspecified conditions (i.e., low ACC). In this paper, we first define a new research paradigm named Multi-In-Domain Face Forgery Detection (MID-FFD), which includes sufficient volumes of real-fake domains for training. Then, the detector should provide definitive real-fake judgments to the domain-unspecified inputs, which simulate the frame-by-frame independent detection scenario in the real world. Meanwhile, to address the domain-dominant issue, we propose a model-agnostic framework termed DevDet (Developer for Detector) to amplify real/fake differences and make them dominant in the feature space. DevDet consists of a Face Forgery Developer (FFDev) and a Dose-Adaptive detector Fine-Tuning strategy (DAFT). Experiments demonstrate our superiority in predicting real-fake under the MID-FFD scenario while maintaining original generalization ability to unseen data.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04837v1",
        "pdf": "https://arxiv.org/pdf/2512.04837v1"
      },
      "arxiv_id": "2512.04837v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04834v1",
      "title": "Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case",
      "authors": [
        "Vignesh Kumar Kembu",
        "Pierandrea Morandini",
        "Marta Bianca Maria Ranzini",
        "Antonino Nocera"
      ],
      "abstract": "Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04834v1",
        "pdf": "https://arxiv.org/pdf/2512.04834v1"
      },
      "arxiv_id": "2512.04834v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04832v1",
      "title": "Tokenizing Buildings: A Transformer for Layout Synthesis",
      "authors": [
        "Manuel Ladron de Guevara",
        "Jinmo Rhee",
        "Ardavan Bidgoli",
        "Vaidas Razgaitis",
        "Michael Bergin"
      ],
      "abstract": "We introduce Small Building Model (SBM), a Transformer-based architecture for layout synthesis in Building Information Modeling (BIM) scenes. We address the question of how to tokenize buildings by unifying heterogeneous feature sets of architectural elements into sequences while preserving compositional structure. Such feature sets are represented as a sparse attribute-feature matrix that captures room properties. We then design a unified embedding module that learns joint representations of categorical and possibly correlated continuous feature groups. Lastly, we train a single Transformer backbone in two modes: an encoder-only pathway that yields high-fidelity room embeddings, and an encoder-decoder pipeline for autoregressive prediction of room entities, referred to as Data-Driven Entity Prediction (DDEP). Experiments across retrieval and generative layout synthesis show that SBM learns compact room embeddings that reliably cluster by type and topology, enabling strong semantic retrieval. In DDEP mode, SBM produces functionally sound layouts, with fewer collisions and boundary violations and improved navigability.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04832v1",
        "pdf": "https://arxiv.org/pdf/2512.04832v1"
      },
      "arxiv_id": "2512.04832v1",
      "comment": "8 pages, 1 page References, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04830v1",
      "title": "FreeGen: Feed-Forward Reconstruction-Generation Co-Training for Free-Viewpoint Driving Scene Synthesis",
      "authors": [
        "Shijie Chen",
        "Peixi Peng"
      ],
      "abstract": "Closed-loop simulation and scalable pre-training for autonomous driving require synthesizing free-viewpoint driving scenes. However, existing datasets and generative pipelines rarely provide consistent off-trajectory observations, limiting large-scale evaluation and training. While recent generative models demonstrate strong visual realism, they struggle to jointly achieve interpolation consistency and extrapolation realism without per-scene optimization. To address this, we propose FreeGen, a feed-forward reconstruction-generation co-training framework for free-viewpoint driving scene synthesis. The reconstruction model provides stable geometric representations to ensure interpolation consistency, while the generation model performs geometry-aware enhancement to improve realism at unseen viewpoints. Through co-training, generative priors are distilled into the reconstruction model to improve off-trajectory rendering, and the refined geometry in turn offers stronger structural guidance for generation. Experiments demonstrate that FreeGen achieves state-of-the-art performance for free-viewpoint driving scene synthesis.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04830v1",
        "pdf": "https://arxiv.org/pdf/2512.04830v1"
      },
      "arxiv_id": "2512.04830v1",
      "comment": "Novel View Synthesis, Driving Scene, Free Trajectory, Image Generation",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04829v1",
      "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
      "authors": [
        "Rasul Tutunov",
        "Alexandre Maraval",
        "Antoine Grosnit",
        "Xihan Li",
        "Jun Wang",
        "Haitham Bou-Ammar"
      ],
      "abstract": "Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension $n=8$, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions $4-16$, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04829v1",
        "pdf": "https://arxiv.org/pdf/2512.04829v1"
      },
      "arxiv_id": "2512.04829v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04827v1",
      "title": "Contract-Driven QoE Auditing for Speech and Singing Services: From MOS Regression to Service Graphs",
      "authors": [
        "Wenzhang Du"
      ],
      "abstract": "Subjective mean opinion scores (MOS) remain the de-facto target for non-intrusive speech and singing quality assessment. However, MOS is a scalar that collapses heterogeneous user expectations, ignores service-level objectives, and is difficult to compare across deployment graphs. We propose a contract-driven QoE auditing framework: each service graph G is evaluated under a set of human-interpretable experience contracts C, yielding a contract-level satisfaction vector Q(G, C). We show that (i) classical MOS regression is a special case with a degenerate contract set, (ii) contract-driven quality is more stable than MOS under graph view transformations (e.g., pooling by system vs. by system type), and (iii) the effective sample complexity of learning contracts is governed by contract semantics rather than merely the dimensionality of C. We instantiate the framework on URGENT2024 MOS (6.9k speech utterances with raw rating vectors) and SingMOS v1 (7,981 singing clips; 80 systems). On URGENT, we train a contract-aware neural auditor on self-supervised WavLM embeddings; on SingMOS, we perform contract-driven graph auditing using released rating vectors and metadata without decoding audio. Empirically, our auditor matches strong MOS predictors in MOS accuracy while providing calibrated contract probabilities; on SingMOS, Q(G, C) exhibits substantially smaller cross-view drift than raw MOS and graph-only baselines; on URGENT, difficulty curves reveal that mis-specified \"simple\" contracts can be harder to learn than richer but better aligned contract sets.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04827v1",
        "pdf": "https://arxiv.org/pdf/2512.04827v1"
      },
      "arxiv_id": "2512.04827v1",
      "comment": "11 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04822v1",
      "title": "Enabling Ethical AI: A case study in using Ontological Context for Justified Agentic AI Decisions",
      "authors": [
        "Liam McGee",
        "James Harvey",
        "Lucy Cull",
        "Andreas Vermeulen",
        "Bart-Floris Visscher",
        "Malvika Sharan"
      ],
      "abstract": "In this preprint, we present A collaborative human-AI approach to building an inspectable semantic layer for Agentic AI. AI agents first propose candidate knowledge structures from diverse data sources; domain experts then validate, correct, and extend these structures, with their feedback used to improve subsequent models. Authors show how this process captures tacit institutional knowledge, improves response quality and efficiency, and mitigates institutional amnesia. We argue for a shift from post-hoc explanation to justifiable Agentic AI, where decisions are grounded in explicit, inspectable evidence and reasoning accessible to both experts and non-specialists.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04822v1",
        "pdf": "https://arxiv.org/pdf/2512.04822v1"
      },
      "arxiv_id": "2512.04822v1",
      "comment": "24 pages including references, with 6 images and 2 tables. Appendices, supporting data and additional reference provided from page 25 to 117",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04821v1",
      "title": "LatentFM: A Latent Flow Matching Approach for Generative Medical Image Segmentation",
      "authors": [
        "Huynh Trinh Ngoc",
        "Hoang Anh Nguyen Kim",
        "Toan Nguyen Hai",
        "Long Tran Quoc"
      ],
      "abstract": "Generative models have achieved remarkable progress with the emergence of flow matching (FM). It has demonstrated strong generative capabilities and attracted significant attention as a simulation-free flow-based framework capable of learning exact data densities. Motivated by these advances, we propose LatentFM, a flow-based model operating in the latent space for medical image segmentation. To model the data distribution, we first design two variational autoencoders (VAEs) to encode both medical images and their corresponding masks into a lower-dimensional latent space. We then estimate a conditional velocity field that guides the flow based on the input image. By sampling multiple latent representations, our method synthesizes diverse segmentation outputs whose pixel-wise variance reliably captures the underlying data distribution, enabling both highly accurate and uncertainty-aware predictions. Furthermore, we generate confidence maps that quantify the model certainty, providing clinicians with richer information for deeper analysis. We conduct experiments on two datasets, ISIC-2018 and CVC-Clinic, and compare our method with several prior baselines, including both deterministic and generative approach models. Through comprehensive evaluations, both qualitative and quantitative results show that our approach achieves superior segmentation accuracy while remaining highly efficient in the latent space.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04821v1",
        "pdf": "https://arxiv.org/pdf/2512.04821v1"
      },
      "arxiv_id": "2512.04821v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04815v1",
      "title": "RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS",
      "authors": [
        "Chuanyu Fu",
        "Guanying Chen",
        "Yuqi Zhang",
        "Kunbin Yao",
        "Yuan Xiong",
        "Chuan Huang",
        "Shuguang Cui",
        "Yasuyuki Matsushita",
        "Xiaochun Cao"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling in-the-wild scenes affected by transient objects and illuminations, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances and illumination variations. To address this, we propose RobustSplat++, a robust solution based on several critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Third, we incorporate the delayed Gaussian growth strategy and mask bootstrapping with appearance modeling to handling in-the-wild scenes including transients and illuminations. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04815v1",
        "pdf": "https://arxiv.org/pdf/2512.04815v1"
      },
      "arxiv_id": "2512.04815v1",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2506.02751",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04814v1",
      "title": "Shared Multi-modal Embedding Space for Face-Voice Association",
      "authors": [
        "Christopher Simic",
        "Korbinian Riedhammer",
        "Tobias Bocklet"
      ],
      "abstract": "The FAME 2026 challenge comprises two demanding tasks: training face-voice associations combined with a multilingual setting that includes testing on languages on which the model was not trained. Our approach consists of separate uni-modal processing pipelines with general face and voice feature extraction, complemented by additional age-gender feature extraction to support prediction. The resulting single-modal features are projected into a shared embedding space and trained with an Adaptive Angular Margin (AAM) loss. Our approach achieved first place in the FAME 2026 challenge, with an average Equal-Error Rate (EER) of 23.99%.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.SD",
        "cs.CV"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04814v1",
        "pdf": "https://arxiv.org/pdf/2512.04814v1"
      },
      "arxiv_id": "2512.04814v1",
      "comment": "Ranked 1st in Fame 2026 Challenge, ICASSP",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.04810v1",
      "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
      "authors": [
        "Xin He",
        "Longhui Wei",
        "Jianbo Ouyang",
        "Lingxi Xie",
        "Qi Tian"
      ],
      "abstract": "We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04810v1",
        "pdf": "https://arxiv.org/pdf/2512.04810v1"
      },
      "arxiv_id": "2512.04810v1",
      "comment": "Project Page: https://emma-umm.github.io/emma/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.04808v1",
      "title": "Setting up for failure: automatic discovery of the neural mechanisms of cognitive errors",
      "authors": [
        "Puria Radmard",
        "Paul M. Bays",
        "Máté Lengyel"
      ],
      "abstract": "Discovering the neural mechanisms underpinning cognition is one of the grand challenges of neuroscience. However, previous approaches for building models of RNN dynamics that explain behaviour required iterative refinement of architectures and/or optimisation objectives, resulting in a piecemeal, and mostly heuristic, human-in-the-loop process. Here, we offer an alternative approach that automates the discovery of viable RNN mechanisms by explicitly training RNNs to reproduce behaviour, including the same characteristic errors and suboptimalities, that humans and animals produce in a cognitive task. Achieving this required two main innovations. First, as the amount of behavioural data that can be collected in experiments is often too limited to train RNNs, we use a non-parametric generative model of behavioural responses to produce surrogate data for training RNNs. Second, to capture all relevant statistical aspects of the data, we developed a novel diffusion model-based approach for training RNNs. To showcase the potential of our approach, we chose a visual working memory task as our test-bed, as behaviour in this task is well known to produce response distributions that are patently multimodal (due to swap errors). The resulting network dynamics correctly qualitative features of macaque neural data. Importantly, these results were not possible to obtain with more traditional approaches, i.e., when only a limited set of behavioural signatures (rather than the full richness of behavioural response distributions) were fitted, or when RNNs were trained for task optimality (instead of reproducing behaviour). Our approach also yields novel predictions about the mechanism of swap errors, which can be readily tested in experiments. These results suggest that fitting RNNs to rich patterns of behaviour provides a powerful way to automatically discover mechanisms of important cognitive functions.",
      "published": "2025-12-04",
      "updated": "2025-12-04",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.04808v1",
        "pdf": "https://arxiv.org/pdf/2512.04808v1"
      },
      "arxiv_id": "2512.04808v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    }
  ]
}