{
  "fetched_at": "2025-12-03T00:25:54.606331",
  "total_papers": 100,
  "papers": [
    {
      "id": "2512.02020v1",
      "title": "EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI",
      "authors": [
        "Jianlei Chang",
        "Ruofeng Mei",
        "Wei Ke",
        "Xiangyu Xu"
      ],
      "abstract": "Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02020v1",
        "pdf": "https://arxiv.org/pdf/2512.02020v1"
      },
      "arxiv_id": "2512.02020v1",
      "comment": "Accepted by AAAI 2026. Project Page: https://efficientflow.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.02019v1",
      "title": "A Diffusion Model Framework for Maximum Entropy Reinforcement Learning",
      "authors": [
        "Sebastian Sanokowski",
        "Kaustubh Patil",
        "Alois Knoll"
      ],
      "abstract": "Diffusion models have achieved remarkable success in data-driven learning and in sampling from complex, unnormalized target distributions. Building on this progress, we reinterpret Maximum Entropy Reinforcement Learning (MaxEntRL) as a diffusion model-based sampling problem. We tackle this problem by minimizing the reverse Kullback-Leibler (KL) divergence between the diffusion policy and the optimal policy distribution using a tractable upper bound. By applying the policy gradient theorem to this objective, we derive a modified surrogate objective for MaxEntRL that incorporates diffusion dynamics in a principled way. This leads to simple diffusion-based variants of Soft Actor-Critic (SAC), Proximal Policy Optimization (PPO) and Wasserstein Policy Optimization (WPO), termed DiffSAC, DiffPPO and DiffWPO. All of these methods require only minor implementation changes to their base algorithm. We find that on standard continuous control benchmarks, DiffSAC, DiffPPO and DiffWPO achieve better returns and higher sample efficiency than SAC and PPO.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02019v1",
        "pdf": "https://arxiv.org/pdf/2512.02019v1"
      },
      "arxiv_id": "2512.02019v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.02018v1",
      "title": "Data-Centric Visual Development for Self-Driving Labs",
      "authors": [
        "Anbang Liu",
        "Guanzhong Hu",
        "Jiayi Wang",
        "Ping Guo",
        "Han Liu"
      ],
      "abstract": "Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02018v1",
        "pdf": "https://arxiv.org/pdf/2512.02018v1"
      },
      "arxiv_id": "2512.02018v1",
      "comment": "11 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.02017v1",
      "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion",
      "authors": [
        "Shaowei Liu",
        "David Yifan Yao",
        "Saurabh Gupta",
        "Shenlong Wang"
      ],
      "abstract": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02017v1",
        "pdf": "https://arxiv.org/pdf/2512.02017v1"
      },
      "arxiv_id": "2512.02017v1",
      "comment": "Accepted to NeurIPS 2025. Project page: https://stevenlsw.github.io/visualsync/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.02016v1",
      "title": "Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now",
      "authors": [
        "Varun Varma Thozhiyoor",
        "Shivam Tripathi",
        "Venkatesh Babu Radhakrishnan",
        "Anand Bhattad"
      ],
      "abstract": "Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio $t_1^2/t_2^2 = h_1/h_2$, a relationship independent of $g$, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises $g_{\\mathrm{eff}}$ from $1.81\\,\\mathrm{m/s^2}$ to $6.43\\,\\mathrm{m/s^2}$ (reaching $65\\%$ of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02016v1",
        "pdf": "https://arxiv.org/pdf/2512.02016v1"
      },
      "arxiv_id": "2512.02016v1",
      "comment": "https://gravity-eval.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.02015v1",
      "title": "Generative Video Motion Editing with 3D Point Tracks",
      "authors": [
        "Yao-Chih Lee",
        "Zhoutong Zhang",
        "Jiahui Huang",
        "Jui-Hsien Wang",
        "Joon-Young Lee",
        "Jia-Bin Huang",
        "Eli Shechtman",
        "Zhengqi Li"
      ],
      "abstract": "Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02015v1",
        "pdf": "https://arxiv.org/pdf/2512.02015v1"
      },
      "arxiv_id": "2512.02015v1",
      "comment": "Project page: https://edit-by-track.github.io",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.02014v1",
      "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
      "authors": [
        "Zhiheng Liu",
        "Weiming Ren",
        "Haozhe Liu",
        "Zijian Zhou",
        "Shoufa Chen",
        "Haonan Qiu",
        "Xiaoke Huang",
        "Zhaochong An",
        "Fanny Yang",
        "Aditya Patel",
        "Viktar Atliha",
        "Tony Ng",
        "Xiao Han",
        "Chuyan Zhu",
        "Chenyang Zhang",
        "Ding Liu",
        "Juan-Manuel Perez-Rua",
        "Sen He",
        "Jürgen Schmidhuber",
        "Wenhu Chen",
        "Ping Luo",
        "Wei Liu",
        "Tao Xiang",
        "Jonas Schult",
        "Yuren Cong"
      ],
      "abstract": "Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02014v1",
        "pdf": "https://arxiv.org/pdf/2512.02014v1"
      },
      "arxiv_id": "2512.02014v1",
      "comment": "Project page: https://tuna-ai.org/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.02012v1",
      "title": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
      "authors": [
        "Zhengyang Geng",
        "Yiyang Lu",
        "Zongze Wu",
        "Eli Shechtman",
        "J. Zico Kolter",
        "Kaiming He"
      ],
      "abstract": "MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\\textbf{improved MeanFlow}$ ($\\textbf{iMF}$) method, trained entirely from scratch, achieves $\\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02012v1",
        "pdf": "https://arxiv.org/pdf/2512.02012v1"
      },
      "arxiv_id": "2512.02012v1",
      "comment": "Technical report",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.02010v1",
      "title": "Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling",
      "authors": [
        "Jack Cook",
        "Junxian Guo",
        "Guangxuan Xiao",
        "Yujun Lin",
        "Song Han"
      ],
      "abstract": "As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02010v1",
        "pdf": "https://arxiv.org/pdf/2512.02010v1"
      },
      "arxiv_id": "2512.02010v1",
      "comment": "10 pages, 5 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.02009v1",
      "title": "AirSim360: A Panoramic Simulation Platform within Drone View",
      "authors": [
        "Xian Ge",
        "Yuling Pan",
        "Yuhang Zhang",
        "Xiang Li",
        "Weijun Zhang",
        "Dizhe Zhang",
        "Zhaoliang Wan",
        "Xin Lin",
        "Xiangkai Zhang",
        "Juntao Liang",
        "Jason Li",
        "Wenjie Jiang",
        "Bo Du",
        "Ming-Hsuan Yang",
        "Lu Qi"
      ],
      "abstract": "The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02009v1",
        "pdf": "https://arxiv.org/pdf/2512.02009v1"
      },
      "arxiv_id": "2512.02009v1",
      "comment": "Project Website: https://insta360-research-team.github.io/AirSim360-website/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.02006v1",
      "title": "MV-TAP: Tracking Any Point in Multi-View Videos",
      "authors": [
        "Jahyeok Koo",
        "Inès Hyeonsu Kim",
        "Mungyeom Kim",
        "Junghyun Park",
        "Seohyun Park",
        "Jaeyeong Kim",
        "Jung Yi",
        "Seokju Cho",
        "Seungryong Kim"
      ],
      "abstract": "Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications. In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information. MV-TAP utilizes camera geometry and a cross-view attention mechanism to aggregate spatio-temporal information across views, enabling more complete and reliable trajectory estimation in multi-view videos. To support this task, we construct a large-scale synthetic training dataset and real-world evaluation sets tailored for multi-view tracking. Extensive experiments demonstrate that MV-TAP outperforms existing point-tracking methods on challenging benchmarks, establishing an effective baseline for advancing research in multi-view point tracking.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02006v1",
        "pdf": "https://arxiv.org/pdf/2512.02006v1"
      },
      "arxiv_id": "2512.02006v1",
      "comment": "Project Page: https://cvlab-kaist.github.io/MV-TAP/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.02005v1",
      "title": "Learning Visual Affordance from Audio",
      "authors": [
        "Lidong Lu",
        "Guo Chen",
        "Zhu Wei",
        "Yicheng Liu",
        "Tong Lu"
      ],
      "abstract": "We introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments object interaction regions from action sounds. Unlike existing approaches that rely on textual instructions or demonstration videos, which often limited by ambiguity or occlusion, audio provides real-time, semantically rich, and visually independent cues for affordance grounding, enabling more intuitive understanding of interaction regions. To support this task, we construct the first AV-AG dataset, comprising a large collection of action sounds, object images, and pixel-level affordance annotations. The dataset also includes an unseen subset to evaluate zero-shot generalization. Furthermore, we propose AVAGFormer, a model equipped with a semantic-conditioned cross-modal mixer and a dual-head decoder that effectively fuses audio and visual signals for mask prediction. Experiments show that AVAGFormer achieves state-of-the-art performance on AV-AG, surpassing baselines from related tasks. Comprehensive analyses highlight the distinctions between AV-AG and AVS, the benefits of end-to-end modeling, and the contribution of each component. Code and dataset have been released on https://jscslld.github.io/AVAGFormer/.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02005v1",
        "pdf": "https://arxiv.org/pdf/2512.02005v1"
      },
      "arxiv_id": "2512.02005v1",
      "comment": "15 pages, 10 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.02004v1",
      "title": "AlignSAE: Concept-Aligned Sparse Autoencoders",
      "authors": [
        "Minglai Yang",
        "Xinyu Guo",
        "Mihai Surdeanu",
        "Liangming Pan"
      ],
      "abstract": "Large Language Models (LLMs) encode factual knowledge within hidden parametric spaces that are difficult to inspect or control. While Sparse Autoencoders (SAEs) can decompose hidden activations into more fine-grained, interpretable features, they often struggle to reliably align these features with human-defined concepts, resulting in entangled and distributed feature representations. To address this, we introduce AlignSAE, a method that aligns SAE features with a defined ontology through a \"pre-train, then post-train\" curriculum. After an initial unsupervised training phase, we apply supervised post-training to bind specific concepts to dedicated latent slots while preserving the remaining capacity for general reconstruction. This separation creates an interpretable interface where specific relations can be inspected and controlled without interference from unrelated features. Empirical results demonstrate that AlignSAE enables precise causal interventions, such as reliable \"concept swaps\", by targeting single, semantically aligned slots.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02004v1",
        "pdf": "https://arxiv.org/pdf/2512.02004v1"
      },
      "arxiv_id": "2512.02004v1",
      "comment": "20 pages, 7 figures, 5 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01996v1",
      "title": "Learning Sim-to-Real Humanoid Locomotion in 15 Minutes",
      "authors": [
        "Younggyo Seo",
        "Carmelo Sferrazza",
        "Juyue Chen",
        "Guanya Shi",
        "Rocky Duan",
        "Pieter Abbeel"
      ],
      "abstract": "Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive scale with thousands of parallel environments through carefully tuned design choices and minimalist reward functions. We demonstrate rapid end-to-end learning of humanoid locomotion controllers on Unitree G1 and Booster T1 robots under strong domain randomization, e.g., randomized dynamics, rough terrain, and push perturbations, as well as fast training of whole-body human-motion tracking policies. We provide videos and open-source implementation at: https://younggyo.me/fastsac-humanoid.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01996v1",
        "pdf": "https://arxiv.org/pdf/2512.01996v1"
      },
      "arxiv_id": "2512.01996v1",
      "comment": "Project website: https://younggyo.me/fastsac-humanoid",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01993v1",
      "title": "RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies",
      "authors": [
        "Guillermo Garcia-Cobo",
        "Maximilian Igl",
        "Peter Karkus",
        "Zhejun Zhang",
        "Michael Watson",
        "Yuxiao Chen",
        "Boris Ivanovic",
        "Marco Pavone"
      ],
      "abstract": "Autonomous driving policies are typically trained via open-loop behavior cloning of human demonstrations. However, such policies suffer from covariate shift when deployed in closed loop, leading to compounding errors. We introduce Rollouts as Demonstrations (RoaD), a simple and efficient method to mitigate covariate shift by leveraging the policy's own closed-loop rollouts as additional training data. During rollout generation, RoaD incorporates expert guidance to bias trajectories toward high-quality behavior, producing informative yet realistic demonstrations for fine-tuning. This approach enables robust closed-loop adaptation with orders of magnitude less data than reinforcement learning, and avoids restrictive assumptions of prior closed-loop supervised fine-tuning (CL-SFT) methods, allowing broader applications domains including end-to-end driving. We demonstrate the effectiveness of RoaD on WOSAC, a large-scale traffic simulation benchmark, where it performs similar or better than the prior CL-SFT method; and in AlpaSim, a high-fidelity neural reconstruction-based simulator for end-to-end driving, where it improves driving score by 41\\% and reduces collisions by 54\\%.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01993v1",
        "pdf": "https://arxiv.org/pdf/2512.01993v1"
      },
      "arxiv_id": "2512.01993v1",
      "comment": "Preprint",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01992v1",
      "title": "LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through Chess",
      "authors": [
        "Sai Kolasani",
        "Maxim Saplin",
        "Nicholas Crispino",
        "Kyle Montgomery",
        "Jared Quincy Davis",
        "Matei Zaharia",
        "Chi Wang",
        "Chenguang Wang"
      ],
      "abstract": "We introduce LLM CHESS, an evaluation framework designed to probe the generalization of reasoning and instruction-following abilities in large language models (LLMs) through extended agentic interaction in the domain of chess. We rank over 50 open and closed source models by playing against a random opponent using a range of behavioral metrics, including win and loss rates, move quality, move legality, hallucinated actions, and game duration. For a subset of top reasoning models, we derive an Elo estimate by playing against a chess engine with variably configured skill, which allows for comparisons between models in an easily understandable way. Despite the simplicity of the instruction-following task and the weakness of the opponent, many state-of-the-art models struggle to complete games or achieve consistent wins. Similar to other benchmarks on complex reasoning tasks, our experiments reveal a clear separation between reasoning and non-reasoning models. However, unlike existing static benchmarks, the stochastic and dynamic nature of LLM CHESS uniquely reduces overfitting and memorization while preventing benchmark saturation, proving difficult even for top reasoning models. To support future work on evaluating reasoning and instruction-following in LLMs, we release our experimental framework, a public leaderboard, and a dataset of associated games.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01992v1",
        "pdf": "https://arxiv.org/pdf/2512.01992v1"
      },
      "arxiv_id": "2512.01992v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01989v1",
      "title": "PAI-Bench: A Comprehensive Benchmark For Physical AI",
      "authors": [
        "Fengzhe Zhou",
        "Jiannan Huang",
        "Jialuo Li",
        "Deva Ramanan",
        "Humphrey Shi"
      ],
      "abstract": "Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01989v1",
        "pdf": "https://arxiv.org/pdf/2512.01989v1"
      },
      "arxiv_id": "2512.01989v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01988v1",
      "title": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
      "authors": [
        "Wei Tang",
        "Yanpeng Sun",
        "Shan Zhang",
        "Xiaofan Li",
        "Piotr Koniusz",
        "Wei Li",
        "Na Zhao",
        "Zechao Li"
      ],
      "abstract": "Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01988v1",
        "pdf": "https://arxiv.org/pdf/2512.01988v1"
      },
      "arxiv_id": "2512.01988v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01987v1",
      "title": "Forecasting in Offline Reinforcement Learning for Non-stationary Environments",
      "authors": [
        "Suzan Ece Ada",
        "Georg Martius",
        "Emre Ugur",
        "Erhan Oztop"
      ],
      "abstract": "Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01987v1",
        "pdf": "https://arxiv.org/pdf/2512.01987v1"
      },
      "arxiv_id": "2512.01987v1",
      "comment": "The Thirty-Ninth Annual Conference on Neural Information Processing Systems, NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01986v1",
      "title": "A robust generalizable device-agnostic deep learning model for sleep-wake determination from triaxial wrist accelerometry",
      "authors": [
        "Nasim Montazeri",
        "Stone Yang",
        "Dominik Luszczynski",
        "John Zhang",
        "Dharmendra Gurve",
        "Andrew Centen",
        "Maged Goubran",
        "Andrew Lim"
      ],
      "abstract": "Study Objectives: Wrist accelerometry is widely used for inferring sleep-wake state. Previous works demonstrated poor wake detection, without cross-device generalizability and validation in different age range and sleep disorders. We developed a robust deep learning model for to detect sleep-wakefulness from triaxial accelerometry and evaluated its validity across three devices and in a large adult population spanning a wide range of ages with and without sleep disorders. Methods: We collected wrist accelerometry simultaneous to polysomnography (PSG) in 453 adults undergoing clinical sleep testing at a tertiary care sleep laboratory, using three devices. We extracted features in 30-second epochs and trained a 3-class model to detect wake, sleep, and sleep with arousals, which was then collapsed into wake vs. sleep using a decision tree. To enhance wake detection, the model was specifically trained on randomly selected subjects with low sleep efficiency and/or high arousal index from one device recording and then tested on the remaining recordings. Results: The model showed high performance with F1 Score of 0.86, sensitivity (sleep) of 0.87, and specificity (wakefulness) of 0.78, and significant and moderate correlation to PSG in predicting total sleep time (R=0.69) and sleep efficiency (R=0.63). Model performance was robust to the presence of sleep disorders, including sleep apnea and periodic limb movements in sleep, and was consistent across all three models of accelerometer. Conclusions: We present a deep model to detect sleep-wakefulness from actigraphy in adults with relative robustness to the presence of sleep disorders and generalizability across diverse commonly used wrist accelerometers.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "q-bio.QM",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01986v1",
        "pdf": "https://arxiv.org/pdf/2512.01986v1"
      },
      "arxiv_id": "2512.01986v1",
      "comment": "27 pages, 5 figures, 5 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01984v1",
      "title": "ECO: Energy-Constrained Operator Learning for Chaotic Dynamics with Boundedness Guarantees",
      "authors": [
        "Andrea Goertzen",
        "Sunbochen Tang",
        "Navid Azizan"
      ],
      "abstract": "Chaos is a fundamental feature of many complex dynamical systems, including weather systems and fluid turbulence. These systems are inherently difficult to predict due to their extreme sensitivity to initial conditions. Many chaotic systems are dissipative and ergodic, motivating data-driven models that aim to learn invariant statistical properties over long time horizons. While recent models have shown empirical success in preserving invariant statistics, they are prone to generating unbounded predictions, which prevent meaningful statistics evaluation. To overcome this, we introduce the Energy-Constrained Operator (ECO) that simultaneously learns the system dynamics while enforcing boundedness in predictions. We leverage concepts from control theory to develop algebraic conditions based on a learnable energy function, ensuring the learned dynamics is dissipative. ECO enforces these algebraic conditions through an efficient closed-form quadratic projection layer, which provides provable trajectory boundedness. To our knowledge, this is the first work establishing such formal guarantees for data-driven chaotic dynamics models. Additionally, the learned invariant level set provides an outer estimate for the strange attractor, a complex structure that is computationally intractable to characterize. We demonstrate empirical success in ECO's ability to generate stable long-horizon forecasts, capturing invariant statistics on systems governed by chaotic PDEs, including the Kuramoto--Sivashinsky and the Navier--Stokes equations.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "eess.SY",
        "cs.LG"
      ],
      "primary_category": "eess.SY",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01984v1",
        "pdf": "https://arxiv.org/pdf/2512.01984v1"
      },
      "arxiv_id": "2512.01984v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01983v1",
      "title": "Feature-Based Semantics-Aware Scheduling for Energy-Harvesting Federated Learning",
      "authors": [
        "Eunjeong Jeong",
        "Giovanni Perin",
        "Howard H. Yang",
        "Nikolaos Pappas"
      ],
      "abstract": "Federated Learning (FL) on resource-constrained edge devices faces a critical challenge: The computational energy required for training Deep Neural Networks (DNNs) often dominates communication costs. However, most existing Energy-Harvesting FL (EHFL) strategies fail to account for this reality, resulting in wasted energy due to redundant local computations. For efficient and proactive resource management, algorithms that predict local update contributions must be devised. We propose a lightweight client scheduling framework using the Version Age of Information (VAoI), a semantics-aware metric that quantifies update timeliness and significance. Crucially, we overcome VAoI's typical prohibitive computational cost, which requires statistical distance over the entire parameter space, by introducing a feature-based proxy. This proxy estimates model redundancy using intermediate-layer extraction from a single forward pass, dramatically reducing computational complexity. Experiments conducted under extreme non-IID data distributions and scarce energy availability demonstrate superior learning performance while achieving energy reduction compared to existing baseline selection policies. Our framework establishes semantics-aware scheduling as a practical and vital solution for EHFL in realistic scenarios where training costs dominate transmission costs.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.IT",
        "cs.NI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01983v1",
        "pdf": "https://arxiv.org/pdf/2512.01983v1"
      },
      "arxiv_id": "2512.01983v1",
      "comment": "This paper is currently under review for presentation at a peer-reviewed conference",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01980v1",
      "title": "Low-Rank Prehab: Preparing Neural Networks for SVD Compression",
      "authors": [
        "Haoran Qin",
        "Shansita Sharma",
        "Ali Abbasi",
        "Chayne Thrash",
        "Soheil Kolouri"
      ],
      "abstract": "Low-rank approximation methods such as singular value decomposition (SVD) and its variants (e.g., Fisher-weighted SVD, Activation SVD) have recently emerged as effective tools for neural network compression. In this setting, decomposition acts as a \"surgical\" intervention, followed by fine-tuning that serves as \"rehab\" to recover accuracy. Inspired by prehabilitation in surgery, we introduce a pre-compression fine-tuning stage, Low-Rank Prehab, that explicitly encourages low-rank structure in weight matrices while preserving task performance. By conditioning the model before SVD, Prehab steers weights toward spectrally compact regions of the parameter space, enabling smoother low-rank approximation and improved recovery. Experiments on large language models (LLMs) and other Transformer-based architectures, including Vision Transformers (ViTs), show that Prehab substantially reduces the immediate accuracy drop after compression and consistently improves post-finetuning performance. Across a wide range of compression ratios, our method outperforms state-of-the-art SVD-based techniques such as SVD-LLM, highlighting the importance of preparing models for compression rather than only improving the compression and recovery stages. Source code is available at https://github.com/niqretnuh/PREHAB-SVD",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01980v1",
        "pdf": "https://arxiv.org/pdf/2512.01980v1"
      },
      "arxiv_id": "2512.01980v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01979v1",
      "title": "Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback",
      "authors": [
        "Aiden Yiliu Li",
        "Bizhi Yu",
        "Daoan Lei",
        "Tianhe Ren",
        "Shilong Liu"
      ],
      "abstract": "GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01979v1",
        "pdf": "https://arxiv.org/pdf/2512.01979v1"
      },
      "arxiv_id": "2512.01979v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01977v1",
      "title": "AI-Driven Optimization under Uncertainty for Mineral Processing Operations",
      "authors": [
        "William Xu",
        "Amir Eskanlou",
        "Mansur Arief",
        "David Zhen Yin",
        "Jef K. Caers"
      ],
      "abstract": "The global capacity for mineral processing must expand rapidly to meet the demand for critical minerals, which are essential for building the clean energy technologies necessary to mitigate climate change. However, the efficiency of mineral processing is severely limited by uncertainty, which arises from both the variability of feedstock and the complexity of process dynamics. To optimize mineral processing circuits under uncertainty, we introduce an AI-driven approach that formulates mineral processing as a Partially Observable Markov Decision Process (POMDP). We demonstrate the capabilities of this approach in handling both feedstock uncertainty and process model uncertainty to optimize the operation of a simulated, simplified flotation cell as an example. We show that by integrating the process of information gathering (i.e., uncertainty reduction) and process optimization, this approach has the potential to consistently perform better than traditional approaches at maximizing an overall objective, such as net present value (NPV). Our methodological demonstration of this optimization-under-uncertainty approach for a synthetic case provides a mathematical and computational framework for later real-world application, with the potential to improve both the laboratory-scale design of experiments and industrial-scale operation of mineral processing circuits without any additional hardware.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01977v1",
        "pdf": "https://arxiv.org/pdf/2512.01977v1"
      },
      "arxiv_id": "2512.01977v1",
      "comment": "27 pages, 13 figures, submitted to Sustainable Earth Resources Communications (SERC)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01975v1",
      "title": "SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning",
      "authors": [
        "Xu Zhang",
        "Jin Yuan",
        "Hanwang Zhang",
        "Guojin Zhong",
        "Yongsheng Zang",
        "Jiacheng Lin",
        "Zhiyong Li"
      ],
      "abstract": "Controllable image semantic understanding tasks, such as captioning or segmentation, necessitate users to input a prompt (e.g., text or bounding boxes) to predict a unique outcome, presenting challenges such as high-cost prompt input or limited information output. This paper introduces a new task ``Image Collaborative Segmentation and Captioning'' (SegCaptioning), which aims to translate a straightforward prompt, like a bounding box around an object, into diverse semantic interpretations represented by (caption, masks) pairs, allowing flexible result selection by users. This task poses significant challenges, including accurately capturing a user's intention from a minimal prompt while simultaneously predicting multiple semantically aligned caption words and masks. Technically, we propose a novel Scene Graph Guided Diffusion Model that leverages structured scene graph features for correlated mask-caption prediction. Initially, we introduce a Prompt-Centric Scene Graph Adaptor to map a user's prompt to a scene graph, effectively capturing his intention. Subsequently, we employ a diffusion process incorporating a Scene Graph Guided Bimodal Transformer to predict correlated caption-mask pairs by uncovering intricate correlations between them. To ensure accurate alignment, we design a Multi-Entities Contrastive Learning loss to explicitly align visual and textual entities by considering inter-modal similarity, resulting in well-aligned caption-mask pairs. Extensive experiments conducted on two datasets demonstrate that SGDiff achieves superior performance in SegCaptioning, yielding promising results for both captioning and segmentation tasks with minimal prompt input.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01975v1",
        "pdf": "https://arxiv.org/pdf/2512.01975v1"
      },
      "arxiv_id": "2512.01975v1",
      "comment": "Accept by AAAI-2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01970v1",
      "title": "From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning",
      "authors": [
        "Sitao Cheng",
        "Xunjian Yin",
        "Ruiwen Zhou",
        "Yuxuan Li",
        "Xinyi Wang",
        "Liangming Pan",
        "William Yang Wang",
        "Victor Zhong"
      ],
      "abstract": "The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01970v1",
        "pdf": "https://arxiv.org/pdf/2512.01970v1"
      },
      "arxiv_id": "2512.01970v1",
      "comment": "Work in Progress. Code and data will be available at https://github.com/sitaocheng/from_atomic_to_composite",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.01960v1",
      "title": "SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation",
      "authors": [
        "Zisu Li",
        "Hengye Lyu",
        "Jiaxin Shi",
        "Yufeng Zeng",
        "Mingming Fan",
        "Hanwang Zhang",
        "Chen Liang"
      ],
      "abstract": "Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01960v1",
        "pdf": "https://arxiv.org/pdf/2512.01960v1"
      },
      "arxiv_id": "2512.01960v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01958v1",
      "title": "Learned-Rule-Augmented Large Language Model Evaluators",
      "authors": [
        "Jie Meng",
        "Jin Mao"
      ],
      "abstract": "Large language models (LLMs) are predominantly used as evaluators for natural language generation (NLG) tasks, but their application to broader evaluation scenarios remains limited. In this work, we explore the potential of LLMs as general evaluators across diverse tasks. Although LLM-based evaluators have made progress in different areas, existing methods struggle to generalize due to their reliance on costly, human-designed evaluation principles, which are often misaligned with both annotated data and LLMs' understanding.To address these challenges, we propose a rule-augmented evaluation paradigm. First, we introduce a rule distillation method that automatically extracts scoring rules from data using an LLM-assisted Monte Carlo Tree Search (MCTS), alleviating scalability issues and improving alignment with data. Second, to enable LLMs to effectively apply the learned rules, we propose two strategies: (1) Chain-of-Rule (CoR), which guides LLM to follow distilled rules, and (2) training a rule-augmented LLM evaluator (RuAE) via reinforcement learning, further bridging the gap between rules and LLMs' reasoning. Extensive experiments on diverse tasks demonstrate the effectiveness and generalizability of our approach across various evaluation scenarios.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01958v1",
        "pdf": "https://arxiv.org/pdf/2512.01958v1"
      },
      "arxiv_id": "2512.01958v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01953v1",
      "title": "KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference",
      "authors": [
        "Sai Gokhale",
        "Devleena Das",
        "Rajeev Patwari",
        "Ashish Sirasao",
        "Elliott Delaye"
      ],
      "abstract": "Long-context Large Language Models (LLMs) face significant memory bottlenecks during inference due to the linear growth of key-value (KV) cache with sequence length. While individual optimization techniques like KV cache quantization, chunked prefill, and model weight quantization have shown promise, their joint effects and optimal configurations for edge deployment remain underexplored. We introduce KV Pareto, a systems-level framework that systematically maps the trade-off frontier between total memory consumption and task accuracy across these three complementary optimization techniques. Our framework evaluates multiple LLM architectures (Qwen, Llama, Mistral) with varying KV quantization schemes (int2/4/8, mixed-precision), granularities (per-token, per-tensor, per-block), and 4-bit weight quantization via AWQ. Our framework identifies model-specific Pareto-optimal configurations that achieve 68-78% total memory reduction with minimal (1-3%) accuracy degradation on long-context tasks. We additionally verify the selected frontiers on additional benchmarks of Needle-in-a-Haystack, GSM8k and MMLU as well as extended context lengths of up to 128k to demonstrate the practical need of joint optimization for efficient LLM inference.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01953v1",
        "pdf": "https://arxiv.org/pdf/2512.01953v1"
      },
      "arxiv_id": "2512.01953v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01952v1",
      "title": "GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment",
      "authors": [
        "Haoyang He",
        "Jay Patrikar",
        "Dong-Ki Kim",
        "Max Smith",
        "Daniel McGann",
        "Ali-akbar Agha-mohammadi",
        "Shayegan Omidshafiei",
        "Sebastian Scherer"
      ],
      "abstract": "Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01952v1",
        "pdf": "https://arxiv.org/pdf/2512.01952v1"
      },
      "arxiv_id": "2512.01952v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01949v1",
      "title": "Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models",
      "authors": [
        "Zhongyu Yang",
        "Dannong Xu",
        "Wei Pang",
        "Yingfang Yuan"
      ],
      "abstract": "The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01949v1",
        "pdf": "https://arxiv.org/pdf/2512.01949v1"
      },
      "arxiv_id": "2512.01949v1",
      "comment": "Published in Transactions on Machine Learning Research, Project in https://01yzzyu.github.io/script.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.01946v1",
      "title": "Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models",
      "authors": [
        "Paul Pacaud",
        "Ricardo Garcia",
        "Shizhe Chen",
        "Cordelia Schmid"
      ],
      "abstract": "Robust robotic manipulation requires reliable failure detection and recovery. Although current Vision-Language Models (VLMs) show promise, their accuracy and generalization are limited by the scarcity of failure data. To address this data gap, we propose an automatic robot failure synthesis approach that procedurally perturbs successful trajectories to generate diverse planning and execution failures. This method produces not only binary classification labels but also fine-grained failure categories and step-by-step reasoning traces in both simulation and the real world. With it, we construct three new failure detection benchmarks: RLBench-Fail, BridgeDataV2-Fail, and UR5-Fail, substantially expanding the diversity and scale of existing failure datasets. We then train Guardian, a VLM with multi-view images for detailed failure reasoning and detection. Guardian achieves state-of-the-art performance on both existing and newly introduced benchmarks. It also effectively improves task success rates when integrated into a state-of-the-art manipulation system in simulation and real robots, demonstrating the impact of our generated failure data.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01946v1",
        "pdf": "https://arxiv.org/pdf/2512.01946v1"
      },
      "arxiv_id": "2512.01946v1",
      "comment": "9 pages, 9 figures, 6 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01945v1",
      "title": "Agentic Policy Optimization via Instruction-Policy Co-Evolution",
      "authors": [
        "Han Zhou",
        "Xingchen Wan",
        "Ivan Vulić",
        "Anna Korhonen"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01945v1",
        "pdf": "https://arxiv.org/pdf/2512.01945v1"
      },
      "arxiv_id": "2512.01945v1",
      "comment": "10 pages, 3 figures, 2 tables (18 pages including references and appendices)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01939v1",
      "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks",
      "authors": [
        "Yanlin Wang",
        "Xinyi Xu",
        "Jiachi Chen",
        "Tingting Bi",
        "Wenchao Gu",
        "Zibin Zheng"
      ],
      "abstract": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01939v1",
        "pdf": "https://arxiv.org/pdf/2512.01939v1"
      },
      "arxiv_id": "2512.01939v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01934v1",
      "title": "Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory",
      "authors": [
        "Chenyi Wang",
        "Yanmao Man",
        "Raymond Muller",
        "Ming Li",
        "Z. Berkay Celik",
        "Ryan Gerdes",
        "Jonathan Petit"
      ],
      "abstract": "Multi-Object Tracking (MOT) is a critical task in computer vision, with applications ranging from surveillance systems to autonomous driving. However, threats to MOT algorithms have yet been widely studied. In particular, incorrect association between the tracked objects and their assigned IDs can lead to severe consequences, such as wrong trajectory predictions. Previous attacks against MOT either focused on hijacking the trackers of individual objects, or manipulating the tracker IDs in MOT by attacking the integrated object detection (OD) module in the digital domain, which are model-specific, non-robust, and only able to affect specific samples in offline datasets. In this paper, we present AdvTraj, the first online and physical ID-manipulation attack against tracking-by-detection MOT, in which an attacker uses adversarial trajectories to transfer its ID to a targeted object to confuse the tracking system, without attacking OD. Our simulation results in CARLA show that AdvTraj can fool ID assignments with 100% success rate in various scenarios for white-box attacks against SORT, which also have high attack transferability (up to 93% attack success rate) against state-of-the-art (SOTA) MOT algorithms due to their common design principles. We characterize the patterns of trajectories generated by AdvTraj and propose two universal adversarial maneuvers that can be performed by a human walker/driver in daily scenarios. Our work reveals under-explored weaknesses in the object association phase of SOTA MOT systems, and provides insights into enhancing the robustness of such systems.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01934v1",
        "pdf": "https://arxiv.org/pdf/2512.01934v1"
      },
      "arxiv_id": "2512.01934v1",
      "comment": "Accepted to Annual Computer Security Applications Conference (ACSAC) 2024",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01930v1",
      "title": "SVRG and Beyond via Posterior Correction",
      "authors": [
        "Nico Daheim",
        "Thomas Möllenhoff",
        "Ming Liang Ang",
        "Mohammad Emtiyaz Khan"
      ],
      "abstract": "Stochastic Variance Reduced Gradient (SVRG) and its variants aim to speed-up training by using gradient corrections, but have seen limited success in deep learning. Here, we show surprising new foundational connections of SVRG to a recently proposed Bayesian method called posterior correction. Specifically, we show that SVRG is recovered as a special case of posterior correction over the isotropic-Gaussian family, while novel extensions are automatically obtained by using more flexible exponential families. We derive two new SVRG variants by using Gaussian families: First, a Newton-like variant that employs novel Hessian corrections, and second, an Adam-like extension that improves pretraining and finetuning of Transformer language models. This is the first work to connect SVRG to Bayes and use it to boost variational training for deep networks.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01930v1",
        "pdf": "https://arxiv.org/pdf/2512.01930v1"
      },
      "arxiv_id": "2512.01930v1",
      "comment": "Preprint. Under review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01925v1",
      "title": "Rectifying LLM Thought from Lens of Optimization",
      "authors": [
        "Junnan Liu",
        "Hongwei Liu",
        "Songyang Zhang",
        "Kai Chen"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01925v1",
        "pdf": "https://arxiv.org/pdf/2512.01925v1"
      },
      "arxiv_id": "2512.01925v1",
      "comment": "Work in progress",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01924v1",
      "title": "Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model",
      "authors": [
        "Kentaro Fujii",
        "Shingo Murata"
      ],
      "abstract": "Robots in uncertain real-world environments must perform both goal-directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We propose a novel deep active inference framework that consists of a world model, an action model, and an abstract world model. The world model encodes environmental dynamics into hidden state representations at slow and fast timescales. The action model compresses action sequences into abstract actions using vector quantization, and the abstract world model predicts future slow states conditioned on the abstract action, enabling low-cost action selection. We evaluate the framework on object-manipulation tasks with a real-world robot. Results show that it achieves high success rates across diverse manipulation tasks and switches between goal-directed and exploratory actions in uncertain settings, while making action selection computationally tractable. These findings highlight the importance of modeling multiple timescale dynamics and abstracting actions and state transitions.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01924v1",
        "pdf": "https://arxiv.org/pdf/2512.01924v1"
      },
      "arxiv_id": "2512.01924v1",
      "comment": "Accepted for publication in IEEE Robotics and Automation Letters (RA-L)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01922v1",
      "title": "Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding",
      "authors": [
        "Zahra Mahdavi",
        "Zahra Khodakaramimaghsoud",
        "Hooman Khaloo",
        "Sina Bakhshandeh Taleshani",
        "Erfan Hashemi",
        "Javad Mirzapour Kaleybar",
        "Omid Nejati Manzari"
      ],
      "abstract": "Large vision-language models (LVLMs) are now central to healthcare applications such as medical visual question answering and imaging report generation. Yet, these models remain vulnerable to hallucination outputs that appear plausible but are in fact incorrect. In the natural image domain, several decoding strategies have been proposed to mitigate hallucinations by reinforcing visual evidence, but most rely on secondary decoding or rollback procedures that substantially slow inference. Moreover, existing solutions are often domain-specific and may introduce misalignment between modalities or between generated and ground-truth content. We introduce Med-VCD, a sparse visual-contrastive decoding method that mitigates hallucinations in medical LVLMs without the time overhead of secondary decoding. Med-VCD incorporates a novel token-sparsification strategy that selects visually informed tokens on the fly, trimming redundancy while retaining critical visual context and thus balancing efficiency with reliability. Evaluations on eight medical datasets, spanning ophthalmology, radiology, and pathology tasks in visual question answering, report generation, and dedicated hallucination benchmarks, show that Med-VCD raises factual accuracy by an average of 13\\% and improves hallucination accuracy by 6\\% relative to baseline medical LVLMs.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01922v1",
        "pdf": "https://arxiv.org/pdf/2512.01922v1"
      },
      "arxiv_id": "2512.01922v1",
      "comment": "",
      "journal_ref": "Computers in Biology and Medicine (2026)",
      "has_code": false
    },
    {
      "id": "2512.01917v1",
      "title": "A Footprint-Aware, High-Resolution Approach for Carbon Flux Prediction Across Diverse Ecosystems",
      "authors": [
        "Jacob Searcy",
        "Anish Dulal",
        "Scott Bridgham",
        "Ashley Cordes",
        "Lillian Aoki",
        "Brendan Bohannan",
        "Qing Zhu",
        "Lucas C. R. Silva"
      ],
      "abstract": "Natural climate solutions (NCS) offer an approach to mitigating carbon dioxide (CO2) emissions. However, monitoring the carbon drawdown of ecosystems over large geographic areas remains challenging. Eddy-flux covariance towers provide ground truth for predictive 'upscaling' models derived from satellite products, but many satellites now produce measurements on spatial scales smaller than a flux tower's footprint. We introduce Footprint-Aware Regression (FAR), a first-of-its-kind, deep-learning framework that simultaneously predicts spatial footprints and pixel-level (30 m scale) estimates of carbon flux. FAR is trained on our AMERI-FAR25 dataset which combines 439 site years of tower data with corresponding Landsat scenes. Our model produces high-resolution predictions and achieves R2 = 0.78 when predicting monthly net ecosystem exchange on test sites from a variety of ecosystems.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01917v1",
        "pdf": "https://arxiv.org/pdf/2512.01917v1"
      },
      "arxiv_id": "2512.01917v1",
      "comment": "29 pages, 7 Figuers",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01913v1",
      "title": "Disentangling Progress in Medical Image Registration: Beyond Trend-Driven Architectures towards Domain-Specific Strategies",
      "authors": [
        "Bailiang Jian",
        "Jiazhen Pan",
        "Rohit Jena",
        "Morteza Ghahremani",
        "Hongwei Bran Li",
        "Daniel Rueckert",
        "Christian Wachinger",
        "Benedikt Wiestler"
      ],
      "abstract": "Medical image registration drives quantitative analysis across organs, modalities, and patient populations. Recent deep learning methods often combine low-level \"trend-driven\" computational blocks from computer vision, such as large-kernel CNNs, Transformers, and state-space models, with high-level registration-specific designs like motion pyramids, correlation layers, and iterative refinement. Yet, their relative contributions remain unclear and entangled. This raises a central question: should future advances in registration focus on importing generic architectural trends or on refining domain-specific design principles? Through a modular framework spanning brain, lung, cardiac, and abdominal registration, we systematically disentangle the influence of these two paradigms. Our evaluation reveals that low-level \"trend-driven\" computational blocks offer only marginal or inconsistent gains, while high-level registration-specific designs consistently deliver more accurate, smoother, and more robust deformations. These domain priors significantly elevate the performance of a standard U-Net baseline, far more than variants incorporating \"trend-driven\" blocks, achieving an average relative improvement of $\\sim3\\%$. All models and experiments are released within a transparent, modular benchmark that enables plug-and-play comparison for new architectures and registration tasks (https://github.com/BailiangJ/rethink-reg). This dynamic and extensible platform establishes a common ground for reproducible and fair evaluation, inviting the community to isolate genuine methodological contributions from domain priors. Our findings advocate a shift in research emphasis: from following architectural trends to embracing domain-specific design principles as the true drivers of progress in learning-based medical image registration.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01913v1",
        "pdf": "https://arxiv.org/pdf/2512.01913v1"
      },
      "arxiv_id": "2512.01913v1",
      "comment": "Submitted to Medical Image Analysis. Journal Extension of arXiv:2407.19274",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01908v1",
      "title": "SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception",
      "authors": [
        "Gurmeher Khurana",
        "Lan Wei",
        "Dandan Zhang"
      ],
      "abstract": "Contact-rich robotic manipulation requires representations that encode local geometry. Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues. Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information. Most self-supervised learning (SSL) frameworks, however, compress feature maps into a global vector, discarding spatial structure and misaligning with the needs of manipulation. To address this, we propose SARL, a spatially-aware SSL framework that augments the Bootstrap Your Own Latent (BYOL) architecture with three map-level objectives, including Saliency Alignment (SAL), Patch-Prototype Distribution Alignment (PPDA), and Region Affinity Matching (RAM), to keep attentional focus, part composition, and geometric relations consistent across views. These losses act on intermediate feature maps, complementing the global objective. SARL consistently outperforms nine SSL baselines across six downstream tasks with fused visual-tactile data. On the geometry-sensitive edge-pose regression task, SARL achieves a Mean Absolute Error (MAE) of 0.3955, a 30% relative improvement over the next-best SSL method (0.5682 MAE) and approaching the supervised upper bound. These findings indicate that, for fused visual-tactile data, the most effective signal is structured spatial equivariance, in which features vary predictably with object geometry, which enables more capable robotic perception.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01908v1",
        "pdf": "https://arxiv.org/pdf/2512.01908v1"
      },
      "arxiv_id": "2512.01908v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01906v1",
      "title": "Delays in Spiking Neural Networks: A State Space Model Approach",
      "authors": [
        "Sanja Karilanova",
        "Subhrakanti Dey",
        "Ayça Özçelikkale"
      ],
      "abstract": "Spiking neural networks (SNNs) are biologically inspired, event-driven models that are suitable for processing temporal data and offer energy-efficient computation when implemented on neuromorphic hardware. In SNNs, richer neuronal dynamic allows capturing more complex temporal dependencies, with delays playing a crucial role by allowing past inputs to directly influence present spiking behavior. We propose a general framework for incorporating delays into SNNs through additional state variables. The proposed mechanism enables each neuron to access a finite temporal input history. The framework is agnostic to neuron models and hence can be seamlessly integrated into standard spiking neuron models such as LIF and adLIF. We analyze how the duration of the delays and the learnable parameters associated with them affect the performance. We investigate the trade-offs in the network architecture due to additional state variables introduced by the delay mechanism. Experiments on the Spiking Heidelberg Digits (SHD) dataset show that the proposed mechanism matches the performance of existing delay-based SNNs while remaining computationally efficient. Moreover, the results illustrate that the incorporation of delays may substantially improve performance in smaller networks.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01906v1",
        "pdf": "https://arxiv.org/pdf/2512.01906v1"
      },
      "arxiv_id": "2512.01906v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01899v1",
      "title": "Provably Safe Model Updates",
      "authors": [
        "Leo Elmecker-Plakolm",
        "Pierre Fasterling",
        "Philip Sosnin",
        "Calvin Tsay",
        "Matthew Wicker"
      ],
      "abstract": "Safety-critical environments are inherently dynamic. Distribution shifts, emerging vulnerabilities, and evolving requirements demand continuous updates to machine learning models. Yet even benign parameter updates can have unintended consequences, such as catastrophic forgetting in classical models or alignment drift in foundation models. Existing heuristic approaches (e.g., regularization, parameter isolation) can mitigate these effects but cannot certify that updated models continue to satisfy required performance specifications. We address this problem by introducing a framework for provably safe model updates. Our approach first formalizes the problem as computing the largest locally invariant domain (LID): a connected region in parameter space where all points are certified to satisfy a given specification. While exact maximal LID computation is intractable, we show that relaxing the problem to parameterized abstract domains (orthotopes, zonotopes) yields a tractable primal-dual formulation. This enables efficient certification of updates - independent of the data or algorithm used - by projecting them onto the safe domain. Our formulation further allows computation of multiple approximately optimal LIDs, incorporation of regularization-inspired biases, and use of lookahead data buffers. Across continual learning and foundation model fine-tuning benchmarks, our method matches or exceeds heuristic baselines for avoiding forgetting while providing formal safety guarantees.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01899v1",
        "pdf": "https://arxiv.org/pdf/2512.01899v1"
      },
      "arxiv_id": "2512.01899v1",
      "comment": "12 pages, 9 figures, submitted to IEEE SaTML 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01895v1",
      "title": "StyleYourSmile: Cross-Domain Face Retargeting Without Paired Multi-Style Data",
      "authors": [
        "Avirup Dey",
        "Vinay Namboodiri"
      ],
      "abstract": "Cross-domain face retargeting requires disentangled control over identity, expressions, and domain-specific stylistic attributes. Existing methods, typically trained on real-world faces, either fail to generalize across domains, need test-time optimizations, or require fine-tuning with carefully curated multi-style datasets to achieve domain-invariant identity representations. In this work, we introduce \\textit{StyleYourSmile}, a novel one-shot cross-domain face retargeting method that eliminates the need for curated multi-style paired data. We propose an efficient data augmentation strategy alongside a dual-encoder framework, for extracting domain-invariant identity cues and capturing domain-specific stylistic variations. Leveraging these disentangled control signals, we condition a diffusion model to retarget facial expressions across domains. Extensive experiments demonstrate that \\textit{StyleYourSmile} achieves superior identity preservation and retargeting fidelity across a wide range of visual domains.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01895v1",
        "pdf": "https://arxiv.org/pdf/2512.01895v1"
      },
      "arxiv_id": "2512.01895v1",
      "comment": "15 pages, 14 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01892v1",
      "title": "Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models",
      "authors": [
        "Heloisa Candello",
        "Muneeza Azmat",
        "Uma Sushmitha Gunturi",
        "Raya Horesh",
        "Rogerio Abreu de Paula",
        "Heloisa Pimentel",
        "Marcelo Carpinette Grave",
        "Aminat Adebiyi",
        "Tiago Machado",
        "Maysa Malfiza Garcia de Macedo"
      ],
      "abstract": "With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and relevance. In a within-subject study design, 57 participants assessed the responses under two conditions: harmful response plus its mitigation and solely mitigated response. Results revealed that participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations. Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts. This contrasts with how language is often treated in the quantitative evaluation of LLMs. We also introduced new metrics for training and evaluating mitigation strategies and insights for human-AI evaluation studies.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01892v1",
        "pdf": "https://arxiv.org/pdf/2512.01892v1"
      },
      "arxiv_id": "2512.01892v1",
      "comment": "16 pages, 2 figures, 6 tables. Under review for publication",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01890v1",
      "title": "Elastic Weight Consolidation for Knowledge Graph Continual Learning: An Empirical Evaluation",
      "authors": [
        "Gaganpreet Jhajj",
        "Fuhua Lin"
      ],
      "abstract": "Knowledge graphs (KGs) require continual updates as new information emerges, but neural embedding models suffer from catastrophic forgetting when learning new tasks sequentially. We evaluate Elastic Weight Consolidation (EWC), a regularization-based continual learning method, on KG link prediction using TransE embeddings on FB15k-237. Across multiple experiments with five random seeds, we find that EWC reduces catastrophic forgetting from 12.62% to 6.85%, a 45.7% reduction compared to naive sequential training. We observe that the task partitioning strategy affects the magnitude of forgetting: relation-based partitioning (grouping triples by relation type) exhibits 9.8 percentage points higher forgetting than randomly partitioned tasks (12.62% vs 2.81%), suggesting that task construction influences evaluation outcomes. While focused on a single embedding model and dataset, our results demonstrate that EWC effectively mitigates catastrophic forgetting in KG continual learning and highlight the importance of evaluation protocol design.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01890v1",
        "pdf": "https://arxiv.org/pdf/2512.01890v1"
      },
      "arxiv_id": "2512.01890v1",
      "comment": "Accepted to NORA Workshop at NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01889v1",
      "title": "KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM",
      "authors": [
        "Zaid Nasser",
        "Mikhail Iumanov",
        "Tianhao Li",
        "Maxim Popov",
        "Jaafar Mahmoud",
        "Malik Mohrat",
        "Ilya Obrubov",
        "Ekaterina Derevyanka",
        "Ivan Sosin",
        "Sergey Kolyubin"
      ],
      "abstract": "We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views). The system performs simultaneous online localization and open-vocabulary semantic mapping by fusing geometric and deep visual features aligned with language embeddings. Our results are competitive with state-of-the-art approaches, while existing solutions either operate offline, need depth data and/or odometry estimation, or lack dynamic scene robustness. KM-ViPE benefits from internet-scale training and uniquely combines online operation, uncalibrated monocular input, and robust handling of dynamic scenes, which makes it a good fit for autonomous robotics and AR/VR applications and advances practical spatial intelligence capabilities for embodied AI.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01889v1",
        "pdf": "https://arxiv.org/pdf/2512.01889v1"
      },
      "arxiv_id": "2512.01889v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01888v1",
      "title": "Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets",
      "authors": [
        "Adrienne M. Propp",
        "Mauro Perego",
        "Eric C. Cyr",
        "Anthony Gruber",
        "Amanda A. Howard",
        "Alexander Heinlein",
        "Panos Stinis",
        "Daniel M. Tartakovsky"
      ],
      "abstract": "Accurate yet efficient surrogate models are essential for large-scale simulations of partial differential equations (PDEs), particularly for uncertainty quantification (UQ) tasks that demand hundreds or thousands of evaluations. We develop a physics-inspired graph neural network (GNN) surrogate that operates directly on unstructured meshes and leverages the flexibility of graph attention. To improve both training efficiency and generalization properties of the model, we introduce a domain decomposition (DD) strategy that partitions the mesh into subdomains, trains local GNN surrogates in parallel, and aggregates their predictions. We then employ transfer learning to fine-tune models across subdomains, accelerating training and improving accuracy in data-limited settings. Applied to ice sheet simulations, our approach accurately predicts full-field velocities on high-resolution meshes, substantially reduces training time relative to training a single global surrogate model, and provides a ripe foundation for UQ objectives. Our results demonstrate that graph-based DD, combined with transfer learning, provides a scalable and reliable pathway for training GNN surrogates on massive PDE-governed systems, with broad potential for application beyond ice sheet dynamics.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "math-ph",
        "math.NA",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01888v1",
        "pdf": "https://arxiv.org/pdf/2512.01888v1"
      },
      "arxiv_id": "2512.01888v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01885v1",
      "title": "TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals",
      "authors": [
        "Florian Bürger",
        "Martim Dias Gomes",
        "Nica Gutu",
        "Adrián E. Granada",
        "Noémie Moreau",
        "Katarzyna Bozek"
      ],
      "abstract": "Tracking cells in time-lapse videos is an essential technique for monitoring cell population dynamics at a single-cell level. Current methods for cell tracking are developed on videos with mostly single, constant signals and do not detect pivotal events such as cell death. Here, we present TransientTrack, a deep learning-based framework for cell tracking in multi-channel microscopy video data with transient fluorescent signals that fluctuate over time following processes such as the circadian rhythm of cells. By identifying key cellular events - mitosis (cell division) and apoptosis (cell death) our method allows us to build complete trajectories, including cell lineage information. TransientTrack is lightweight and performs matching on cell detection embeddings directly, without the need for quantification of tracking-specific cell features. Furthermore, our approach integrates Transformer Networks, multi-stage matching using all detection boxes, and the interpolation of missing tracklets with the Kalman Filter. This unified framework achieves strong performance across diverse conditions, effectively tracking cells and capturing cell division and death. We demonstrate the use of TransientTrack in an analysis of the efficacy of a chemotherapeutic drug at a single-cell level. The proposed framework could further advance quantitative studies of cancer cell dynamics, enabling detailed characterization of treatment response and resistance mechanisms. The code is available at https://github.com/bozeklab/TransientTrack.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV",
        "q-bio.CB",
        "q-bio.QM"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01885v1",
        "pdf": "https://arxiv.org/pdf/2512.01885v1"
      },
      "arxiv_id": "2512.01885v1",
      "comment": "13 pages, 7 figures, 2 tables. This work has been submitted to IEEE Transactions on Medical Imaging",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01882v1",
      "title": "New Spiking Architecture for Multi-Modal Decision-Making in Autonomous Vehicles",
      "authors": [
        "Aref Ghoreishee",
        "Abhishek Mishra",
        "Lifeng Zhou",
        "John Walsh",
        "Nagarajan Kandasamy"
      ],
      "abstract": "This work proposes an end-to-end multi-modal reinforcement learning framework for high-level decision-making in autonomous vehicles. The framework integrates heterogeneous sensory input, including camera images, LiDAR point clouds, and vehicle heading information, through a cross-attention transformer-based perception module. Although transformers have become the backbone of modern multi-modal architectures, their high computational cost limits their deployment in resource-constrained edge environments. To overcome this challenge, we propose a spiking temporal-aware transformer-like architecture that uses ternary spiking neurons for computationally efficient multi-modal fusion. Comprehensive evaluations across multiple tasks in the Highway Environment demonstrate the effectiveness and efficiency of the proposed approach for real-time autonomous decision-making.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01882v1",
        "pdf": "https://arxiv.org/pdf/2512.01882v1"
      },
      "arxiv_id": "2512.01882v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01881v1",
      "title": "Unifying Sign and Magnitude for Optimizing Deep Vision Networks via ThermoLion",
      "authors": [
        "Ahmed Nebli"
      ],
      "abstract": "The training of deep vision models is fundamentally a signal recovery problem amidst high-dimensional stochastic noise. Current optimization paradigms impose a static compromise on information channel capacity. For instance, magnitude-based methods, such as AdamW, operate on the assumption that gradient norms are high-fidelity curvature signals. While this allows for precision in smooth regimes, it leads to catastrophic noise amplification when applied to rugged, non-convex landscapes. Conversely, sign-based methods (e.g., Lion) perform a radical 1-bit quantization of the gradient, which aims to provide robust regularization at the cost of discarding fine-grained descent information. We propose that optimal convergence requires neither static prior, but rather a dynamic modulation of the update bitrate. We introduce \\textbf{ThermoLion}, a vision-centric framework that utilizes local Signal-to-Noise Ratio (SNR) gating to autonomously transition parameters between a \"low-bit\" exploration phase and a \"high-precision\" exploitation phase. Furthermore, we introduce a Momentum Alignment mechanism that detects constructive interference between historical drift and instantaneous gradients to accelerate convergence during stable trajectories. Empirical benchmarks across 12 diverse vision datasets (including CIFAR, SVHN, and GTSRB) demonstrate that ThermoLion serves as a hyperparameter-free generalist, surpassing both AdamW and Lion in convergence speed and terminal accuracy without architecture-specific tuning.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01881v1",
        "pdf": "https://arxiv.org/pdf/2512.01881v1"
      },
      "arxiv_id": "2512.01881v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01880v1",
      "title": "Predicting Human Chess Moves: An AI Assisted Analysis of Chess Games Using Skill-group Specific n-gram Language Models",
      "authors": [
        "Daren Zhong",
        "Dingcheng Huang",
        "Clayton Greenberg"
      ],
      "abstract": "Chess, a deterministic game with perfect information, has long served as a benchmark for studying strategic decision-making and artificial intelligence. Traditional chess engines or tools for analysis primarily focus on calculating optimal moves, often neglecting the variability inherent in human chess playing, particularly across different skill levels.\n  To overcome this limitation, we propose a novel and computationally efficient move prediction framework that approaches chess move prediction as a behavioral analysis task. The framework employs n-gram language models to capture move patterns characteristic of specific player skill levels. By dividing players into seven distinct skill groups, from novice to expert, we trained separate models using data from the open-source chess platform Lichess. The framework dynamically selects the most suitable model for prediction tasks and generates player moves based on preceding sequences.\n  Evaluation on real-world game data demonstrates that the model selector module within the framework can classify skill levels with an accuracy of up to 31.7\\% when utilizing early game information (16 half-moves). The move prediction framework also shows substantial accuracy improvements, with our Selector Assisted Accuracy being up to 39.1\\% more accurate than our benchmark accuracy. The computational efficiency of the framework further enhances its suitability for real-time chess analysis.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01880v1",
        "pdf": "https://arxiv.org/pdf/2512.01880v1"
      },
      "arxiv_id": "2512.01880v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01878v1",
      "title": "Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning",
      "authors": [
        "Gaganpreet Jhajj",
        "Fuhua Lin"
      ],
      "abstract": "In this work, we propose that reasoning in knowledge graph (KG) networks can be guided by surprise minimization. Entities that are close in graph distance will have lower surprise than those farther apart. This connects the Free Energy Principle (FEP) from neuroscience to KG systems, where the KG serves as the agent's generative model. We formalize surprise using the shortest-path distance in directed graphs and provide a framework for KG-based agents. Graph distance appears in graph neural networks as message passing depth and in model-based reinforcement learning as world model trajectories. This work-in-progress study explores whether distance-based surprise can extend recent work showing that syntax minimizes surprise and free energy via tree structures.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01878v1",
        "pdf": "https://arxiv.org/pdf/2512.01878v1"
      },
      "arxiv_id": "2512.01878v1",
      "comment": "Accepted to NORA Workshop at NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01870v1",
      "title": "Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees",
      "authors": [
        "Alessandro Breccia",
        "Federica Gerace",
        "Marco Lippi",
        "Gabriele Sicuro",
        "Pierluigi Contucci"
      ],
      "abstract": "We study whether a Large Language Model can learn the deterministic sequence of trees generated by the iterated prime factorization of the natural numbers. Each integer is mapped into a rooted planar tree and the resulting sequence $ \\mathbb{N}\\mathcal{T}$ defines an arithmetic text with measurable statistical structure. A transformer network (the GPT-2 architecture) is trained from scratch on the first $10^{11}$ elements to subsequently test its predictive ability under next-word and masked-word prediction tasks. Our results show that the model partially learns the internal grammar of $\\mathbb{N}\\mathcal{T}$, capturing non-trivial regularities and correlations. This suggests that learnability may extend beyond empirical data to the very structure of arithmetic.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.AI",
        "cond-mat.dis-nn",
        "math-ph",
        "math.NT"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01870v1",
        "pdf": "https://arxiv.org/pdf/2512.01870v1"
      },
      "arxiv_id": "2512.01870v1",
      "comment": "21 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01868v1",
      "title": "The Mean-Field Dynamics of Transformers",
      "authors": [
        "Philippe Rigollet"
      ],
      "abstract": "We develop a mathematical framework that interprets Transformer attention as an interacting particle system and studies its continuum (mean-field) limits. By idealizing attention continuous on the sphere, we connect Transformer dynamics to Wasserstein gradient flows, synchronization models (Kuramoto), and mean-shift clustering. Central to our results is a global clustering phenomenon whereby tokens cluster asymptotically after long metastable states where they are arranged into multiple clusters. We further analyze a tractable equiangular reduction to obtain exact clustering rates, show how commonly used normalization schemes alter contraction speeds, and identify a phase transition for long-context attention. The results highlight both the mechanisms that drive representation collapse and the regimes that preserve expressive, multi-cluster structure in deep attention architectures.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "math-ph",
        "math.DS",
        "math.PR"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01868v1",
        "pdf": "https://arxiv.org/pdf/2512.01868v1"
      },
      "arxiv_id": "2512.01868v1",
      "comment": "to appear as Proceedings of the ICM2026, Philadelphia, USA",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01865v1",
      "title": "Cross-Lingual Interleaving for Speech Language Models",
      "authors": [
        "Adel Moumen",
        "Guangzhi Sun",
        "Philip C. Woodland"
      ],
      "abstract": "Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also release an EN-FR training dataset, TinyStories (~42k hours), together with EN-FR spoken StoryCloze and TopicCloze benchmarks for cross-lingual semantic evaluation, both synthetically generated using GPT-4. On 360M and 1B SLMs under matched training-token budgets, interleaving improves monolingual semantic accuracy, enables robust cross-lingual continuation, and strengthens cross-lingual hidden-state alignment. Taken together, these results indicate that cross-lingual interleaving is a simple, scalable route to building multilingual SLMs that understand and converse across languages. All resources will be made open-source to support reproducibility.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01865v1",
        "pdf": "https://arxiv.org/pdf/2512.01865v1"
      },
      "arxiv_id": "2512.01865v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01863v1",
      "title": "Topological Order in Deep State",
      "authors": [
        "Ahmed Abouelkomsan",
        "Max Geier",
        "Liang Fu"
      ],
      "abstract": "Topologically ordered states are among the most interesting quantum phases of matter that host emergent quasi-particles having fractional charge and obeying fractional quantum statistics. Theoretical study of such states is however challenging owing to their strong-coupling nature that prevents conventional mean-field treatment. Here, we demonstrate that an attention-based deep neural network provides an expressive variational wavefunction that discovers fractional Chern insulator ground states purely through energy minimization without prior knowledge and achieves remarkable accuracy. We introduce an efficient method to extract ground state topological degeneracy -- a hallmark of topological order -- from a single optimized real-space wavefunction in translation-invariant systems by decomposing it into different many-body momentum sectors. Our results establish neural network variational Monte Carlo as a versatile tool for discovering strongly correlated topological phases.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cond-mat.mes-hall",
        "cond-mat.str-el",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mes-hall",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01863v1",
        "pdf": "https://arxiv.org/pdf/2512.01863v1"
      },
      "arxiv_id": "2512.01863v1",
      "comment": "5 pages + 6 SM",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01853v1",
      "title": "COACH: Collaborative Agents for Contextual Highlighting - A Multi-Agent Framework for Sports Video Analysis",
      "authors": [
        "Tsz-To Wong",
        "Ching-Chun Huang",
        "Hong-Han Shuai"
      ],
      "abstract": "Intelligent sports video analysis demands a comprehensive understanding of temporal context, from micro-level actions to macro-level game strategies. Existing end-to-end models often struggle with this temporal hierarchy, offering solutions that lack generalization, incur high development costs for new tasks, and suffer from poor interpretability. To overcome these limitations, we propose a reconfigurable Multi-Agent System (MAS) as a foundational framework for sports video understanding. In our system, each agent functions as a distinct \"cognitive tool\" specializing in a specific aspect of analysis. The system's architecture is not confined to a single temporal dimension or task. By leveraging iterative invocation and flexible composition of these agents, our framework can construct adaptive pipelines for both short-term analytic reasoning (e.g., Rally QA) and long-term generative summarization (e.g., match summaries). We demonstrate the adaptability of this framework using two representative tasks in badminton analysis, showcasing its ability to bridge fine-grained event detection and global semantic organization. This work presents a paradigm shift towards a flexible, scalable, and interpretable system for robust, cross-task sports video intelligence.The project homepage is available at https://aiden1020.github.io/COACH-project-page",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01853v1",
        "pdf": "https://arxiv.org/pdf/2512.01853v1"
      },
      "arxiv_id": "2512.01853v1",
      "comment": "Accepted by AAAI 2026 Workshop LaMAS",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01852v1",
      "title": "BHRAM-IL: A Benchmark for Hallucination Recognition and Assessment in Multiple Indian Languages",
      "authors": [
        "Hrishikesh Terdalkar",
        "Kirtan Bhojani",
        "Aryan Dongare",
        "Omm Aditya Behera"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in multilingual applications but often generate plausible yet incorrect or misleading outputs, known as hallucinations. While hallucination detection has been studied extensively in English, under-resourced Indian languages remain largely unexplored. We present BHRAM-IL, a benchmark for hallucination recognition and assessment in multiple Indian languages, covering Hindi, Gujarati, Marathi, Odia, along with English. The benchmark comprises 36,047 curated questions across nine categories spanning factual, numerical, reasoning, and linguistic tasks. We evaluate 14 state-of-the-art multilingual LLMs on a benchmark subset of 10,265 questions, analyzing cross-lingual and factual hallucinations across languages, models, scales, categories, and domains using category-specific metrics normalized to (0,1) range. Aggregation over all categories and models yields a primary score of 0.23 and a language-corrected fuzzy score of 0.385, demonstrating the usefulness of BHRAM-IL for hallucination-focused evaluation. The dataset, and the code for generation and evaluation are available on GitHub (https://github.com/sambhashana/BHRAM-IL/) and HuggingFace (https://huggingface.co/datasets/sambhashana/BHRAM-IL/) to support future research in multilingual hallucination detection and mitigation.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01852v1",
        "pdf": "https://arxiv.org/pdf/2512.01852v1"
      },
      "arxiv_id": "2512.01852v1",
      "comment": "Accepted at BHASHA Workshop @ IJCNLP/AACL 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01850v1",
      "title": "Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching",
      "authors": [
        "Yue Pan",
        "Tao Sun",
        "Liyuan Zhu",
        "Lucas Nunes",
        "Iro Armeni",
        "Jens Behley",
        "Cyrill Stachniss"
      ],
      "abstract": "Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01850v1",
        "pdf": "https://arxiv.org/pdf/2512.01850v1"
      },
      "arxiv_id": "2512.01850v1",
      "comment": "22 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01843v1",
      "title": "PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models",
      "authors": [
        "Zeqing Wang",
        "Keze Wang",
        "Lei Zhang"
      ],
      "abstract": "Driven by the growing capacity and training scale, Text-to-Video (T2V) generation models have recently achieved substantial progress in video quality, length, and instruction-following capability. However, whether these models can understand physics and generate physically plausible videos remains a question. While Vision-Language Models (VLMs) have been widely used as general-purpose evaluators in various applications, they struggle to identify the physically impossible content from generated videos. To investigate this issue, we construct a \\textbf{PID} (\\textbf{P}hysical \\textbf{I}mplausibility \\textbf{D}etection) dataset, which consists of a \\textit{test split} of 500 manually annotated videos and a \\textit{train split} of 2,588 paired videos, where each implausible video is generated by carefully rewriting the caption of its corresponding real-world video to induce T2V models producing physically implausible content. With the constructed dataset, we introduce a lightweight fine-tuning approach, enabling VLMs to not only detect physically implausible events but also generate textual explanations on the violated physical principles. Taking the fine-tuned VLM as a physical plausibility detector and explainer, namely \\textbf{PhyDetEx}, we benchmark a series of state-of-the-art T2V models to assess their adherence to physical laws. Our findings show that although recent T2V models have made notable progress toward generating physically plausible content, understanding and adhering to physical laws remains a challenging issue, especially for open-source models. Our dataset, training code, and checkpoints are available at \\href{https://github.com/Zeqing-Wang/PhyDetEx}{https://github.com/Zeqing-Wang/PhyDetEx}.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01843v1",
        "pdf": "https://arxiv.org/pdf/2512.01843v1"
      },
      "arxiv_id": "2512.01843v1",
      "comment": "17 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01834v1",
      "title": "Mitigating Gender Bias in Depression Detection via Counterfactual Inference",
      "authors": [
        "Mingxuan Hu",
        "Hongbo Ma",
        "Xinlan Wu",
        "Ziqi Liu",
        "Jiaqi Liu",
        "Yangbin Chen"
      ],
      "abstract": "Audio-based depression detection models have demonstrated promising performance but often suffer from gender bias due to imbalanced training data. Epidemiological statistics show a higher prevalence of depression in females, leading models to learn spurious correlations between gender and depression. Consequently, models tend to over-diagnose female patients while underperforming on male patients, raising significant fairness concerns. To address this, we propose a novel Counterfactual Debiasing Framework grounded in causal inference. We construct a causal graph to model the decision-making process and identify gender bias as the direct causal effect of gender on the prediction. During inference, we employ counterfactual inference to estimate and subtract this direct effect, ensuring the model relies primarily on authentic acoustic pathological features. Extensive experiments on the DAIC-WOZ dataset using two advanced acoustic backbones demonstrate that our framework not only significantly reduces gender bias but also improves overall detection performance compared to existing debiasing strategies.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01834v1",
        "pdf": "https://arxiv.org/pdf/2512.01834v1"
      },
      "arxiv_id": "2512.01834v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01831v1",
      "title": "Deconstructing Generative Diversity: An Information Bottleneck Analysis of Discrete Latent Generative Models",
      "authors": [
        "Yudi Wu",
        "Wenhao Zhao",
        "Dianbo Liu"
      ],
      "abstract": "Generative diversity varies significantly across discrete latent generative models such as AR, MIM, and Diffusion. We propose a diagnostic framework, grounded in Information Bottleneck (IB) theory, to analyze the underlying strategies resolving this behavior. The framework models generation as a conflict between a 'Compression Pressure' - a drive to minimize overall codebook entropy - and a 'Diversity Pressure' - a drive to maximize conditional entropy given an input. We further decompose this diversity into two primary sources: 'Path Diversity', representing the choice of high-level generative strategies, and 'Execution Diversity', the randomness in executing a chosen strategy. To make this decomposition operational, we introduce three zero-shot, inference-time interventions that directly perturb the latent generative process and reveal how models allocate and express diversity. Application of this probe-based framework to representative AR, MIM, and Diffusion systems reveals three distinct strategies: \"Diversity-Prioritized\" (MIM), \"Compression-Prioritized\" (AR), and \"Decoupled\" (Diffusion). Our analysis provides a principled explanation for their behavioral differences and informs a novel inference-time diversity enhancement technique.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01831v1",
        "pdf": "https://arxiv.org/pdf/2512.01831v1"
      },
      "arxiv_id": "2512.01831v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01830v1",
      "title": "OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic",
      "authors": [
        "Songyan Zhang",
        "Wenhui Huang",
        "Zhan Chen",
        "Chua Jiahao Collister",
        "Qihang Huang",
        "Chen Lv"
      ],
      "abstract": "Recently, two-stage fine-tuning strategies, e.g., acquiring essential driving knowledge through supervised fine-tuning (SFT) and further enhancing decision-making and planning via reinforcement fine-tuning (RFT), have shown strong potential in advancing the knowledge-driven autonomous driving (AD) paradigm. However, the learning nature of SFT still limits the generalization of reasoning, thereby constraining the full potential of driving performance. Meanwhile, current RFT approaches are primarily applied to downstream tasks, since scene understanding is an open-ended problem where corresponding rewards are difficult to quantify. To address these limitations, we propose OpenREAD, an OPEN-ended REasoning reinforced vision-language model (VLM)-based autonomous driving (AD) framework that enables end-to-end RFT across the full spectrum from high-level reasoning to low-level trajectory planning. Specifically, we begin by constructing large-scale Chain-of-Thought (CoT) annotations on open-source driving-related knowledge datasets, and employ the powerful Qwen3 large language model (LLM) as the critic in RFT to quantify reasoning quality for open-ended questions during reward modeling. Extensive experiments confirm that joint end-to-end RFT yields substantial improvements in both upstream and downstream tasks, enabling OpenREAD to achieve state-of-the-art performance on reasoning and planning benchmarks.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01830v1",
        "pdf": "https://arxiv.org/pdf/2512.01830v1"
      },
      "arxiv_id": "2512.01830v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01827v1",
      "title": "CauSight: Learning to Supersense for Visual Causal Discovery",
      "authors": [
        "Yize Zhang",
        "Meiqi Chen",
        "Sirui Chen",
        "Bo Peng",
        "Yanxi Zhang",
        "Tianyu Li",
        "Chaochao Lu"
      ],
      "abstract": "Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), a large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, a novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with a designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over a threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01827v1",
        "pdf": "https://arxiv.org/pdf/2512.01827v1"
      },
      "arxiv_id": "2512.01827v1",
      "comment": "project page: https://github.com/OpenCausaLab/CauSight",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.01822v1",
      "title": "InnoGym: Benchmarking the Innovation Potential of AI Agents",
      "authors": [
        "Jintian Zhang",
        "Kewei Xu",
        "Jingsheng Zheng",
        "Zhuoyun Yu",
        "Yuqi Zhu",
        "Yujie Luo",
        "Lanning Wei",
        "Shuofei Qiao",
        "Lun Du",
        "Da Zheng",
        "Shumin Deng",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01822v1",
        "pdf": "https://arxiv.org/pdf/2512.01822v1"
      },
      "arxiv_id": "2512.01822v1",
      "comment": "Work in progress",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01821v1",
      "title": "Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling",
      "authors": [
        "Meng Cao",
        "Haokun Lin",
        "Haoyuan Li",
        "Haoran Tang",
        "Rongtao Xu",
        "Dong An",
        "Xue Liu",
        "Ian Reid",
        "Xiaodan Liang"
      ],
      "abstract": "Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01821v1",
        "pdf": "https://arxiv.org/pdf/2512.01821v1"
      },
      "arxiv_id": "2512.01821v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01820v1",
      "title": "Dimension-free error estimate for diffusion model and optimal scheduling",
      "authors": [
        "Valentin de Bortoli",
        "Romuald Elie",
        "Anna Kazeykina",
        "Zhenjie Ren",
        "Jiacheng Zhang"
      ],
      "abstract": "Diffusion generative models have emerged as powerful tools for producing synthetic data from an empirically observed distribution. A common approach involves simulating the time-reversal of an Ornstein-Uhlenbeck (OU) process initialized at the true data distribution. Since the score function associated with the OU process is typically unknown, it is approximated using a trained neural network. This approximation, along with finite time simulation, time discretization and statistical approximation, introduce several sources of error whose impact on the generated samples must be carefully understood. Previous analyses have quantified the error between the generated and the true data distributions in terms of Wasserstein distance or Kullback-Leibler (KL) divergence. However, both metrics present limitations: KL divergence requires absolute continuity between distributions, while Wasserstein distance, though more general, leads to error bounds that scale poorly with dimension, rendering them impractical in high-dimensional settings. In this work, we derive an explicit, dimension-free bound on the discrepancy between the generated and the true data distributions. The bound is expressed in terms of a smooth test functional with bounded first and second derivatives. The key novelty lies in the use of this weaker, functional metric to obtain dimension-independent guarantees, at the cost of higher regularity on the test functions. As an application, we formulate and solve a variational problem to minimize the time-discretization error, leading to the derivation of an optimal time-scheduling strategy for the reverse-time diffusion. Interestingly, this scheduler has appeared previously in the literature in a different context; our analysis provides a new justification for its optimality, now grounded in minimizing the discretization bias in generative sampling.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.PR",
        "math.ST"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01820v1",
        "pdf": "https://arxiv.org/pdf/2512.01820v1"
      },
      "arxiv_id": "2512.01820v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01819v1",
      "title": "Decision Tree Embedding by Leaf-Means",
      "authors": [
        "Cencheng Shen",
        "Yuexiao Dong",
        "Carey E. Priebe"
      ],
      "abstract": "Decision trees and random forest remain highly competitive for classification on medium-sized, standard datasets due to their robustness, minimal preprocessing requirements, and interpretability. However, a single tree suffers from high estimation variance, while large ensembles reduce this variance at the cost of substantial computational overhead and diminished interpretability. In this paper, we propose Decision Tree Embedding (DTE), a fast and effective method that leverages the leaf partitions of a trained classification tree to construct an interpretable feature representation. By using the sample means within each leaf region as anchor points, DTE maps inputs into an embedding space defined by the tree's partition structure, effectively circumventing the high variance inherent in decision-tree splitting rules. We further introduce an ensemble extension based on additional bootstrap trees, and pair the resulting embedding with linear discriminant analysis for classification. We establish several population-level theoretical properties of DTE, including its preservation of conditional density under mild conditions and a characterization of the resulting classification error. Empirical studies on synthetic and real datasets demonstrate that DTE strikes a strong balance between accuracy and computational efficiency, outperforming or matching random forest and shallow neural networks while requiring only a fraction of their training time in most cases. Overall, the proposed DTE method can be viewed either as a scalable decision tree classifier that improves upon standard split rules, or as a neural network model whose weights are learned from tree-derived anchor points, achieving an intriguing integration of both paradigms.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01819v1",
        "pdf": "https://arxiv.org/pdf/2512.01819v1"
      },
      "arxiv_id": "2512.01819v1",
      "comment": "9 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01818v1",
      "title": "Forget Less, Retain More: A Lightweight Regularizer for Rehearsal-Based Continual Learning",
      "authors": [
        "Lama Alssum",
        "Hasan Abed Al Kader Hammoud",
        "Motasem Alfarra",
        "Juan C Leon Alcazar",
        "Bernard Ghanem"
      ],
      "abstract": "Deep neural networks suffer from catastrophic forgetting, where performance on previous tasks degrades after training on a new task. This issue arises due to the model's tendency to overwrite previously acquired knowledge with new information. We present a novel approach to address this challenge, focusing on the intersection of memory-based methods and regularization approaches. We formulate a regularization strategy, termed Information Maximization (IM) regularizer, for memory-based continual learning methods, which is based exclusively on the expected label distribution, thus making it class-agnostic. As a consequence, IM regularizer can be directly integrated into various rehearsal-based continual learning methods, reducing forgetting and favoring faster convergence. Our empirical validation shows that, across datasets and regardless of the number of tasks, our proposed regularization strategy consistently improves baseline performance at the expense of a minimal computational overhead. The lightweight nature of IM ensures that it remains a practical and scalable solution, making it applicable to real-world continual learning scenarios where efficiency is paramount. Finally, we demonstrate the data-agnostic nature of our regularizer by applying it to video data, which presents additional challenges due to its temporal structure and higher memory requirements. Despite the significant domain gap, our experiments show that IM regularizer also improves the performance of video continual learning methods.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01818v1",
        "pdf": "https://arxiv.org/pdf/2512.01818v1"
      },
      "arxiv_id": "2512.01818v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01816v1",
      "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights",
      "authors": [
        "Juanxi Tian",
        "Siyuan Li",
        "Conghui He",
        "Lijun Wu",
        "Cheng Tan"
      ],
      "abstract": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01816v1",
        "pdf": "https://arxiv.org/pdf/2512.01816v1"
      },
      "arxiv_id": "2512.01816v1",
      "comment": "35 pages, 12 figures, 10 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01810v1",
      "title": "DeepCAVE: A Visualization and Analysis Tool for Automated Machine Learning",
      "authors": [
        "Sarah Segel",
        "Helena Graf",
        "Edward Bergman",
        "Kristina Thieme",
        "Marcel Wever",
        "Alexander Tornede",
        "Frank Hutter",
        "Marius Lindauer"
      ],
      "abstract": "Hyperparameter optimization (HPO), as a central paradigm of AutoML, is crucial for leveraging the full potential of machine learning (ML) models; yet its complexity poses challenges in understanding and debugging the optimization process. We present DeepCAVE, a tool for interactive visualization and analysis, providing insights into HPO. Through an interactive dashboard, researchers, data scientists, and ML engineers can explore various aspects of the HPO process and identify issues, untouched potentials, and new insights about the ML model being tuned. By empowering users with actionable insights, DeepCAVE contributes to the interpretability of HPO and ML on a design level and aims to foster the development of more robust and efficient methodologies in the future.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01810v1",
        "pdf": "https://arxiv.org/pdf/2512.01810v1"
      },
      "arxiv_id": "2512.01810v1",
      "comment": "",
      "journal_ref": "Journal of Machine Learning Research (2025)",
      "has_code": false
    },
    {
      "id": "2512.01809v1",
      "title": "Much Ado About Noising: Dispelling the Myths of Generative Robotic Control",
      "authors": [
        "Chaoyi Pan",
        "Giri Anantharaman",
        "Nai-Chieh Huang",
        "Claire Jin",
        "Daniel Pfrommer",
        "Chenyang Yuan",
        "Frank Permenter",
        "Guannan Qu",
        "Nicholas Boffi",
        "Guanya Shi",
        "Max Simchowitz"
      ],
      "abstract": "Generative models, like flows and diffusions, have recently emerged as popular and efficacious policy parameterizations in robotics. There has been much speculation as to the factors underlying their successes, ranging from capturing multi-modal action distribution to expressing more complex behaviors. In this work, we perform a comprehensive evaluation of popular generative control policies (GCPs) on common behavior cloning (BC) benchmarks. We find that GCPs do not owe their success to their ability to capture multi-modality or to express more complex observation-to-action mappings. Instead, we find that their advantage stems from iterative computation, as long as intermediate steps are supervised during training and this supervision is paired with a suitable level of stochasticity. As a validation of our findings, we show that a minimum iterative policy (MIP), a lightweight two-step regression-based policy, essentially matches the performance of flow GCPs, and often outperforms distilled shortcut models. Our results suggest that the distribution-fitting component of GCPs is less salient than commonly believed, and point toward new design spaces focusing solely on control performance. Project page: https://simchowitzlabpublic.github.io/much-ado-about-noising-project/",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01809v1",
        "pdf": "https://arxiv.org/pdf/2512.01809v1"
      },
      "arxiv_id": "2512.01809v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01803v1",
      "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
      "authors": [
        "Xavier Thomas",
        "Youngsun Lim",
        "Ananya Srinivasan",
        "Audrey Zheng",
        "Deepti Ghadiyaram"
      ],
      "abstract": "Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01803v1",
        "pdf": "https://arxiv.org/pdf/2512.01803v1"
      },
      "arxiv_id": "2512.01803v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01801v1",
      "title": "GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation",
      "authors": [
        "Yunfei Li",
        "Xiao Ma",
        "Jiafeng Xu",
        "Yu Cui",
        "Zhongren Cui",
        "Zhigang Han",
        "Liqun Huang",
        "Tao Kong",
        "Yuxiao Liu",
        "Hao Niu",
        "Wanli Peng",
        "Jingchao Qiao",
        "Zeyu Ren",
        "Haixin Shi",
        "Zhi Su",
        "Jiawen Tian",
        "Yuyang Xiao",
        "Shenyu Zhang",
        "Liwei Zheng",
        "Hang Li",
        "Yonghui Wu"
      ],
      "abstract": "We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting $Q$-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundations models to specialize into reliable real-world experts.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01801v1",
        "pdf": "https://arxiv.org/pdf/2512.01801v1"
      },
      "arxiv_id": "2512.01801v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01797v1",
      "title": "H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons",
      "authors": [
        "Cheng Gao",
        "Huimin Chen",
        "Chaojun Xiao",
        "Zhiyi Chen",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01797v1",
        "pdf": "https://arxiv.org/pdf/2512.01797v1"
      },
      "arxiv_id": "2512.01797v1",
      "comment": "20 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01789v1",
      "title": "SAM3-UNet: Simplified Adaptation of Segment Anything Model 3",
      "authors": [
        "Xinyu Xiong",
        "Zihuang Wu",
        "Lei Lu",
        "Yufa Xia"
      ],
      "abstract": "In this paper, we introduce SAM3-UNet, a simplified variant of Segment Anything Model 3 (SAM3), designed to adapt SAM3 for downstream tasks at a low cost. Our SAM3-UNet consists of three components: a SAM3 image encoder, a simple adapter for parameter-efficient fine-tuning, and a lightweight U-Net-style decoder. Preliminary experiments on multiple tasks, such as mirror detection and salient object detection, demonstrate that the proposed SAM3-UNet outperforms the prior SAM2-UNet and other state-of-the-art methods, while requiring less than 6 GB of GPU memory during training with a batch size of 12. The code is publicly available at https://github.com/WZH0120/SAM3-UNet.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01789v1",
        "pdf": "https://arxiv.org/pdf/2512.01789v1"
      },
      "arxiv_id": "2512.01789v1",
      "comment": "Technical Report",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01788v1",
      "title": "Learned Image Compression for Earth Observation: Implications for Downstream Segmentation Tasks",
      "authors": [
        "Christian Mollière",
        "Iker Cumplido",
        "Marco Zeulner",
        "Lukas Liesenhoff",
        "Matthias Schubert",
        "Julia Gottfriedsen"
      ],
      "abstract": "The rapid growth of data from satellite-based Earth observation (EO) systems poses significant challenges in data transmission and storage. We evaluate the potential of task-specific learned compression algorithms in this context to reduce data volumes while retaining crucial information. In detail, we compare traditional compression (JPEG 2000) versus a learned compression approach (Discretized Mixed Gaussian Likelihood) on three EO segmentation tasks: Fire, cloud, and building detection. Learned compression notably outperforms JPEG 2000 for large-scale, multi-channel optical imagery in both reconstruction quality (PSNR) and segmentation accuracy. However, traditional codecs remain competitive on smaller, single-channel thermal infrared datasets due to limited data and architectural constraints. Additionally, joint end-to-end optimization of compression and segmentation models does not improve performance over standalone optimization.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01788v1",
        "pdf": "https://arxiv.org/pdf/2512.01788v1"
      },
      "arxiv_id": "2512.01788v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01786v1",
      "title": "Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems",
      "authors": [
        "Xiaochuan Li",
        "Ke Wang",
        "Girija Gouda",
        "Shubham Choudhary",
        "Yaqun Wang",
        "Linwei Hu",
        "Joel Vaughan",
        "Freddy Lecue"
      ],
      "abstract": "As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, we propose LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. Our method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features. This enables a fully adaptive evaluation where, for each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights. Experiments on summarization and RAG benchmarks show that our dynamic jury system achieves significantly higher correlation with human judgment than both single-judge and static-jury baselines. These results highlight the promise of adaptive, learning-based juries for building scalable, more reliable and trustworthy evaluation systems for modern LLMs in high-stakes domains.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01786v1",
        "pdf": "https://arxiv.org/pdf/2512.01786v1"
      },
      "arxiv_id": "2512.01786v1",
      "comment": "66 pages, 22 figures, 37 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01783v1",
      "title": "The Active and Noise-Tolerant Strategic Perceptron",
      "authors": [
        "Maria-Florina Blacan",
        "Hedyeh Beyhaghi"
      ],
      "abstract": "We initiate the study of active learning algorithms for classifying strategic agents. Active learning is a well-established framework in machine learning in which the learner selectively queries labels, often achieving substantially higher accuracy and efficiency than classical supervised methods-especially in settings where labeling is costly or time-consuming, such as hiring, admissions, and loan decisions. Strategic classification, however, addresses scenarios where agents modify their features to obtain more favorable outcomes, resulting in observed data that is not truthful. Such manipulation introduces challenges beyond those in learning from clean data. Our goal is to design active and noise-tolerant algorithms that remain effective in strategic environments-algorithms that classify strategic agents accurately while issuing as few label requests as possible. The central difficulty is to simultaneously account for strategic manipulation and preserve the efficiency gains of active learning.\n  Our main result is an algorithm for actively learning linear separators in the strategic setting that preserves the exponential improvement in label complexity over passive learning previously obtained only in the non-strategic case. Specifically, for data drawn uniformly from the unit sphere, we show that a modified version of the Active Perceptron algorithm [DKM05,YZ17] achieves excess error $ε$ using only $\\tilde{O}(d \\ln \\frac{1}ε)$ label queries and incurs at most $\\tilde{O}(d \\ln \\frac{1}ε)$ additional mistakes relative to the optimal classifier, even in the nonrealizable case, when a $\\tildeΩ(ε)$ fraction of inputs have inconsistent labels with the optimal classifier. The algorithm is computationally efficient and, under these distributional assumptions, requires substantially fewer label queries than prior work on strategic Perceptron [ABBN21].",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cs.GT"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01783v1",
        "pdf": "https://arxiv.org/pdf/2512.01783v1"
      },
      "arxiv_id": "2512.01783v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01782v1",
      "title": "Dual Randomized Smoothing: Beyond Global Noise Variance",
      "authors": [
        "Chenhao Sun",
        "Yuhao Mao",
        "Martin Vechev"
      ],
      "abstract": "Randomized Smoothing (RS) is a prominent technique for certifying the robustness of neural networks against adversarial perturbations. With RS, achieving high accuracy at small radii requires a small noise variance, while achieving high accuracy at large radii requires a large noise variance. However, the global noise variance used in the standard RS formulation leads to a fundamental limitation: there exists no global noise variance that simultaneously achieves strong performance at both small and large radii. To break through the global variance limitation, we propose a dual RS framework which enables input-dependent noise variances. To achieve that, we first prove that RS remains valid with input-dependent noise variances, provided the variance is locally constant around each input. Building on this result, we introduce two components which form our dual RS framework: (i) a variance estimator first predicts an optimal noise variance for each input, (ii) this estimated variance is then used by a standard RS classifier. The variance estimator is independently smoothed via RS to ensure local constancy, enabling flexible design. We also introduce training strategies to iteratively optimize the two components. Extensive experiments on CIFAR-10 show that our dual RS method provides strong performance for both small and large radii-unattainable with global noise variance-while incurring only a 60% computational overhead at inference. Moreover, it consistently outperforms prior input-dependent noise approaches across most radii, with particularly large gains at radii 0.5, 0.75, and 1.0, achieving relative improvements of 19%, 24%, and 21%, respectively. On ImageNet, dual RS remains effective across all radii. Additionally, the dual RS framework naturally provides a routing perspective for certified robustness, improving the accuracy-robustness trade-off with off-the-shelf expert RS models.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01782v1",
        "pdf": "https://arxiv.org/pdf/2512.01782v1"
      },
      "arxiv_id": "2512.01782v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01775v1",
      "title": "How Does RL Post-training Induce Skill Composition? A Case Study on Countdown",
      "authors": [
        "Simon Park",
        "Simran Kaur",
        "Sanjeev Arora"
      ],
      "abstract": "While reinforcement learning (RL) successfully enhances reasoning in large language models, its role in fostering compositional generalization (the ability to synthesize novel skills from known components) is often conflated with mere length generalization. To this end, we study what RL post-training teaches about skill composition and how the structure of the composition affects the skill transfer. We focus on the Countdown task (given n numbers and a target, form an expression that evaluates to the target) and analyze model solutions as expression trees, where each subtree corresponds to a reusable subtask and thus can be viewed as a ``skill.'' Tracking tree shapes and their success rates over training, we find: (i) out-of-distribution (OOD) generalization to larger n and to unseen tree shapes, indicating compositional reuse of subtasks; (ii) a structure-dependent hierarchy of learnability -- models master shallow balanced trees (workload is balanced between subtasks) before deep unbalanced ones, with persistent fragility on right-heavy structures (even when the composition depth is the same as some left-heavy structures). Our diagnostic reveals what is learned, in what order, and where generalization fails, clarifying how RL-only post-training induces OOD generalization beyond what standard metrics such as pass@k reveal.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01775v1",
        "pdf": "https://arxiv.org/pdf/2512.01775v1"
      },
      "arxiv_id": "2512.01775v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01774v1",
      "title": "Evaluating SAM2 for Video Semantic Segmentation",
      "authors": [
        "Syed Hesham Syed Ariff",
        "Yun Liu",
        "Guolei Sun",
        "Jing Yang",
        "Henghui Ding",
        "Xue Geng",
        "Xudong Jiang"
      ],
      "abstract": "The Segmentation Anything Model 2 (SAM2) has proven to be a powerful foundation model for promptable visual object segmentation in both images and videos, capable of storing object-aware memories and transferring them temporally through memory blocks. While SAM2 excels in video object segmentation by providing dense segmentation masks based on prompts, extending it to dense Video Semantic Segmentation (VSS) poses challenges due to the need for spatial accuracy, temporal consistency, and the ability to track multiple objects with complex boundaries and varying scales. This paper explores the extension of SAM2 for VSS, focusing on two primary approaches and highlighting firsthand observations and common challenges faced during this process. The first approach involves using SAM2 to extract unique objects as masks from a given image, with a segmentation network employed in parallel to generate and refine initial predictions. The second approach utilizes the predicted masks to extract unique feature vectors, which are then fed into a simple network for classification. The resulting classifications and masks are subsequently combined to produce the final segmentation. Our experiments suggest that leveraging SAM2 enhances overall performance in VSS, primarily due to its precise predictions of object boundaries.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01774v1",
        "pdf": "https://arxiv.org/pdf/2512.01774v1"
      },
      "arxiv_id": "2512.01774v1",
      "comment": "17 pages, 3 figures and 7 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01771v1",
      "title": "Robust Rigid and Non-Rigid Medical Image Registration Using Learnable Edge Kernels",
      "authors": [
        "Ahsan Raza Siyal",
        "Markus Haltmeier",
        "Ruth Steiger",
        "Malik Galijasevic",
        "Elke Ruth Gizewski",
        "Astrid Ellen Grams"
      ],
      "abstract": "Medical image registration is crucial for various clinical and research applications including disease diagnosis or treatment planning which require alignment of images from different modalities, time points, or subjects. Traditional registration techniques often struggle with challenges such as contrast differences, spatial distortions, and modality-specific variations. To address these limitations, we propose a method that integrates learnable edge kernels with learning-based rigid and non-rigid registration techniques. Unlike conventional layers that learn all features without specific bias, our approach begins with a predefined edge detection kernel, which is then perturbed with random noise. These kernels are learned during training to extract optimal edge features tailored to the task. This adaptive edge detection enhances the registration process by capturing diverse structural features critical in medical imaging. To provide clearer insight into the contribution of each component in our design, we introduce four variant models for rigid registration and four variant models for non-rigid registration. We evaluated our approach using a dataset provided by the Medical University across three setups: rigid registration without skull removal, with skull removal, and non-rigid registration. Additionally, we assessed performance on two publicly available datasets. Across all experiments, our method consistently outperformed state-of-the-art techniques, demonstrating its potential to improve multi-modal image alignment and anatomical structure analysis.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01771v1",
        "pdf": "https://arxiv.org/pdf/2512.01771v1"
      },
      "arxiv_id": "2512.01771v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01769v1",
      "title": "VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis",
      "authors": [
        "Hafsa Billah"
      ],
      "abstract": "Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain.\n  This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV",
        "cs.DB"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01769v1",
        "pdf": "https://arxiv.org/pdf/2512.01769v1"
      },
      "arxiv_id": "2512.01769v1",
      "comment": "This is a report submitted as part of PhD proposal defense of Hafsa Billah",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01766v1",
      "title": "On the Unreasonable Effectiveness of Last-layer Retraining",
      "authors": [
        "John C. Hill",
        "Tyler LaBonte",
        "Xinchen Zhang",
        "Vidya Muthukumar"
      ],
      "abstract": "Last-layer retraining (LLR) methods -- wherein the last layer of a neural network is reinitialized and retrained on a held-out set following ERM training -- have garnered interest as an efficient approach to rectify dependence on spurious correlations and improve performance on minority groups. Surprisingly, LLR has been found to improve worst-group accuracy even when the held-out set is an imbalanced subset of the training set. We initially hypothesize that this ``unreasonable effectiveness'' of LLR is explained by its ability to mitigate neural collapse through the held-out set, resulting in the implicit bias of gradient descent benefiting robustness. Our empirical investigation does not support this hypothesis. Instead, we present strong evidence for an alternative hypothesis: that the success of LLR is primarily due to better group balance in the held-out set. We conclude by showing how the recent algorithms CB-LLR and AFR perform implicit group-balancing to elicit a robustness improvement.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01766v1",
        "pdf": "https://arxiv.org/pdf/2512.01766v1"
      },
      "arxiv_id": "2512.01766v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01763v1",
      "title": "HiconAgent: History Context-aware Policy Optimization for GUI Agents",
      "authors": [
        "Xurui Zhou",
        "Gongwei Chen",
        "Yuquan Xie",
        "Zaijing Li",
        "Kaiwen Zhou",
        "Shuai Wang",
        "Shuo Yang",
        "Zhuotao Tian",
        "Rui Shao"
      ],
      "abstract": "Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01763v1",
        "pdf": "https://arxiv.org/pdf/2512.01763v1"
      },
      "arxiv_id": "2512.01763v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01759v1",
      "title": "Weight Space Representation Learning with Neural Fields",
      "authors": [
        "Zhuoqian Yang",
        "Mathieu Salzmann",
        "Sabine Süsstrunk"
      ],
      "abstract": "In this work, we investigate the potential of weights to serve as effective representations, focusing on neural fields. Our key insight is that constraining the optimization space through a pre-trained base model and low-rank adaptation (LoRA) can induce structure in weight space. Across reconstruction, generation, and analysis tasks on 2D and 3D data, we find that multiplicative LoRA weights achieve high representation quality while exhibiting distinctiveness and semantic structure. When used with latent diffusion models, multiplicative LoRA weights enable higher-quality generation than existing weight-space methods.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01759v1",
        "pdf": "https://arxiv.org/pdf/2512.01759v1"
      },
      "arxiv_id": "2512.01759v1",
      "comment": "12 pages body, 9 pages appendix",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01756v1",
      "title": "Mofasa: A Step Change in Metal-Organic Framework Generation",
      "authors": [
        "Vaidotas Simkus",
        "Anders Christensen",
        "Steven Bennett",
        "Ian Johnson",
        "Mark Neumann",
        "James Gin",
        "Jonathan Godwin",
        "Benjamin Rhodes"
      ],
      "abstract": "Mofasa is an all-atom latent diffusion model with state-of-the-art performance for generating Metal-Organic Frameworks (MOFs). These are highly porous crystalline materials used to harvest water from desert air, capture carbon dioxide, store toxic gases and catalyse chemical reactions. In recognition of their value, the development of MOFs recently received a Nobel Prize in Chemistry.\n  In many ways, MOFs are well-suited for exploiting generative models in chemistry: they are rationally-designable materials with a large combinatorial design space and strong structure-property couplings. And yet, to date, a high performance generative model has been lacking. To fill this gap, we introduce Mofasa, a general-purpose latent diffusion model that jointly samples positions, atom-types and lattice vectors for systems as large as 500 atoms. Mofasa avoids handcrafted assembly algorithms common in the literature, unlocking the simultaneous discovery of metal nodes, linkers and topologies.\n  To help the scientific community build on our work, we release MofasaDB, an annotated library of hundreds of thousands of sampled MOF structures, along with a user-friendly web interface for search and discovery: https://mofux.ai/ .",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01756v1",
        "pdf": "https://arxiv.org/pdf/2512.01756v1"
      },
      "arxiv_id": "2512.01756v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01755v1",
      "title": "FreqEdit: Preserving High-Frequency Features for Robust Multi-Turn Image Editing",
      "authors": [
        "Yucheng Liao",
        "Jiajun Liang",
        "Kaiqian Cui",
        "Baoquan Zhao",
        "Haoran Xie",
        "Wei Liu",
        "Qing Li",
        "Xudong Mao"
      ],
      "abstract": "Instruction-based image editing through natural language has emerged as a powerful paradigm for intuitive visual manipulation. While recent models achieve impressive results on single edits, they suffer from severe quality degradation under multi-turn editing. Through systematic analysis, we identify progressive loss of high-frequency information as the primary cause of this quality degradation. We present FreqEdit, a training-free framework that enables stable editing across 10+ consecutive iterations. Our approach comprises three synergistic components: (1) high-frequency feature injection from reference velocity fields to preserve fine-grained details, (2) an adaptive injection strategy that spatially modulates injection strength for precise region-specific control, and (3) a path compensation mechanism that periodically recalibrates the editing trajectory to prevent over-constraint. Extensive experiments demonstrate that FreqEdit achieves superior performance in both identity preservation and instruction following compared to seven state-of-the-art baselines.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01755v1",
        "pdf": "https://arxiv.org/pdf/2512.01755v1"
      },
      "arxiv_id": "2512.01755v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01750v1",
      "title": "Multimodal Mixture-of-Experts for ISAC in Low-Altitude Wireless Networks",
      "authors": [
        "Kai Zhang",
        "Wentao Yu",
        "Hengtao He",
        "Shenghui Song",
        "Jun Zhang",
        "Khaled B. Letaief"
      ],
      "abstract": "Integrated sensing and communication (ISAC) is a key enabler for low-altitude wireless networks (LAWNs), providing simultaneous environmental perception and data transmission in complex aerial scenarios. By combining heterogeneous sensing modalities such as visual, radar, lidar, and positional information, multimodal ISAC can improve both situational awareness and robustness of LAWNs. However, most existing multimodal fusion approaches use static fusion strategies that treat all modalities equally and cannot adapt to channel heterogeneity or time-varying modality reliability in dynamic low-altitude environments. To address this fundamental limitation, we propose a mixture-of-experts (MoE) framework for multimodal ISAC in LAWNs. Each modality is processed by a dedicated expert network, and a lightweight gating module adaptively assigns fusion weights according to the instantaneous informativeness and reliability of each modality. To improve scalability under the stringent energy constraints of aerial platforms, we further develop a sparse MoE variant that selectively activates only a subset of experts, thereby reducing computation overhead while preserving the benefits of adaptive fusion. Comprehensive simulations on three typical ISAC tasks in LAWNs demonstrate that the proposed frameworks consistently outperform conventional multimodal fusion baselines in terms of learning performance and training sample efficiency.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01750v1",
        "pdf": "https://arxiv.org/pdf/2512.01750v1"
      },
      "arxiv_id": "2512.01750v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01748v1",
      "title": "SA-ADP: Sensitivity-Aware Adaptive Differential Privacy for Large Language Models",
      "authors": [
        "Stella Etuk",
        "Ashraf Matrawy"
      ],
      "abstract": "Despite advances in the use of large language models (LLMs) in downstream tasks, their ability to memorize information has raised privacy concerns. Therefore, protecting personally identifiable information (PII) during LLM training remains a fundamental challenge. Conventional methods like Differential Privacy-Stochastic Gradient Descent (DP-SGD) provide robust privacy protection via uniform noising, protecting PII regardless of its distinct sensitivity. This comes at the expense of the model's utility, leading to a trade-off. In this paper, we propose SA-ADP, a sensitivity-aware approach that allocates noise based on the sensitivity of individual PII. We evaluated our method on four datasets (ABCD, CUSTOMERSIM, Wikitext-2, and UNSW-NB15 ). Our results show that SA-ADP achieves results comparable to the baseline (No-DP) and the conventional DP-SGD. This means that our method did not degrade the model's utility while still maintaining strong privacy protection.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01748v1",
        "pdf": "https://arxiv.org/pdf/2512.01748v1"
      },
      "arxiv_id": "2512.01748v1",
      "comment": "It is a 5-page paper with 5 figures and 1 Table",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01738v1",
      "title": "MSPT: Efficient Large-Scale Physical Modeling via Parallelized Multi-Scale Attention",
      "authors": [
        "Pedro M. P. Curvo",
        "Jan-Willem van de Meent",
        "Maksim Zhdanov"
      ],
      "abstract": "A key scalability challenge in neural solvers for industrial-scale physics simulations is efficiently capturing both fine-grained local interactions and long-range global dependencies across millions of spatial elements. We introduce the Multi-Scale Patch Transformer (MSPT), an architecture that combines local point attention within patches with global attention to coarse patch-level representations. To partition the input domain into spatially-coherent patches, we employ ball trees, which handle irregular geometries efficiently. This dual-scale design enables MSPT to scale to millions of points on a single GPU. We validate our method on standard PDE benchmarks (elasticity, plasticity, fluid dynamics, porous flow) and large-scale aerodynamic datasets (ShapeNet-Car, Ahmed-ML), achieving state-of-the-art accuracy with substantially lower memory footprint and computational cost.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01738v1",
        "pdf": "https://arxiv.org/pdf/2512.01738v1"
      },
      "arxiv_id": "2512.01738v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01735v1",
      "title": "Automating modeling in mechanics: LLMs as designers of physics-constrained neural networks for constitutive modeling of materials",
      "authors": [
        "Marius Tacke",
        "Matthias Busch",
        "Kian Abdolazizi",
        "Jonas Eichinger",
        "Kevin Linka",
        "Christian Cyron",
        "Roland Aydin"
      ],
      "abstract": "Large language model (LLM)-based agentic frameworks increasingly adopt the paradigm of dynamically generating task-specific agents. We suggest that not only agents but also specialized software modules for scientific and engineering tasks can be generated on demand. We demonstrate this concept in the field of solid mechanics. There, so-called constitutive models are required to describe the relationship between mechanical stress and body deformation. Constitutive models are essential for both the scientific understanding and industrial application of materials. However, even recent data-driven methods of constitutive modeling, such as constitutive artificial neural networks (CANNs), still require substantial expert knowledge and human labor. We present a framework in which an LLM generates a CANN on demand, tailored to a given material class and dataset provided by the user. The framework covers LLM-based architecture selection, integration of physical constraints, and complete code generation. Evaluation on three benchmark problems demonstrates that LLM-generated CANNs achieve accuracy comparable to or greater than manually engineered counterparts, while also exhibiting reliable generalization to unseen loading scenarios and extrapolation to large deformations. These findings indicate that LLM-based generation of physics-constrained neural networks can substantially reduce the expertise required for constitutive modeling and represent a step toward practical end-to-end automation.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01735v1",
        "pdf": "https://arxiv.org/pdf/2512.01735v1"
      },
      "arxiv_id": "2512.01735v1",
      "comment": "Currently under review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01732v1",
      "title": "Beyond Scaffold: A Unified Spatio-Temporal Gradient Tracking Method",
      "authors": [
        "Yan Huang",
        "Jinming Xu",
        "Jiming Chen",
        "Karl Henrik Johansson"
      ],
      "abstract": "In distributed and federated learning algorithms, communication overhead is often reduced by performing multiple local updates between communication rounds. However, due to data heterogeneity across nodes and the local gradient noise within each node, this strategy can lead to the drift of local models away from the global optimum. To address this issue, we revisit the well-known federated learning method Scaffold (Karimireddy et al., 2020) under a gradient tracking perspective, and propose a unified spatio-temporal gradient tracking algorithm, termed ST-GT, for distributed stochastic optimization over time-varying graphs. ST-GT tracks the global gradient across neighboring nodes to mitigate data heterogeneity, while maintaining a running average of local gradients to substantially suppress noise, with slightly more storage overhead. Without assuming bounded data heterogeneity, we prove that ST-GT attains a linear convergence rate for strongly convex problems and a sublinear rate for nonconvex cases. Notably, ST-GT achieves the first linear speed-up in communication complexity with respect to the number of local updates per round $τ$ for the strongly-convex setting. Compared to traditional gradient tracking methods, ST-GT reduces the topology-dependent noise term from $σ^2$ to $σ^2/τ$, where $σ^2$ denotes the noise level, thereby improving communication efficiency.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01732v1",
        "pdf": "https://arxiv.org/pdf/2512.01732v1"
      },
      "arxiv_id": "2512.01732v1",
      "comment": "13 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01723v1",
      "title": "Probabilistic Neuro-Symbolic Reasoning for Sparse Historical Data: A Framework Integrating Bayesian Inference, Causal Models, and Game-Theoretic Allocation",
      "authors": [
        "Saba Kublashvili"
      ],
      "abstract": "Modeling historical events poses fundamental challenges for machine learning: extreme data scarcity (N << 100), heterogeneous and noisy measurements, missing counterfactuals, and the requirement for human interpretable explanations. We present HistoricalML, a probabilistic neuro-symbolic framework that addresses these challenges through principled integration of (1) Bayesian uncertainty quantification to separate epistemic from aleatoric uncertainty, (2) structural causal models for counterfactual reasoning under confounding, (3) cooperative game theory (Shapley values) for fair allocation modeling, and (4) attention based neural architectures for context dependent factor weighting. We provide theoretical analysis showing that our approach achieves consistent estimation in the sparse data regime when strong priors from domain knowledge are available, and that Shapley based allocation satisfies axiomatic fairness guarantees that pure regression approaches cannot provide. We instantiate the framework on two historical case studies: the 19th century partition of Africa (N = 7 colonial powers) and the Second Punic War (N = 2 factions). Our model identifies Germany's +107.9 percent discrepancy as a quantifiable structural tension preceding World War I, with tension factor 36.43 and 0.79 naval arms race correlation. For the Punic Wars, Monte Carlo battle simulations achieve a 57.3 percent win probability for Carthage at Cannae and 57.8 percent for Rome at Zama, aligning with historical outcomes. Counterfactual analysis reveals that Carthaginian political support (support score 6.4 vs Napoleon's 7.1), rather than military capability, was the decisive factor.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.AI",
        "cs.GT",
        "math.PR"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01723v1",
        "pdf": "https://arxiv.org/pdf/2512.01723v1"
      },
      "arxiv_id": "2512.01723v1",
      "comment": "Preprint. Code and simulation notebooks available at the GitHub repository: https://github.com/Saba-Kublashvili/bayesian-computational-modeling.-",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.01716v1",
      "title": "Common Structure Discovery in Collections of Bipartite Networks: Application to Pollination Systems",
      "authors": [
        "Louis Lacoste",
        "Pierre Barbillon",
        "Sophie Donnet"
      ],
      "abstract": "Bipartite networks are widely used to encode the ecological interactions. Being able to compare the organization of bipartite networks is a first step toward a better understanding of how environmental factors shape community structure and resilience. Yet current methods for structure detection in bipartite networks overlook shared patterns across collections of networks. We introduce the \\emph{colBiSBM}, a family of probabilistic models for collections of bipartite networks that extends the classical Latent Block Model (LBM). The proposed framework assumes that networks are independent realizations of a shared mesoscale structure, encoded through common inter-block connectivity parameters. We establish identifiability conditions for the different variants of \\emph{colBiSBM} and develop a variational EM algorithm for parameter estimation, coupled with an adaptation of the Integrated Classification Likelihood (ICL) criterion for model selection. We demonstrate how our approach can be used to classify networks based on their topology or organization. Simulation studies highlight the ability of \\emph{colBiSBM} to recover common structures, improve clustering performance, and enhance link prediction by borrowing strength across networks. An application to plant--pollinator networks highlights how the method uncovers shared ecological roles and partitions networks into sub-collections with similar connectivity patterns. These results illustrate the methodological and practical advantages of joint modeling over separate network analyses in the study of bipartite systems.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01716v1",
        "pdf": "https://arxiv.org/pdf/2512.01716v1"
      },
      "arxiv_id": "2512.01716v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.01708v1",
      "title": "Differentially Private and Federated Structure Learning in Bayesian Networks",
      "authors": [
        "Ghita Fassy El Fehri",
        "Aurélien Bellet",
        "Philippe Bastien"
      ],
      "abstract": "Learning the structure of a Bayesian network from decentralized data poses two major challenges: (i) ensuring rigorous privacy guarantees for participants, and (ii) avoiding communication costs that scale poorly with dimensionality. In this work, we introduce Fed-Sparse-BNSL, a novel federated method for learning linear Gaussian Bayesian network structures that addresses both challenges. By combining differential privacy with greedy updates that target only a few relevant edges per participant, Fed-Sparse-BNSL efficiently uses the privacy budget while keeping communication costs low. Our careful algorithmic design preserves model identifiability and enables accurate structure estimation. Experiments on synthetic and real datasets demonstrate that Fed-Sparse-BNSL achieves utility close to non-private baselines while offering substantially stronger privacy and communication efficiency.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01708v1",
        "pdf": "https://arxiv.org/pdf/2512.01708v1"
      },
      "arxiv_id": "2512.01708v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    }
  ]
}