{
  "fetched_at": "2025-12-12T00:26:42.715876",
  "total_papers": 100,
  "papers": [
    {
      "id": "2512.09929v1",
      "title": "Closing the Train-Test Gap in World Models for Gradient-Based Planning",
      "authors": [
        "Arjun Parthasarathy",
        "Nimit Kalra",
        "Rohun Agrawal",
        "Yann LeCun",
        "Oumayma Bounou",
        "Pavel Izmailov",
        "Micah Goldblum"
      ],
      "abstract": "World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09929v1",
        "pdf": "https://arxiv.org/pdf/2512.09929v1"
      },
      "arxiv_id": "2512.09929v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09925v1",
      "title": "GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures",
      "authors": [
        "Patrick Noras",
        "Jun Myeong Choi",
        "Didier Stricker",
        "Pieter Peers",
        "Roni Sengupta"
      ],
      "abstract": "Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09925v1",
        "pdf": "https://arxiv.org/pdf/2512.09925v1"
      },
      "arxiv_id": "2512.09925v1",
      "comment": "23 pages, 18 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09924v1",
      "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
      "authors": [
        "Xinyu Liu",
        "Hangjie Yuan",
        "Yujie Wei",
        "Jiazheng Xing",
        "Yujin Han",
        "Jiahao Pan",
        "Yanbiao Ma",
        "Chi-Min Chan",
        "Kang Zhao",
        "Shiwei Zhang",
        "Wenhan Luo",
        "Yike Guo"
      ],
      "abstract": "Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09924v1",
        "pdf": "https://arxiv.org/pdf/2512.09924v1"
      },
      "arxiv_id": "2512.09924v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09923v1",
      "title": "Splatent: Splatting Diffusion Latents for Novel View Synthesis",
      "authors": [
        "Or Hirschorn",
        "Omer Sela",
        "Inbar Huberman-Spiegelglas",
        "Netalee Efrat",
        "Eli Alshan",
        "Ianir Ideses",
        "Frederic Devernay",
        "Yochai Zvik",
        "Lior Fritz"
      ],
      "abstract": "Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09923v1",
        "pdf": "https://arxiv.org/pdf/2512.09923v1"
      },
      "arxiv_id": "2512.09923v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09920v1",
      "title": "LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating",
      "authors": [
        "Junting Chen",
        "Yunchuan Li",
        "Panfeng Jiang",
        "Jiacheng Du",
        "Zixuan Chen",
        "Chenrui Tie",
        "Jiajun Deng",
        "Lin Shao"
      ],
      "abstract": "Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09920v1",
        "pdf": "https://arxiv.org/pdf/2512.09920v1"
      },
      "arxiv_id": "2512.09920v1",
      "comment": "8 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09914v1",
      "title": "FALCON: Few-step Accurate Likelihoods for Continuous Flows",
      "authors": [
        "Danyal Rehman",
        "Tara Akhound-Sadegh",
        "Artem Gazizov",
        "Yoshua Bengio",
        "Alexander Tong"
      ],
      "abstract": "Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09914v1",
        "pdf": "https://arxiv.org/pdf/2512.09914v1"
      },
      "arxiv_id": "2512.09914v1",
      "comment": "Preprint; NeurIPS 2025 MLSB",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09913v1",
      "title": "NordFKB: a fine-grained benchmark dataset for geospatial AI in Norway",
      "authors": [
        "Sander Riisøen Jyhne",
        "Aditya Gupta",
        "Ben Worsley",
        "Marianne Andersen",
        "Ivar Oveland",
        "Alexander Salveson Nossum"
      ],
      "abstract": "We present NordFKB, a fine-grained benchmark dataset for geospatial AI in Norway, derived from the authoritative, highly accurate, national Felles KartdataBase (FKB). The dataset contains high-resolution orthophotos paired with detailed annotations for 36 semantic classes, including both per-class binary segmentation masks in GeoTIFF format and COCO-style bounding box annotations. Data is collected from seven geographically diverse areas, ensuring variation in climate, topography, and urbanization. Only tiles containing at least one annotated object are included, and training/validation splits are created through random sampling across areas to ensure representative class and context distributions. Human expert review and quality control ensures high annotation accuracy. Alongside the dataset, we release a benchmarking repository with standardized evaluation protocols and tools for semantic segmentation and object detection, enabling reproducible and comparable research. NordFKB provides a robust foundation for advancing AI methods in mapping, land administration, and spatial planning, and paves the way for future expansions in coverage, temporal scope, and data modalities.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09913v1",
        "pdf": "https://arxiv.org/pdf/2512.09913v1"
      },
      "arxiv_id": "2512.09913v1",
      "comment": "8 pages, 2 figures, 2 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09912v1",
      "title": "Supervised learning pays attention",
      "authors": [
        "Erin Craig",
        "Robert Tibshirani"
      ],
      "abstract": "In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.\n  Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09912v1",
        "pdf": "https://arxiv.org/pdf/2512.09912v1"
      },
      "arxiv_id": "2512.09912v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09910v1",
      "title": "Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach",
      "authors": [
        "Salvador Carrión",
        "Francisco Casacuberta"
      ],
      "abstract": "Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09910v1",
        "pdf": "https://arxiv.org/pdf/2512.09910v1"
      },
      "arxiv_id": "2512.09910v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09909v1",
      "title": "STACHE: Local Black-Box Explanations for Reinforcement Learning Policies",
      "authors": [
        "Andrew Elashkin",
        "Orna Grumberg"
      ],
      "abstract": "Reinforcement learning agents often behave unexpectedly in sparse-reward or safety-critical environments, creating a strong need for reliable debugging and verification tools. In this paper, we propose STACHE, a comprehensive framework for generating local, black-box explanations for an agent's specific action within discrete Markov games. Our method produces a Composite Explanation consisting of two complementary components: (1) a Robustness Region, the connected neighborhood of states where the agent's action remains invariant, and (2) Minimal Counterfactuals, the smallest state perturbations required to alter that decision. By exploiting the structure of factored state spaces, we introduce an exact, search-based algorithm that circumvents the fidelity gaps of surrogate models. Empirical validation on Gymnasium environments demonstrates that our framework not only explains policy actions, but also effectively captures the evolution of policy logic during training - from erratic, unstable behavior to optimized, robust strategies - providing actionable insights into agent sensitivity and decision boundaries.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09909v1",
        "pdf": "https://arxiv.org/pdf/2512.09909v1"
      },
      "arxiv_id": "2512.09909v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09908v1",
      "title": "Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective",
      "authors": [
        "Antonio Lorenzin",
        "Fabio Zanasi"
      ],
      "abstract": "Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.AI",
        "cs.LO",
        "math.CT"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09908v1",
        "pdf": "https://arxiv.org/pdf/2512.09908v1"
      },
      "arxiv_id": "2512.09908v1",
      "comment": "36 pages. A preliminary version of this work was presented at CALCO 2025, under the title \"An Algebraic Approach to Moralisation and Triangulation of Probabilistic Graphical Models''",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09907v1",
      "title": "VisualActBench: Can VLMs See and Act like a Human?",
      "authors": [
        "Daoan Zhang",
        "Pai Liu",
        "Xiaofei Zhou",
        "Yuan Ge",
        "Guangchen Lan",
        "Jing Bi",
        "Christopher Brinton",
        "Ehsan Hoque",
        "Jiebo Luo"
      ],
      "abstract": "Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models' human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs' ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09907v1",
        "pdf": "https://arxiv.org/pdf/2512.09907v1"
      },
      "arxiv_id": "2512.09907v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09903v1",
      "title": "YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos",
      "authors": [
        "Ryan Meegan",
        "Adam D'Souza",
        "Bryan Bo Cao",
        "Shubham Jain",
        "Kristin Dana"
      ],
      "abstract": "Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09903v1",
        "pdf": "https://arxiv.org/pdf/2512.09903v1"
      },
      "arxiv_id": "2512.09903v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09898v1",
      "title": "Visual Heading Prediction for Autonomous Aerial Vehicles",
      "authors": [
        "Reza Ahmari",
        "Ahmad Mohammadi",
        "Vahid Hemmati",
        "Mohammed Mynuddin",
        "Parham Kebria",
        "Mahmoud Nabil Mahmoud",
        "Xiaohong Yuan",
        "Abdollah Homaifar"
      ],
      "abstract": "The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506° and a root mean squared error of 0.1957°, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.MA",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09898v1",
        "pdf": "https://arxiv.org/pdf/2512.09898v1"
      },
      "arxiv_id": "2512.09898v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09897v1",
      "title": "SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments",
      "authors": [
        "Haoye Lu",
        "Pavan Seshadri",
        "Kaheer Suleman"
      ],
      "abstract": "Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09897v1",
        "pdf": "https://arxiv.org/pdf/2512.09897v1"
      },
      "arxiv_id": "2512.09897v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09895v1",
      "title": "Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science",
      "authors": [
        "Jane Greenberg",
        "Scott McClellan",
        "Addy Ireland",
        "Robert Sammarco",
        "Colton Gerber",
        "Christopher B. Rauch",
        "Mat Kelly",
        "John Kunze",
        "Yuan An",
        "Eric Toberer"
      ],
      "abstract": "Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09895v1",
        "pdf": "https://arxiv.org/pdf/2512.09895v1"
      },
      "arxiv_id": "2512.09895v1",
      "comment": "Metadata and Semantics Research Conference 2025, 14 pages, 7 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09894v1",
      "title": "Exploring Protein Language Model Architecture-Induced Biases for Antibody Comprehension",
      "authors": [
        "Mengren",
        "Liu",
        "Yixiang Zhang",
        "Yiming",
        "Zhang"
      ],
      "abstract": "Recent advances in protein language models (PLMs) have demonstrated remarkable capabilities in understanding protein sequences. However, the extent to which different model architectures capture antibody-specific biological properties remains unexplored. In this work, we systematically investigate how architectural choices in PLMs influence their ability to comprehend antibody sequence characteristics and functions. We evaluate three state-of-the-art PLMs-AntiBERTa, BioBERT, and ESM2--against a general-purpose language model (GPT-2) baseline on antibody target specificity prediction tasks. Our results demonstrate that while all PLMs achieve high classification accuracy, they exhibit distinct biases in capturing biological features such as V gene usage, somatic hypermutation patterns, and isotype information. Through attention attribution analysis, we show that antibody-specific models like AntiBERTa naturally learn to focus on complementarity-determining regions (CDRs), while general protein models benefit significantly from explicit CDR-focused training strategies. These findings provide insights into the relationship between model architecture and biological feature extraction, offering valuable guidance for future PLM development in computational antibody design.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09894v1",
        "pdf": "https://arxiv.org/pdf/2512.09894v1"
      },
      "arxiv_id": "2512.09894v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09892v1",
      "title": "Provably Learning from Modern Language Models via Low Logit Rank",
      "authors": [
        "Noah Golowich",
        "Allen Liu",
        "Abhishek Shetty"
      ],
      "abstract": "While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix.\n  In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09892v1",
        "pdf": "https://arxiv.org/pdf/2512.09892v1"
      },
      "arxiv_id": "2512.09892v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09890v1",
      "title": "Analysis of Dirichlet Energies as Over-smoothing Measures",
      "authors": [
        "Anna Bison",
        "Alessandro Sperduti"
      ],
      "abstract": "We analyze the distinctions between two functionals often used as over-smoothing measures: the Dirichlet energies induced by the unnormalized graph Laplacian and the normalized graph Laplacian. We demonstrate that the latter fails to satisfy the axiomatic definition of a node-similarity measure proposed by Rusch \\textit{et al.} By formalizing fundamental spectral properties of these two definitions, we highlight critical distinctions necessary to select the metric that is spectrally compatible with the GNN architecture, thereby resolving ambiguities in monitoring the dynamics.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09890v1",
        "pdf": "https://arxiv.org/pdf/2512.09890v1"
      },
      "arxiv_id": "2512.09890v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09886v1",
      "title": "HPM-KD: Hierarchical Progressive Multi-Teacher Framework for Knowledge Distillation and Efficient Model Compression",
      "authors": [
        "Gustavo Coelho Haase",
        "Paulo Henrique Dourado da Silva"
      ],
      "abstract": "Knowledge Distillation (KD) has emerged as a promising technique for model compression but faces critical limitations: (1) sensitivity to hyperparameters requiring extensive manual tuning, (2) capacity gap when distilling from very large teachers to small students, (3) suboptimal coordination in multi-teacher scenarios, and (4) inefficient use of computational resources. We present \\textbf{HPM-KD}, a framework that integrates six synergistic components: (i) Adaptive Configuration Manager via meta-learning that eliminates manual hyperparameter tuning, (ii) Progressive Distillation Chain with automatically determined intermediate models, (iii) Attention-Weighted Multi-Teacher Ensemble that learns dynamic per-sample weights, (iv) Meta-Learned Temperature Scheduler that adapts temperature throughout training, (v) Parallel Processing Pipeline with intelligent load balancing, and (vi) Shared Optimization Memory for cross-experiment reuse. Experiments on CIFAR-10, CIFAR-100, and tabular datasets demonstrate that HPM-KD: achieves 10x-15x compression while maintaining 85% accuracy retention, eliminates the need for manual tuning, and reduces training time by 30-40% via parallelization. Ablation studies confirm independent contribution of each component (0.10-0.98 pp). HPM-KD is available as part of the open-source DeepBridge library.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09886v1",
        "pdf": "https://arxiv.org/pdf/2512.09886v1"
      },
      "arxiv_id": "2512.09886v1",
      "comment": "9 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09882v1",
      "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing",
      "authors": [
        "Justin W. Lin",
        "Eliot Krzysztof Jones",
        "Donovan Julian Jasper",
        "Ethan Jun-shen Ho",
        "Anna Wu",
        "Arnold Tianyi Yang",
        "Neil Perry",
        "Andy Zou",
        "Matt Fredrikson",
        "J. Zico Kolter",
        "Percy Liang",
        "Dan Boneh",
        "Daniel E. Ho"
      ],
      "abstract": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09882v1",
        "pdf": "https://arxiv.org/pdf/2512.09882v1"
      },
      "arxiv_id": "2512.09882v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09874v1",
      "title": "Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs",
      "authors": [
        "Pius Horn",
        "Janis Keuper"
      ],
      "abstract": "Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09874v1",
        "pdf": "https://arxiv.org/pdf/2512.09874v1"
      },
      "arxiv_id": "2512.09874v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09872v1",
      "title": "FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning",
      "authors": [
        "Khurram Khalil",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "Generative Artificial Intelligence models, such as Large Language Models (LLMs) and Large Vision Models (VLMs), exhibit state-of-the-art performance but remain vulnerable to hardware-based threats, specifically bit-flip attacks (BFAs). Existing BFA discovery methods lack generalizability and struggle to scale, often failing to analyze the vast parameter space and complex interdependencies of modern foundation models in a reasonable time. This paper proposes FlipLLM, a reinforcement learning (RL) architecture-agnostic framework that formulates BFA discovery as a sequential decision-making problem. FlipLLM combines sensitivity-guided layer pruning with Q-learning to efficiently identify minimal, high-impact bit sets that can induce catastrophic failure. We demonstrate the effectiveness and generalizability of FlipLLM by applying it to a diverse set of models, including prominent text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, and DeepSeek-V2 7B), VLMs such as LLaVA 1.6, and datasets, such as MMLU, MMLU-Pro, VQAv2, and TextVQA. Our results show that FlipLLM can identify critical bits that are vulnerable to BFAs up to 2.5x faster than SOTA methods. We demonstrate that flipping the FlipLLM-identified bits plummets the accuracy of LLaMA 3.1 8B from 69.9% to ~0.2%, and for LLaVA's VQA score from 78% to almost 0%, by flipping as few as 5 and 7 bits, respectively. Further analysis reveals that applying standard hardware protection mechanisms, such as ECC SECDED, to the FlipLLM-identified bit locations completely mitigates the BFA impact, demonstrating the practical value of our framework in guiding hardware-level defenses. FlipLLM offers the first scalable and adaptive methodology for exploring the BFA vulnerability of both language and multimodal foundation models, paving the way for comprehensive hardware-security evaluation.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09872v1",
        "pdf": "https://arxiv.org/pdf/2512.09872v1"
      },
      "arxiv_id": "2512.09872v1",
      "comment": "Accepted in IEEE HOST 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09871v1",
      "title": "Diffusion Posterior Sampler for Hyperspectral Unmixing with Spectral Variability Modeling",
      "authors": [
        "Yimin Zhu",
        "Lincoln Linlin Xu"
      ],
      "abstract": "Linear spectral mixture models (LMM) provide a concise form to disentangle the constituent materials (endmembers) and their corresponding proportions (abundance) in a single pixel. The critical challenges are how to model the spectral prior distribution and spectral variability. Prior knowledge and spectral variability can be rigorously modeled under the Bayesian framework, where posterior estimation of Abundance is derived by combining observed data with endmember prior distribution. Considering the key challenges and the advantages of the Bayesian framework, a novel method using a diffusion posterior sampler for semiblind unmixing, denoted as DPS4Un, is proposed to deal with these challenges with the following features: (1) we view the pretrained conditional spectrum diffusion model as a posterior sampler, which can combine the learned endmember prior with observation to get the refined abundance distribution. (2) Instead of using the existing spectral library as prior, which may raise bias, we establish the image-based endmember bundles within superpixels, which are used to train the endmember prior learner with diffusion model. Superpixels make sure the sub-scene is more homogeneous. (3) Instead of using the image-level data consistency constraint, the superpixel-based data fidelity term is proposed. (4) The endmember is initialized as Gaussian noise for each superpixel region, DPS4Un iteratively updates the abundance and endmember, contributing to spectral variability modeling. The experimental results on three real-world benchmark datasets demonstrate that DPS4Un outperforms the state-of-the-art hyperspectral unmixing methods.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09871v1",
        "pdf": "https://arxiv.org/pdf/2512.09871v1"
      },
      "arxiv_id": "2512.09871v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09867v1",
      "title": "MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI",
      "authors": [
        "Fengli Wu",
        "Vaidehi Patil",
        "Jaehong Yoon",
        "Yue Zhang",
        "Mohit Bansal"
      ],
      "abstract": "Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the \"right to be forgotten\". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09867v1",
        "pdf": "https://arxiv.org/pdf/2512.09867v1"
      },
      "arxiv_id": "2512.09867v1",
      "comment": "Dataset and Code: https://github.com/fengli-wu/MedForget",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.09864v1",
      "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
      "authors": [
        "Hao Lu",
        "Ziyang Liu",
        "Guangfeng Jiang",
        "Yuanfei Luo",
        "Sheng Chen",
        "Yangang Zhang",
        "Ying-Cong Chen"
      ],
      "abstract": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09864v1",
        "pdf": "https://arxiv.org/pdf/2512.09864v1"
      },
      "arxiv_id": "2512.09864v1",
      "comment": "Project Page: https://seed-uniugp.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.09851v1",
      "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
      "authors": [
        "Yuyang Li",
        "Yinghan Chen",
        "Zihang Zhao",
        "Puhao Li",
        "Tengyu Liu",
        "Siyuan Huang",
        "Yixin Zhu"
      ],
      "abstract": "Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09851v1",
        "pdf": "https://arxiv.org/pdf/2512.09851v1"
      },
      "arxiv_id": "2512.09851v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09850v1",
      "title": "Conformal Bandits: Bringing statistical validity and reward efficiency to the small-gap regime",
      "authors": [
        "Simone Cuonzo",
        "Nina Deliu"
      ],
      "abstract": "We introduce Conformal Bandits, a novel framework integrating Conformal Prediction (CP) into bandit problems, a classic paradigm for sequential decision-making under uncertainty. Traditional regret-minimisation bandit strategies like Thompson Sampling and Upper Confidence Bound (UCB) typically rely on distributional assumptions or asymptotic guarantees; further, they remain largely focused on regret, neglecting their statistical properties. We address this gap. Through the adoption of CP, we bridge the regret-minimising potential of a decision-making bandit policy with statistical guarantees in the form of finite-time prediction coverage.\n  We demonstrate the potential of it Conformal Bandits through simulation studies and an application to portfolio allocation, a typical small-gap regime, where differences in arm rewards are far too small for classical policies to achieve optimal regret bounds in finite sample. Motivated by this, we showcase our framework's practical advantage in terms of regret in small-gap settings, as well as its added value in achieving nominal coverage guarantees where classical UCB policies fail. Focusing on our application of interest, we further illustrate how integrating hidden Markov models to capture the regime-switching behaviour of financial markets, enhances the exploration-exploitation trade-off, and translates into higher risk-adjusted regret efficiency returns, while preserving coverage guarantees.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09850v1",
        "pdf": "https://arxiv.org/pdf/2512.09850v1"
      },
      "arxiv_id": "2512.09850v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09847v1",
      "title": "From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities",
      "authors": [
        "Shijia Feng",
        "Michael Wray",
        "Walterio Mayol-Cuevas"
      ],
      "abstract": "Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09847v1",
        "pdf": "https://arxiv.org/pdf/2512.09847v1"
      },
      "arxiv_id": "2512.09847v1",
      "comment": "Accepted by WACV 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09841v1",
      "title": "ChronusOmni: Improving Time Awareness of Omni Large Language Models",
      "authors": [
        "Yijing Chen",
        "Yihan Wu",
        "Kaisi Guan",
        "Yuchen Ren",
        "Yuyue Wang",
        "Ruihua Song",
        "Liyun Ru"
      ],
      "abstract": "Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09841v1",
        "pdf": "https://arxiv.org/pdf/2512.09841v1"
      },
      "arxiv_id": "2512.09841v1",
      "comment": "Code available at https://github.com/YJCX330/Chronus/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.09836v1",
      "title": "Fast Factorized Learning: Powered by In-Memory Database Systems",
      "authors": [
        "Bernhard Stöckl",
        "Maximilian E. Schüle"
      ],
      "abstract": "Learning models over factorized joins avoids redundant computations by identifying and pre-computing shared cofactors. Previous work has investigated the performance gain when computing cofactors on traditional disk-based database systems. Due to the absence of published code, the experiments could not be reproduced on in-memory database systems. This work describes the implementation when using cofactors for in-database factorized learning. We benchmark our open-source implementation for learning linear regression on factorized joins with PostgreSQL -- as a disk-based database system -- and HyPer -- as an in-memory engine. The evaluation shows a performance gain of factorized learning on in-memory database systems by 70\\% to non-factorized learning and by a factor of 100 compared to disk-based database systems. Thus, modern database engines can contribute to the machine learning pipeline by pre-computing aggregates prior to data extraction to accelerate training.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09836v1",
        "pdf": "https://arxiv.org/pdf/2512.09836v1"
      },
      "arxiv_id": "2512.09836v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09835v1",
      "title": "Predicting the Containment Time of California Wildfires Using Machine Learning",
      "authors": [
        "Shashank Bhardwaj"
      ],
      "abstract": "California's wildfire season keeps getting worse over the years, overwhelming the emergency response teams. These fires cause massive destruction to both property and human life. Because of these reasons, there's a growing need for accurate and practical predictions that can help assist with resources allocation for the Wildfire managers or the response teams. In this research, we built machine learning models to predict the number of days it will require to fully contain a wildfire in California. Here, we addressed an important gap in the current literature. Most prior research has concentrated on wildfire risk or how fires spread, and the few that examine the duration typically predict it in broader categories rather than a continuous measure. This research treats the wildfire duration prediction as a regression task, which allows for more detailed and precise forecasts rather than just the broader categorical predictions used in prior work. We built the models by combining three publicly available datasets from California Department of Forestry and Fire Protection's Fire and Resource Assessment Program (FRAP). This study compared the performance of baseline ensemble regressor, Random Forest and XGBoost, with a Long Short-Term Memory (LSTM) neural network. The results show that the XGBoost model slightly outperforms the Random Forest model, likely due to its superior handling of static features in the dataset. The LSTM model, on the other hand, performed worse than the ensemble models because the dataset lacked temporal features. Overall, this study shows that, depending on the feature availability, Wildfire managers or Fire management authorities can select the most appropriate model to accurately predict wildfire containment duration and allocate resources effectively.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09835v1",
        "pdf": "https://arxiv.org/pdf/2512.09835v1"
      },
      "arxiv_id": "2512.09835v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09831v1",
      "title": "Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning",
      "authors": [
        "Chainarong Amornbunchornvej"
      ],
      "abstract": "This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.\n  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-\"the No-Null-Space Leadership Condition\"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.\n  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09831v1",
        "pdf": "https://arxiv.org/pdf/2512.09831v1"
      },
      "arxiv_id": "2512.09831v1",
      "comment": "The first draft of cognitive geometry model",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09830v1",
      "title": "LLMs in Interpreting Legal Documents",
      "authors": [
        "Simone Corbo"
      ],
      "abstract": "This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09830v1",
        "pdf": "https://arxiv.org/pdf/2512.09830v1"
      },
      "arxiv_id": "2512.09830v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09829v1",
      "title": "RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning",
      "authors": [
        "Khurram Khalil",
        "Muhammad Mahad Khaliq",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \\textbf{2.2$\\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \\textbf{99\\%} compared to random fault injection, all while achieving \\textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \\textbf{12.8$\\times$} improvement in \\textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09829v1",
        "pdf": "https://arxiv.org/pdf/2512.09829v1"
      },
      "arxiv_id": "2512.09829v1",
      "comment": "Accepted in the IEEE DATE 2026 conference",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09824v1",
      "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
      "authors": [
        "Xianghao Kong",
        "Zeyu Zhang",
        "Yuwei Guo",
        "Zhuoran Zhao",
        "Songchun Zhang",
        "Anyi Rao"
      ],
      "abstract": "Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09824v1",
        "pdf": "https://arxiv.org/pdf/2512.09824v1"
      },
      "arxiv_id": "2512.09824v1",
      "comment": "Project page: https://refkxh.github.io/BiCo_Webpage/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.09817v1",
      "title": "A roadmap of geospatial soil quality analysis systems",
      "authors": [
        "Habiba BEN ABDERRAHMANE",
        "Slimane Oulad-Naoui",
        "Benameur ZIANI"
      ],
      "abstract": "Soil quality (SQ) plays a crucial role in sustainable agriculture, environmental conservation, and land-use planning. Traditional SQ assessment techniques rely on costly, labor-intensive sampling and laboratory analysis, limiting their spatial and temporal coverage. Advances in Geographic Information Systems (GIS), remote sensing, and machine learning (ML) enabled efficient SQ evaluation. This paper presents a comprehensive roadmap distinguishing it from previous reviews by proposing a unified and modular pipeline that integrates multi-source soil data, GIS and remote sensing tools, and machine learning techniques to support transparent and scalable soil quality assessment. It also includes practical applications. Contrary to existing studies that predominantly target isolated soil parameters or specific modeling methodologies, this approach consolidates recent advancements in Geographic Information Systems (GIS), remote sensing technologies, and machine learning algorithms within the entire soil quality assessment pipeline. It also addresses existing challenges and limitations while exploring future developments and emerging trends in the field that can deliver the next generation of soil quality systems making them more transparent, adaptive, and aligned with sustainable land management.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.CE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09817v1",
        "pdf": "https://arxiv.org/pdf/2512.09817v1"
      },
      "arxiv_id": "2512.09817v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09814v1",
      "title": "DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation",
      "authors": [
        "Zhizhong Wang",
        "Tianyi Chu",
        "Zeyi Huang",
        "Nanyang Wang",
        "Kehan Li"
      ],
      "abstract": "Personalized Text-to-Image (PT2I) generation aims to produce customized images based on reference images. A prominent interest pertains to the integration of an image prompt adapter to facilitate zero-shot PT2I without test-time fine-tuning. However, current methods grapple with three fundamental challenges: 1. the elusive equilibrium between Concept Preservation (CP) and Prompt Following (PF), 2. the difficulty in retaining fine-grained concept details in reference images, and 3. the restricted scalability to extend to multi-subject personalization. To tackle these challenges, we present Dynamic Image Prompt Adapter (DynaIP), a cutting-edge plugin to enhance the fine-grained concept fidelity, CP-PF balance, and subject scalability of SOTA T2I multimodal diffusion transformers (MM-DiT) for PT2I generation. Our key finding is that MM-DiT inherently exhibit decoupling learning behavior when injecting reference image features into its dual branches via cross attentions. Based on this, we design an innovative Dynamic Decoupling Strategy that removes the interference of concept-agnostic information during inference, significantly enhancing the CP-PF balance and further bolstering the scalability of multi-subject compositions. Moreover, we identify the visual encoder as a key factor affecting fine-grained CP and reveal that the hierarchical features of commonly used CLIP can capture visual information at diverse granularity levels. Therefore, we introduce a novel Hierarchical Mixture-of-Experts Feature Fusion Module to fully leverage the hierarchical features of CLIP, remarkably elevating the fine-grained concept fidelity while also providing flexible control of visual granularity. Extensive experiments across single- and multi-subject PT2I tasks verify that our DynaIP outperforms existing approaches, marking a notable advancement in the field of PT2l generation.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09814v1",
        "pdf": "https://arxiv.org/pdf/2512.09814v1"
      },
      "arxiv_id": "2512.09814v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09810v1",
      "title": "Incorporating Fairness in Neighborhood Graphs for Fair Spectral Clustering",
      "authors": [
        "Adithya K Moorthy",
        "V Vijaya Saradhi",
        "Bhanu Prasad"
      ],
      "abstract": "Graph clustering plays a pivotal role in unsupervised learning methods like spectral clustering, yet traditional methods for graph clustering often perpetuate bias through unfair graph constructions that may underrepresent some groups. The current research introduces novel approaches for constructing fair k-nearest neighbor (kNN) and fair epsilon-neighborhood graphs that proactively enforce demographic parity during graph formation. By incorporating fairness constraints at the earliest stage of neighborhood selection steps, our approaches incorporate proportional representation of sensitive features into the local graph structure while maintaining geometric consistency.Our work addresses a critical gap in pre-processing for fair spectral clustering, demonstrating that topological fairness in graph construction is essential for achieving equitable clustering outcomes. Widely used graph construction methods like kNN and epsilon-neighborhood graphs propagate edge based disparate impact on sensitive groups, leading to biased clustering results. Providing representation of each sensitive group in the neighborhood of every node leads to fairer spectral clustering results because the topological features of the graph naturally reflect equitable group ratios. This research fills an essential shortcoming in fair unsupervised learning, by illustrating how topological fairness in graph construction inherently facilitates fairer spectral clustering results without the need for changes to the clustering algorithm itself. Thorough experiments on three synthetic datasets, seven real-world tabular datasets, and three real-world image datasets prove that our fair graph construction methods surpass the current baselines in graph clustering tasks.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09810v1",
        "pdf": "https://arxiv.org/pdf/2512.09810v1"
      },
      "arxiv_id": "2512.09810v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09806v1",
      "title": "CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image Processing",
      "authors": [
        "Jianfei Li",
        "Ines Rosellon-Inclan",
        "Gitta Kutyniok",
        "Jean-Luc Starck"
      ],
      "abstract": "U-Net and other U-shaped architectures have achieved significant success in image deconvolution tasks. However, challenges have emerged, as these methods might generate unrealistic artifacts or hallucinations, which can interfere with analysis in safety-critical scenarios. This paper introduces a novel approach for quantifying and comprehending hallucination artifacts to ensure trustworthy computer vision models. Our method, termed the Conformal Hallucination Estimation Metric (CHEM), is applicable to any image reconstruction model, enabling efficient identification and quantification of hallucination artifacts. It offers two key advantages: it leverages wavelet and shearlet representations to efficiently extract hallucinations of image features and uses conformalized quantile regression to assess hallucination levels in a distribution-free manner. Furthermore, from an approximation theoretical perspective, we explore the reasons why U-shaped networks are prone to hallucinations. We test the proposed approach on the CANDELS astronomical image dataset with models such as U-Net, SwinUNet, and Learnlets, and provide new perspectives on hallucination from different aspects in deep learning-based image processing.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09806v1",
        "pdf": "https://arxiv.org/pdf/2512.09806v1"
      },
      "arxiv_id": "2512.09806v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09804v1",
      "title": "OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations",
      "authors": [
        "Jens Albrecht",
        "Robert Lehmann",
        "Aleksandra Poltermann",
        "Eric Rudolph",
        "Philipp Steigerwald",
        "Mara Stieler"
      ],
      "abstract": "This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09804v1",
        "pdf": "https://arxiv.org/pdf/2512.09804v1"
      },
      "arxiv_id": "2512.09804v1",
      "comment": "Submitted to LREC 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09801v1",
      "title": "Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation",
      "authors": [
        "Tien-Dat Chung",
        "Ba-Thinh Lam",
        "Thanh-Huy Nguyen",
        "Thien Nguyen",
        "Nguyen Lan Vi Vu",
        "Hoang-Loc Cao",
        "Phat Kim Huynh",
        "Min Xu"
      ],
      "abstract": "Semi-supervised learning (SSL) has become a promising direction for medical image segmentation, enabling models to learn from limited labeled data alongside abundant unlabeled samples. However, existing SSL approaches for multi-modal medical imaging often struggle to exploit the complementary information between modalities due to semantic discrepancies and misalignment across MRI sequences. To address this, we propose a novel semi-supervised multi-modal framework that explicitly enhances modality-specific representations and facilitates adaptive cross-modal information fusion. Specifically, we introduce a Modality-specific Enhancing Module (MEM) to strengthen semantic cues unique to each modality via channel-wise attention, and a learnable Complementary Information Fusion (CIF) module to adaptively exchange complementary knowledge between modalities. The overall framework is optimized using a hybrid objective combining supervised segmentation loss and cross-modal consistency regularization on unlabeled data. Extensive experiments on the BraTS 2019 (HGG subset) demonstrate that our method consistently outperforms strong semi-supervised and multi-modal baselines under 1\\%, 5\\%, and 10\\% labeled data settings, achieving significant improvements in both Dice and Sensitivity scores. Ablation studies further confirm the complementary effects of our proposed MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness under scarce supervision.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09801v1",
        "pdf": "https://arxiv.org/pdf/2512.09801v1"
      },
      "arxiv_id": "2512.09801v1",
      "comment": "9 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09800v1",
      "title": "Ariel-ML: Computing Parallelization with Embedded Rust for Neural Networks on Heterogeneous Multi-core Microcontrollers",
      "authors": [
        "Zhaolan Huang",
        "Kaspar Schleiser",
        "Gyungmin Myung",
        "Emmanuel Baccelli"
      ],
      "abstract": "Low-power microcontroller (MCU) hardware is currently evolving from single-core architectures to predominantly multi-core architectures. In parallel, new embedded software building blocks are more and more written in Rust, while C/C++ dominance fades in this domain. On the other hand, small artificial neural networks (ANN) of various kinds are increasingly deployed in edge AI use cases, thus deployed and executed directly on low-power MCUs. In this context, both incremental improvements and novel innovative services will have to be continuously retrofitted using ANNs execution in software embedded on sensing/actuating systems already deployed in the field. However, there was so far no Rust embedded software platform automating parallelization for inference computation on multi-core MCUs executing arbitrary TinyML models. This paper thus fills this gap by introducing Ariel-ML, a novel toolkit we designed combining a generic TinyML pipeline and an embedded Rust software platform which can take full advantage of multi-core capabilities of various 32bit microcontroller families (Arm Cortex-M, RISC-V, ESP-32). We published the full open source code of its implementation, which we used to benchmark its capabilities using a zoo of various TinyML models. We show that Ariel-ML outperforms prior art in terms of inference latency as expected, and we show that, compared to pre-existing toolkits using embedded C/C++, Ariel-ML achieves comparable memory footprints. Ariel-ML thus provides a useful basis for TinyML practitioners and resource-constrained embedded Rust developers.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09800v1",
        "pdf": "https://arxiv.org/pdf/2512.09800v1"
      },
      "arxiv_id": "2512.09800v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09797v1",
      "title": "M3Net: A Multi-Metric Mixture of Experts Network Digital Twin with Graph Neural Networks",
      "authors": [
        "Blessed Guda",
        "Carlee Joe-Wong"
      ],
      "abstract": "The rise of 5G/6G network technologies promises to enable applications like autonomous vehicles and virtual reality, resulting in a significant increase in connected devices and necessarily complicating network management. Even worse, these applications often have strict, yet heterogeneous, performance requirements across metrics like latency and reliability. Much recent work has thus focused on developing the ability to predict network performance. However, traditional methods for network modeling, like discrete event simulators and emulation, often fail to balance accuracy and scalability. Network Digital Twins (NDTs), augmented by machine learning, present a viable solution by creating virtual replicas of physical networks for real- time simulation and analysis. State-of-the-art models, however, fall short of full-fledged NDTs, as they often focus only on a single performance metric or simulated network data. We introduce M3Net, a Multi-Metric Mixture-of-experts (MoE) NDT that uses a graph neural network architecture to estimate multiple performance metrics from an expanded set of network state data in a range of scenarios. We show that M3Net significantly enhances the accuracy of flow delay predictions by reducing the MAPE (Mean Absolute Percentage Error) from 20.06% to 17.39%, while also achieving 66.47% and 78.7% accuracy on jitter and packets dropped for each flow",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.NI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09797v1",
        "pdf": "https://arxiv.org/pdf/2512.09797v1"
      },
      "arxiv_id": "2512.09797v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09796v1",
      "title": "Knowledge Diversion for Efficient Morphology Control and Policy Transfer",
      "authors": [
        "Fu Feng",
        "Ruixiao Shi",
        "Yucheng Xie",
        "Jianlu Shen",
        "Jing Wang",
        "Xin Geng"
      ],
      "abstract": "Universal morphology control aims to learn a universal policy that generalizes across heterogeneous agent morphologies, with Transformer-based controllers emerging as a popular choice. However, such architectures incur substantial computational costs, resulting in high deployment overhead, and existing methods exhibit limited cross-task generalization, necessitating training from scratch for each new task. To this end, we propose \\textbf{DivMorph}, a modular training paradigm that leverages knowledge diversion to learn decomposable controllers. DivMorph factorizes randomly initialized Transformer weights into factor units via SVD prior to training and employs dynamic soft gating to modulate these units based on task and morphology embeddings, separating them into shared \\textit{learngenes} and morphology- and task-specific \\textit{tailors}, thereby achieving knowledge disentanglement. By selectively activating relevant components, DivMorph enables scalable and efficient policy deployment while supporting effective policy transfer to novel tasks. Extensive experiments demonstrate that DivMorph achieves state-of-the-art performance, achieving a 3$\\times$ improvement in sample efficiency over direct finetuning for cross-task transfer and a 17$\\times$ reduction in model size for single-agent deployment.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09796v1",
        "pdf": "https://arxiv.org/pdf/2512.09796v1"
      },
      "arxiv_id": "2512.09796v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09792v1",
      "title": "FastPose-ViT: A Vision Transformer for Real-Time Spacecraft Pose Estimation",
      "authors": [
        "Pierre Ancey",
        "Andrew Price",
        "Saqib Javed",
        "Mathieu Salzmann"
      ],
      "abstract": "Estimating the 6-degrees-of-freedom (6DoF) pose of a spacecraft from a single image is critical for autonomous operations like in-orbit servicing and space debris removal. Existing state-of-the-art methods often rely on iterative Perspective-n-Point (PnP)-based algorithms, which are computationally intensive and ill-suited for real-time deployment on resource-constrained edge devices. To overcome these limitations, we propose FastPose-ViT, a Vision Transformer (ViT)-based architecture that directly regresses the 6DoF pose. Our approach processes cropped images from object bounding boxes and introduces a novel mathematical formalism to map these localized predictions back to the full-image scale. This formalism is derived from the principles of projective geometry and the concept of \"apparent rotation\", where the model predicts an apparent rotation matrix that is then corrected to find the true orientation. We demonstrate that our method outperforms other non-PnP strategies and achieves performance competitive with state-of-the-art PnP-based techniques on the SPEED dataset. Furthermore, we validate our model's suitability for real-world space missions by quantizing it and deploying it on power-constrained edge hardware. On the NVIDIA Jetson Orin Nano, our end-to-end pipeline achieves a latency of ~75 ms per frame under sequential execution, and a non-blocking throughput of up to 33 FPS when stages are scheduled concurrently.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09792v1",
        "pdf": "https://arxiv.org/pdf/2512.09792v1"
      },
      "arxiv_id": "2512.09792v1",
      "comment": "Accepted to WACV 2026. Preprint version",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09786v1",
      "title": "TinyDéjàVu: Smaller Memory Footprint & Faster Inference on Sensor Data Streams with Always-On Microcontrollers",
      "authors": [
        "Zhaolan Huang",
        "Emmanuel Baccelli"
      ],
      "abstract": "Always-on sensors are increasingly expected to embark a variety of tiny neural networks and to continuously perform inference on time-series of the data they sense. In order to fit lifetime and energy consumption requirements when operating on battery, such hardware uses microcontrollers (MCUs) with tiny memory budget e.g., 128kB of RAM. In this context, optimizing data flows across neural network layers becomes crucial. In this paper, we introduce TinyDéjàVu, a new framework and novel algorithms we designed to drastically reduce the RAM footprint required by inference using various tiny ML models for sensor data time-series on typical microcontroller hardware. We publish the implementation of TinyDéjàVu as open source, and we perform reproducible benchmarks on hardware. We show that TinyDéjàVu can save more than 60% of RAM usage and eliminate up to 90% of redundant compute on overlapping sliding window inputs.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG",
        "cs.PF",
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09786v1",
        "pdf": "https://arxiv.org/pdf/2512.09786v1"
      },
      "arxiv_id": "2512.09786v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09784v1",
      "title": "Predicting Polymer Solubility in Solvents Using SMILES Strings",
      "authors": [
        "Andrew Reinhard"
      ],
      "abstract": "Understanding and predicting polymer solubility in various solvents is critical for applications ranging from recycling to pharmaceutical formulation. This work presents a deep learning framework that predicts polymer solubility, expressed as weight percent (wt%), directly from SMILES representations of both polymers and solvents. A dataset of 8,049 polymer solvent pairs at 25 deg C was constructed from calibrated molecular dynamics simulations (Zhou et al., 2023), and molecular descriptors and fingerprints were combined into a 2,394 feature representation per sample. A fully connected neural network with six hidden layers was trained using the Adam optimizer and evaluated using mean squared error loss, achieving strong agreement between predicted and actual solubility values. Generalizability was demonstrated using experimentally measured data from the Materials Genome Project, where the model maintained high accuracy on 25 unseen polymer solvent combinations. These findings highlight the viability of SMILES based machine learning models for scalable solubility prediction and high-throughput solvent screening, supporting applications in green chemistry, polymer processing, and materials design.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09784v1",
        "pdf": "https://arxiv.org/pdf/2512.09784v1"
      },
      "arxiv_id": "2512.09784v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09780v1",
      "title": "Physics-Aware Heterogeneous GNN Architecture for Real-Time BESS Optimization in Unbalanced Distribution Systems",
      "authors": [
        "Aoxiang Ma",
        "Salah Ghamizi",
        "Jun Cao",
        "Pedro Rodriguez"
      ],
      "abstract": "Battery energy storage systems (BESS) have become increasingly vital in three-phase unbalanced distribution grids for maintaining voltage stability and enabling optimal dispatch. However, existing deep learning approaches often lack explicit three-phase representation, making it difficult to accurately model phase-specific dynamics and enforce operational constraints--leading to infeasible dispatch solutions. This paper demonstrates that by embedding detailed three-phase grid information--including phase voltages, unbalanced loads, and BESS states--into heterogeneous graph nodes, diverse GNN architectures (GCN, GAT, GraphSAGE, GPS) can jointly predict network state variables with high accuracy. Moreover, a physics-informed loss function incorporates critical battery constraints--SoC and C-rate limits--via soft penalties during training. Experimental validation on the CIGRE 18-bus distribution system shows that this embedding-loss approach achieves low prediction errors, with bus voltage MSEs of 6.92e-07 (GCN), 1.21e-06 (GAT), 3.29e-05 (GPS), and 9.04e-07 (SAGE). Importantly, the physics-informed method ensures nearly zero SoC and C-rate constraint violations, confirming its effectiveness for reliable, constraint-compliant dispatch.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09780v1",
        "pdf": "https://arxiv.org/pdf/2512.09780v1"
      },
      "arxiv_id": "2512.09780v1",
      "comment": "5 pages, 2 figures, 3 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09779v1",
      "title": "PathCo-LatticE: Pathology-Constrained Lattice-Of Experts Framework for Fully-supervised Few-Shot Cardiac MRI Segmentation",
      "authors": [
        "Mohamed Elbayumi",
        "Mohammed S. M. Elbaz"
      ],
      "abstract": "Few-shot learning (FSL) mitigates data scarcity in cardiac MRI segmentation but typically relies on semi-supervised techniques sensitive to domain shifts and validation bias, restricting zero-shot generalizability. We propose PathCo-LatticE, a fully supervised FSL framework that replaces unlabeled data with pathology-guided synthetic supervision. First, our Virtual Patient Engine models continuous latent disease trajectories from sparse clinical anchors, using generative modeling to synthesize physiologically plausible, fully labeled 3D cohorts. Second, Self-Reinforcing Interleaved Validation (SIV) provides a leakage-free protocol that evaluates models online with progressively challenging synthetic samples, eliminating the need for real validation data. Finally, a dynamic Lattice-of-Experts (LoE) organizes specialized networks within a pathology-aware topology and activates the most relevant experts per input, enabling robust zero-shot generalization to unseen data without target-domain fine-tuning. We evaluated PathCo-LatticE in a strict out-of-distribution (OOD) setting, deriving all anchors and severity statistics from a single-source domain (ACDC) and performing zero-shot testing on the multi-center, multi-vendor M&Ms dataset. PathCo-LatticE outperforms four state-of-the-art FSL methods by 4.2-11% Dice starting from only 7 labeled anchors, and approaches fully supervised performance (within 1% Dice) with only 19 labeled anchors. The method shows superior harmonization across four vendors and generalization to unseen pathologies. [Code will be made publicly available].",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09779v1",
        "pdf": "https://arxiv.org/pdf/2512.09779v1"
      },
      "arxiv_id": "2512.09779v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09778v1",
      "title": "Optimal certification of constant-local Hamiltonians",
      "authors": [
        "Junseo Lee",
        "Myeongjin Shin"
      ],
      "abstract": "We study the problem of certifying local Hamiltonians from real-time access to their dynamics. Given oracle access to $e^{-itH}$ for an unknown $k$-local Hamiltonian $H$ and a fully specified target Hamiltonian $H_0$, the goal is to decide whether $H$ is exactly equal to $H_0$ or differs from $H_0$ by at least $\\varepsilon$ in normalized Frobenius norm, while minimizing the total evolution time. We introduce the first intolerant Hamiltonian certification protocol that achieves optimal performance for all constant-locality Hamiltonians. For general $n$-qubit, $k$-local, traceless Hamiltonians, our procedure uses $O(c^k/\\varepsilon)$ total evolution time for a universal constant $c$, and succeeds with high probability. In particular, for $O(1)$-local Hamiltonians, the total evolution time becomes $Θ(1/\\varepsilon)$, matching the known $Ω(1/\\varepsilon)$ lower bounds and achieving the gold-standard Heisenberg-limit scaling. Prior certification methods either relied on implementing inverse evolution of $H$, required controlled access to $e^{-itH}$, or achieved near-optimal guarantees only in restricted settings such as the Ising case ($k=2$). In contrast, our algorithm requires neither inverse evolution nor controlled operations: it uses only forward real-time dynamics and achieves optimal intolerant certification for all constant-locality Hamiltonians.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "quant-ph",
        "cs.CC",
        "cs.DS",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09778v1",
        "pdf": "https://arxiv.org/pdf/2512.09778v1"
      },
      "arxiv_id": "2512.09778v1",
      "comment": "29 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09775v1",
      "title": "Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition",
      "authors": [
        "Vladimir Balditsyn",
        "Philippe Lalanda",
        "German Vega",
        "Stéphanie Chollet"
      ],
      "abstract": "The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09775v1",
        "pdf": "https://arxiv.org/pdf/2512.09775v1"
      },
      "arxiv_id": "2512.09775v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09773v1",
      "title": "Stylized Meta-Album: Group-bias injection with style transfer to study robustness against distribution shifts",
      "authors": [
        "Romain Mussard",
        "Aurélien Gauffre",
        "Ihsan Ullah",
        "Thanh Gia Hieu Khuong",
        "Massih-Reza Amini",
        "Isabelle Guyon",
        "Lisheng Sun-Hosoya"
      ],
      "abstract": "We introduce Stylized Meta-Album (SMA), a new image classification meta-dataset comprising 24 datasets (12 content datasets, and 12 stylized datasets), designed to advance studies on out-of-distribution (OOD) generalization and related topics. Created using style transfer techniques from 12 subject classification datasets, SMA provides a diverse and extensive set of 4800 groups, combining various subjects (objects, plants, animals, human actions, textures) with multiple styles. SMA enables flexible control over groups and classes, allowing us to configure datasets to reflect diverse benchmark scenarios. While ideally, data collection would capture extensive group diversity, practical constraints often make this infeasible. SMA addresses this by enabling large and configurable group structures through flexible control over styles, subject classes, and domains-allowing datasets to reflect a wide range of real-world benchmark scenarios. This design not only expands group and class diversity, but also opens new methodological directions for evaluating model performance across diverse group and domain configurations-including scenarios with many minority groups, varying group imbalance, and complex domain shifts-and for studying fairness, robustness, and adaptation under a broader range of realistic conditions. To demonstrate SMA's effectiveness, we implemented two benchmarks: (1) a novel OOD generalization and group fairness benchmark leveraging SMA's domain, class, and group diversity to evaluate existing benchmarks. Our findings reveal that while simple balancing and algorithms utilizing group information remain competitive as claimed in previous benchmarks, increasing group diversity significantly impacts fairness, altering the superiority and relative rankings of algorithms. We also propose to use \\textit{Top-M worst group accuracy} as a new hyperparameter tuning metric, demonstrating broader fairness during optimization and delivering better final worst-group accuracy for larger group diversity. (2) An unsupervised domain adaptation (UDA) benchmark utilizing SMA's group diversity to evaluate UDA algorithms across more scenarios, offering a more comprehensive benchmark with lower error bars (reduced by 73\\% and 28\\% in closed-set setting and UniDA setting, respectively) compared to existing efforts. These use cases highlight SMA's potential to significantly impact the outcomes of conventional benchmarks.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09773v1",
        "pdf": "https://arxiv.org/pdf/2512.09773v1"
      },
      "arxiv_id": "2512.09773v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09757v1",
      "title": "Circuits, Features, and Heuristics in Molecular Transformers",
      "authors": [
        "Kristof Varadi",
        "Mark Marosi",
        "Peter Antal"
      ],
      "abstract": "Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09757v1",
        "pdf": "https://arxiv.org/pdf/2512.09757v1"
      },
      "arxiv_id": "2512.09757v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09742v1",
      "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs",
      "authors": [
        "Jan Betley",
        "Jorio Cocola",
        "Dylan Feng",
        "James Chua",
        "Andy Arditi",
        "Anna Sztyber-Betley",
        "Owain Evans"
      ],
      "abstract": "LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. \"Q: Favorite music? A: Wagner\"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09742v1",
        "pdf": "https://arxiv.org/pdf/2512.09742v1"
      },
      "arxiv_id": "2512.09742v1",
      "comment": "70 pages, 47 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09736v1",
      "title": "Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation",
      "authors": [
        "Jingtian Yan",
        "Zhifei Li",
        "William Kang",
        "Stephen F. Smith",
        "Jiaoyang Li"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09736v1",
        "pdf": "https://arxiv.org/pdf/2512.09736v1"
      },
      "arxiv_id": "2512.09736v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09730v1",
      "title": "Interpreto: An Explainability Library for Transformers",
      "authors": [
        "Antonin Poché",
        "Thomas Mullor",
        "Gabriele Sarti",
        "Frédéric Boisnard",
        "Corentin Friedrich",
        "Charlotte Claye",
        "François Hoofd",
        "Raphael Bernas",
        "Céline Hudelot",
        "Fanny Jourdan"
      ],
      "abstract": "Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.\n  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.\n  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09730v1",
        "pdf": "https://arxiv.org/pdf/2512.09730v1"
      },
      "arxiv_id": "2512.09730v1",
      "comment": "Equal contribution: Poché and Jourdan",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09729v1",
      "title": "Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method",
      "authors": [
        "Laurynas Adomaitis",
        "Vincent Israel-Jost",
        "Alexei Grinbaum"
      ],
      "abstract": "We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09729v1",
        "pdf": "https://arxiv.org/pdf/2512.09729v1"
      },
      "arxiv_id": "2512.09729v1",
      "comment": "23 pages. Data available on GitHub at https://github.com/LA-NS/ethics-readiness-levels",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.09727v1",
      "title": "Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions",
      "authors": [
        "Junlin Xiao",
        "Victor-Alexandru Darvariu",
        "Bruno Lacerda",
        "Nick Hawes"
      ],
      "abstract": "Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09727v1",
        "pdf": "https://arxiv.org/pdf/2512.09727v1"
      },
      "arxiv_id": "2512.09727v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09723v1",
      "title": "Mixture of Lookup Key-Value Experts",
      "authors": [
        "Zongcheng Wang"
      ],
      "abstract": "Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \\textbf{M}ixture \\textbf{o}f \\textbf{L}ookup \\textbf{K}ey-\\textbf{V}alue Experts (\\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09723v1",
        "pdf": "https://arxiv.org/pdf/2512.09723v1"
      },
      "arxiv_id": "2512.09723v1",
      "comment": "Preliminary Version; Work in Progress",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09706v1",
      "title": "Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning",
      "authors": [
        "Kaichen He",
        "Zihao Wang",
        "Muyao Li",
        "Anji Liu",
        "Yitao Liang"
      ],
      "abstract": "The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09706v1",
        "pdf": "https://arxiv.org/pdf/2512.09706v1"
      },
      "arxiv_id": "2512.09706v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09700v1",
      "title": "LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery",
      "authors": [
        "Seon-Hoon Kim",
        "Hyeji Sim",
        "Youeyun Jung",
        "Ok-Chul Jung",
        "Yerin Kim"
      ],
      "abstract": "Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09700v1",
        "pdf": "https://arxiv.org/pdf/2512.09700v1"
      },
      "arxiv_id": "2512.09700v1",
      "comment": "16 pages, 8 figures, 9 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09690v1",
      "title": "A data-driven approach to linking design features with manufacturing process data for sustainable product development",
      "authors": [
        "Jiahang Li",
        "Lucas Cazzonelli",
        "Jacqueline Höllig",
        "Markus Doellken",
        "Sven Matthiesen"
      ],
      "abstract": "The growing adoption of Industrial Internet of Things (IIoT) technologies enables automated, real-time collection of manufacturing process data, unlocking new opportunities for data-driven product development. Current data-driven methods are generally applied within specific domains, such as design or manufacturing, with limited exploration of integrating design features and manufacturing process data. Since design decisions significantly affect manufacturing outcomes, such as error rates, energy consumption, and processing times, the lack of such integration restricts the potential for data-driven product design improvements. This paper presents a data-driven approach to mapping and analyzing the relationship between design features and manufacturing process data. A comprehensive system architecture is developed to ensure continuous data collection and integration. The linkage between design features and manufacturing process data serves as the basis for developing a machine learning model that enables automated design improvement suggestions. By integrating manufacturing process data with sustainability metrics, this approach opens new possibilities for sustainable product development.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09690v1",
        "pdf": "https://arxiv.org/pdf/2512.09690v1"
      },
      "arxiv_id": "2512.09690v1",
      "comment": "This is the preprint of a paper accepted for the CIRP Design Conference 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09687v1",
      "title": "Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized",
      "authors": [
        "Er Jin",
        "Yang Zhang",
        "Yongli Mou",
        "Yanfei Dong",
        "Stefan Decker",
        "Kenji Kawaguchi",
        "Johannes Stegmaier"
      ],
      "abstract": "Recent advances in generative models have demonstrated an exceptional ability to produce highly realistic images. However, previous studies show that generated images often resemble the training data, and this problem becomes more severe as the model size increases. Memorizing training data can lead to legal challenges, including copyright infringement, violations of portrait rights, and trademark violations. Existing approaches to mitigating memorization mainly focus on manipulating the denoising sampling process to steer image embeddings away from the memorized embedding space or employ unlearning methods that require training on datasets containing specific sets of memorized concepts. However, existing methods often incur substantial computational overhead during sampling, or focus narrowly on removing one or more groups of target concepts, imposing a significant limitation on their scalability. To understand and mitigate these problems, our work, UniForget, offers a new perspective on understanding the root cause of memorization. Our work demonstrates that specific parts of the model are responsible for copyrighted content generation. By applying model pruning, we can effectively suppress the probability of generating copyrighted content without targeting specific concepts while preserving the general generative capabilities of the model. Additionally, we show that our approach is both orthogonal and complementary to existing unlearning methods, thereby highlighting its potential to improve current unlearning and de-memorization techniques.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09687v1",
        "pdf": "https://arxiv.org/pdf/2512.09687v1"
      },
      "arxiv_id": "2512.09687v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09682v1",
      "title": "Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies",
      "authors": [
        "Mika Persson",
        "Jonas Lidman",
        "Jacob Ljungberg",
        "Samuel Sandelius",
        "Adam Andersson"
      ],
      "abstract": "This work presents a conceptual study on the application of Multi-Agent Reinforcement Learning (MARL) for decentralized control of unmanned aerial vehicles to relay a critical data package to a known position. For this purpose, a family of deterministic games is introduced, designed for scaling studies for MARL. A robust baseline policy is proposed, which is based on restricting agent motion envelopes and applying Dijkstra's algorithm. Experimental results show that two off-the-shelf MARL algorithms perform competitively with the baseline for a small number of agents, but scalability issues arise as the number of agents increase.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.GT",
        "cs.MA"
      ],
      "primary_category": "eess.SY",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09682v1",
        "pdf": "https://arxiv.org/pdf/2512.09682v1"
      },
      "arxiv_id": "2512.09682v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09678v1",
      "title": "The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization",
      "authors": [
        "Alexey Kravatskiy",
        "Ivan Kozyrev",
        "Nikolai Kozlov",
        "Alexander Vinogradov",
        "Daniil Merkulov",
        "Ivan Oseledets"
      ],
      "abstract": "In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm underlying the Muon update, we leverage duals of the Ky Fan $k$-norms to introduce a family of Muon-like algorithms we name Fanions, which are closely related to Dion. By working with duals of convex combinations of the Ky Fan $k$-norms with either the Frobenius norm or the $l_\\infty$ norm, we construct the families of F-Fanions and S-Fanions, respectively. Their most prominent members are F-Muon and S-Muon. We complement our theoretical analysis with an extensive empirical study of these algorithms across a wide range of tasks and settings, demonstrating that F-Muon and S-Muon consistently match Muon's performance, while outperforming vanilla Muon on a synthetic linear least squares problem.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09678v1",
        "pdf": "https://arxiv.org/pdf/2512.09678v1"
      },
      "arxiv_id": "2512.09678v1",
      "comment": "31 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09673v1",
      "title": "Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power",
      "authors": [
        "Yuzhu Chen",
        "Tian Qin",
        "Xinmei Tian",
        "Fengxiang He",
        "Dacheng Tao"
      ],
      "abstract": "Equivariant neural networks encode symmetry as an inductive bias and have achieved strong empirical performance in wide domains. However, their expressive power remains not well understood. Focusing on 2-layer ReLU networks, this paper investigates the impact of equivariance constraints on the expressivity of equivariant and layer-wise equivariant networks. By examining the boundary hyperplanes and the channel vectors of ReLU networks, we construct an example showing that equivariance constraints could strictly limit expressive power. However, we demonstrate that this drawback can be compensated via enlarging the model size. Furthermore, we show that despite a larger model size, the resulting architecture could still correspond to a hypothesis space with lower complexity, implying superior generalizability for equivariant networks.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09673v1",
        "pdf": "https://arxiv.org/pdf/2512.09673v1"
      },
      "arxiv_id": "2512.09673v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09670v1",
      "title": "An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence",
      "authors": [
        "Gil Weissman",
        "Amir Ivry",
        "Israel Cohen"
      ],
      "abstract": "The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV",
        "eess.SY"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09670v1",
        "pdf": "https://arxiv.org/pdf/2512.09670v1"
      },
      "arxiv_id": "2512.09670v1",
      "comment": "Under review at IEEE Transactions on Geoscience and Remote Sensing (TGRS). 13 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09665v1",
      "title": "OxEnsemble: Fair Ensembles for Low-Data Classification",
      "authors": [
        "Jonathan Rystrøm",
        "Zihao Fu",
        "Chris Russell"
      ],
      "abstract": "We address the problem of fair classification in settings where data is scarce and unbalanced across demographic groups. Such low-data regimes are common in domains like medical imaging, where false negatives can have fatal consequences.\n  We propose a novel approach \\emph{OxEnsemble} for efficiently training ensembles and enforcing fairness in these low-data regimes. Unlike other approaches, we aggregate predictions across ensemble members, each trained to satisfy fairness constraints. By construction, \\emph{OxEnsemble} is both data-efficient, carefully reusing held-out data to enforce fairness reliably, and compute-efficient, requiring little more compute than used to fine-tune or evaluate an existing model. We validate this approach with new theoretical guarantees. Experimentally, our approach yields more consistent outcomes and stronger fairness-accuracy trade-offs than existing methods across multiple challenging medical imaging classification datasets.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09665v1",
        "pdf": "https://arxiv.org/pdf/2512.09665v1"
      },
      "arxiv_id": "2512.09665v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09664v1",
      "title": "SynthPix: A lightspeed PIV images generator",
      "authors": [
        "Antonio Terpin",
        "Alan Bonomi",
        "Francesco Banelli",
        "Raffaello D'Andrea"
      ],
      "abstract": "We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.DC",
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.DC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09664v1",
        "pdf": "https://arxiv.org/pdf/2512.09664v1"
      },
      "arxiv_id": "2512.09664v1",
      "comment": "Code: https://github.com/antonioterpin/synthpix",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.09663v1",
      "title": "IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting",
      "authors": [
        "Tao Zhang",
        "Yuyang Hong",
        "Yang Xia",
        "Kun Ding",
        "Zeyu Zhang",
        "Ying Wang",
        "Shiming Xiang",
        "Chunhong Pan"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09663v1",
        "pdf": "https://arxiv.org/pdf/2512.09663v1"
      },
      "arxiv_id": "2512.09663v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09662v1",
      "title": "Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection",
      "authors": [
        "Paloma Piot",
        "David Otero",
        "Patricia Martín-Rodilla",
        "Javier Parapar"
      ],
      "abstract": "Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $κ$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09662v1",
        "pdf": "https://arxiv.org/pdf/2512.09662v1"
      },
      "arxiv_id": "2512.09662v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09654v1",
      "title": "Membership and Dataset Inference Attacks on Large Audio Generative Models",
      "authors": [
        "Jakub Proboszcz",
        "Paweł Kochanski",
        "Karol Korszun",
        "Donato Crisostomi",
        "Giorgio Strano",
        "Emanuele Rodolà",
        "Kamil Deja",
        "Jan Dubinski"
      ],
      "abstract": "Generative audio models, based on diffusion and autoregressive architectures, have advanced rapidly in both quality and expressiveness. This progress, however, raises pressing copyright concerns, as such models are often trained on vast corpora of artistic and commercial works. A central question is whether one can reliably verify if an artist's material was included in training, thereby providing a means for copyright holders to protect their content. In this work, we investigate the feasibility of such verification through membership inference attacks (MIA) on open-source generative audio models, which attempt to determine whether a specific audio sample was part of the training set. Our empirical results show that membership inference alone is of limited effectiveness at scale, as the per-sample membership signal is weak for models trained on large and diverse datasets. However, artists and media owners typically hold collections of works rather than isolated samples. Building on prior work in text and vision domains, in this work we focus on dataset inference (DI), which aggregates diverse membership evidence across multiple samples. We find that DI is successful in the audio domain, offering a more practical mechanism for assessing whether an artist's works contributed to model training. Our results suggest DI as a promising direction for copyright protection and dataset accountability in the era of large audio generative models.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09654v1",
        "pdf": "https://arxiv.org/pdf/2512.09654v1"
      },
      "arxiv_id": "2512.09654v1",
      "comment": "NeurIPS 2025 AI for Music Workshop NeurIPS 2025 Workshop on Creativity & Generative AI",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09646v1",
      "title": "VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification",
      "authors": [
        "Wanyue Zhang",
        "Lin Geng Foo",
        "Thabo Beeler",
        "Rishabh Dabral",
        "Christian Theobalt"
      ],
      "abstract": "Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09646v1",
        "pdf": "https://arxiv.org/pdf/2512.09646v1"
      },
      "arxiv_id": "2512.09646v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09644v1",
      "title": "Kaapana: A Comprehensive Open-Source Platform for Integrating AI in Medical Imaging Research Environments",
      "authors": [
        "Ünal Akünal",
        "Markus Bujotzek",
        "Stefan Denner",
        "Benjamin Hamm",
        "Klaus Kades",
        "Philipp Schader",
        "Jonas Scherer",
        "Marco Nolden",
        "Peter Neher",
        "Ralf Floca",
        "Klaus Maier-Hein"
      ],
      "abstract": "Developing generalizable AI for medical imaging requires both access to large, multi-center datasets and standardized, reproducible tooling within research environments. However, leveraging real-world imaging data in clinical research environments is still hampered by strict regulatory constraints, fragmented software infrastructure, and the challenges inherent in conducting large-cohort multicentre studies. This leads to projects that rely on ad-hoc toolchains that are hard to reproduce, difficult to scale beyond single institutions and poorly suited for collaboration between clinicians and data scientists. We present Kaapana, a comprehensive open-source platform for medical imaging research that is designed to bridge this gap. Rather than building single-use, site-specific tooling, Kaapana provides a modular, extensible framework that unifies data ingestion, cohort curation, processing workflows and result inspection under a common user interface. By bringing the algorithm to the data, it enables institutions to keep control over their sensitive data while still participating in distributed experimentation and model development. By integrating flexible workflow orchestration with user-facing applications for researchers, Kaapana reduces technical overhead, improves reproducibility and enables conducting large-scale, collaborative, multi-centre imaging studies. We describe the core concepts of the platform and illustrate how they can support diverse use cases, from local prototyping to nation-wide research networks. The open-source codebase is available at https://github.com/kaapana/kaapana",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09644v1",
        "pdf": "https://arxiv.org/pdf/2512.09644v1"
      },
      "arxiv_id": "2512.09644v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09633v1",
      "title": "Benchmarking SAM2-based Trackers on FMOX",
      "authors": [
        "Senem Aktas",
        "Charles Markham",
        "John McDonald",
        "Rozenn Dahyot"
      ],
      "abstract": "Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09633v1",
        "pdf": "https://arxiv.org/pdf/2512.09633v1"
      },
      "arxiv_id": "2512.09633v1",
      "comment": "",
      "journal_ref": "33rd International Conference on Artificial Intelligence and Cognitive Science (AICS 2025), December, 2025, Dublin, Ireland",
      "has_code": false
    },
    {
      "id": "2512.09629v1",
      "title": "An End-to-end Planning Framework with Agentic LLMs and PDDL",
      "authors": [
        "Emanuele La Malfa",
        "Ping Zhu",
        "Samuele Marro",
        "Sara Bernardini",
        "Michael Wooldridge"
      ],
      "abstract": "We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09629v1",
        "pdf": "https://arxiv.org/pdf/2512.09629v1"
      },
      "arxiv_id": "2512.09629v1",
      "comment": "Code: https://github.com/EmanueleLM/MultiAgentPlanning",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.09626v1",
      "title": "Beyond Sequences: A Benchmark for Atomic Hand-Object Interaction Using a Static RNN Encoder",
      "authors": [
        "Yousef Azizi Movahed",
        "Fatemeh Ziaeetabar"
      ],
      "abstract": "Reliably predicting human intent in hand-object interactions is an open challenge for computer vision. Our research concentrates on a fundamental sub-problem: the fine-grained classification of atomic interaction states, namely 'approaching', 'grabbing', and 'holding'. To this end, we introduce a structured data engineering process that converts raw videos from the MANIAC dataset into 27,476 statistical-kinematic feature vectors. Each vector encapsulates relational and dynamic properties from a short temporal window of motion. Our initial hypothesis posited that sequential modeling would be critical, leading us to compare static classifiers (MLPs) against temporal models (RNNs). Counter-intuitively, the key discovery occurred when we set the sequence length of a Bidirectional RNN to one (seq_length=1). This modification converted the network's function, compelling it to act as a high-capacity static feature encoder. This architectural change directly led to a significant accuracy improvement, culminating in a final score of 97.60%. Of particular note, our optimized model successfully overcame the most challenging transitional class, 'grabbing', by achieving a balanced F1-score of 0.90. These findings provide a new benchmark for low-level hand-object interaction recognition using structured, interpretable features and lightweight architectures.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09626v1",
        "pdf": "https://arxiv.org/pdf/2512.09626v1"
      },
      "arxiv_id": "2512.09626v1",
      "comment": "Code available at: https://github.com/YousefAMovahed/beyond-sequences-hoi-benchmark",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.09621v1",
      "title": "Semantic-Aware Cooperative Communication and Computation Framework in Vehicular Networks",
      "authors": [
        "Jingbo Zhang",
        "Maoxin Ji",
        "Qiong Wu",
        "Pingyi Fan",
        "Kezhi Wang",
        "Wen Chen"
      ],
      "abstract": "Semantic Communication (SC) combined with Vehicular edge computing (VEC) provides an efficient edge task processing paradigm for Internet of Vehicles (IoV). Focusing on highway scenarios, this paper proposes a Tripartite Cooperative Semantic Communication (TCSC) framework, which enables Vehicle Users (VUs) to perform semantic task offloading via Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communications. Considering task latency and the number of semantic symbols, the framework constructs a Mixed-Integer Nonlinear Programming (MINLP) problem, which is transformed into two subproblems. First, we innovatively propose a multi-agent proximal policy optimization task offloading optimization method based on parametric distribution noise (MAPPO-PDN) to solve the optimization problem of the number of semantic symbols; second, linear programming (LP) is used to solve offloading ratio. Simulations show that performance of this scheme is superior to that of other algorithms.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09621v1",
        "pdf": "https://arxiv.org/pdf/2512.09621v1"
      },
      "arxiv_id": "2512.09621v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09617v1",
      "title": "FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation",
      "authors": [
        "Hubert Kompanowski",
        "Varun Jampani",
        "Aaryaman Vasishta",
        "Binh-Son Hua"
      ],
      "abstract": "Multiview diffusion models have rapidly emerged as a powerful tool for content creation with spatial consistency across viewpoints, offering rich visual realism without requiring explicit geometry and appearance representation. However, compared to meshes or radiance fields, existing multiview diffusion models offer limited appearance manipulation, particularly in terms of material, texture, or style.\n  In this paper, we present a lightweight adaptation technique for appearance transfer in multiview diffusion models. Our method learns to combine object identity from an input image with appearance cues rendered in a separate reference image, producing multi-view-consistent output that reflects the desired materials, textures, or styles. This allows explicit specification of appearance parameters at generation time while preserving the underlying object geometry and view coherence. We leverage three diffusion denoising processes responsible for generating the original object, the reference, and the target images, and perform reverse sampling to aggregate a small subset of layer-wise self-attention features from the object and the reference to influence the target generation. Our method requires only a few training examples to introduce appearance awareness to pretrained multiview models. The experiments show that our method provides a simple yet effective way toward multiview generation with diverse appearance, advocating the adoption of implicit generative 3D representations in practice.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09617v1",
        "pdf": "https://arxiv.org/pdf/2512.09617v1"
      },
      "arxiv_id": "2512.09617v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09616v1",
      "title": "Rethinking Chain-of-Thought Reasoning for Videos",
      "authors": [
        "Yiwu Zhong",
        "Zi-Yuan Hu",
        "Yin Li",
        "Liwei Wang"
      ],
      "abstract": "Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09616v1",
        "pdf": "https://arxiv.org/pdf/2512.09616v1"
      },
      "arxiv_id": "2512.09616v1",
      "comment": "Technical report",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09610v1",
      "title": "ImageTalk: Designing a Multimodal AAC Text Generation System Driven by Image Recognition and Natural Language Generation",
      "authors": [
        "Boyin Yang",
        "Puming Jiang",
        "Per Ola Kristensson"
      ],
      "abstract": "People living with Motor Neuron Disease (plwMND) frequently encounter speech and motor impairments that necessitate a reliance on augmentative and alternative communication (AAC) systems. This paper tackles the main challenge that traditional symbol-based AAC systems offer a limited vocabulary, while text entry solutions tend to exhibit low communication rates. To help plwMND articulate their needs about the system efficiently and effectively, we iteratively design and develop a novel multimodal text generation system called ImageTalk through a tailored proxy-user-based and an end-user-based design phase. The system demonstrates pronounced keystroke savings of 95.6%, coupled with consistent performance and high user satisfaction. We distill three design guidelines for AI-assisted text generation systems design and outline four user requirement levels tailored for AAC purposes, guiding future research in this field.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09610v1",
        "pdf": "https://arxiv.org/pdf/2512.09610v1"
      },
      "arxiv_id": "2512.09610v1",
      "comment": "24 pages, 10 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09607v1",
      "title": "UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories",
      "authors": [
        "Yanghong Mei",
        "Yirong Yang",
        "Longteng Guo",
        "Qunbo Wang",
        "Ming-Ming Yu",
        "Xingjian He",
        "Wenjun Wu",
        "Jing Liu"
      ],
      "abstract": "Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09607v1",
        "pdf": "https://arxiv.org/pdf/2512.09607v1"
      },
      "arxiv_id": "2512.09607v1",
      "comment": "9 pages, 5 figures, accepted to AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09592v1",
      "title": "CS3D: An Efficient Facial Expression Recognition via Event Vision",
      "authors": [
        "Zhe Wang",
        "Qijin Song",
        "Yucen Peng",
        "Weibang Bai"
      ],
      "abstract": "Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\\% of the original C3D required on the same device.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09592v1",
        "pdf": "https://arxiv.org/pdf/2512.09592v1"
      },
      "arxiv_id": "2512.09592v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09591v1",
      "title": "Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models",
      "authors": [
        "Magnus Ruud Kjaer",
        "Rahul Thapa",
        "Gauri Ganjoo",
        "Hyatt Moore",
        "Poul Joergen Jennum",
        "Brandon M. Westover",
        "James Zou",
        "Emmanuel Mignot",
        "Bryan He",
        "Andreas Brink-Kjaer"
      ],
      "abstract": "Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09591v1",
        "pdf": "https://arxiv.org/pdf/2512.09591v1"
      },
      "arxiv_id": "2512.09591v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09586v1",
      "title": "Graph-Based Bayesian Optimization for Quantum Circuit Architecture Search with Uncertainty Calibrated Surrogates",
      "authors": [
        "Prashant Kumar Choudhary",
        "Nouhaila Innan",
        "Muhammad Shafique",
        "Rajeev Singh"
      ],
      "abstract": "Quantum circuit design is a key bottleneck for practical quantum machine learning on complex, real-world data. We present an automated framework that discovers and refines variational quantum circuits (VQCs) using graph-based Bayesian optimization with a graph neural network (GNN) surrogate. Circuits are represented as graphs and mutated and selected via an expected improvement acquisition function informed by surrogate uncertainty with Monte Carlo dropout. Candidate circuits are evaluated with a hybrid quantum-classical variational classifier on the next generation firewall telemetry and network internet of things (NF-ToN-IoT-V2) cybersecurity dataset, after feature selection and scaling for quantum embedding. We benchmark our pipeline against an MLP-based surrogate, random search, and greedy GNN selection. The GNN-guided optimizer consistently finds circuits with lower complexity and competitive or superior classification accuracy compared to all baselines. Robustness is assessed via a noise study across standard quantum noise channels, including amplitude damping, phase damping, thermal relaxation, depolarizing, and readout bit flip noise. The implementation is fully reproducible, with time benchmarking and export of best found circuits, providing a scalable and interpretable route to automated quantum circuit discovery.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "cs.NI"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09586v1",
        "pdf": "https://arxiv.org/pdf/2512.09586v1"
      },
      "arxiv_id": "2512.09586v1",
      "comment": "17 pages, 13 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09583v1",
      "title": "UnReflectAnything: RGB-Only Highlight Removal by Rendering Synthetic Specular Supervision",
      "authors": [
        "Alberto Rota",
        "Mert Kiray",
        "Mert Asim Karaoglu",
        "Patrick Ruhkamp",
        "Elena De Momi",
        "Nassir Navabm",
        "Benjamin Busam"
      ],
      "abstract": "Specular highlights distort appearance, obscure texture, and hinder geometric reasoning in both natural and surgical imagery. We present UnReflectAnything, an RGB-only framework that removes highlights from a single image by predicting a highlight map together with a reflection-free diffuse reconstruction. The model uses a frozen vision transformer encoder to extract multi-scale features, a lightweight head to localize specular regions, and a token-level inpainting module that restores corrupted feature patches before producing the final diffuse image. To overcome the lack of paired supervision, we introduce a Virtual Highlight Synthesis pipeline that renders physically plausible specularities using monocular geometry, Fresnel-aware shading, and randomized lighting which enables training on arbitrary RGB images with correct geometric structure. UnReflectAnything generalizes across natural and surgical domains where non-Lambertian surfaces and non-uniform lighting create severe highlights and it achieves competitive performance with state-of-the-art results on several benchmarks. Project Page: https://alberto-rota.github.io/UnReflectAnything/",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09583v1",
        "pdf": "https://arxiv.org/pdf/2512.09583v1"
      },
      "arxiv_id": "2512.09583v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09580v1",
      "title": "Content-Adaptive Image Retouching Guided by Attribute-Based Text Representation",
      "authors": [
        "Hancheng Zhu",
        "Xinyu Liu",
        "Rui Yao",
        "Kunyang Sun",
        "Leida Li",
        "Abdulmotaleb El Saddik"
      ],
      "abstract": "Image retouching has received significant attention due to its ability to achieve high-quality visual content. Existing approaches mainly rely on uniform pixel-wise color mapping across entire images, neglecting the inherent color variations induced by image content. This limitation hinders existing approaches from achieving adaptive retouching that accommodates both diverse color distributions and user-defined style preferences. To address these challenges, we propose a novel Content-Adaptive image retouching method guided by Attribute-based Text Representation (CA-ATP). Specifically, we propose a content-adaptive curve mapping module, which leverages a series of basis curves to establish multiple color mapping relationships and learns the corresponding weight maps, enabling content-aware color adjustments. The proposed module can capture color diversity within the image content, allowing similar color values to receive distinct transformations based on their spatial context. In addition, we propose an attribute text prediction module that generates text representations from multiple image attributes, which explicitly represent user-defined style preferences. These attribute-based text representations are subsequently integrated with visual features via a multimodal model, providing user-friendly guidance for image retouching. Extensive experiments on several public datasets demonstrate that our method achieves state-of-the-art performance.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09580v1",
        "pdf": "https://arxiv.org/pdf/2512.09580v1"
      },
      "arxiv_id": "2512.09580v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09579v1",
      "title": "Hands-on Evaluation of Visual Transformers for Object Recognition and Detection",
      "authors": [
        "Dimitrios N. Vlachogiannis",
        "Dimitrios A. Koutsomitropoulos"
      ],
      "abstract": "Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09579v1",
        "pdf": "https://arxiv.org/pdf/2512.09579v1"
      },
      "arxiv_id": "2512.09579v1",
      "comment": "",
      "journal_ref": "37th International Conference on Tools with Artificial Intelligence (ICTAI 2025)",
      "has_code": false
    },
    {
      "id": "2512.09577v1",
      "title": "Auto-BenchmarkCard: Automated Synthesis of Benchmark Documentation",
      "authors": [
        "Aris Hofmann",
        "Inge Vejsbjerg",
        "Dhaval Salwala",
        "Elizabeth M. Daly"
      ],
      "abstract": "We present Auto-BenchmarkCard, a workflow for generating validated descriptions of AI benchmarks. Benchmark documentation is often incomplete or inconsistent, making it difficult to interpret and compare benchmarks across tasks or domains. Auto-BenchmarkCard addresses this gap by combining multi-agent data extraction from heterogeneous sources (e.g., Hugging Face, Unitxt, academic papers) with LLM-driven synthesis. A validation phase evaluates factual accuracy through atomic entailment scoring using the FactReasoner tool. This workflow has the potential to promote transparency, comparability, and reusability in AI benchmark reporting, enabling researchers and practitioners to better navigate and evaluate benchmark choices.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09577v1",
        "pdf": "https://arxiv.org/pdf/2512.09577v1"
      },
      "arxiv_id": "2512.09577v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09576v1",
      "title": "Seeing Soil from Space: Towards Robust and Scalable Remote Soil Nutrient Analysis",
      "authors": [
        "David Seu",
        "Nicolas Longepe",
        "Gabriel Cioltea",
        "Erik Maidik",
        "Calin Andrei"
      ],
      "abstract": "Environmental variables are increasingly affecting agricultural decision-making, yet accessible and scalable tools for soil assessment remain limited. This study presents a robust and scalable modeling system for estimating soil properties in croplands, including soil organic carbon (SOC), total nitrogen (N), available phosphorus (P), exchangeable potassium (K), and pH, using remote sensing data and environmental covariates. The system employs a hybrid modeling approach, combining the indirect methods of modeling soil through proxies and drivers with direct spectral modeling. We extend current approaches by using interpretable physics-informed covariates derived from radiative transfer models (RTMs) and complex, nonlinear embeddings from a foundation model. We validate the system on a harmonized dataset that covers Europes cropland soils across diverse pedoclimatic zones. Evaluation is conducted under a robust validation framework that enforces strict spatial blocking, stratified splits, and statistically distinct train-test sets, which deliberately make the evaluation harder and produce more realistic error estimates for unseen regions. The models achieved their highest accuracy for SOC and N. This performance held across unseen locations, under both spatial cross-validation and an independent test set. SOC obtained a MAE of 5.12 g/kg and a CCC of 0.77, and N obtained a MAE of 0.44 g/kg and a CCC of 0.77. We also assess uncertainty through conformal calibration, achieving 90 percent coverage at the target confidence level. This study contributes to the digital advancement of agriculture through the application of scalable, data-driven soil analysis frameworks that can be extended to related domains requiring quantitative soil evaluation, such as carbon markets.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV",
        "physics.geo-ph"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09576v1",
        "pdf": "https://arxiv.org/pdf/2512.09576v1"
      },
      "arxiv_id": "2512.09576v1",
      "comment": "23 pages, 13 figures, 13 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09573v1",
      "title": "Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment",
      "authors": [
        "Yuan Li",
        "Zitang Sun",
        "Yen-Ju Chen",
        "Shin'ya Nishida"
      ],
      "abstract": "Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09573v1",
        "pdf": "https://arxiv.org/pdf/2512.09573v1"
      },
      "arxiv_id": "2512.09573v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09572v1",
      "title": "Lazy Diffusion: Mitigating spectral collapse in generative diffusion-based stable autoregressive emulation of turbulent flows",
      "authors": [
        "Anish Sambamurthy",
        "Ashesh Chattopadhyay"
      ],
      "abstract": "Turbulent flows posses broadband, power-law spectra in which multiscale interactions couple high-wavenumber fluctuations to large-scale dynamics. Although diffusion-based generative models offer a principled probabilistic forecasting framework, we show that standard DDPMs induce a fundamental \\emph{spectral collapse}: a Fourier-space analysis of the forward SDE reveals a closed-form, mode-wise signal-to-noise ratio (SNR) that decays monotonically in wavenumber, $|k|$ for spectra $S(k)\\!\\propto\\!|k|^{-λ}$, rendering high-wavenumber modes indistinguishable from noise and producing an intrinsic spectral bias. We reinterpret the noise schedule as a spectral regularizer and introduce power-law schedules $β(τ)\\!\\propto\\!τ^γ$ that preserve fine-scale structure deeper into diffusion time, along with \\emph{Lazy Diffusion}, a one-step distillation method that leverages the learned score geometry to bypass long reverse-time trajectories and prevent high-$k$ degradation. Applied to high-Reynolds-number 2D Kolmogorov turbulence and $1/12^\\circ$ Gulf of Mexico ocean reanalysis, these methods resolve spectral collapse, stabilize long-horizon autoregression, and restore physically realistic inertial-range scaling. Together, they show that naïve Gaussian scheduling is structurally incompatible with power-law physics and that physics-aware diffusion processes can yield accurate, efficient, and fully probabilistic surrogates for multiscale dynamical systems.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "physics.flu-dyn",
        "cs.AI",
        "math.DS",
        "nlin.CD",
        "physics.ao-ph"
      ],
      "primary_category": "physics.flu-dyn",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09572v1",
        "pdf": "https://arxiv.org/pdf/2512.09572v1"
      },
      "arxiv_id": "2512.09572v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09570v1",
      "title": "The Gender Code: Gendering the Global Governance of Artificial Intelligence",
      "authors": [
        "Jelena Cupac"
      ],
      "abstract": "This paper examines how international AI governance frameworks address gender issues and gender-based harms. The analysis covers binding regulations, such as the EU AI Act; soft law instruments, like the UNESCO Recommendations on AI Ethics; and global initiatives, such as the Global Partnership on AI (GPAI). These instruments reveal emerging trends, including the integration of gender concerns into broader human rights frameworks, a shift toward explicit gender-related provisions, and a growing emphasis on inclusivity and diversity. Yet, some critical gaps persist, including inconsistent treatment of gender across governance documents, limited engagement with intersectionality, and a lack of robust enforcement mechanisms. However, this paper argues that effective AI governance must be intersectional, enforceable, and inclusive. This is key to moving beyond tokenism toward meaningful equity and preventing reinforcement of existing inequalities. The study contributes to ethical AI debates by highlighting the importance of gender-sensitive governance in building a just technological future.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09570v1",
        "pdf": "https://arxiv.org/pdf/2512.09570v1"
      },
      "arxiv_id": "2512.09570v1",
      "comment": "The paper is part of the Handbook on the Global Governance of Artificial Intelligence, forthcoming with Edward Elgar Publishing",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09566v1",
      "title": "Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search",
      "authors": [
        "Junkai Ji",
        "Zhangfan Yang",
        "Dong Xu",
        "Ruibin Bai",
        "Jianqiang Li",
        "Tingjun Hou",
        "Zexuan Zhu"
      ],
      "abstract": "Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09566v1",
        "pdf": "https://arxiv.org/pdf/2512.09566v1"
      },
      "arxiv_id": "2512.09566v1",
      "comment": "21 pages, 5 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09565v1",
      "title": "From Graphs to Gates: DNS-HyXNet, A Lightweight and Deployable Sequential Model for Real-Time DNS Tunnel Detection",
      "authors": [
        "Faraz Ali",
        "Muhammad Afaq",
        "Mahmood Niazi",
        "Muzammil Behzad"
      ],
      "abstract": "Domain Name System (DNS) tunneling remains a covert channel for data exfiltration and command-and-control communication. Although graph-based methods such as GraphTunnel achieve strong accuracy, they introduce significant latency and computational overhead due to recursive parsing and graph construction, limiting their suitability for real-time deployment. This work presents DNS-HyXNet, a lightweight extended Long Short-Term Memory (xLSTM) hybrid framework designed for efficient sequence-based DNS tunnel detection. DNS-HyXNet integrates tokenized domain embeddings with normalized numerical DNS features and processes them through a two-layer xLSTM network that directly learns temporal dependencies from packet sequences, eliminating the need for graph reconstruction and enabling single-stage multi-class classification. The model was trained and evaluated on two public benchmark datasets with carefully tuned hyperparameters to ensure low memory consumption and fast inference. Across all experimental splits of the DNS-Tunnel-Datasets, DNS-HyXNet achieved up to 99.99% accuracy, with macro-averaged precision, recall, and F1-scores exceeding 99.96%, and demonstrated a per-sample detection latency of just 0.041 ms, confirming its scalability and real-time readiness. These results show that sequential modeling with xLSTM can effectively replace computationally expensive recursive graph generation, offering a deployable and energy-efficient alternative for real-time DNS tunnel detection on commodity hardware.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09565v1",
        "pdf": "https://arxiv.org/pdf/2512.09565v1"
      },
      "arxiv_id": "2512.09565v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09563v1",
      "title": "System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection",
      "authors": [
        "Binglin Wu",
        "Jiaxiu Zou",
        "Xianneng Li"
      ],
      "abstract": "The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09563v1",
        "pdf": "https://arxiv.org/pdf/2512.09563v1"
      },
      "arxiv_id": "2512.09563v1",
      "comment": "Accepted at CCL 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09555v1",
      "title": "Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment",
      "authors": [
        "Yuan Li",
        "Zitang Sun",
        "Yen-ju Chen",
        "Shin'ya Nishida"
      ],
      "abstract": "Recent progress in BIQA has been driven by VLMs, whose semantic reasoning abilities suggest that they might extract visual features, generate descriptive text, and infer quality in a human-like manner. However, these models often produce textual descriptions that contradict their final quality predictions, and the predicted scores can change unstably during inference - behaviors not aligned with human reasoning. To understand these issues, we analyze the factors that cause contradictory assessments and instability. We first estimate the relationship between the final quality predictions and the generated visual features, finding that the predictions are not fully grounded in the features and that the logical connection between them is weak. Moreover, decoding intermediate VLM layers shows that the model frequently relies on a limited set of candidate tokens, which contributes to prediction instability. To encourage more human-like reasoning, we introduce a two-stage tuning method that explicitly separates visual perception from quality inference. In the first stage, the model learns visual features; in the second, it infers quality solely from these features. Experiments on SPAQ and KONIQ demonstrate that our approach reduces prediction instability from 22.00% to 12.39% and achieves average gains of 0.3124/0.3507 in SRCC/PLCC across LIVE, CSIQ, SPAQ, and KONIQ compared to the baseline. Further analyses show that our method improves both stability and the reliability of the inference process.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09555v1",
        "pdf": "https://arxiv.org/pdf/2512.09555v1"
      },
      "arxiv_id": "2512.09555v1",
      "comment": "Accepted to the ICONIP (International Conference on Neural Information Processing), 2025",
      "journal_ref": "Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment. In: Taniguchi, T., et al. Neural Information Processing. ICONIP 2025. Lecture Notes in Computer Science, vol 16310. Springer, Singapore",
      "has_code": false
    },
    {
      "id": "2512.09546v1",
      "title": "A Dual-Domain Convolutional Network for Hyperspectral Single-Image Super-Resolution",
      "authors": [
        "Murat Karayaka",
        "Usman Muhammad",
        "Jorma Laaksonen",
        "Md Ziaul Hoque",
        "Tapio Seppänen"
      ],
      "abstract": "This study presents a lightweight dual-domain super-resolution network (DDSRNet) that combines Spatial-Net with the discrete wavelet transform (DWT). Specifically, our proposed model comprises three main components: (1) a shallow feature extraction module, termed Spatial-Net, which performs residual learning and bilinear interpolation; (2) a low-frequency enhancement branch based on the DWT that refines coarse image structures; and (3) a shared high-frequency refinement branch that simultaneously enhances the LH (horizontal), HL (vertical), and HH (diagonal) wavelet subbands using a single CNN with shared weights. As a result, the DWT enables subband decomposition, while the inverse DWT reconstructs the final high-resolution output. By doing so, the integration of spatial- and frequency-domain learning enables DDSRNet to achieve highly competitive performance with low computational cost on three hyperspectral image datasets, demonstrating its effectiveness for hyperspectral image super-resolution.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09546v1",
        "pdf": "https://arxiv.org/pdf/2512.09546v1"
      },
      "arxiv_id": "2512.09546v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.09543v1",
      "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs",
      "authors": [
        "Arihant Tripathy",
        "Ch Pavan Harshit",
        "Karthik Vaidhyanathan"
      ],
      "abstract": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.\n  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.\n  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.\n  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.\n  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.",
      "published": "2025-12-10",
      "updated": "2025-12-10",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.09543v1",
        "pdf": "https://arxiv.org/pdf/2512.09543v1"
      },
      "arxiv_id": "2512.09543v1",
      "comment": "8 pages, 5 figures, 1 table. Accepted to AGENT 2026 (ICSE 2026 workshop)",
      "journal_ref": "",
      "has_code": false
    }
  ]
}