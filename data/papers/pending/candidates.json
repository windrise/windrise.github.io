{
  "fetched_at": "2025-12-16T00:27:01.029527",
  "total_papers": 100,
  "papers": [
    {
      "id": "2512.11800v1",
      "title": "Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance",
      "authors": [
        "Jan U. Müller",
        "Robin Tim Landsgesell",
        "Leif Van Holland",
        "Patrick Stotko",
        "Reinhard Klein"
      ],
      "abstract": "The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11800v1",
        "pdf": "https://arxiv.org/pdf/2512.11800v1"
      },
      "arxiv_id": "2512.11800v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11799v1",
      "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
      "authors": [
        "Ye Fang",
        "Tong Wu",
        "Valentin Deschaintre",
        "Duygu Ceylan",
        "Iliyan Georgiev",
        "Chun-Hao Paul Huang",
        "Yiwei Hu",
        "Xuelin Chen",
        "Tuanfeng Yang Wang"
      ],
      "abstract": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11799v1",
        "pdf": "https://arxiv.org/pdf/2512.11799v1"
      },
      "arxiv_id": "2512.11799v1",
      "comment": "Project Page: https://aleafy.github.io/vrgbx",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.11798v1",
      "title": "Particulate: Feed-Forward 3D Object Articulation",
      "authors": [
        "Ruining Li",
        "Yuxin Yao",
        "Chuanxia Zheng",
        "Christian Rupprecht",
        "Joan Lasenby",
        "Shangzhe Wu",
        "Andrea Vedaldi"
      ],
      "abstract": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11798v1",
        "pdf": "https://arxiv.org/pdf/2512.11798v1"
      },
      "arxiv_id": "2512.11798v1",
      "comment": "Project page: https://ruiningli.com/particulate",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11797v1",
      "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis",
      "authors": [
        "Junjie Ye",
        "Rong Xue",
        "Basile Van Hoorick",
        "Pavel Tokmakov",
        "Muhammad Zubair Irshad",
        "Yue Wang",
        "Vitor Guizilini"
      ],
      "abstract": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11797v1",
        "pdf": "https://arxiv.org/pdf/2512.11797v1"
      },
      "arxiv_id": "2512.11797v1",
      "comment": "Project page: https://jay-ye.github.io/AnchorDream/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.11793v1",
      "title": "A General Algorithm for Detecting Higher-Order Interactions via Random Sequential Additions",
      "authors": [
        "Ahmad Shamail",
        "Claire McWhite"
      ],
      "abstract": "Many systems exhibit complex interactions between their components: some features or actions amplify each other's effects, others provide redundant information, and some contribute independently. We present a simple geometric method for discovering interactions and redundancies: when elements are added in random sequential orders and their contributions plotted over many trials, characteristic L-shaped patterns emerge that directly reflect interaction structure. The approach quantifies how the contribution of each element depends on those added before it, revealing patterns that distinguish interaction, independence, and redundancy on a unified scale. When pairwise contributions are visualized as two--dimensional point clouds, redundant pairs form L--shaped patterns where only the first-added element contributes, while synergistic pairs form L--shaped patterns where only elements contribute together. Independent elements show order--invariant distributions. We formalize this with the L--score, a continuous measure ranging from $-1$ (perfect synergy, e.g. $Y=X_1X_2$) to $0$ (independence) to $+1$ (perfect redundancy, $X_1 \\approx X_2$). The relative scaling of the L--shaped arms reveals feature dominance in which element consistently provides more information. Although computed only from pairwise measurements, higher--order interactions among three or more elements emerge naturally through consistent cross--pair relationships (e.g. AB, AC, BC). The method is metric--agnostic and broadly applicable to any domain where performance can be evaluated incrementally over non-repeating element sequences, providing a unified geometric approach to uncovering interaction structure.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11793v1",
        "pdf": "https://arxiv.org/pdf/2512.11793v1"
      },
      "arxiv_id": "2512.11793v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11792v1",
      "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
      "authors": [
        "Yang Fei",
        "George Stoica",
        "Jingyuan Liu",
        "Qifeng Chen",
        "Ranjay Krishna",
        "Xiaojuan Wang",
        "Benlin Liu"
      ],
      "abstract": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11792v1",
        "pdf": "https://arxiv.org/pdf/2512.11792v1"
      },
      "arxiv_id": "2512.11792v1",
      "comment": "Project Website: https://sam2videox.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.11791v1",
      "title": "Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs",
      "authors": [
        "Wentao Jiang",
        "Vamsi Varra",
        "Caitlin Perez-Stable",
        "Harrison Zhu",
        "Meredith Apicella",
        "Nicole Nyamongo"
      ],
      "abstract": "Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Frequency Spectral Gating (HFSG) module and stem-skip connections to capture subtle textures; and (3) a clinical trust mechanism employing K-fold ensemble and Test-Time Augmentation (TTA) to generate pixel-wise uncertainty maps. Extensive validation on an expert-annotated clinical cohort demonstrates superior performance, achieving a Dice score of 85.05% and significantly reducing boundary error (95% Hausdorff Distance improved from 44.79 px to 29.95 px), consistently outperforming strong CNN (ResNet-50 and UNet++) and Transformer (MiT-B5) baselines. Notably, our framework demonstrates high reliability with zero catastrophic failures and provides interpretable entropy maps to identify ambiguous regions for clinician review. Our approach suggests that the proposed framework establishes a robust and reliable standard for automated vitiligo assessment.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11791v1",
        "pdf": "https://arxiv.org/pdf/2512.11791v1"
      },
      "arxiv_id": "2512.11791v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11784v1",
      "title": "Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective",
      "authors": [
        "Etienne Boursier",
        "Claire Boyer"
      ],
      "abstract": "Softmax attention is a central component of transformer architectures, yet its nonlinear structure poses significant challenges for theoretical analysis. We develop a unified, measure-based framework for studying single-layer softmax attention under both finite and infinite prompts. For i.i.d. Gaussian inputs, we lean on the fact that the softmax operator converges in the infinite-prompt limit to a linear operator acting on the underlying input-token measure. Building on this insight, we establish non-asymptotic concentration bounds for the output and gradient of softmax attention, quantifying how rapidly the finite-prompt model approaches its infinite-prompt counterpart, and prove that this concentration remains stable along the entire training trajectory in general in-context learning settings with sub-Gaussian tokens. In the case of in-context linear regression, we use the tractable infinite-prompt dynamics to analyze training at finite prompt length. Our results allow optimization analyses developed for linear attention to transfer directly to softmax attention when prompts are sufficiently long, showing that large-prompt softmax attention inherits the analytical structure of its linear counterpart. This, in turn, provides a principled and broadly applicable toolkit for studying the training dynamics and statistical behavior of softmax attention layers in large prompt regimes.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11784v1",
        "pdf": "https://arxiv.org/pdf/2512.11784v1"
      },
      "arxiv_id": "2512.11784v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11783v1",
      "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously",
      "authors": [
        "Andrew Adiletta",
        "Kathryn Adiletta",
        "Kemal Derya",
        "Berk Sunar"
      ],
      "abstract": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.\n  Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11783v1",
        "pdf": "https://arxiv.org/pdf/2512.11783v1"
      },
      "arxiv_id": "2512.11783v1",
      "comment": "13 pages, 5 Figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11782v1",
      "title": "MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator",
      "authors": [
        "Peiqing Yang",
        "Shangchen Zhou",
        "Kai Hao",
        "Qingyi Tao"
      ],
      "abstract": "Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11782v1",
        "pdf": "https://arxiv.org/pdf/2512.11782v1"
      },
      "arxiv_id": "2512.11782v1",
      "comment": "Project page: https://pq-yang.github.io/projects/MatAnyone2/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.11781v1",
      "title": "Agile Flight Emerges from Multi-Agent Competitive Racing",
      "authors": [
        "Vineet Pasumarti",
        "Lorenzo Bianchi",
        "Antonio Loquercio"
      ],
      "abstract": "Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.\n  Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11781v1",
        "pdf": "https://arxiv.org/pdf/2512.11781v1"
      },
      "arxiv_id": "2512.11781v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11779v1",
      "title": "Conditional Coverage Diagnostics for Conformal Prediction",
      "authors": [
        "Sacha Braun",
        "David Holzmüller",
        "Michael I. Jordan",
        "Francis Bach"
      ],
      "abstract": "Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11779v1",
        "pdf": "https://arxiv.org/pdf/2512.11779v1"
      },
      "arxiv_id": "2512.11779v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11776v1",
      "title": "The Adaptive Vekua Cascade: A Differentiable Spectral-Analytic Solver for Physics-Informed Representation",
      "authors": [
        "Vladimer Khasia"
      ],
      "abstract": "Coordinate-based neural networks have emerged as a powerful tool for representing continuous physical fields, yet they face two fundamental pathologies: spectral bias, which hinders the learning of high-frequency dynamics, and the curse of dimensionality, which causes parameter explosion in discrete feature grids. We propose the Adaptive Vekua Cascade (AVC), a hybrid architecture that bridges deep learning and classical approximation theory. AVC decouples manifold learning from function approximation by using a deep network to learn a diffeomorphic warping of the physical domain, projecting complex spatiotemporal dynamics onto a latent manifold where the solution is represented by a basis of generalized analytic functions. Crucially, we replace the standard gradient-descent output layer with a differentiable linear solver, allowing the network to optimally resolve spectral coefficients in a closed form during the forward pass. We evaluate AVC on a suite of five rigorous physics benchmarks, including high-frequency Helmholtz wave propagation, sparse medical reconstruction, and unsteady 3D Navier-Stokes turbulence. Our results demonstrate that AVC achieves state-of-the-art accuracy while reducing parameter counts by orders of magnitude (e.g., 840 parameters vs. 4.2 million for 3D grids) and converging 2-3x faster than implicit neural representations. This work establishes a new paradigm for memory-efficient, spectrally accurate scientific machine learning. The code is available at https://github.com/VladimerKhasia/vecua.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11776v1",
        "pdf": "https://arxiv.org/pdf/2512.11776v1"
      },
      "arxiv_id": "2512.11776v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11771v1",
      "title": "Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints",
      "authors": [
        "Kai Yao",
        "Marc Juarez"
      ],
      "abstract": "Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11771v1",
        "pdf": "https://arxiv.org/pdf/2512.11771v1"
      },
      "arxiv_id": "2512.11771v1",
      "comment": "This work has been accepted for publication in the 4th IEEE Conference on Secure and Trustworthy Machine Learning (IEEE SaTML 2026). The final version will be available on IEEE Xplore",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11767v1",
      "title": "Learning Minimal Representations of Fermionic Ground States",
      "authors": [
        "Felix Frohnert",
        "Emiel Koridon",
        "Stefano Polla"
      ],
      "abstract": "We introduce an unsupervised machine-learning framework that discovers optimally compressed representations of quantum many-body ground states. Using an autoencoder neural network architecture on data from $L$-site Fermi-Hubbard models, we identify minimal latent spaces with a sharp reconstruction quality threshold at $L-1$ latent dimensions, matching the system's intrinsic degrees of freedom. We demonstrate the use of the trained decoder as a differentiable variational ansatz to minimize energy directly within the latent space. Crucially, this approach circumvents the $N$-representability problem, as the learned manifold implicitly restricts the optimization to physically valid quantum states.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "quant-ph",
        "cond-mat.str-el",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11767v1",
        "pdf": "https://arxiv.org/pdf/2512.11767v1"
      },
      "arxiv_id": "2512.11767v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11763v1",
      "title": "Reducing Domain Gap with Diffusion-Based Domain Adaptation for Cell Counting",
      "authors": [
        "Mohammad Dehghanmanshadi",
        "Wallapak Tavanapong"
      ],
      "abstract": "Generating realistic synthetic microscopy images is critical for training deep learning models in label-scarce environments, such as cell counting with many cells per image. However, traditional domain adaptation methods often struggle to bridge the domain gap when synthetic images lack the complex textures and visual patterns of real samples. In this work, we adapt the Inversion-Based Style Transfer (InST) framework originally designed for artistic style transfer to biomedical microscopy images. Our method combines latent-space Adaptive Instance Normalization with stochastic inversion in a diffusion model to transfer the style from real fluorescence microscopy images to synthetic ones, while weakly preserving content structure.\n  We evaluate the effectiveness of our InST-based synthetic dataset for downstream cell counting by pre-training and fine-tuning EfficientNet-B0 models on various data sources, including real data, hard-coded synthetic data, and the public Cell200-s dataset. Models trained with our InST-synthesized images achieve up to 37\\% lower Mean Absolute Error (MAE) compared to models trained on hard-coded synthetic data, and a 52\\% reduction in MAE compared to models trained on Cell200-s (from 53.70 to 25.95 MAE). Notably, our approach also outperforms models trained on real data alone (25.95 vs. 27.74 MAE). Further improvements are achieved when combining InST-synthesized data with lightweight domain adaptation techniques such as DACS with CutMix. These findings demonstrate that InST-based style transfer most effectively reduces the domain gap between synthetic and real microscopy data. Our approach offers a scalable path for enhancing cell counting performance while minimizing manual labeling effort. The source code and resources are publicly available at: https://github.com/MohammadDehghan/InST-Microscopy.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11763v1",
        "pdf": "https://arxiv.org/pdf/2512.11763v1"
      },
      "arxiv_id": "2512.11763v1",
      "comment": "Accepted at ICMLA 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11760v1",
      "title": "SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning",
      "authors": [
        "Aditya Tripathi",
        "Karan Sharma",
        "Rahul Mishra",
        "Tapas Kumar Maiti"
      ],
      "abstract": "Federated Learning (FL) distributes model training across clients who retain their data locally, but this architecture exposes a fundamental vulnerability: Byzantine clients can inject arbitrarily corrupted updates that degrade or subvert the global model. While robust aggregation methods (including Krum, Bulyan, and coordinate-wise defenses) offer theoretical guarantees under idealized assumptions, their effectiveness erodes substantially when client data distributions are heterogeneous (non-IID) and adversaries can observe or approximate the defense mechanism.\n  This paper introduces SpectralKrum, a defense that fuses spectral subspace estimation with geometric neighbor-based selection. The core insight is that benign optimization trajectories, despite per-client heterogeneity, concentrate near a low-dimensional manifold that can be estimated from historical aggregates. SpectralKrum projects incoming updates into this learned subspace, applies Krum selection in compressed coordinates, and filters candidates whose orthogonal residual energy exceeds a data-driven threshold. The method requires no auxiliary data, operates entirely on model updates, and preserves FL privacy properties.\n  We evaluate SpectralKrum against eight robust baselines across seven attack scenarios on CIFAR-10 with Dirichlet-distributed non-IID partitions (alpha = 0.1). Experiments spanning over 56,000 training rounds show that SpectralKrum is competitive against directional and subspace-aware attacks (adaptive-steer, buffer-drift), but offers limited advantage under label-flip and min-max attacks where malicious updates remain spectrally indistinguishable from benign ones.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11760v1",
        "pdf": "https://arxiv.org/pdf/2512.11760v1"
      },
      "arxiv_id": "2512.11760v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11750v1",
      "title": "LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems",
      "authors": [
        "Ernesto Casablanca",
        "Oliver Schön",
        "Paolo Zuliani",
        "Sadegh Soudjani"
      ],
      "abstract": "Ensuring the safety of AI-enabled systems, particularly in high-stakes domains such as autonomous driving and healthcare, has become increasingly critical. Traditional formal verification tools fall short when faced with systems that embed both opaque, black-box AI components and complex stochastic dynamics. To address these challenges, we introduce LUCID (Learning-enabled Uncertainty-aware Certification of stochastIc Dynamical systems), a verification engine for certifying safety of black-box stochastic dynamical systems from a finite dataset of random state transitions. As such, LUCID is the first known tool capable of establishing quantified safety guarantees for such systems. Thanks to its modular architecture and extensive documentation, LUCID is designed for easy extensibility. LUCID employs a data-driven methodology rooted in control barrier certificates, which are learned directly from system transition data, to ensure formal safety guarantees. We use conditional mean embeddings to embed data into a reproducing kernel Hilbert space (RKHS), where an RKHS ambiguity set is constructed that can be inflated to robustify the result to out-of-distribution behavior. A key innovation within LUCID is its use of a finite Fourier kernel expansion to reformulate a semi-infinite non-convex optimization problem into a tractable linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. LUCID thus offers a robust and efficient verification framework, able to handle the complexities of modern black-box systems while providing formal guarantees of safety. These unique capabilities are demonstrated on challenging benchmarks.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "eess.SY",
        "cs.LG"
      ],
      "primary_category": "eess.SY",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11750v1",
        "pdf": "https://arxiv.org/pdf/2512.11750v1"
      },
      "arxiv_id": "2512.11750v1",
      "comment": "The manuscript has been accepted for publication in the main track of AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11749v1",
      "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
      "authors": [
        "Minglei Shi",
        "Haolin Wang",
        "Borui Zhang",
        "Wenzhao Zheng",
        "Bohan Zeng",
        "Ziyang Yuan",
        "Xiaoshi Wu",
        "Yuanxing Zhang",
        "Huan Yang",
        "Xintao Wang",
        "Pengfei Wan",
        "Kun Gai",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11749v1",
        "pdf": "https://arxiv.org/pdf/2512.11749v1"
      },
      "arxiv_id": "2512.11749v1",
      "comment": "Code Repository: https://github.com/KlingTeam/SVG-T2I; Model Weights: https://huggingface.co/KlingTeam/SVG-T2I",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.11748v1",
      "title": "Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation",
      "authors": [
        "Mohammed El Fallaki Idrissi",
        "Jad Mounayer",
        "Sebastian Rodriguez",
        "Fodil Meraghni",
        "Francisco Chinesta"
      ],
      "abstract": "This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are linked in the latent space using regression techniques, allowing efficient transitions between design and their associated sPGD modes. By empowering design exploration and optimization, this framework also advances digital and hybrid twin development, enhancing predictive modeling and real-time decision-making in engineering applications. The developed framework is demonstrated on two-phase microstructures, in which the multiparametric solutions account for variations in two key material parameters.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11748v1",
        "pdf": "https://arxiv.org/pdf/2512.11748v1"
      },
      "arxiv_id": "2512.11748v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11745v1",
      "title": "mViSE: A Visual Search Engine for Analyzing Multiplex IHC Brain Tissue Images",
      "authors": [
        "Liqiang Huang",
        "Rachel W. Mills",
        "Saikiran Mandula",
        "Lin Bai",
        "Mahtab Jeyhani",
        "John Redell",
        "Hien Van Nguyen",
        "Saurabh Prasad",
        "Dragan Maric",
        "Badrinath Roysam"
      ],
      "abstract": "Whole-slide multiplex imaging of brain tissue generates massive information-dense images that are challenging to analyze and require custom software. We present an alternative query-driven programming-free strategy using a multiplex visual search engine (mViSE) that learns the multifaceted brain tissue chemoarchitecture, cytoarchitecture, and myeloarchitecture. Our divide-and-conquer strategy organizes the data into panels of related molecular markers and uses self-supervised learning to train a multiplex encoder for each panel with explicit visual confirmation of successful learning. Multiple panels can be combined to process visual queries for retrieving similar communities of individual cells or multicellular niches using information-theoretic methods. The retrievals can be used for diverse purposes including tissue exploration, delineating brain regions and cortical cell layers, profiling and comparing brain regions without computer programming. We validated mViSE's ability to retrieve single cells, proximal cell pairs, tissue patches, delineate cortical layers, brain regions and sub-regions. mViSE is provided as an open-source QuPath plug-in.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11745v1",
        "pdf": "https://arxiv.org/pdf/2512.11745v1"
      },
      "arxiv_id": "2512.11745v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11743v1",
      "title": "CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks",
      "authors": [
        "Yongsheng Huang",
        "Peibo Duan",
        "Yujie Wu",
        "Kai Sun",
        "Zhipeng Liu",
        "Changsheng Zhang",
        "Bin Zhang",
        "Mingkun Xu"
      ],
      "abstract": "Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability. In this paper, we introduce a new SNN paradigm, named Cognition-aware SNN (CogniSNN), by incorporating Random Graph Architecture (RGA). Furthermore, we address the issues of network degradation and dimensional mismatch in deep pathways by introducing an improved pure spiking residual mechanism alongside an adaptive pooling strategy. Then, we design a Key Pathway-based Learning without Forgetting (KP-LwF) approach, which selectively reuses critical neural pathways while retaining historical knowledge, enabling efficient multi-task transfer. Finally, we propose a Dynamic Growth Learning (DGL) algorithm that allows neurons and synapses to grow dynamically along the internal temporal dimension. Extensive experiments demonstrate that CogniSNN achieves performance comparable to, or even surpassing, current state-of-the-art SNNs on neuromorphic datasets and Tiny-ImageNet. The Pathway-Reusability enhances the network's continuous learning capability across different scenarios, while the dynamic growth algorithm improves robustness against interference and mitigates the fixed-timestep constraints during neuromorphic chip deployment. This work demonstrates the potential of SNNs with random graph structures in advancing brain-inspired intelligence and lays the foundation for their practical application on neuromorphic hardware.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11743v1",
        "pdf": "https://arxiv.org/pdf/2512.11743v1"
      },
      "arxiv_id": "2512.11743v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11727v1",
      "title": "ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning",
      "authors": [
        "Yuze He",
        "Ferdi Kossmann",
        "Srinivasan Seshan",
        "Peter Steenkiste"
      ],
      "abstract": "Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.DC",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.DC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11727v1",
        "pdf": "https://arxiv.org/pdf/2512.11727v1"
      },
      "arxiv_id": "2512.11727v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11724v1",
      "title": "From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines",
      "authors": [
        "Titaya Mairittha",
        "Tanakon Sawanglok",
        "Panuwit Raden",
        "Jirapast Buntub",
        "Thanapat Warunee",
        "Napat Asawachaisuvikrom",
        "Thanaphum Saiwongin"
      ],
      "abstract": "While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations of conversational rhythm; (2) Expressive Flattening, where the loss of paralinguistic cues leads to literal, inappropriate responses; and (3) Repair Rigidity, where architectural gating prevents users from correcting errors in real-time. Through system-level analysis, we demonstrate that these friction points should not be understood as defects or failures, but as structural consequences of a modular design that prioritizes control over fluidity. We conclude that building natural spoken AI is an infrastructure design challenge, requiring a shift from optimizing isolated components to carefully choreographing the seams between them.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11724v1",
        "pdf": "https://arxiv.org/pdf/2512.11724v1"
      },
      "arxiv_id": "2512.11724v1",
      "comment": "6 pages, 1 figure",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11722v1",
      "title": "Weak-to-Strong Generalization Enables Fully Automated De Novo Training of Multi-head Mask-RCNN Model for Segmenting Densely Overlapping Cell Nuclei in Multiplex Whole-slice Brain Images",
      "authors": [
        "Lin Bai",
        "Xiaoyang Li",
        "Liqiang Huang",
        "Quynh Nguyen",
        "Hien Van Nguyen",
        "Saurabh Prasad",
        "Dragan Maric",
        "John Redell",
        "Pramod Dash",
        "Badrinath Roysam"
      ],
      "abstract": "We present a weak to strong generalization methodology for fully automated training of a multi-head extension of the Mask-RCNN method with efficient channel attention for reliable segmentation of overlapping cell nuclei in multiplex cyclic immunofluorescent (IF) whole-slide images (WSI), and present evidence for pseudo-label correction and coverage expansion, the key phenomena underlying weak to strong generalization. This method can learn to segment de novo a new class of images from a new instrument and/or a new imaging protocol without the need for human annotations. We also present metrics for automated self-diagnosis of segmentation quality in production environments, where human visual proofreading of massive WSI images is unaffordable. Our method was benchmarked against five current widely used methods and showed a significant improvement. The code, sample WSI images, and high-resolution segmentation results are provided in open form for community adoption and adaptation.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11722v1",
        "pdf": "https://arxiv.org/pdf/2512.11722v1"
      },
      "arxiv_id": "2512.11722v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11720v1",
      "title": "Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation",
      "authors": [
        "Yan Zhang",
        "Han Zou",
        "Lincong Feng",
        "Cong Xie",
        "Ruiqi Yu",
        "Zhenpeng Zhan"
      ],
      "abstract": "Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled with a DiT-style backbone, allowing us to inherit architectural and training advances from modern text-to-image models and better capture high-variance 2D pose distributions. On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation. Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning. See supplementary videos at https://hot-dance.github.io",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11720v1",
        "pdf": "https://arxiv.org/pdf/2512.11720v1"
      },
      "arxiv_id": "2512.11720v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11719v1",
      "title": "Referring Change Detection in Remote Sensing Imagery",
      "authors": [
        "Yilmaz Korkmaz",
        "Jay N. Paranjape",
        "Celso M. de Melo",
        "Vishal M. Patel"
      ],
      "abstract": "Change detection in remote sensing imagery is essential for applications such as urban planning, environmental monitoring, and disaster management. Traditional change detection methods typically identify all changes between two temporal images without distinguishing the types of transitions, which can lead to results that may not align with specific user needs. Although semantic change detection methods have attempted to address this by categorizing changes into predefined classes, these methods rely on rigid class definitions and fixed model architectures, making it difficult to mix datasets with different label sets or reuse models across tasks, as the output channels are tightly coupled with the number and type of semantic classes. To overcome these limitations, we introduce Referring Change Detection (RCD), which leverages natural language prompts to detect specific classes of changes in remote sensing images. By integrating language understanding with visual analysis, our approach allows users to specify the exact type of change they are interested in. However, training models for RCD is challenging due to the limited availability of annotated data and severe class imbalance in existing datasets. To address this, we propose a two-stage framework consisting of (I) \\textbf{RCDNet}, a cross-modal fusion network designed for referring change detection, and (II) \\textbf{RCDGen}, a diffusion-based synthetic data generation pipeline that produces realistic post-change images and change maps for a specified category using only pre-change image, without relying on semantic segmentation masks and thereby significantly lowering the barrier to scalable data creation. Experiments across multiple datasets show that our framework enables scalable and targeted change detection. Project website is here: https://yilmazkorkmaz1.github.io/RCD.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11719v1",
        "pdf": "https://arxiv.org/pdf/2512.11719v1"
      },
      "arxiv_id": "2512.11719v1",
      "comment": "2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11715v1",
      "title": "EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing",
      "authors": [
        "Wei Chow",
        "Linfeng Li",
        "Lingdong Kong",
        "Zefeng Li",
        "Qi Xu",
        "Hang Song",
        "Tian Ye",
        "Xian Wang",
        "Jinbin Bai",
        "Shilin Xu",
        "Xiangtai Li",
        "Junting Pan",
        "Shaoteng Liu",
        "Ran Zhou",
        "Tianshu Yang",
        "Songhua Liu"
      ],
      "abstract": "Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinement, MGTs exhibit a localized decoding paradigm that endows them with the inherent capacity to explicitly preserve non-relevant regions during the editing process. Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT. We first demonstrate that MGT's cross-attention maps provide informative localization signals for localizing edit-relevant regions and devise a multi-layer attention consolidation scheme that refines these maps to achieve fine-grained and precise localization. On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas. To train EditMGT, we construct CrispEdit-2M, a high-resolution dataset spanning seven diverse editing categories. Without introducing additional parameters, we adapt a pre-trained text-to-image MGT into an image editing model through attention injection. Extensive experiments across four standard benchmarks demonstrate that, with fewer than 1B parameters, our model achieves similarity performance while enabling 6 times faster editing. Moreover, it delivers comparable or superior editing quality, with improvements of 3.6% and 17.6% on style change and style transfer tasks, respectively.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.MM",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11715v1",
        "pdf": "https://arxiv.org/pdf/2512.11715v1"
      },
      "arxiv_id": "2512.11715v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11705v1",
      "title": "High-Dimensional Surrogate Modeling for Closed-Loop Learning of Neural-Network-Parameterized Model Predictive Control",
      "authors": [
        "Sebastian Hirt",
        "Valentinus Suwanto",
        "Hendrik Alsmeier",
        "Maik Pfefferkorn",
        "Rolf Findeisen"
      ],
      "abstract": "Learning controller parameters from closed-loop data has been shown to improve closed-loop performance. Bayesian optimization, a widely used black-box and sample-efficient learning method, constructs a probabilistic surrogate of the closed-loop performance from few experiments and uses it to select informative controller parameters. However, it typically struggles with dense high-dimensional controller parameterizations, as they may appear, for example, in tuning model predictive controllers, because standard surrogate models fail to capture the structure of such spaces. This work suggests that the use of Bayesian neural networks as surrogate models may help to mitigate this limitation. Through a comparison between Gaussian processes with Matern kernels, finite-width Bayesian neural networks, and infinite-width Bayesian neural networks on a cart-pole task, we find that Bayesian neural network surrogate models achieve faster and more reliable convergence of the closed-loop cost and enable successful optimization of parameterizations with hundreds of dimensions. Infinite-width Bayesian neural networks also maintain performance in settings with more than one thousand parameters, whereas Matern-kernel Gaussian processes rapidly lose effectiveness. These results indicate that Bayesian neural network surrogate models may be suitable for learning dense high-dimensional controller parameterizations and offer practical guidance for selecting surrogate models in learning-based controller design.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11705v1",
        "pdf": "https://arxiv.org/pdf/2512.11705v1"
      },
      "arxiv_id": "2512.11705v1",
      "comment": "7 pages, 2 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11695v1",
      "title": "Particle Image Velocimetry Refinement via Consensus ADMM",
      "authors": [
        "Alan Bonomi",
        "Francesco Banelli",
        "Antonio Terpin"
      ],
      "abstract": "Particle Image Velocimetry (PIV) is an imaging technique in experimental fluid dynamics that quantifies flow fields around bluff bodies by analyzing the displacement of neutrally buoyant tracer particles immersed in the fluid. Traditional PIV approaches typically depend on tuning parameters specific to the imaging setup, making the performance sensitive to variations in illumination, flow conditions, and seeding density. On the other hand, even state-of-the-art machine learning methods for flow quantification are fragile outside their training set. In our experiments, we observed that flow quantification would improve if different tunings (or algorithms) were applied to different regions of the same image pair. In this work, we parallelize the instantaneous flow quantification with multiple algorithms and adopt a consensus framework based on the alternating direction method of multipliers, seamlessly incorporating priors such as smoothness and incompressibility. We perform several numerical experiments to demonstrate the benefits of this approach. For instance, we achieve a decrease in end-point-error of up to 20% of a dense-inverse-search estimator at an inference rate of 60Hz, and we show how this performance boost can be increased further with outlier rejection. Our method is implemented in JAX, effectively exploiting hardware acceleration, and integrated in Flow Gym, enabling (i) reproducible comparisons with the state-of-the-art, (ii) testing different base algorithms, (iii) straightforward deployment for active fluids control applications.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "physics.flu-dyn",
        "cs.CV",
        "eess.IV",
        "math.OC"
      ],
      "primary_category": "physics.flu-dyn",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11695v1",
        "pdf": "https://arxiv.org/pdf/2512.11695v1"
      },
      "arxiv_id": "2512.11695v1",
      "comment": "Code: https://github.com/antonioterpin/flowgym",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.11691v1",
      "title": "Text images processing system using artificial intelligence models",
      "authors": [
        "Aya Kaysan Bahjat"
      ],
      "abstract": "This is to present a text image classifier device that identifies textual content in images and then categorizes each image into one of four predefined categories, including Invoice, Form, Letter, or Report. The device supports a gallery mode, in which users browse files on flash disks, hard disk drives, or microSD cards, and a live mode which renders feeds of cameras connected to it. Its design is specifically aimed at addressing pragmatic challenges, such as changing light, random orientation, curvature or partial coverage of text, low resolution, and slightly visible text. The steps of the processing process are divided into four steps: image acquisition and preprocessing, textual elements detection with the help of DBNet++ (Differentiable Binarization Network Plus) model, BART (Bidirectional Auto-Regressive Transformers) model that classifies detected textual elements, and the presentation of the results through a user interface written in Python and PyQt5. All the stages are connected in such a way that they form a smooth workflow. The system achieved a text recognition rate of about 94.62% when tested over ten hours on the mentioned Total-Text dataset, that includes high resolution images, created so as to represent a wide range of problematic conditions. These experimental results support the effectiveness of the suggested methodology to practice, mixed-source text categorization, even in uncontrolled imaging conditions.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11691v1",
        "pdf": "https://arxiv.org/pdf/2512.11691v1"
      },
      "arxiv_id": "2512.11691v1",
      "comment": "8 pages, 12 figures, article",
      "journal_ref": "International Journal of Engineering in Computer Science 2025; 7(2): 255-262",
      "has_code": false
    },
    {
      "id": "2512.11686v1",
      "title": "Stable spectral neural operator for learning stiff PDE systems from limited data",
      "authors": [
        "Rui Zhang",
        "Han Wan",
        "Yang Liu",
        "Hao Sun"
      ],
      "abstract": "Accurate modeling of spatiotemporal dynamics is crucial to understanding complex phenomena across science and engineering. However, this task faces a fundamental challenge when the governing equations are unknown and observational data are sparse. System stiffness, the coupling of multiple time-scales, further exacerbates this problem and hinders long-term prediction. Existing methods fall short: purely data-driven methods demand massive datasets, whereas physics-aware approaches are constrained by their reliance on known equations and fine-grained time steps. To overcome these limitations, we introduce an equation-free learning framework, namely, the Stable Spectral Neural Operator (SSNO), for modeling stiff partial differential equation (PDE) systems based on limited data. Instead of encoding specific equation terms, SSNO embeds spectrally inspired structures in its architecture, yielding strong inductive biases for learning the underlying physics. It automatically learns local and global spatial interactions in the frequency domain, while handling system stiffness with a robust integrating factor time-stepping scheme. Demonstrated across multiple 2D and 3D benchmarks in Cartesian and spherical geometries, SSNO achieves prediction errors one to two orders of magnitude lower than leading models. Crucially, it shows remarkable data efficiency, requiring only very few (2--5) training trajectories for robust generalization to out-of-distribution conditions. This work offers a robust and generalizable approach to learning stiff spatiotemporal dynamics from limited data without explicit \\textit{a priori} knowledge of PDE terms.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "physics.comp-ph",
        "cs.LG"
      ],
      "primary_category": "physics.comp-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11686v1",
        "pdf": "https://arxiv.org/pdf/2512.11686v1"
      },
      "arxiv_id": "2512.11686v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11683v1",
      "title": "Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection",
      "authors": [
        "Qiushi Guo"
      ],
      "abstract": "Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11683v1",
        "pdf": "https://arxiv.org/pdf/2512.11683v1"
      },
      "arxiv_id": "2512.11683v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11682v1",
      "title": "MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition",
      "authors": [
        "Tim Cofala",
        "Christian Kalfar",
        "Jingge Xiao",
        "Johanna Schrader",
        "Michelle Tang",
        "Wolfgang Nejdl"
      ],
      "abstract": "Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11682v1",
        "pdf": "https://arxiv.org/pdf/2512.11682v1"
      },
      "arxiv_id": "2512.11682v1",
      "comment": "7 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11680v1",
      "title": "Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing",
      "authors": [
        "Xu Zhang",
        "Jiabin Fang",
        "Zhuoming Ding",
        "Jin Yuan",
        "Xuan Liu",
        "Qianjun Zhang",
        "Zhiyong Li"
      ],
      "abstract": "Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11680v1",
        "pdf": "https://arxiv.org/pdf/2512.11680v1"
      },
      "arxiv_id": "2512.11680v1",
      "comment": "12 pages, 5 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11676v1",
      "title": "Stochastics of shapes and Kunita flows",
      "authors": [
        "Stefan Sommer",
        "Gefan Yang",
        "Elizabeth Louise Baker"
      ],
      "abstract": "Stochastic processes of evolving shapes are used in applications including evolutionary biology, where morphology changes stochastically as a function of evolutionary processes. Due to the non-linear and often infinite-dimensional nature of shape spaces, the mathematical construction of suitable stochastic shape processes is far from immediate. We define and formalize properties that stochastic shape processes should ideally satisfy to be compatible with the shape structure, and we link this to Kunita flows that, when acting on shape spaces, induce stochastic processes that satisfy these criteria by their construction. We couple this with a survey of other relevant shape stochastic processes and show how bridge sampling techniques can be used to condition shape stochastic processes on observed data thereby allowing for statistical inference of parameters of the stochastic dynamics.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "math.PR",
        "cs.CV"
      ],
      "primary_category": "math.PR",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11676v1",
        "pdf": "https://arxiv.org/pdf/2512.11676v1"
      },
      "arxiv_id": "2512.11676v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11668v1",
      "title": "Bridging Streaming Continual Learning via In-Context Large Tabular Models",
      "authors": [
        "Afonso Lourenço",
        "João Gama",
        "Eric P. Xing",
        "Goreti Marreiros"
      ],
      "abstract": "In streaming scenarios, models must learn continuously, adapting to concept drifts without erasing previously acquired knowledge. However, existing research communities address these challenges in isolation. Continual Learning (CL) focuses on long-term retention and mitigating catastrophic forgetting, often without strict real-time constraints. Stream Learning (SL) emphasizes rapid, efficient adaptation to high-frequency data streams, but typically neglects forgetting. Recent efforts have tried to combine these paradigms, yet no clear algorithmic overlap exists. We argue that large in-context tabular models (LTMs) provide a natural bridge for Streaming Continual Learning (SCL). In our view, unbounded streams should be summarized on-the-fly into compact sketches that can be consumed by LTMs. This recovers the classical SL motivation of compressing massive streams with fixed-size guarantees, while simultaneously aligning with the experience-replay desiderata of CL. To clarify this bridge, we show how the SL and CL communities implicitly adopt a divide-to-conquer strategy to manage the tension between plasticity (performing well on the current distribution) and stability (retaining past knowledge), while also imposing a minimal complexity constraint that motivates diversification (avoiding redundancy in what is stored) and retrieval (re-prioritizing past information when needed). Within this perspective, we propose structuring SCL with LTMs around two core principles of data selection for in-context learning: (1) distribution matching, which balances plasticity and stability, and (2) distribution compression, which controls memory size through diversification and retrieval mechanisms.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11668v1",
        "pdf": "https://arxiv.org/pdf/2512.11668v1"
      },
      "arxiv_id": "2512.11668v1",
      "comment": "Streaming Continual Learning AAAI Bridge 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11661v1",
      "title": "From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews",
      "authors": [
        "Brenda Nogueira",
        "Werner Geyer",
        "Andrew Anderson",
        "Toby Jia-Jun Li",
        "Dongwhi Kim",
        "Nuno Moniz",
        "Nitesh V. Chawla"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \\textit{pain points} in using LLMs to investigate related work. We identified three recurring gaps: (i) lack of trust in outputs, (ii) persistent verification burden, and (iii) requiring multiple tools. This motivates our proposal of six design goals and a high-level framework that operationalizes them through improved related papers visualization, verification at every step, and human-feedback alignment with generation-guided explanations. Overall, by grounding our work in the practical, day-to-day needs of researchers, we designed a framework that addresses these limitations and models real-world LLM-assisted writing, advancing trust through verifiable actions and fostering practical collaboration between researchers and AI systems.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11661v1",
        "pdf": "https://arxiv.org/pdf/2512.11661v1"
      },
      "arxiv_id": "2512.11661v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11654v1",
      "title": "Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation",
      "authors": [
        "Luca Cazzola",
        "Ahed Alboody"
      ],
      "abstract": "The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11654v1",
        "pdf": "https://arxiv.org/pdf/2512.11654v1"
      },
      "arxiv_id": "2512.11654v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11653v1",
      "title": "Causal Inference in Energy Demand Prediction",
      "authors": [
        "Chutian Ma",
        "Grigorii Pomazkin",
        "Giacinto Paolo Saggese",
        "Paul Smith"
      ],
      "abstract": "Energy demand prediction is critical for grid operators, industrial energy\n  consumers, and service providers. Energy demand is influenced by multiple\n  factors, including weather conditions (e.g. temperature, humidity, wind\n  speed, solar radiation), and calendar information (e.g. hour of day and\n  month of year), which further affect daily work and life schedules. These\n  factors are causally interdependent, making the problem more complex than\n  simple correlation-based learning techniques satisfactorily allow for. We\n  propose a structural causal model that explains the causal relationship\n  between these variables. A full analysis is performed to validate our causal\n  beliefs, also revealing important insights consistent with prior studies.\n  For example, our causal model reveals that energy demand responds to\n  temperature fluctuations with season-dependent sensitivity. Additionally, we\n  find that energy demand exhibits lower variance in winter due to the\n  decoupling effect between temperature changes and daily activity patterns.\n  We then build a Bayesian model, which takes advantage of the causal insights\n  we learned as prior knowledge. The model is trained and tested on unseen\n  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on\n  the test set. The model also demonstrates strong robustness, as the\n  cross-validation across two years of data yields an average MAPE of 3.88 percent.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11653v1",
        "pdf": "https://arxiv.org/pdf/2512.11653v1"
      },
      "arxiv_id": "2512.11653v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11645v1",
      "title": "FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint",
      "authors": [
        "Jiapeng Tang",
        "Kai Li",
        "Chengxiang Yin",
        "Liuhao Ge",
        "Fei Jiang",
        "Jiu Xu",
        "Matthias Nießner",
        "Christian Häne",
        "Timur Bagautdinov",
        "Egor Zakharov",
        "Peihong Guo"
      ],
      "abstract": "We introduce FactorPortrait, a video diffusion method for controllable portrait animation that enables lifelike synthesis from disentangled control signals of facial expressions, head movement, and camera viewpoints. Given a single portrait image, a driving video, and camera trajectories, our method animates the portrait by transferring facial expressions and head movements from the driving video while simultaneously enabling novel view synthesis from arbitrary viewpoints. We utilize a pre-trained image encoder to extract facial expression latents from the driving video as control signals for animation generation. Such latents implicitly capture nuanced facial expression dynamics with identity and pose information disentangled, and they are efficiently injected into the video diffusion transformer through our proposed expression controller. For camera and head pose control, we employ Plücker ray maps and normal maps rendered from 3D body mesh tracking. To train our model, we curate a large-scale synthetic dataset containing diverse combinations of camera viewpoints, head poses, and facial expression dynamics. Extensive experiments demonstrate that our method outperforms existing approaches in realism, expressiveness, control accuracy, and view consistency.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11645v1",
        "pdf": "https://arxiv.org/pdf/2512.11645v1"
      },
      "arxiv_id": "2512.11645v1",
      "comment": "Project page: https://tangjiapeng.github.io/FactorPortrait/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.11635v1",
      "title": "Automating Historical Insight Extraction from Large-Scale Newspaper Archives via Neural Topic Modeling",
      "authors": [
        "Keerthana Murugaraj",
        "Salima Lamsiyah",
        "Marten During",
        "Martin Theobald"
      ],
      "abstract": "Extracting coherent and human-understandable themes from large collections of unstructured historical newspaper archives presents significant challenges due to topic evolution, Optical Character Recognition (OCR) noise, and the sheer volume of text. Traditional topic-modeling methods, such as Latent Dirichlet Allocation (LDA), often fall short in capturing the complexity and dynamic nature of discourse in historical texts. To address these limitations, we employ BERTopic. This neural topic-modeling approach leverages transformerbased embeddings to extract and classify topics, which, despite its growing popularity, still remains underused in historical research. Our study focuses on articles published between 1955 and 2018, specifically examining discourse on nuclear power and nuclear safety. We analyze various topic distributions across the corpus and trace their temporal evolution to uncover long-term trends and shifts in public discourse. This enables us to more accurately explore patterns in public discourse, including the co-occurrence of themes related to nuclear power and nuclear weapons and their shifts in topic importance over time. Our study demonstrates the scalability and contextual sensitivity of BERTopic as an alternative to traditional approaches, offering richer insights into historical discourses extracted from newspaper archives. These findings contribute to historical, nuclear, and social-science research while reflecting on current limitations and proposing potential directions for future work.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11635v1",
        "pdf": "https://arxiv.org/pdf/2512.11635v1"
      },
      "arxiv_id": "2512.11635v1",
      "comment": "This is a preprint of a manuscript submitted to Digital Scholarship in the Humanities (Oxford University Press). The paper is currently under peer review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11624v1",
      "title": "Fast and Explicit: Slice-to-Volume Reconstruction via 3D Gaussian Primitives with Analytic Point Spread Function Modeling",
      "authors": [
        "Maik Dannecker",
        "Steven Jia",
        "Nil Stolt-Ansó",
        "Nadine Girard",
        "Guillaume Auzias",
        "François Rousseau",
        "Daniel Rueckert"
      ],
      "abstract": "Recovering high-fidelity 3D images from sparse or degraded 2D images is a fundamental challenge in medical imaging, with broad applications ranging from 3D ultrasound reconstruction to MRI super-resolution. In the context of fetal MRI, high-resolution 3D reconstruction of the brain from motion-corrupted low-resolution 2D acquisitions is a prerequisite for accurate neurodevelopmental diagnosis. While implicit neural representations (INRs) have recently established state-of-the-art performance in self-supervised slice-to-volume reconstruction (SVR), they suffer from a critical computational bottleneck: accurately modeling the image acquisition physics requires expensive stochastic Monte Carlo sampling to approximate the point spread function (PSF). In this work, we propose a shift from neural network based implicit representations to Gaussian based explicit representations. By parameterizing the HR 3D image volume as a field of anisotropic Gaussian primitives, we leverage the property of Gaussians being closed under convolution and thus derive a \\textit{closed-form analytical solution} for the forward model. This formulation reduces the previously intractable acquisition integral to an exact covariance addition ($\\mathbfΣ_{obs} = \\mathbfΣ_{HR} + \\mathbfΣ_{PSF}$), effectively bypassing the need for compute-intensive stochastic sampling while ensuring exact gradient propagation. We demonstrate that our approach matches the reconstruction quality of self-supervised state-of-the-art SVR frameworks while delivering a 5$\\times$--10$\\times$ speed-up on neonatal and fetal data. With convergence often reached in under 30 seconds, our framework paves the way towards translation into clinical routine of real-time fetal 3D MRI. Code will be public at {https://github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR}.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11624v1",
        "pdf": "https://arxiv.org/pdf/2512.11624v1"
      },
      "arxiv_id": "2512.11624v1",
      "comment": "Under Review for MIDL 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11616v1",
      "title": "A Fast Interpretable Fuzzy Tree Learner",
      "authors": [
        "Javier Fumanal-Idocin",
        "Raquel Fernandez-Peralta",
        "Javier Andreu-Perez"
      ],
      "abstract": "Fuzzy rule-based systems have been mostly used in interpretable decision-making because of their interpretable linguistic rules. However, interpretability requires both sensible linguistic partitions and small rule-base sizes, which are not guaranteed by many existing fuzzy rule-mining algorithms. Evolutionary approaches can produce high-quality models but suffer from prohibitive computational costs, while neural-based methods like ANFIS have problems retaining linguistic interpretations. In this work, we propose an adaptation of classical tree-based splitting algorithms from crisp rules to fuzzy trees, combining the computational efficiency of greedy algoritms with the interpretability advantages of fuzzy logic. This approach achieves interpretable linguistic partitions and substantially improves running time compared to evolutionary-based approaches while maintaining competitive predictive performance. Our experiments on tabular classification benchmarks proof that our method achieves comparable accuracy to state-of-the-art fuzzy classifiers with significantly lower computational cost and produces more interpretable rule bases with constrained complexity. Code is available in: https://github.com/Fuminides/fuzzy_greedy_tree_public",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG",
        "cs.SC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11616v1",
        "pdf": "https://arxiv.org/pdf/2512.11616v1"
      },
      "arxiv_id": "2512.11616v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11614v1",
      "title": "Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols",
      "authors": [
        "Björn Deiseroth",
        "Max Henning Höth",
        "Kristian Kersting",
        "Letitia Parcalabescu"
      ],
      "abstract": "Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11614v1",
        "pdf": "https://arxiv.org/pdf/2512.11614v1"
      },
      "arxiv_id": "2512.11614v1",
      "comment": "34 pages, 19 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11612v1",
      "title": "Embodied Image Compression",
      "authors": [
        "Chunyi Li",
        "Rui Qing",
        "Jianbo Zhang",
        "Yuan Tian",
        "Xiangyang Zhu",
        "Zicheng Zhang",
        "Xiaohong Liu",
        "Weisi Lin",
        "Guangtao Zhai"
      ],
      "abstract": "Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11612v1",
        "pdf": "https://arxiv.org/pdf/2512.11612v1"
      },
      "arxiv_id": "2512.11612v1",
      "comment": "15 pages, 12 figures, 3 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11611v1",
      "title": "Using GUI Agent for Electronic Design Automation",
      "authors": [
        "Chunyi Li",
        "Longfei Li",
        "Zicheng Zhang",
        "Xiaohong Liu",
        "Min Tang",
        "Weisi Lin",
        "Guangtao Zhai"
      ],
      "abstract": "Graphical User Interface (GUI) agents adopt an end-to-end paradigm that maps a screenshot to an action sequence, thereby automating repetitive tasks in virtual environments. However, existing GUI agents are evaluated almost exclusively on commodity software such as Microsoft Word and Excel. Professional Computer-Aided Design (CAD) suites promise an order-of-magnitude higher economic return, yet remain the weakest performance domain for existing agents and are still far from replacing expert Electronic-Design-Automation (EDA) engineers. We therefore present the first systematic study that deploys GUI agents for EDA workflows. Our contributions are: (1) a large-scale dataset named GUI-EDA, including 5 CAD tools and 5 physical domains, comprising 2,000+ high-quality screenshot-answer-action pairs recorded by EDA scientists and engineers during real-world component design; (2) a comprehensive benchmark that evaluates 30+ mainstream GUI agents, demonstrating that EDA tasks constitute a major, unsolved challenge; and (3) an EDA-specialized metric named EDAgent, equipped with a reflection mechanism that achieves reliable performance on industrial CAD software and, for the first time, outperforms Ph.D. students majored in Electrical Engineering. This work extends GUI agents from generic office automation to specialized, high-value engineering domains and offers a new avenue for advancing EDA productivity. The dataset will be released at: https://github.com/aiben-ch/GUI-EDA.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.AR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11611v1",
        "pdf": "https://arxiv.org/pdf/2512.11611v1"
      },
      "arxiv_id": "2512.11611v1",
      "comment": "17 pages, 15 figures, 8 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11593v1",
      "title": "Neural Network-based Partial-Linear Single-Index Models for Environmental Mixtures Analysis",
      "authors": [
        "Hyungrok Do",
        "Yuyan Wang",
        "Mengling Liu",
        "Myeonggyun Lee"
      ],
      "abstract": "Evaluating the health effects of complex environmental mixtures remains a central challenge in environmental health research. Existing approaches vary in their flexibility, interpretability, scalability, and support for diverse outcome types, often limiting their utility in real-world applications. To address these limitations, we propose a neural network-based partial-linear single-index (NeuralPLSI) modeling framework that bridges semiparametric regression modeling interpretability with the expressive power of deep learning. The NeuralPLSI model constructs an interpretable exposure index via a learnable projection and models its relationship with the outcome through a flexible neural network. The framework accommodates continuous, binary, and time-to-event outcomes, and supports inference through a bootstrap-based procedure that yields confidence intervals for key model parameters. We evaluated NeuralPLSI through simulation studies under a range of scenarios and applied it to data from the National Health and Nutrition Examination Survey (NHANES) to demonstrate its practical utility. Together, our contributions establish NeuralPLSI as a scalable, interpretable, and versatile modeling tool for mixture analysis. To promote adoption and reproducibility, we release a user-friendly open-source software package that implements the proposed methodology and supports downstream visualization and inference (\\texttt{https://github.com/hyungrok-do/NeuralPLSI}).",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "stat.AP",
        "cs.LG"
      ],
      "primary_category": "stat.AP",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11593v1",
        "pdf": "https://arxiv.org/pdf/2512.11593v1"
      },
      "arxiv_id": "2512.11593v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11588v1",
      "title": "AI Benchmark Democratization and Carpentry",
      "authors": [
        "Gregor von Laszewski",
        "Wesley Brewer",
        "Jeyan Thiyagalingam",
        "Juri Papay",
        "Armstrong Foundjem",
        "Piotr Luszczek",
        "Murali Emani",
        "Shirley V. Moore",
        "Vijay Janapa Reddi",
        "Matthew D. Sinclair",
        "Sebastian Lobentanzer",
        "Sujata Goswami",
        "Benjamin Hawks",
        "Marco Colombo",
        "Nhan Tran",
        "Christine R. Kirkpatrick",
        "Abdulkareem Alsudais",
        "Gregg Barrett",
        "Tianhao Li",
        "Kirsten Morehouse",
        "Shivaram Venkataraman",
        "Rutwik Jain",
        "Kartik Mathur",
        "Victor Lu",
        "Tejinder Singh",
        "Khojasteh Z. Mirza",
        "Kongtao Chen",
        "Sasidhar Kunapuli",
        "Gavin Farrell",
        "Renato Umeton",
        "Geoffrey C. Fox"
      ],
      "abstract": "Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.\n  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.\n  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11588v1",
        "pdf": "https://arxiv.org/pdf/2512.11588v1"
      },
      "arxiv_id": "2512.11588v1",
      "comment": "43 pages, 2 figures, 7 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11587v1",
      "title": "Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration",
      "authors": [
        "Alexander Tyurin"
      ],
      "abstract": "Even for the gradient descent (GD) method applied to neural network training, understanding its optimization dynamics, including convergence rate, iterate trajectories, function value oscillations, and especially its implicit acceleration, remains a challenging problem. We analyze nonlinear models with the logistic loss and show that the steps of GD reduce to those of generalized perceptron algorithms (Rosenblatt, 1958), providing a new perspective on the dynamics. This reduction yields significantly simpler algorithmic steps, which we analyze using classical linear algebra tools. Using these tools, we demonstrate on a minimalistic example that the nonlinearity in a two-layer model can provably yield a faster iteration complexity $\\tilde{O}(\\sqrt{d})$ compared to $Ω(d)$ achieved by linear models, where $d$ is the number of features. This helps explain the optimization dynamics and the implicit acceleration phenomenon observed in neural networks. The theoretical results are supported by extensive numerical experiments. We believe that this alternative view will further advance research on the optimization of neural networks.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG",
        "math.NA",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11587v1",
        "pdf": "https://arxiv.org/pdf/2512.11587v1"
      },
      "arxiv_id": "2512.11587v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11584v1",
      "title": "Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents",
      "authors": [
        "Stefan Tabakov",
        "Asen Popov",
        "Dimitar Dimitrov",
        "S. Ensiye Kiyamousavi",
        "Vladimir Hristov",
        "Boris Kraychev"
      ],
      "abstract": "Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11584v1",
        "pdf": "https://arxiv.org/pdf/2512.11584v1"
      },
      "arxiv_id": "2512.11584v1",
      "comment": "The 41st ACM/SIGAPP Symposium On Applied Computing",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11582v1",
      "title": "Brain-Semantoks: Learning Semantic Tokens of Brain Dynamics with a Self-Distilled Foundation Model",
      "authors": [
        "Sam Gijsen",
        "Marc-Andre Schulz",
        "Kerstin Ritter"
      ],
      "abstract": "The development of foundation models for functional magnetic resonance imaging (fMRI) time series holds significant promise for predicting phenotypes related to disease and cognition. Current models, however, are often trained using a mask-and-reconstruct objective on small brain regions. This focus on low-level information leads to representations that are sensitive to noise and temporal fluctuations, necessitating extensive fine-tuning for downstream tasks. We introduce Brain-Semantoks, a self-supervised framework designed specifically to learn abstract representations of brain dynamics. Its architecture is built on two core innovations: a semantic tokenizer that aggregates noisy regional signals into robust tokens representing functional networks, and a self-distillation objective that enforces representational stability across time. We show that this objective is stabilized through a novel training curriculum, ensuring the model robustly learns meaningful features from low signal-to-noise time series. We demonstrate that learned representations enable strong performance on a variety of downstream tasks even when only using a linear probe. Furthermore, we provide comprehensive scaling analyses indicating more unlabeled data reliably results in out-of-distribution performance gains without domain adaptation.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG",
        "cs.CV",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11582v1",
        "pdf": "https://arxiv.org/pdf/2512.11582v1"
      },
      "arxiv_id": "2512.11582v1",
      "comment": "Code and pretrained models available at https://github.com/SamGijsen/Brain-Semantoks",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.11580v1",
      "title": "Safe Bayesian optimization across noise models via scenario programming",
      "authors": [
        "Abdullah Tokmak",
        "Thomas B. Schön",
        "Dominik Baumann"
      ],
      "abstract": "Safe Bayesian optimization (BO) with Gaussian processes is an effective tool for tuning control policies in safety-critical real-world systems, specifically due to its sample efficiency and safety guarantees. However, most safe BO algorithms assume homoscedastic sub-Gaussian measurement noise, an assumption that does not hold in many relevant applications. In this article, we propose a straightforward yet rigorous approach for safe BO across noise models, including homoscedastic sub-Gaussian and heteroscedastic heavy-tailed distributions. We provide a high-probability bound on the measurement noise via the scenario approach, integrate these bounds into high probability confidence intervals, and prove safety and optimality for our proposed safe BO algorithm. We deploy our algorithm in synthetic examples and in tuning a controller for the Franka Emika manipulator in simulation.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "math.OC",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "math.OC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11580v1",
        "pdf": "https://arxiv.org/pdf/2512.11580v1"
      },
      "arxiv_id": "2512.11580v1",
      "comment": "Accepted for publication (IEEE Control System Letters)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11575v1",
      "title": "In-Context Learning for Seismic Data Processing",
      "authors": [
        "Fabian Fuchs",
        "Mario Ruben Fernandez",
        "Norman Ettrich",
        "Janis Keuper"
      ],
      "abstract": "Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11575v1",
        "pdf": "https://arxiv.org/pdf/2512.11575v1"
      },
      "arxiv_id": "2512.11575v1",
      "comment": "Source code available under https://codeberg.org/fuchsfa/in-context-learning-seismic",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.11574v1",
      "title": "Evaluating Foundation Models' 3D Understanding Through Multi-View Correspondence Analysis",
      "authors": [
        "Valentina Lilova",
        "Toyesh Chakravorty",
        "Julian I. Bibo",
        "Emma Boccaletti",
        "Brandon Li",
        "Lívia Baxová",
        "Cees G. M. Snoek",
        "Mohammadreza Salehi"
      ],
      "abstract": "Benchmarking 3D spatial understanding of foundation models is essential for real-world applications such as robotics and autonomous driving. Existing evaluations often rely on downstream finetuning with linear heads or task-specific decoders, making it difficult to isolate the intrinsic 3D reasoning ability of pretrained encoders. In this work, we introduce a novel benchmark for in-context 3D scene understanding that requires no finetuning and directly probes the quality of dense visual features. Building on the Hummingbird framework, which evaluates in-context 2D scene understanding, we extend the setup to the 3D Multi-View ImageNet (MVImgNet) dataset. Given a set of images from objects in specific angles (keys), we benchmark the performance of segmenting novel views (queries) and report the scores in 4 categories of easy, medium, hard, and extreme based on the key-query view contrast. We benchmark 8 state-of-the-art foundation models and show DINO-based encoders remain competitive across large viewpoint shifts, while 3D-aware models like VGGT require dedicated multi-view adjustments. Our code is publicly available at https://github.com/ToyeshC/open-hummingbird-3d-eval .",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11574v1",
        "pdf": "https://arxiv.org/pdf/2512.11574v1"
      },
      "arxiv_id": "2512.11574v1",
      "comment": "NeurIPS 2025 UniReps workshop",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11573v1",
      "title": "Visualizing token importance for black-box language models",
      "authors": [
        "Paulius Rauba",
        "Qiyao Wei",
        "Mihaela van der Schaar"
      ],
      "abstract": "We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11573v1",
        "pdf": "https://arxiv.org/pdf/2512.11573v1"
      },
      "arxiv_id": "2512.11573v1",
      "comment": "",
      "journal_ref": "Proceedings of the 28th International Conference on Artifi- cial Intelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand. PMLR: Volume 258",
      "has_code": false
    },
    {
      "id": "2512.11561v1",
      "title": "Fully Inductive Node Representation Learning via Graph View Transformation",
      "authors": [
        "Dooho Lee",
        "Myeong Kong",
        "Minho Jeong",
        "Jaemin Yoo"
      ],
      "abstract": "Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11561v1",
        "pdf": "https://arxiv.org/pdf/2512.11561v1"
      },
      "arxiv_id": "2512.11561v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11560v1",
      "title": "Multi-temporal Calving Front Segmentation",
      "authors": [
        "Marcel Dreier",
        "Nora Gourmelon",
        "Dakota Pyles",
        "Fei Wu",
        "Matthias Braun",
        "Thorsten Seehaus",
        "Andreas Maier",
        "Vincent Christlein"
      ],
      "abstract": "The calving fronts of marine-terminating glaciers undergo constant changes. These changes significantly affect the glacier's mass and dynamics, demanding continuous monitoring. To address this need, deep learning models were developed that can automatically delineate the calving front in Synthetic Aperture Radar imagery. However, these models often struggle to correctly classify areas affected by seasonal conditions such as ice melange or snow-covered surfaces. To address this issue, we propose to process multiple frames from a satellite image time series of the same glacier in parallel and exchange temporal information between the corresponding feature maps to stabilize each prediction. We integrate our approach into the current state-of-the-art architecture Tyrion and accomplish a new state-of-the-art performance on the CaFFe benchmark dataset. In particular, we achieve a Mean Distance Error of 184.4 m and a mean Intersection over Union of 83.6.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11560v1",
        "pdf": "https://arxiv.org/pdf/2512.11560v1"
      },
      "arxiv_id": "2512.11560v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11558v1",
      "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
      "authors": [
        "Zhenyang Cai",
        "Jiaming Zhang",
        "Junjie Zhao",
        "Ziyi Zeng",
        "Yanchao Li",
        "Jingyi Liang",
        "Junying Chen",
        "Yunjin Yang",
        "Jiajun You",
        "Shuzhi Deng",
        "Tongfei Wang",
        "Wanting Chen",
        "Chunxiu Hao",
        "Ruiqi Xie",
        "Zhenwei Wen",
        "Xiangyi Feng",
        "Zou Ting",
        "Jin Zou Lin",
        "Jianquan Li",
        "Guangjun Yu",
        "Liangyi Chen",
        "Junwen Wang",
        "Shan Jiang",
        "Benyou Wang"
      ],
      "abstract": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11558v1",
        "pdf": "https://arxiv.org/pdf/2512.11558v1"
      },
      "arxiv_id": "2512.11558v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11557v1",
      "title": "3DTeethSAM: Taming SAM2 for 3D Teeth Segmentation",
      "authors": [
        "Zhiguo Lu",
        "Jianwen Lou",
        "Mingjun Ma",
        "Hairong Jin",
        "Youyi Zheng",
        "Kun Zhou"
      ],
      "abstract": "3D teeth segmentation, involving the localization of tooth instances and their semantic categorization in 3D dental models, is a critical yet challenging task in digital dentistry due to the complexity of real-world dentition. In this paper, we propose 3DTeethSAM, an adaptation of the Segment Anything Model 2 (SAM2) for 3D teeth segmentation. SAM2 is a pretrained foundation model for image and video segmentation, demonstrating a strong backbone in various downstream scenarios. To adapt SAM2 for 3D teeth data, we render images of 3D teeth models from predefined views, apply SAM2 for 2D segmentation, and reconstruct 3D results using 2D-3D projections. Since SAM2's performance depends on input prompts and its initial outputs often have deficiencies, and given its class-agnostic nature, we introduce three light-weight learnable modules: (1) a prompt embedding generator to derive prompt embeddings from image embeddings for accurate mask decoding, (2) a mask refiner to enhance SAM2's initial segmentation results, and (3) a mask classifier to categorize the generated masks. Additionally, we incorporate Deformable Global Attention Plugins (DGAP) into SAM2's image encoder. The DGAP enhances both the segmentation accuracy and the speed of the training process. Our method has been validated on the 3DTeethSeg benchmark, achieving an IoU of 91.90% on high-resolution 3D teeth meshes, establishing a new state-of-the-art in the field.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11557v1",
        "pdf": "https://arxiv.org/pdf/2512.11557v1"
      },
      "arxiv_id": "2512.11557v1",
      "comment": "Accepted by AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11548v1",
      "title": "SSL-MedSAM2: A Semi-supervised Medical Image Segmentation Framework Powered by Few-shot Learning of SAM2",
      "authors": [
        "Zhendi Gong",
        "Xin Chen"
      ],
      "abstract": "Despite the success of deep learning based models in medical image segmentation, most state-of-the-art (SOTA) methods perform fully-supervised learning, which commonly rely on large scale annotated training datasets. However, medical image annotation is highly time-consuming, hindering its clinical applications. Semi-supervised learning (SSL) has been emerged as an appealing strategy in training with limited annotations, largely reducing the labelling cost. We propose a novel SSL framework SSL-MedSAM2, which contains a training-free few-shot learning branch TFFS-MedSAM2 based on the pretrained large foundation model Segment Anything Model 2 (SAM2) for pseudo label generation, and an iterative fully-supervised learning branch FSL-nnUNet based on nnUNet for pseudo label refinement. The results on MICCAI2025 challenge CARE-LiSeg (Liver Segmentation) demonstrate an outstanding performance of SSL-MedSAM2 among other methods. The average dice scores on the test set in GED4 and T1 MRI are 0.9710 and 0.9648 respectively, and the Hausdorff distances are 20.07 and 21.97 respectively. The code is available via https://github.com/naisops/SSL-MedSAM2/tree/main.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11548v1",
        "pdf": "https://arxiv.org/pdf/2512.11548v1"
      },
      "arxiv_id": "2512.11548v1",
      "comment": "Accepted by MICCAI 2025 CARE Challenge, waiting for publication",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11547v1",
      "title": "Elastic-Net Multiple Kernel Learning: Combining Multiple Data Sources for Prediction",
      "authors": [
        "Janaina Mourão-Miranda",
        "Zakria Hussain",
        "Konstantinos Tsirlis",
        "Christophe Phillips",
        "John Shawe-Taylor"
      ],
      "abstract": "Multiple Kernel Learning (MKL) models combine several kernels in supervised and unsupervised settings to integrate multiple data representations or sources, each represented by a different kernel. MKL seeks an optimal linear combination of base kernels that maximizes a generalized performance measure under a regularization constraint. Various norms have been used to regularize the kernel weights, including $l1$, $l2$ and $lp$, as well as the \"elastic-net\" penalty, which combines $l1$- and $l2$-norm to promote both sparsity and the selection of correlated kernels. This property makes elastic-net regularized MKL (ENMKL) especially valuable when model interpretability is critical and kernels capture correlated information, such as in neuroimaging. Previous ENMKL methods have followed a two-stage procedure: fix kernel weights, train a support vector machine (SVM) with the weighted kernel, and then update the weights via gradient descent, cutting-plane methods, or surrogate functions. Here, we introduce an alternative ENMKL formulation that yields a simple analytical update for the kernel weights. We derive explicit algorithms for both SVM and kernel ridge regression (KRR) under this framework, and implement them in the open-source Pattern Recognition for Neuroimaging Toolbox (PRoNTo). We evaluate these ENMKL algorithms against $l1$-norm MKL and against SVM (or KRR) trained on the unweighted sum of kernels across three neuroimaging applications. Our results show that ENMKL matches or outperforms $l1$-norm MKL in all tasks and only underperforms standard SVM in one scenario. Crucially, ENMKL produces sparser, more interpretable models by selectively weighting correlated kernels.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11547v1",
        "pdf": "https://arxiv.org/pdf/2512.11547v1"
      },
      "arxiv_id": "2512.11547v1",
      "comment": "Technical Report",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11546v1",
      "title": "Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting",
      "authors": [
        "Federico Pennino",
        "Maurizio Gabbrielli"
      ],
      "abstract": "The standard paradigm for training deep learning models on sensor data assumes that more data is always better. However, raw sensor streams are often imbalanced and contain significant redundancy, meaning that not all data points contribute equally to model generalization. In this paper, we show that, in some cases, \"less is more\" when considering datasets. We do this by reframing the data selection problem: rather than tuning model hyperparameters, we fix the model and optimize the composition of the training data itself. We introduce a framework for discovering the optimal \"training diet\" from a large, unlabeled time series corpus. Our framework first uses a large-scale encoder and k-means clustering to partition the dataset into distinct, behaviorally consistent clusters. These clusters represent the fundamental 'ingredients' available for training. We then employ the Optuna optimization framework to search the high-dimensional space of possible data mixtures. For each trial, Optuna proposes a specific sampling ratio for each cluster, and a new training set is constructed based on this recipe. A smaller target model is then trained and evaluated. Our experiments reveal that this data-centric search consistently discovers data mixtures that yield models with significantly higher performance compared to baselines trained on the entire dataset. Specifically - evaluated on PMSM dataset - our method improved performance from a baseline MSE of 1.70 to 1.37, a 19.41% improvement.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11546v1",
        "pdf": "https://arxiv.org/pdf/2512.11546v1"
      },
      "arxiv_id": "2512.11546v1",
      "comment": "Accepted ACM SAC 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11545v1",
      "title": "Graph Embedding with Mel-spectrograms for Underwater Acoustic Target Recognition",
      "authors": [
        "Sheng Feng",
        "Shuqing Ma",
        "Xiaoqian Zhu"
      ],
      "abstract": "Underwater acoustic target recognition (UATR) is extremely challenging due to the complexity of ship-radiated noise and the variability of ocean environments. Although deep learning (DL) approaches have achieved promising results, most existing models implicitly assume that underwater acoustic data lie in a Euclidean space. This assumption, however, is unsuitable for the inherently complex topology of underwater acoustic signals, which exhibit non-stationary, non-Gaussian, and nonlinear characteristics. To overcome this limitation, this paper proposes the UATR-GTransformer, a non-Euclidean DL model that integrates Transformer architectures with graph neural networks (GNNs). The model comprises three key components: a Mel patchify block, a GTransformer block, and a classification head. The Mel patchify block partitions the Mel-spectrogram into overlapping patches, while the GTransformer block employs a Transformer Encoder to capture mutual information between split patches to generate Mel-graph embeddings. Subsequently, a GNN enhances these embeddings by modeling local neighborhood relationships, and a feed-forward network (FFN) further performs feature transformation. Experiments results based on two widely used benchmark datasets demonstrate that the UATR-GTransformer achieves performance competitive with state-of-the-art methods. In addition, interpretability analysis reveals that the proposed model effectively extracts rich frequency-domain information, highlighting its potential for applications in ocean engineering.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11545v1",
        "pdf": "https://arxiv.org/pdf/2512.11545v1"
      },
      "arxiv_id": "2512.11545v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11544v1",
      "title": "AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives",
      "authors": [
        "Yuan Shen",
        "Xiaojun Wu",
        "Linghua Yu"
      ],
      "abstract": "This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of \"AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)\". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11544v1",
        "pdf": "https://arxiv.org/pdf/2512.11544v1"
      },
      "arxiv_id": "2512.11544v1",
      "comment": "47 pages, 2 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11542v1",
      "title": "Infinity and Beyond: Compositional Alignment in VAR and Diffusion T2I Models",
      "authors": [
        "Hossein Shahabadi",
        "Niki Sepasian",
        "Arash Marioriyad",
        "Ali Sharifi-Zarchi",
        "Mahdieh Soleymani Baghshah"
      ],
      "abstract": "Achieving compositional alignment between textual descriptions and generated images - covering objects, attributes, and spatial relationships - remains a core challenge for modern text-to-image (T2I) models. Although diffusion-based architectures have been widely studied, the compositional behavior of emerging Visual Autoregressive (VAR) models is still largely unexamined. We benchmark six diverse T2I systems - SDXL, PixArt-$α$, Flux-Dev, Flux-Schnell, Infinity-2B, and Infinity-8B - across the full T2I-CompBench++ and GenEval suites, evaluating alignment in color and attribute binding, spatial relations, numeracy, and complex multi-object prompts. Across both benchmarks, Infinity-8B achieves the strongest overall compositional alignment, while Infinity-2B also matches or exceeds larger diffusion models in several categories, highlighting favorable efficiency-performance trade-offs. In contrast, SDXL and PixArt-$α$ show persistent weaknesses in attribute-sensitive and spatial tasks. These results provide the first systematic comparison of VAR and diffusion approaches to compositional alignment and establish unified baselines for the future development of the T2I model.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11542v1",
        "pdf": "https://arxiv.org/pdf/2512.11542v1"
      },
      "arxiv_id": "2512.11542v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11541v1",
      "title": "A Multi-Criteria Automated MLOps Pipeline for Cost-Effective Cloud-Based Classifier Retraining in Response to Data Distribution Shifts",
      "authors": [
        "Emmanuel K. Katalay",
        "David O. Dimandja",
        "Jordan F. Masakuna"
      ],
      "abstract": "The performance of machine learning (ML) models often deteriorates when the underlying data distribution changes over time, a phenomenon known as data distribution drift. When this happens, ML models need to be retrained and redeployed. ML Operations (MLOps) is often manual, i.e., humans trigger the process of model retraining and redeployment. In this work, we present an automated MLOps pipeline designed to address neural network classifier retraining in response to significant data distribution changes. Our MLOps pipeline employs multi-criteria statistical techniques to detect distribution shifts and triggers model updates only when necessary, ensuring computational efficiency and resource optimization. We demonstrate the effectiveness of our framework through experiments on several benchmark anomaly detection data sets, showing significant improvements in model accuracy and robustness compared to traditional retraining strategies. Our work provides a foundation for deploying more reliable and adaptive ML systems in dynamic real-world settings, where data distribution changes are common.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11541v1",
        "pdf": "https://arxiv.org/pdf/2512.11541v1"
      },
      "arxiv_id": "2512.11541v1",
      "comment": "11 pages, 3 figures and 2 tables. Preliminary results on an automated MLOps pipeline",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11534v1",
      "title": "HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning",
      "authors": [
        "Yiqing Yang",
        "Kin-Man Lam"
      ],
      "abstract": "Key frame selection in video understanding presents significant challenges. Traditional top-K selection methods, which score frames independently, often fail to optimize the selection as a whole. This independent scoring frequently results in selecting frames that are temporally clustered and visually redundant. Additionally, training lightweight selectors using pseudo labels generated offline by Multimodal Large Language Models (MLLMs) prevents the supervisory signal from dynamically adapting to task objectives. To address these limitations, we propose an end-to-end trainable, task-adaptive framework for frame selection. A Chain-of-Thought approach guides a Small Language Model (SLM) to generate task-specific implicit query vectors, which are combined with multimodal features to enable dynamic frame scoring. We further define a continuous set-level objective function that incorporates relevance, coverage, and redundancy, enabling differentiable optimization via Gumbel-Softmax to select optimal frame combinations at the set level. Finally, student-teacher mutual learning is employed, where the student selector (SLM) and teacher reasoner (MLLM) are trained to align their frame importance distributions via KL divergence. Combined with cross-entropy loss, this enables end-to-end optimization, eliminating reliance on static pseudo labels. Experiments across various benchmarks, including Video-MME, LongVideoBench, MLVU, and NExT-QA, demonstrate that our method significantly outperforms existing approaches.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11534v1",
        "pdf": "https://arxiv.org/pdf/2512.11534v1"
      },
      "arxiv_id": "2512.11534v1",
      "comment": "18 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11532v1",
      "title": "Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems",
      "authors": [
        "Chong Tang",
        "Hao Dai",
        "Jagmohan Chauhan"
      ],
      "abstract": "The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.DC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11532v1",
        "pdf": "https://arxiv.org/pdf/2512.11532v1"
      },
      "arxiv_id": "2512.11532v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11530v1",
      "title": "Parametric Numerical Integration with (Differential) Machine Learning",
      "authors": [
        "Álvaro Leitao",
        "Jonatan Ráfales"
      ],
      "abstract": "In this work, we introduce a machine/deep learning methodology to solve parametric integrals. Besides classical machine learning approaches, we consider a differential learning framework that incorporates derivative information during training, emphasizing its advantageous properties. Our study covers three representative problem classes: statistical functionals (including moments and cumulative distribution functions), approximation of functions via Chebyshev expansions, and integrals arising directly from differential equations. These examples range from smooth closed-form benchmarks to challenging numerical integrals. Across all cases, the differential machine learning-based approach consistently outperforms standard architectures, achieving lower mean squared error, enhanced scalability, and improved sample efficiency.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11530v1",
        "pdf": "https://arxiv.org/pdf/2512.11530v1"
      },
      "arxiv_id": "2512.11530v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11529v1",
      "title": "xGR: Efficient Generative Recommendation Serving at Scale",
      "authors": [
        "Qingxiao Sun",
        "Tongxuan Liu",
        "Shen Zhang",
        "Siyu Wu",
        "Peijun Yang",
        "Haotian Liang",
        "Menxin Li",
        "Xiaolong Ma",
        "Zhiwei Liang",
        "Ziyi Ren",
        "Minchao Zhang",
        "Xinyu Liu",
        "Ke Zhang",
        "Depei Qian",
        "Hailong Yang"
      ],
      "abstract": "Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11529v1",
        "pdf": "https://arxiv.org/pdf/2512.11529v1"
      },
      "arxiv_id": "2512.11529v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11526v1",
      "title": "Contrastive Time Series Forecasting with Anomalies",
      "authors": [
        "Joel Ekstrand",
        "Zahra Taghiyarrenani",
        "Slawomir Nowaczyk"
      ],
      "abstract": "Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11526v1",
        "pdf": "https://arxiv.org/pdf/2512.11526v1"
      },
      "arxiv_id": "2512.11526v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11525v1",
      "title": "NeuralOGCM: Differentiable Ocean Modeling with Learnable Physics",
      "authors": [
        "Hao Wu",
        "Yuan Gao",
        "Fan Xu",
        "Fan Zhang",
        "Guangliang Liu",
        "Yuxuan Liang",
        "Xiaomeng Huang"
      ],
      "abstract": "High-precision scientific simulation faces a long-standing trade-off between computational efficiency and physical fidelity. To address this challenge, we propose NeuralOGCM, an ocean modeling framework that fuses differentiable programming with deep learning. At the core of NeuralOGCM is a fully differentiable dynamical solver, which leverages physics knowledge as its core inductive bias. The learnable physics integration captures large-scale, deterministic physical evolution, and transforms key physical parameters (e.g., diffusion coefficients) into learnable parameters, enabling the model to autonomously optimize its physical core via end-to-end training. Concurrently, a deep neural network learns to correct for subgrid-scale processes and discretization errors not captured by the physics model. Both components work in synergy, with their outputs integrated by a unified ODE solver. Experiments demonstrate that NeuralOGCM maintains long-term stability and physical consistency, significantly outperforming traditional numerical models in speed and pure AI baselines in accuracy. Our work paves a new path for building fast, stable, and physically-plausible models for scientific computing.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11525v1",
        "pdf": "https://arxiv.org/pdf/2512.11525v1"
      },
      "arxiv_id": "2512.11525v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11524v1",
      "title": "Super-Resolved Canopy Height Mapping from Sentinel-2 Time Series Using LiDAR HD Reference Data across Metropolitan France",
      "authors": [
        "Ekaterina Kalinicheva",
        "Florian Helen",
        "Stéphane Mermoz",
        "Florian Mouret",
        "Milena Planells"
      ],
      "abstract": "Fine-scale forest monitoring is essential for understanding canopy structure and its dynamics, which are key indicators of carbon stocks, biodiversity, and forest health. Deep learning is particularly effective for this task, as it integrates spectral, temporal, and spatial signals that jointly reflect the canopy structure. To address this need, we introduce THREASURE-Net, a novel end-to-end framework for Tree Height Regression And Super-Resolution. The model is trained on Sentinel-2 time series using reference height metrics derived from LiDAR HD data at multiple spatial resolutions over Metropolitan France to produce annual height maps. We evaluate three model variants, producing tree-height predictions at 2.5 m, 5 m, and 10 m resolution. THREASURE-Net does not rely on any pretrained model nor on reference very high resolution optical imagery to train its super-resolution module; instead, it learns solely from LiDAR-derived height information. Our approach outperforms existing state-of-the-art methods based on Sentinel data and is competitive with methods based on very high resolution imagery. It can be deployed to generate high-precision annual canopy-height maps, achieving mean absolute errors of 2.62 m, 2.72 m, and 2.88 m at 2.5 m, 5 m, and 10 m resolution, respectively. These results highlight the potential of THREASURE-Net for scalable and cost-effective structural monitoring of temperate forests using only freely available satellite data. The source code for THREASURE-Net is available at: https://github.com/Global-Earth-Observation/threasure-net.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11524v1",
        "pdf": "https://arxiv.org/pdf/2512.11524v1"
      },
      "arxiv_id": "2512.11524v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11510v1",
      "title": "Reconstruction as a Bridge for Event-Based Visual Question Answering",
      "authors": [
        "Hanyue Lou",
        "Jiayi Zhou",
        "Yang Zhang",
        "Boyu Li",
        "Yi Wang",
        "Guangnan Ye",
        "Boxin Shi"
      ],
      "abstract": "Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11510v1",
        "pdf": "https://arxiv.org/pdf/2512.11510v1"
      },
      "arxiv_id": "2512.11510v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11509v1",
      "title": "Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs",
      "authors": [
        "Mohor Banerjee",
        "Nadya Yuki Wangsajaya",
        "Syed Ali Redha Alsagoff",
        "Min Sen Tan",
        "Zachary Choy Kit Chun",
        "Alvin Chan Guo Wei"
      ],
      "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11509v1",
        "pdf": "https://arxiv.org/pdf/2512.11509v1"
      },
      "arxiv_id": "2512.11509v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11508v1",
      "title": "On Geometric Understanding and Learned Data Priors in VGGT",
      "authors": [
        "Jelena Bratulić",
        "Sudhanshu Mittal",
        "Thomas Brox",
        "Christian Rupprecht"
      ],
      "abstract": "The Visual Geometry Grounded Transformer (VGGT) is a 3D foundation model that infers camera geometry and scene structure in a single feed-forward pass. Trained in a supervised, single-step fashion on large datasets, VGGT raises a key question: does it build upon geometric concepts like traditional multi-view methods, or does it rely primarily on learned appearance-based data-driven priors? In this work, we conduct a systematic analysis of VGGT's internal mechanisms to uncover whether geometric understanding emerges within its representations. By probing intermediate features, analyzing attention patterns, and performing interventions, we examine how the model implements its functionality. Our findings reveal that VGGT implicitly performs correspondence matching within its global attention layers and encodes epipolar geometry, despite being trained without explicit geometric constraints. We further investigate VGGT's dependence on its learned data priors. Using spatial input masking and perturbation experiments, we assess its robustness to occlusions, appearance variations, and camera configurations, comparing it with classical multi-stage pipelines. Together, these insights highlight how VGGT internalizes geometric structure while using learned data-driven priors.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11508v1",
        "pdf": "https://arxiv.org/pdf/2512.11508v1"
      },
      "arxiv_id": "2512.11508v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11507v1",
      "title": "SSA3D: Text-Conditioned Assisted Self-Supervised Framework for Automatic Dental Abutment Design",
      "authors": [
        "Mianjie Zheng",
        "Xinquan Yang",
        "Along He",
        "Xuguang Li",
        "Feilie Zhong",
        "Xuefen Liu",
        "Kun Tang",
        "Zhicheng Zhang",
        "Linlin Shen"
      ],
      "abstract": "Abutment design is a critical step in dental implant restoration. However, manual design involves tedious measurement and fitting, and research on automating this process with AI is limited, due to the unavailability of large annotated datasets. Although self-supervised learning (SSL) can alleviate data scarcity, its need for pre-training and fine-tuning results in high computational costs and long training times. In this paper, we propose a Self-supervised assisted automatic abutment design framework (SS$A^3$D), which employs a dual-branch architecture with a reconstruction branch and a regression branch. The reconstruction branch learns to restore masked intraoral scan data and transfers the learned structural information to the regression branch. The regression branch then predicts the abutment parameters under supervised learning, which eliminates the separate pre-training and fine-tuning process. We also design a Text-Conditioned Prompt (TCP) module to incorporate clinical information (such as implant location, system, and series) into SS$A^3$D. This guides the network to focus on relevant regions and constrains the parameter predictions. Extensive experiments on a collected dataset show that SS$A^3$D saves half of the training time and achieves higher accuracy than traditional SSL methods. It also achieves state-of-the-art performance compared to other methods, significantly improving the accuracy and efficiency of automated abutment design.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11507v1",
        "pdf": "https://arxiv.org/pdf/2512.11507v1"
      },
      "arxiv_id": "2512.11507v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11506v1",
      "title": "EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection",
      "authors": [
        "Georgios Kaoukis",
        "Ioannis Aris Koufopoulos",
        "Psaroudaki Eleni",
        "Danae Pla Karidi",
        "Evaggelia Pitoura",
        "George Papastefanatos",
        "Panayiotis Tsaparas"
      ],
      "abstract": "As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11506v1",
        "pdf": "https://arxiv.org/pdf/2512.11506v1"
      },
      "arxiv_id": "2512.11506v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11505v1",
      "title": "BAID: A Benchmark for Bias Assessment of AI Detectors",
      "authors": [
        "Priyam Basu",
        "Yunfeng Zhang",
        "Vipul Raheja"
      ],
      "abstract": "AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11505v1",
        "pdf": "https://arxiv.org/pdf/2512.11505v1"
      },
      "arxiv_id": "2512.11505v1",
      "comment": "Accepted at the workshop on Agentic AI Benchmarks and Applications for Enterprise Tasks at AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11503v1",
      "title": "TSkel-Mamba: Temporal Dynamic Modeling via State Space Model for Human Skeleton-based Action Recognition",
      "authors": [
        "Yanan Liu",
        "Jun Liu",
        "Hao Zhang",
        "Dan Xu",
        "Hossein Rahmani",
        "Mohammed Bennamoun",
        "Qiuhong Ke"
      ],
      "abstract": "Skeleton-based action recognition has garnered significant attention in the computer vision community. Inspired by the recent success of the selective state-space model (SSM) Mamba in modeling 1D temporal sequences, we propose TSkel-Mamba, a hybrid Transformer-Mamba framework that effectively captures both spatial and temporal dynamics. In particular, our approach leverages Spatial Transformer for spatial feature learning while utilizing Mamba for temporal modeling. Mamba, however, employs separate SSM blocks for individual channels, which inherently limits its ability to model inter-channel dependencies. To better adapt Mamba for skeleton data and enhance Mamba`s ability to model temporal dependencies, we introduce a Temporal Dynamic Modeling (TDM) block, which is a versatile plug-and-play component that integrates a novel Multi-scale Temporal Interaction (MTI) module. The MTI module employs multi-scale Cycle operators to capture cross-channel temporal interactions, a critical factor in action recognition. Extensive experiments on NTU-RGB+D 60, NTU-RGB+D 120, NW-UCLA and UAV-Human datasets demonstrate that TSkel-Mamba achieves state-of-the-art performance while maintaining low inference time, making it both efficient and highly effective.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11503v1",
        "pdf": "https://arxiv.org/pdf/2512.11503v1"
      },
      "arxiv_id": "2512.11503v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11499v1",
      "title": "FRQI Pairs method for image classification using Quantum Recurrent Neural Network",
      "authors": [
        "Rafał Potempa",
        "Michał Kordasz",
        "Sundas Naqeeb Khan",
        "Krzysztof Werner",
        "Kamil Wereszczyński",
        "Krzysztof Simiński",
        "Krzysztof A. Cyran"
      ],
      "abstract": "This study aims to introduce the FRQI Pairs method to a wider audience, a novel approach to image classification using Quantum Recurrent Neural Networks (QRNN) with Flexible Representation for Quantum Images (FRQI).\n  The study highlights an innovative approach to use quantum encoded data for an image classification task, suggesting that such quantum-based approaches could significantly reduce the complexity of quantum algorithms. Comparison of the FRQI Pairs method with contemporary techniques underscores the promise of integrating quantum computing principles with neural network architectures for the development of quantum machine learning.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11499v1",
        "pdf": "https://arxiv.org/pdf/2512.11499v1"
      },
      "arxiv_id": "2512.11499v1",
      "comment": "This is a preprint of a paper submitted to the 2025 11th International Conference on Control, Decision and Information Technologies (CoDIT). Copyright may be transferred to IEEE upon acceptance",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11490v1",
      "title": "VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing",
      "authors": [
        "Emanuel Sánchez Aimar",
        "Gulnaz Zhambulova",
        "Fahad Shahbaz Khan",
        "Yonghao Xu",
        "Michael Felsberg"
      ],
      "abstract": "Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11490v1",
        "pdf": "https://arxiv.org/pdf/2512.11490v1"
      },
      "arxiv_id": "2512.11490v1",
      "comment": "21 pages, 7 figures, under review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11482v1",
      "title": "Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models",
      "authors": [
        "Melih Catal",
        "Pooja Rani",
        "Harald C. Gall"
      ],
      "abstract": "Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11482v1",
        "pdf": "https://arxiv.org/pdf/2512.11482v1"
      },
      "arxiv_id": "2512.11482v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11480v1",
      "title": "CADMorph: Geometry-Driven Parametric CAD Editing via a Plan-Generate-Verify Loop",
      "authors": [
        "Weijian Ma",
        "Shizhao Sun",
        "Ruiyu Wang",
        "Jiang Bian"
      ],
      "abstract": "A Computer-Aided Design (CAD) model encodes an object in two coupled forms: a parametric construction sequence and its resulting visible geometric shape. During iterative design, adjustments to the geometric shape inevitably require synchronized edits to the underlying parametric sequence, called geometry-driven parametric CAD editing. The task calls for 1) preserving the original sequence's structure, 2) ensuring each edit's semantic validity, and 3) maintaining high shape fidelity to the target shape, all under scarce editing data triplets. We present CADMorph, an iterative plan-generate-verify framework that orchestrates pretrained domain-specific foundation models during inference: a parameter-to-shape (P2S) latent diffusion model and a masked-parameter-prediction (MPP) model. In the planning stage, cross-attention maps from the P2S model pinpoint the segments that need modification and offer editing masks. The MPP model then infills these masks with semantically valid edits in the generation stage. During verification, the P2S model embeds each candidate sequence in shape-latent space, measures its distance to the target shape, and selects the closest one. The three stages leverage the inherent geometric consciousness and design knowledge in pretrained priors, and thus tackle structure preservation, semantic validity, and shape fidelity respectively. Besides, both P2S and MPP models are trained without triplet data, bypassing the data-scarcity bottleneck. CADMorph surpasses GPT-4o and specialized CAD baselines, and supports downstream applications such as iterative editing and reverse-engineering enhancement.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11480v1",
        "pdf": "https://arxiv.org/pdf/2512.11480v1"
      },
      "arxiv_id": "2512.11480v1",
      "comment": "NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11474v1",
      "title": "General-purpose AI models can generate actionable knowledge on agroecological crop protection",
      "authors": [
        "Kris A. G. Wyckhuys"
      ],
      "abstract": "Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11474v1",
        "pdf": "https://arxiv.org/pdf/2512.11474v1"
      },
      "arxiv_id": "2512.11474v1",
      "comment": "33 pages, 3 figures, 3 tables, 1 supplementary table",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11470v1",
      "title": "Rethinking Expert Trajectory Utilization in LLM Post-training",
      "authors": [
        "Bowen Ding",
        "Yuhan Chen",
        "Jiayang Lv",
        "Jiyao Yuan",
        "Qi Zhu",
        "Shuangshuang Tian",
        "Dantong Zhu",
        "Futing Wang",
        "Heyuan Deng",
        "Fei Mi",
        "Lifeng Shang",
        "Tao Lin"
      ],
      "abstract": "While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11470v1",
        "pdf": "https://arxiv.org/pdf/2512.11470v1"
      },
      "arxiv_id": "2512.11470v1",
      "comment": "24 pages, 5 figures, under review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11469v1",
      "title": "Three methods, one problem: Classical and AI approaches to no-three-in-line",
      "authors": [
        "Pranav Ramanathan",
        "Thomas Prellberg",
        "Matthew Lewis",
        "Prathamesh Dinesh Joshi",
        "Raj Abhijit Dandekar",
        "Rajat Dandekar",
        "Sreedath Panat"
      ],
      "abstract": "The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11469v1",
        "pdf": "https://arxiv.org/pdf/2512.11469v1"
      },
      "arxiv_id": "2512.11469v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11465v1",
      "title": "DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation",
      "authors": [
        "Mohamed Abdelsamad",
        "Michael Ulrich",
        "Bin Yang",
        "Miao Zhang",
        "Yakov Miron",
        "Abhinav Valada"
      ],
      "abstract": "Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11465v1",
        "pdf": "https://arxiv.org/pdf/2512.11465v1"
      },
      "arxiv_id": "2512.11465v1",
      "comment": "AAAI-26",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11464v1",
      "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
      "authors": [
        "Han Lin",
        "Xichen Pan",
        "Ziqi Huang",
        "Ji Hou",
        "Jialiang Wang",
        "Weifeng Chen",
        "Zecheng He",
        "Felix Juefei-Xu",
        "Junzhe Sun",
        "Zhipeng Fan",
        "Ali Thabet",
        "Mohit Bansal",
        "Chu Wang"
      ],
      "abstract": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11464v1",
        "pdf": "https://arxiv.org/pdf/2512.11464v1"
      },
      "arxiv_id": "2512.11464v1",
      "comment": "Project page: https://metacanvas.github.io",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.11458v1",
      "title": "Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation",
      "authors": [
        "Jingmin Zhu",
        "Anqi Zhu",
        "Hossein Rahmani",
        "Jun Liu",
        "Mohammed Bennamoun",
        "Qiuhong Ke"
      ],
      "abstract": "We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11458v1",
        "pdf": "https://arxiv.org/pdf/2512.11458v1"
      },
      "arxiv_id": "2512.11458v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11448v1",
      "title": "Hyperbolic Gaussian Blurring Mean Shift: A Statistical Mode-Seeking Framework for Clustering in Curved Spaces",
      "authors": [
        "Arghya Pratihar",
        "Arnab Seal",
        "Swagatam Das",
        "Inesh Chattopadhyay"
      ],
      "abstract": "Clustering is a fundamental unsupervised learning task for uncovering patterns in data. While Gaussian Blurring Mean Shift (GBMS) has proven effective for identifying arbitrarily shaped clusters in Euclidean space, it struggles with datasets exhibiting hierarchical or tree-like structures. In this work, we introduce HypeGBMS, a novel extension of GBMS to hyperbolic space. Our method replaces Euclidean computations with hyperbolic distances and employs Möbius-weighted means to ensure that all updates remain consistent with the geometry of the space. HypeGBMS effectively captures latent hierarchies while retaining the density-seeking behavior of GBMS. We provide theoretical insights into convergence and computational complexity, along with empirical results that demonstrate improved clustering quality in hierarchical datasets. This work bridges classical mean-shift clustering and hyperbolic representation learning, offering a principled approach to density-based clustering in curved spaces. Extensive experimental evaluations on $11$ real-world datasets demonstrate that HypeGBMS significantly outperforms conventional mean-shift clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11448v1",
        "pdf": "https://arxiv.org/pdf/2512.11448v1"
      },
      "arxiv_id": "2512.11448v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11446v1",
      "title": "YawDD+: Frame-level Annotations for Accurate Yawn Prediction",
      "authors": [
        "Ahmed Mujtaba",
        "Gleb Radchenko",
        "Marc Masana",
        "Radu Prodan"
      ],
      "abstract": "Driver fatigue remains a leading cause of road accidents, with 24\\% of crashes involving drowsy drivers. While yawning serves as an early behavioral indicator of fatigue, existing machine learning approaches face significant challenges due to video-annotated datasets that introduce systematic noise from coarse temporal annotations. We develop a semi-automated labeling pipeline with human-in-the-loop verification, which we apply to YawDD, enabling more accurate model training. Training the established MNasNet classifier and YOLOv11 detector architectures on YawDD+ improves frame accuracy by up to 6\\% and mAP by 5\\% over video-level supervision, achieving 99.34\\% classification accuracy and 95.69\\% detection mAP. The resulting approach deliver up to 59.8 FPS on edge AI hardware (NVIDIA Jetson Nano), confirming that enhanced data quality alone supports on-device yawning monitoring without server-side computation.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11446v1",
        "pdf": "https://arxiv.org/pdf/2512.11446v1"
      },
      "arxiv_id": "2512.11446v1",
      "comment": "This paper is submitted at European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11438v1",
      "title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
      "authors": [
        "Tariq Berrada Ifriqi",
        "John Nguyen",
        "Karteek Alahari",
        "Jakob Verbeek",
        "Ricky T. Q. Chen"
      ],
      "abstract": "We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11438v1",
        "pdf": "https://arxiv.org/pdf/2512.11438v1"
      },
      "arxiv_id": "2512.11438v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11433v1",
      "title": "Back to the Baseline: Examining Baseline Effects on Explainability Metrics",
      "authors": [
        "Agustin Martin Picard",
        "Thibaut Boissin",
        "Varshini Subhash",
        "Rémi Cadène",
        "Thomas Fel"
      ],
      "abstract": "Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11433v1",
        "pdf": "https://arxiv.org/pdf/2512.11433v1"
      },
      "arxiv_id": "2512.11433v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11426v1",
      "title": "AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints",
      "authors": [
        "Shuowei Cai",
        "Yansong Ning",
        "Hao Liu"
      ],
      "abstract": "Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11426v1",
        "pdf": "https://arxiv.org/pdf/2512.11426v1"
      },
      "arxiv_id": "2512.11426v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11423v1",
      "title": "JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion",
      "authors": [
        "Chaochao Li",
        "Ruikui Wang",
        "Liangbo Zhou",
        "Jinheng Feng",
        "Huaishao Luo",
        "Huan Zhang",
        "Youzheng Wu",
        "Xiaodong He"
      ],
      "abstract": "Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11423v1",
        "pdf": "https://arxiv.org/pdf/2512.11423v1"
      },
      "arxiv_id": "2512.11423v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11421v1",
      "title": "Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance",
      "authors": [
        "Gonca Gürsun"
      ],
      "abstract": "Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.\n  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11421v1",
        "pdf": "https://arxiv.org/pdf/2512.11421v1"
      },
      "arxiv_id": "2512.11421v1",
      "comment": "Accepted to AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11415v1",
      "title": "Emergence of Nonequilibrium Latent Cycles in Unsupervised Generative Modeling",
      "authors": [
        "Marco Baiesi",
        "Alberto Rosso"
      ],
      "abstract": "We show that nonequilibrium dynamics can play a constructive role in unsupervised machine learning by inducing the spontaneous emergence of latent-state cycles. We introduce a model in which visible and hidden variables interact through two independently parametrized transition matrices, defining a Markov chain whose steady state is intrinsically out of equilibrium. Likelihood maximization drives this system toward nonequilibrium steady states with finite entropy production, reduced self-transition probabilities, and persistent probability currents in the latent space. These cycles are not imposed by the architecture but arise from training, and models that develop them avoid the low-log-likelihood regime associated with nearly reversible dynamics while more faithfully reproducing the empirical distribution of data classes. Compared with equilibrium approaches such as restricted Boltzmann machines, our model breaks the detailed balance between the forward and backward conditional transitions and relies on a log-likelihood gradient that depends explicitly on the last two steps of the Markov chain. Hence, this exploration of the interface between nonequilibrium statistical physics and modern machine learning suggests that introducing irreversibility into latent-variable models can enhance generative performance.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cond-mat.stat-mech",
        "cs.LG"
      ],
      "primary_category": "cond-mat.stat-mech",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11415v1",
        "pdf": "https://arxiv.org/pdf/2512.11415v1"
      },
      "arxiv_id": "2512.11415v1",
      "comment": "8 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.11412v1",
      "title": "Task-Specific Sparse Feature Masks for Molecular Toxicity Prediction with Chemical Language Models",
      "authors": [
        "Kwun Sy Lee",
        "Jiawei Chen",
        "Fuk Sheng Ford Chung",
        "Tianyu Zhao",
        "Zhenyuan Chen",
        "Debby D. Wang"
      ],
      "abstract": "Reliable in silico molecular toxicity prediction is a cornerstone of modern drug discovery, offering a scalable alternative to experimental screening. However, the black-box nature of state-of-the-art models remains a significant barrier to adoption, as high-stakes safety decisions demand verifiable structural insights alongside predictive performance. To address this, we propose a novel multi-task learning (MTL) framework designed to jointly enhance accuracy and interpretability. Our architecture integrates a shared chemical language model with task-specific attention modules. By imposing an L1 sparsity penalty on these modules, the framework is constrained to focus on a minimal set of salient molecular fragments for each distinct toxicity endpoint. The resulting framework is trained end-to-end and is readily adaptable to various transformer-based backbones. Evaluated on the ClinTox, SIDER, and Tox21 benchmark datasets, our approach consistently outperforms both single-task and standard MTL baselines. Crucially, the sparse attention weights provide chemically intuitive visualizations that reveal the specific fragments influencing predictions, thereby enhancing insight into the model's decision-making process.",
      "published": "2025-12-12",
      "updated": "2025-12-12",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "q-bio.BM"
      ],
      "primary_category": "cs.CE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.11412v1",
        "pdf": "https://arxiv.org/pdf/2512.11412v1"
      },
      "arxiv_id": "2512.11412v1",
      "comment": "6 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    }
  ]
}