{
  "fetched_at": "2025-11-26T00:25:17.781202",
  "total_papers": 100,
  "papers": [
    {
      "id": "2511.19437v1",
      "title": "LumiTex: Towards High-Fidelity PBR Texture Generation with Illumination Context",
      "authors": [
        "Jingzhi Bao",
        "Hongze Chen",
        "Lingting Zhu",
        "Chenyu Liu",
        "Runze Zhang",
        "Keyang Luo",
        "Zeyu Hu",
        "Weikai Chen",
        "Yingda Yin",
        "Xin Wang",
        "Zehong Lin",
        "Jun Zhang",
        "Xiaoguang Han"
      ],
      "abstract": "Physically-based rendering (PBR) provides a principled standard for realistic material-lighting interactions in computer graphics. Despite recent advances in generating PBR textures, existing methods fail to address two fundamental challenges: 1) materials decomposition from image prompts under limited illumination cues, and 2) seamless and view-consistent texture completion. To this end, we propose LumiTex, an end-to-end framework that comprises three key components: (1) a multi-branch generation scheme that disentangles albedo and metallic-roughness under shared illumination priors for robust material understanding, (2) a lighting-aware material attention mechanism that injects illumination context into the decoding process for physically grounded generation of albedo, metallic, and roughness maps, and (3) a geometry-guided inpainting module based on a large view synthesis model that enriches texture coverage and ensures seamless, view-consistent UV completion. Extensive experiments demonstrate that LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19437v1",
        "pdf": "https://arxiv.org/pdf/2511.19437v1"
      },
      "arxiv_id": "2511.19437v1",
      "comment": "Project page: https://lumitex.vercel.app",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19436v1",
      "title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection",
      "authors": [
        "Qiang Wang",
        "Xinyuan Gao",
        "SongLin Dong",
        "Jizhou Han",
        "Jiangyang Li",
        "Yuhang He",
        "Yihong Gong"
      ],
      "abstract": "We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19436v1",
        "pdf": "https://arxiv.org/pdf/2511.19436v1"
      },
      "arxiv_id": "2511.19436v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19435v1",
      "title": "Are Image-to-Video Models Good Zero-Shot Image Editors?",
      "authors": [
        "Zechuan Zhang",
        "Zhenyuan Chen",
        "Zongxin Yang",
        "Yi Yang"
      ],
      "abstract": "Large-scale video diffusion models show strong world simulation and temporal reasoning abilities, but their use as zero-shot image editors remains underexplored. We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing. IF-Edit addresses three key challenges: prompt misalignment, redundant temporal latents, and blurry late-stage frames. It includes (1) a chain-of-thought prompt enhancement module that transforms static editing instructions into temporally grounded reasoning prompts; (2) a temporal latent dropout strategy that compresses frame latents after the expert-switch point, accelerating denoising while preserving semantic and temporal coherence; and (3) a self-consistent post-refinement step that sharpens late-stage frames using a short still-video trajectory. Experiments on four public benchmarks, covering non-rigid editing, physical and temporal reasoning, and general instruction edits, show that IF-Edit performs strongly on reasoning-centric tasks while remaining competitive on general-purpose edits. Our study provides a systematic view of video diffusion models as image editors and highlights a simple recipe for unified video-image generative reasoning.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19435v1",
        "pdf": "https://arxiv.org/pdf/2511.19435v1"
      },
      "arxiv_id": "2511.19435v1",
      "comment": "technical report",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19434v1",
      "title": "Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts",
      "authors": [
        "Yasin Esfandiari",
        "Stefan Bauer",
        "Sebastian U. Stich",
        "Andrea Dittadi"
      ],
      "abstract": "Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19434v1",
        "pdf": "https://arxiv.org/pdf/2511.19434v1"
      },
      "arxiv_id": "2511.19434v1",
      "comment": "ICLR 2025 DeLTa workshop",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19433v1",
      "title": "Mixture of Horizons in Action Chunking",
      "authors": [
        "Dong Jing",
        "Gang Wang",
        "Jiaqi Liu",
        "Weiliang Tang",
        "Zelong Sun",
        "Yunchao Yao",
        "Zhenyu Wei",
        "Yunhui Liu",
        "Zhiwu Lu",
        "Mingyu Ding"
      ],
      "abstract": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19433v1",
        "pdf": "https://arxiv.org/pdf/2511.19433v1"
      },
      "arxiv_id": "2511.19433v1",
      "comment": "15 pages, 14 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19431v1",
      "title": "Cloud4D",
      "authors": [
        "Jacob Lin",
        "Edward Gryspeerdt",
        "Ronald Clark"
      ],
      "abstract": "There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($<10\\%$) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "physics.ao-ph"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19431v1",
        "pdf": "https://arxiv.org/pdf/2511.19431v1"
      },
      "arxiv_id": "2511.19431v1",
      "comment": "NeurIPS 2025 Spotlight, project page: https://cloud4d.jacob-lin.com/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19430v1",
      "title": "Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution",
      "authors": [
        "Dingkang Liang",
        "Cheng Zhang",
        "Xiaopeng Xu",
        "Jianzhong Ju",
        "Zhenbo Luo",
        "Xiang Bai"
      ],
      "abstract": "Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19430v1",
        "pdf": "https://arxiv.org/pdf/2511.19430v1"
      },
      "arxiv_id": "2511.19430v1",
      "comment": "Accepted to AAAI 2026 (Oral). The code is available at \\url{https://github.com/H-EmbodVis/GRANT}",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.19428v1",
      "title": "Flow Map Distillation Without Data",
      "authors": [
        "Shangyuan Tong",
        "Nanye Ma",
        "Saining Xie",
        "Tommi Jaakkola"
      ],
      "abstract": "State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19428v1",
        "pdf": "https://arxiv.org/pdf/2511.19428v1"
      },
      "arxiv_id": "2511.19428v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19426v1",
      "title": "Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction",
      "authors": [
        "Yun Zhou",
        "Yaoting Wang",
        "Guangquan Jie",
        "Jinyu Liu",
        "Henghui Ding"
      ],
      "abstract": "SAM3D has garnered widespread attention for its strong 3D object reconstruction capabilities. However, a key limitation remains: SAM3D cannot reconstruct specific objects referred to by textual descriptions, a capability that is essential for practical applications such as 3D editing, game development, and virtual environments. To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image. Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance. Our results demonstrate that Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction. Code is available at: https://github.com/FudanCVL/Ref-SAM3D.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19426v1",
        "pdf": "https://arxiv.org/pdf/2511.19426v1"
      },
      "arxiv_id": "2511.19426v1",
      "comment": "Code: https://github.com/FudanCVL/Ref-SAM3D",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.19427v1",
      "title": "Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering",
      "authors": [
        "Jayanaka L. Dantanarayana",
        "Savini Kashmira",
        "Thakee Nathees",
        "Zichen Zhang",
        "Krisztian Flautner",
        "Lingjia Tang",
        "Jason Mars"
      ],
      "abstract": "AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19427v1",
        "pdf": "https://arxiv.org/pdf/2511.19427v1"
      },
      "arxiv_id": "2511.19427v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19425v1",
      "title": "SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation",
      "authors": [
        "Tianrun Chen",
        "Runlong Cao",
        "Xinda Yu",
        "Lanyun Zhu",
        "Chaotao Ding",
        "Deyi Ji",
        "Cheng Chen",
        "Qi Zhu",
        "Chunyan Xu",
        "Papa Mao",
        "Ying Zang"
      ],
      "abstract": "The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19425v1",
        "pdf": "https://arxiv.org/pdf/2511.19425v1"
      },
      "arxiv_id": "2511.19425v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19423v1",
      "title": "Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design",
      "authors": [
        "Bruno Jacob",
        "Khushbu Agarwal",
        "Marcel Baer",
        "Peter Rice",
        "Simone Raugei"
      ],
      "abstract": "We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design. Using metalloproteins (e.g., ferredoxins) as a case study, Genie-CAT integrates four capabilities -- literature-grounded reasoning through retrieval-augmented generation (RAG), structural parsing of Protein Data Bank files, electrostatic potential calculations, and machine-learning prediction of redox properties -- into a unified agentic workflow. By coupling natural-language reasoning with data-driven and physics-based computation, the system generates mechanistically interpretable, testable hypotheses linking sequence, structure, and function. In proof-of-concept demonstrations, Genie-CAT autonomously identifies residue-level modifications near [Fe--S] clusters that affect redox tuning, reproducing expert-derived hypotheses in a fraction of the time. The framework highlights how AI agents combining language models with domain-specific tools can bridge symbolic reasoning and numerical simulation, transforming LLMs from conversational assistants into partners for computational discovery.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19423v1",
        "pdf": "https://arxiv.org/pdf/2511.19423v1"
      },
      "arxiv_id": "2511.19423v1",
      "comment": "10 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19422v1",
      "title": "SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning",
      "authors": [
        "David Jiahao Fu",
        "Aryan Gupta",
        "Aaron Councilman",
        "David Grove",
        "Yu-Xiong Wang",
        "Vikram Adve"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19422v1",
        "pdf": "https://arxiv.org/pdf/2511.19422v1"
      },
      "arxiv_id": "2511.19422v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19418v1",
      "title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens",
      "authors": [
        "Yiming Qin",
        "Bomin Wei",
        "Jiaxin Ge",
        "Konstantinos Kallidromitis",
        "Stephanie Fu",
        "Trevor Darrell",
        "Xudong Wang"
      ],
      "abstract": "Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19418v1",
        "pdf": "https://arxiv.org/pdf/2511.19418v1"
      },
      "arxiv_id": "2511.19418v1",
      "comment": "Project page: https://wakalsprojectpage.github.io/comt-website/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.19417v1",
      "title": "Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration",
      "authors": [
        "James Y. Huang",
        "Sheng Zhang",
        "Qianchu Liu",
        "Guanghui Qin",
        "Tinghui Zhu",
        "Tristan Naumann",
        "Muhao Chen",
        "Hoifung Poon"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19417v1",
        "pdf": "https://arxiv.org/pdf/2511.19417v1"
      },
      "arxiv_id": "2511.19417v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19413v1",
      "title": "UniGame: Turning a Unified Multimodal Model Into Its Own Adversary",
      "authors": [
        "Zhaolong Su",
        "Wang Lu",
        "Hao Chen",
        "Sharon Li",
        "Jindong Wang"
      ],
      "abstract": "Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19413v1",
        "pdf": "https://arxiv.org/pdf/2511.19413v1"
      },
      "arxiv_id": "2511.19413v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19405v1",
      "title": "Learning Robust Social Strategies with Large Language Models",
      "authors": [
        "Dereck Piche",
        "Mohammed Muqeeth",
        "Milad Aghajohari",
        "Juan Duque",
        "Michael Noukhovitch",
        "Aaron Courville"
      ],
      "abstract": "As agentic AI becomes more widespread, agents with distinct and possibly conflicting goals will interact in complex ways. These multi-agent interactions pose a fundamental challenge, particularly in social dilemmas, where agents' individual incentives can undermine collective welfare. While reinforcement learning (RL) has been effective for aligning large language models (LLMs) in the single-agent regime, prior small-network results suggest that standard RL in multi-agent settings often converges to defecting, self-interested policies. We show the same effect in LLMs: despite cooperative priors, RL-trained LLM agents develop opportunistic behavior that can exploit even advanced closed-source models. To address this tendency of RL to converge to poor equilibria, we adapt a recent opponent-learning awareness algorithm, Advantage Alignment, to fine-tune LLMs toward multi-agent cooperation and non-exploitability. We then introduce a group-relative baseline that simplifies advantage computation in iterated games, enabling multi-agent training at LLM scale. We also contribute a novel social dilemma environment, Trust and Split, which requires natural language communication to achieve high collective welfare. Across a wide range of social dilemmas, policies learned with Advantage Alignment achieve higher collective payoffs while remaining robust against exploitation by greedy agents.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19405v1",
        "pdf": "https://arxiv.org/pdf/2511.19405v1"
      },
      "arxiv_id": "2511.19405v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19404v1",
      "title": "Nonparametric Instrumental Variable Regression with Observed Covariates",
      "authors": [
        "Zikai Shen",
        "Zonghao Chen",
        "Dimitri Meunier",
        "Ingo Steinwart",
        "Arthur Gretton",
        "Zhu Li"
      ],
      "abstract": "We study the problem of nonparametric instrumental variable regression with observed covariates, which we refer to as NPIV-O. Compared with standard nonparametric instrumental variable regression (NPIV), the additional observed covariates facilitate causal identification and enables heterogeneous causal effect estimation. However, the presence of observed covariates introduces two challenges for its theoretical analysis. First, it induces a partial identity structure, which renders previous NPIV analyses - based on measures of ill-posedness, stability conditions, or link conditions - inapplicable. Second, it imposes anisotropic smoothness on the structural function. To address the first challenge, we introduce a novel Fourier measure of partial smoothing; for the second challenge, we extend the existing kernel 2SLS instrumental variable algorithm with observed covariates, termed KIV-O, to incorporate Gaussian kernel lengthscales adaptive to the anisotropic smoothness. We prove upper $L^2$-learning rates for KIV-O and the first $L^2$-minimax lower learning rates for NPIV-O. Both rates interpolate between known optimal rates of NPIV and nonparametric regression (NPR). Interestingly, we identify a gap between our upper and lower bounds, which arises from the choice of kernel lengthscales tuned to minimize a projected risk. Our theoretical analysis also applies to proximal causal inference, an emerging framework for causal effect estimation that shares the same conditional moment restriction as NPIV-O.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19404v1",
        "pdf": "https://arxiv.org/pdf/2511.19404v1"
      },
      "arxiv_id": "2511.19404v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19401v1",
      "title": "In-Video Instructions: Visual Signals as Generative Control",
      "authors": [
        "Gongfan Fang",
        "Xinyin Ma",
        "Xinchao Wang"
      ],
      "abstract": "Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19401v1",
        "pdf": "https://arxiv.org/pdf/2511.19401v1"
      },
      "arxiv_id": "2511.19401v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19399v1",
      "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
      "authors": [
        "Rulin Shao",
        "Akari Asai",
        "Shannon Zejiang Shen",
        "Hamish Ivison",
        "Varsha Kishore",
        "Jingming Zhuo",
        "Xinran Zhao",
        "Molly Park",
        "Samuel G. Finlayson",
        "David Sontag",
        "Tyler Murray",
        "Sewon Min",
        "Pradeep Dasigi",
        "Luca Soldaini",
        "Faeze Brahman",
        "Wen-tau Yih",
        "Tongshuang Wu",
        "Luke Zettlemoyer",
        "Yoon Kim",
        "Hannaneh Hajishirzi",
        "Pang Wei Koh"
      ],
      "abstract": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19399v1",
        "pdf": "https://arxiv.org/pdf/2511.19399v1"
      },
      "arxiv_id": "2511.19399v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19398v1",
      "title": "PTF Testing Lower Bounds for Non-Gaussian Component Analysis",
      "authors": [
        "Ilias Diakonikolas",
        "Daniel M. Kane",
        "Sihan Liu",
        "Thanasis Pittas"
      ],
      "abstract": "This work studies information-computation gaps for statistical problems. A common approach for providing evidence of such gaps is to show sample complexity lower bounds (that are stronger than the information-theoretic optimum) against natural models of computation. A popular such model in the literature is the family of low-degree polynomial tests. While these tests are defined in such a way that make them easy to analyze, the class of algorithms that they rule out is somewhat restricted. An important goal in this context has been to obtain lower bounds against the stronger and more natural class of low-degree Polynomial Threshold Function (PTF) tests, i.e., any test that can be expressed as comparing some low-degree polynomial of the data to a threshold. Proving lower bounds against PTF tests has turned out to be challenging. Indeed, we are not aware of any non-trivial PTF testing lower bounds in the literature.\n  In this paper, we establish the first non-trivial PTF testing lower bounds for a range of statistical tasks. Specifically, we prove a near-optimal PTF testing lower bound for Non-Gaussian Component Analysis (NGCA). Our NGCA lower bound implies similar lower bounds for a number of other statistical problems. Our proof leverages a connection to recent work on pseudorandom generators for PTFs and recent techniques developed in that context. At the technical level, we develop several tools of independent interest, including novel structural results for analyzing the behavior of low-degree polynomials restricted to random directions.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.DS",
        "cs.IT",
        "cs.LG",
        "math.ST",
        "stat.ML"
      ],
      "primary_category": "cs.DS",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19398v1",
        "pdf": "https://arxiv.org/pdf/2511.19398v1"
      },
      "arxiv_id": "2511.19398v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19396v1",
      "title": "Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments",
      "authors": [
        "Jorge Ortigoso-Narro",
        "Jose A. Belloch",
        "Adrian Amor-Martin",
        "Sandra Roger",
        "Maximo Cobos"
      ],
      "abstract": "Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19396v1",
        "pdf": "https://arxiv.org/pdf/2511.19396v1"
      },
      "arxiv_id": "2511.19396v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19394v1",
      "title": "BackSplit: The Importance of Sub-dividing the Background in Biomedical Lesion Segmentation",
      "authors": [
        "Rachit Saluja",
        "Asli Cihangir",
        "Ruining Deng",
        "Johannes C. Paetzold",
        "Fengbei Liu",
        "Mert R. Sabuncu"
      ],
      "abstract": "Segmenting small lesions in medical images remains notoriously difficult. Most prior work tackles this challenge by either designing better architectures, loss functions, or data augmentation schemes; and collecting more labeled data. We take a different view, arguing that part of the problem lies in how the background is modeled. Common lesion segmentation collapses all non-lesion pixels into a single \"background\" class, ignoring the rich anatomical context in which lesions appear. In reality, the background is highly heterogeneous-composed of tissues, organs, and other structures that can now be labeled manually or inferred automatically using existing segmentation models.\n  In this paper, we argue that training with fine-grained labels that sub-divide the background class, which we call BackSplit, is a simple yet powerful paradigm that can offer a significant performance boost without increasing inference costs. From an information theoretic standpoint, we prove that BackSplit increases the expected Fisher Information relative to conventional binary training, leading to tighter asymptotic bounds and more stable optimization. With extensive experiments across multiple datasets and architectures, we empirically show that BackSplit consistently boosts small-lesion segmentation performance, even when auxiliary labels are generated automatically using pretrained segmentation models. Additionally, we demonstrate that auxiliary labels derived from interactive segmentation frameworks exhibit the same beneficial effect, demonstrating its robustness, simplicity, and broad applicability.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19394v1",
        "pdf": "https://arxiv.org/pdf/2511.19394v1"
      },
      "arxiv_id": "2511.19394v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19390v1",
      "title": "Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme",
      "authors": [
        "Rudy Morel",
        "Francesco Pio Ramunno",
        "Jeff Shen",
        "Alberto Bietti",
        "Kyunghyun Cho",
        "Miles Cranmer",
        "Siavash Golkar",
        "Olexandr Gugnin",
        "Geraud Krawezik",
        "Tanya Marwah",
        "Michael McCabe",
        "Lucas Meyer",
        "Payel Mukhopadhyay",
        "Ruben Ohana",
        "Liam Parker",
        "Helen Qu",
        "François Rozet",
        "K. D. Leka",
        "François Lanusse",
        "David Fouhey",
        "Shirley Ho"
      ],
      "abstract": "Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "astro-ph.SR",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19390v1",
        "pdf": "https://arxiv.org/pdf/2511.19390v1"
      },
      "arxiv_id": "2511.19390v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19380v1",
      "title": "UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval",
      "authors": [
        "Maroun Ayli",
        "Youssef Bakouny",
        "Tushar Sharma",
        "Nader Jalloul",
        "Hani Seifeddine",
        "Rima Kilany"
      ],
      "abstract": "Enterprise software companies maintain thousands of user interface screens across products and versions, creating critical challenges for design consistency, pattern discovery, and compliance check. Existing approaches rely on visual similarity or text semantics, lacking explicit modeling of structural properties fundamental to user interface (UI) composition. We present a novel graph-based representation that converts UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, potentially generalizable to document layouts, architectural diagrams, and other structured visual domains. A contrastive graph autoencoder learns embeddings preserving multi-level similarity across visual, structural, and semantic properties. The comprehensive analysis demonstrates that our structural embeddings achieve better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in the expressiveness of the UI representation. We implement this representation in UISearch, a multi-modal search framework that combines structural embeddings with semantic search through a composable query language. On 20,396 financial software UIs, UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency (P95: 124ms), scaling to 20,000+ screens. The hybrid indexing architecture enables complex queries and supports fine-grained UI distinction impossible with vision-only approaches.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19380v1",
        "pdf": "https://arxiv.org/pdf/2511.19380v1"
      },
      "arxiv_id": "2511.19380v1",
      "comment": "12 pages, 2 figures, 3 algorithms, 4 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19379v1",
      "title": "Efficiency vs. Fidelity: A Comparative Analysis of Diffusion Probabilistic Models and Flow Matching on Low-Resource Hardware",
      "authors": [
        "Srishti Gupta",
        "Yashasvee Taiwade"
      ],
      "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have established a new state-of-the-art in generative image synthesis, yet their deployment is hindered by significant computational overhead during inference, often requiring up to 1,000 iterative steps. This study presents a rigorous comparative analysis of DDPMs against the emerging Flow Matching (Rectified Flow) paradigm, specifically isolating their geometric and efficiency properties on low-resource hardware. By implementing both frameworks on a shared Time-Conditioned U-Net backbone using the MNIST dataset, we demonstrate that Flow Matching significantly outperforms Diffusion in efficiency. Our geometric analysis reveals that Flow Matching learns a highly rectified transport path (Curvature $\\mathcal{C} \\approx 1.02$), which is near-optimal, whereas Diffusion trajectories remain stochastic and tortuous ($\\mathcal{C} \\approx 3.45$). Furthermore, we establish an ``efficiency frontier'' at $N=10$ function evaluations, where Flow Matching retains high fidelity while Diffusion collapses. Finally, we show via numerical sensitivity analysis that the learned vector field is sufficiently linear to render high-order ODE solvers (Runge-Kutta 4) unnecessary, validating the use of lightweight Euler solvers for edge deployment. \\textbf{This work concludes that Flow Matching is the superior algorithmic choice for real-time, resource-constrained generative tasks.}",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19379v1",
        "pdf": "https://arxiv.org/pdf/2511.19379v1"
      },
      "arxiv_id": "2511.19379v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19368v1",
      "title": "LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems",
      "authors": [
        "Tianyang Duan",
        "Zongyuan Zhang",
        "Zheng Lin",
        "Songxiao Guo",
        "Xiuxian Guan",
        "Guangyu Wu",
        "Zihan Fang",
        "Haotian Meng",
        "Xia Du",
        "Ji-Zhe Zhou",
        "Heming Cui",
        "Jun Luo",
        "Yue Gao"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19368v1",
        "pdf": "https://arxiv.org/pdf/2511.19368v1"
      },
      "arxiv_id": "2511.19368v1",
      "comment": "15 pages, 9 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19367v1",
      "title": "An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification",
      "authors": [
        "Saniah Kayenat Chowdhury",
        "Rusab Sarmun",
        "Muhammad E. H. Chowdhury",
        "Sohaib Bassam Zoghoul",
        "Israa Al-Hashimi",
        "Adam Mushtak",
        "Amith Khandakar"
      ],
      "abstract": "Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable \"black box\" manner, our method offers both state-of-the-art performance and transparent decision support.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19367v1",
        "pdf": "https://arxiv.org/pdf/2511.19367v1"
      },
      "arxiv_id": "2511.19367v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19365v1",
      "title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation",
      "authors": [
        "Zehong Ma",
        "Longhui Wei",
        "Shuai Wang",
        "Shiliang Zhang",
        "Qi Tian"
      ],
      "abstract": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19365v1",
        "pdf": "https://arxiv.org/pdf/2511.19365v1"
      },
      "arxiv_id": "2511.19365v1",
      "comment": "Project Page: https://zehong-ma.github.io/DeCo. Code Repository: https://github.com/Zehong-Ma/DeCo",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.19364v1",
      "title": "Neural surrogates for designing gravitational wave detectors",
      "authors": [
        "Carlos Ruiz-Gonzalez",
        "Sören Arlt",
        "Sebastian Lehner",
        "Arturs Berzins",
        "Yehonathan Drori",
        "Rana X Adhikari",
        "Johannes Brandstetter",
        "Mario Krenn"
      ],
      "abstract": "Physics simulators are essential in science and engineering, enabling the analysis, control, and design of complex systems. In experimental sciences, they are increasingly used to automate experimental design, often via combinatorial search and optimization. However, as the setups grow more complex, the computational cost of traditional, CPU-based simulators becomes a major limitation. Here, we show how neural surrogate models can significantly reduce reliance on such slow simulators while preserving accuracy. Taking the design of interferometric gravitational wave detectors as a representative example, we train a neural network to surrogate the gravitational wave physics simulator Finesse, which was developed by the LIGO community. Despite that small changes in physical parameters can change the output by orders of magnitudes, the model rapidly predicts the quality and feasibility of candidate designs, allowing an efficient exploration of large design spaces. Our algorithm loops between training the surrogate, inverse designing new experiments, and verifying their properties with the slow simulator for further training. Assisted by auto-differentiation and GPU parallelism, our method proposes high-quality experiments much faster than direct optimization. Solutions that our algorithm finds within hours outperform designs that take five days for the optimizer to reach. Though shown in the context of gravitational wave detectors, our framework is broadly applicable to other domains where simulator bottlenecks hinder optimization and discovery.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "astro-ph.IM",
        "gr-qc",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19364v1",
        "pdf": "https://arxiv.org/pdf/2511.19364v1"
      },
      "arxiv_id": "2511.19364v1",
      "comment": "20 pages, 7 figures, 4 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19359v1",
      "title": "Enhancing Conformal Prediction via Class Similarity",
      "authors": [
        "Ariel Fargion",
        "Lahav Dabah",
        "Tom Tirer"
      ],
      "abstract": "Conformal Prediction (CP) has emerged as a powerful statistical framework for high-stakes classification applications. Instead of predicting a single class, CP generates a prediction set, guaranteed to include the true label with a pre-specified probability. The performance of different CP methods is typically assessed by their average prediction set size. In setups where the classes can be partitioned into semantic groups, e.g., diseases that require similar treatment, users can benefit from prediction sets that are not only small on average, but also contain a small number of semantically different groups. This paper begins by addressing this problem and ultimately offers a widely applicable tool for boosting any CP method on any dataset. First, given a class partition, we propose augmenting the CP score function with a term that penalizes predictions with out-of-group errors. We theoretically analyze this strategy and prove its advantages for group-related metrics. Surprisingly, we show mathematically that, for common class partitions, it can also reduce the average set size of any CP score function. Our analysis reveals the class similarity factors behind this improvement and motivates us to propose a model-specific variant, which does not require any human semantic partition and can further reduce the prediction set size. Finally, we present an extensive empirical study, encompassing prominent CP methods, multiple models, and several datasets, which demonstrates that our class-similarity-based approach consistently enhances CP methods.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19359v1",
        "pdf": "https://arxiv.org/pdf/2511.19359v1"
      },
      "arxiv_id": "2511.19359v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19356v1",
      "title": "Growing with the Generator: Self-paced GRPO for Video Generation",
      "authors": [
        "Rui Li",
        "Yuanzhi Liang",
        "Ziqi Ni",
        "Haibing Huang",
        "Chi Zhang",
        "Xuelong Li"
      ],
      "abstract": "Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-aware GRPO framework in which reward feedback co-evolves with the generator. Our method introduces a progressive reward mechanism that automatically shifts its emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases. This self-paced curriculum alleviates reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization. Experiments on VBench across multiple video generation backbones demonstrate consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards, validating the effectiveness and generality of Self-Paced GRPO.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19356v1",
        "pdf": "https://arxiv.org/pdf/2511.19356v1"
      },
      "arxiv_id": "2511.19356v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19355v1",
      "title": "Leveraging LLMs for reward function design in reinforcement learning control tasks",
      "authors": [
        "Franklin Cardenoso",
        "Wouter Caarls"
      ],
      "abstract": "The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19355v1",
        "pdf": "https://arxiv.org/pdf/2511.19355v1"
      },
      "arxiv_id": "2511.19355v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19351v1",
      "title": "CellFMCount: A Fluorescence Microscopy Dataset, Benchmark, and Methods for Cell Counting",
      "authors": [
        "Abdurahman Ali Mohammed",
        "Catherine Fonder",
        "Ying Wei",
        "Wallapak Tavanapong",
        "Donald S Sakaguchi",
        "Qi Li",
        "Surya K. Mallapragada"
      ],
      "abstract": "Accurate cell counting is essential in various biomedical research and clinical applications, including cancer diagnosis, stem cell research, and immunology. Manual counting is labor-intensive and error-prone, motivating automation through deep learning techniques. However, training reliable deep learning models requires large amounts of high-quality annotated data, which is difficult and time-consuming to produce manually. Consequently, existing cell-counting datasets are often limited, frequently containing fewer than $500$ images. In this work, we introduce a large-scale annotated dataset comprising $3{,}023$ images from immunocytochemistry experiments related to cellular differentiation, containing over $430{,}000$ manually annotated cell locations. The dataset presents significant challenges: high cell density, overlapping and morphologically diverse cells, a long-tailed distribution of cell count per image, and variation in staining protocols. We benchmark three categories of existing methods: regression-based, crowd-counting, and cell-counting techniques on a test set with cell counts ranging from $10$ to $2{,}126$ cells per image. We also evaluate how the Segment Anything Model (SAM) can be adapted for microscopy cell counting using only dot-annotated datasets. As a case study, we implement a density-map-based adaptation of SAM (SAM-Counter) and report a mean absolute error (MAE) of $22.12$, which outperforms existing approaches (second-best MAE of $27.46$). Our results underscore the value of the dataset and the benchmarking framework for driving progress in automated cell counting and provide a robust foundation for future research and development.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19351v1",
        "pdf": "https://arxiv.org/pdf/2511.19351v1"
      },
      "arxiv_id": "2511.19351v1",
      "comment": "The IEEE International Conference on Data Mining (ICDM) 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19350v1",
      "title": "Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric",
      "authors": [
        "Nikita Neveditsin",
        "Pawan Lingras",
        "Vijay Mago"
      ],
      "abstract": "Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at https://anonymous.4open.science/r/towards_clustering-0C2E.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19350v1",
        "pdf": "https://arxiv.org/pdf/2511.19350v1"
      },
      "arxiv_id": "2511.19350v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19347v1",
      "title": "Artificial Intelligence Driven Workflow for Accelerating Design of Novel Photosensitizers",
      "authors": [
        "Hongyi Wang",
        "Xiuli Zheng",
        "Weimin Liu",
        "Zitian Tang",
        "Sheng Gong"
      ],
      "abstract": "The discovery of high-performance photosensitizers has long been hindered by the time-consuming and resource-intensive nature of traditional trial-and-error approaches. Here, we present \\textbf{A}I-\\textbf{A}ccelerated \\textbf{P}hoto\\textbf{S}ensitizer \\textbf{I}nnovation (AAPSI), a closed-loop workflow that integrates expert knowledge, scaffold-based molecule generation, and Bayesian optimization to accelerate the design of novel photosensitizers. The scaffold-driven generation in AAPSI ensures structural novelty and synthetic feasibility, while the iterative AI-experiment loop accelerates the discovery of novel photosensitizers. AAPSI leverages a curated database of 102,534 photosensitizer-solvent pairs and generate 6,148 synthetically accessible candidates. These candidates are screened via graph transformers trained to predict singlet oxygen quantum yield ($φ_Δ$) and absorption maxima ($λ_{max}$), following experimental validation. This work generates several novel candidates for photodynamic therapy (PDT), among which the hypocrellin-based candidate HB4Ph exhibits exceptional performance at the Pareto frontier of high quantum yield of singlet oxygen and long absorption maxima among current photosensitizers ($φ_Δ$=0.85, $λ_{max}$=650nm).",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.LG",
        "physics.chem-ph"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19347v1",
        "pdf": "https://arxiv.org/pdf/2511.19347v1"
      },
      "arxiv_id": "2511.19347v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19344v1",
      "title": "Annotation-Free Class-Incremental Learning",
      "authors": [
        "Hari Chandana Kuchibhotla",
        "K S Ananth",
        "Vineeth N Balasubramanian"
      ],
      "abstract": "Despite significant progress in continual learning ranging from architectural novelty to clever strategies for mitigating catastrophic forgetting most existing methods rest on a strong but unrealistic assumption the availability of labeled data throughout the learning process. In real-world scenarios, however, data often arrives sequentially and without annotations, rendering conventional approaches impractical. In this work, we revisit the fundamental assumptions of continual learning and ask: Can current systems adapt when labels are absent and tasks emerge incrementally over time? To this end, we introduce Annotation-Free Class-Incremental Learning (AFCIL), a more realistic and challenging paradigm where unlabeled data arrives continuously, and the learner must incrementally acquire new classes without any supervision. To enable effective learning under AFCIL, we propose CrossWorld CL, a Cross Domain World Guided Continual Learning framework that incorporates external world knowledge as a stable auxiliary source. The method retrieves semantically related ImageNet classes for each downstream category, maps downstream and ImageNet features through a cross domain alignment strategy and finally introduce a novel replay strategy. This design lets the model uncover semantic structure without annotations while keeping earlier knowledge intact. Across four datasets, CrossWorld-CL surpasses CLIP baselines and existing continual and unlabeled learning methods, underscoring the benefit of world knowledge for annotation free continual learning.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19344v1",
        "pdf": "https://arxiv.org/pdf/2511.19344v1"
      },
      "arxiv_id": "2511.19344v1",
      "comment": "18 pages, 6 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19343v1",
      "title": "Syn-GRPO: Self-Evolving Data Synthesis for MLLM Perception Reasoning",
      "authors": [
        "Qihan Huang",
        "Haofei Zhang",
        "Rong Wei",
        "Yi Wang",
        "Rui Tang",
        "Mingli Song",
        "Jie Song"
      ],
      "abstract": "RL (reinforcement learning) methods (e.g., GRPO) for MLLM (Multimodal LLM) perception ability has attracted wide research interest owing to its remarkable generalization ability. Nevertheless, existing reinforcement learning methods still face the problem of low data quality, where data samples cannot elicit diverse responses from MLLMs, thus restricting the exploration scope for MLLM reinforcement learning. Some methods attempt to mitigate this problem by imposing constraints on entropy, but none address it at its root. Therefore, to tackle this problem, this work proposes Syn-GRPO (Synthesis-GRPO), which employs an online data generator to synthesize high-quality training data with diverse responses in GRPO training. Specifically, Syn-GRPO consists of two components: (1) data server; (2) GRPO workflow. The data server synthesizes new samples from existing ones using an image generation model, featuring a decoupled and asynchronous scheme to achieve high generation efficiency. The GRPO workflow provides the data server with the new image descriptions, and it leverages a diversity reward to supervise the MLLM to predict image descriptions for synthesizing samples with diverse responses. Experiment results across three visual perception tasks demonstrate that Syn-GRPO improves the data quality by a large margin, achieving significant superior performance to existing MLLM perception methods, and Syn-GRPO presents promising potential for scaling long-term self-evolving RL. Our code is available at https://github.com/hqhQAQ/Syn-GRPO.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19343v1",
        "pdf": "https://arxiv.org/pdf/2511.19343v1"
      },
      "arxiv_id": "2511.19343v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19342v1",
      "title": "Explicit Tonal Tension Conditioning via Dual-Level Beam Search for Symbolic Music Generation",
      "authors": [
        "Maral Ebrahimzadeh",
        "Gilberto Bernardes",
        "Sebastian Stober"
      ],
      "abstract": "State-of-the-art symbolic music generation models have recently achieved remarkable output quality, yet explicit control over compositional features, such as tonal tension, remains challenging. We propose a novel approach that integrates a computational tonal tension model, based on tonal interval vector analysis, into a Transformer framework. Our method employs a two-level beam search strategy during inference. At the token level, generated candidates are re-ranked using model probability and diversity metrics to maintain overall quality. At the bar level, a tension-based re-ranking is applied to ensure that the generated music aligns with a desired tension curve. Objective evaluations indicate that our approach effectively modulates tonal tension, and subjective listening tests confirm that the system produces outputs that align with the target tension. These results demonstrate that explicit tension conditioning through a dual-level beam search provides a powerful and intuitive tool to guide AI-generated music. Furthermore, our experiments demonstrate that our method can generate multiple distinct musical interpretations under the same tension condition.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19342v1",
        "pdf": "https://arxiv.org/pdf/2511.19342v1"
      },
      "arxiv_id": "2511.19342v1",
      "comment": "12 pages, 2 Figures, Accepted at the 17th International Symposium on Computer Music Multidisciplinary Research (CMMR) 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19339v1",
      "title": "POUR: A Provably Optimal Method for Unlearning Representations via Neural Collapse",
      "authors": [
        "Anjie Le",
        "Can Peng",
        "Yuyuan Liu",
        "J. Alison Noble"
      ],
      "abstract": "In computer vision, machine unlearning aims to remove the influence of specific visual concepts or training images without retraining from scratch. Studies show that existing approaches often modify the classifier while leaving internal representations intact, resulting in incomplete forgetting. In this work, we extend the notion of unlearning to the representation level, deriving a three-term interplay between forgetting efficacy, retention fidelity, and class separation. Building on Neural Collapse theory, we show that the orthogonal projection of a simplex Equiangular Tight Frame (ETF) remains an ETF in a lower dimensional space, yielding a provably optimal forgetting operator. We further introduce the Representation Unlearning Score (RUS) to quantify representation-level forgetting and retention fidelity. Building on this, we introduce POUR (Provably Optimal Unlearning of Representations), a geometric projection method with closed-form (POUR-P) and a feature-level unlearning variant under a distillation scheme (POUR-D). Experiments on CIFAR-10/100 and PathMNIST demonstrate that POUR achieves effective unlearning while preserving retained knowledge, outperforming state-of-the-art unlearning methods on both classification-level and representation-level metrics.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19339v1",
        "pdf": "https://arxiv.org/pdf/2511.19339v1"
      },
      "arxiv_id": "2511.19339v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19335v1",
      "title": "High-throughput validation of phase formability and simulation accuracy of Cantor alloys",
      "authors": [
        "Changjun Cheng",
        "Daniel Persaud",
        "Kangming Li",
        "Michael J. Moorehead",
        "Natalie Page",
        "Christian Lavoie",
        "Beatriz Diaz Moreno",
        "Adrien Couet",
        "Samuel E Lofland",
        "Jason Hattrick-Simpers"
      ],
      "abstract": "High-throughput methods enable accelerated discovery of novel materials in complex systems such as high-entropy alloys, which exhibit intricate phase stability across vast compositional spaces. Computational approaches, including Density Functional Theory (DFT) and calculation of phase diagrams (CALPHAD), facilitate screening of phase formability as a function of composition and temperature. However, the integration of computational predictions with experimental validation remains challenging in high-throughput studies. In this work, we introduce a quantitative confidence metric to assess the agreement between predictions and experimental observations, providing a quantitative measure of the confidence of machine learning models trained on either DFT or CALPHAD input in accounting for experimental evidence. The experimental dataset was generated via high-throughput in-situ synchrotron X-ray diffraction on compositionally varied FeNiMnCr alloy libraries, heated from room temperature to ~1000 °C. Agreement between the observed and predicted phases was evaluated using either temperature-independent phase classification or a model that incorporates a temperature-dependent probability of phase formation. This integrated approach demonstrates where strong overall agreement between computation and experiment exists, while also identifying key discrepancies, particularly in FCC/BCC predictions at Mn-rich regions to inform future model refinement.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.LG"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19335v1",
        "pdf": "https://arxiv.org/pdf/2511.19335v1"
      },
      "arxiv_id": "2511.19335v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19330v1",
      "title": "Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data",
      "authors": [
        "Dominik Luszczynski"
      ],
      "abstract": "A common method of attacking deep learning models is through adversarial attacks, which occur when an attacker specifically modifies the input of a model to produce an incorrect result. Adversarial attacks have been deeply investigated in the image domain; however, there is less research in the time-series domain and very little for forecasting financial data. To address these concerns, this study aims to build upon previous research on adversarial attacks for time-series data by introducing two new slope-based methods aimed to alter the trends of the predicted stock forecast generated by an N-HiTS model. Compared to the normal N-HiTS predictions, the two new slope-based methods, the General Slope Attack and Least-Squares Slope Attack, can manipulate N-HiTS predictions by doubling the slope. These new slope attacks can bypass standard security mechanisms, such as a discriminator that filters real and perturbed inputs, reducing a 4-layered CNN's specificity to 28% and accuracy to 57%. Furthermore, the slope based methods were incorporated into a GAN architecture as a means of generating realistic synthetic data, while simultaneously fooling the model. Finally, this paper also proposes a sample malware designed to inject an adversarial attack in the model inference library, proving that ML-security research should not only focus on making the model safe, but also securing the entire pipeline.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19330v1",
        "pdf": "https://arxiv.org/pdf/2511.19330v1"
      },
      "arxiv_id": "2511.19330v1",
      "comment": "13 pages, 6 figures, 4 tables, preprint; Total including Appendix: 21 pages, 11 figures, 7 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19328v1",
      "title": "Understanding the Staged Dynamics of Transformers in Learning Latent Structure",
      "authors": [
        "Rohan Saha",
        "Farzane Aminmansour",
        "Alona Fyshe"
      ],
      "abstract": "While transformers can discover latent structure from context, the dynamics of how they acquire different components of the latent structure remain poorly understood. In this work, we use the Alchemy benchmark, to investigate the dynamics of latent structure learning. We train a small decoder-only transformer on three task variants: 1) inferring missing rules from partial contextual information, 2) composing simple rules to solve multi-step sequences, and 3) decomposing complex multi-step examples to infer intermediate steps. By factorizing each task into interpretable events, we show that the model acquires capabilities in discrete stages, first learning the coarse grained rules, before learning the complete latent structure. We also identify a crucial asymmetry, where the model can compose fundamental rules robustly, but struggles to decompose complex examples to discover the fundamental rules. These findings offer new insights into understanding how a transformer model learns latent structures, providing a granular view of how these capabilities evolve during training.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19328v1",
        "pdf": "https://arxiv.org/pdf/2511.19328v1"
      },
      "arxiv_id": "2511.19328v1",
      "comment": "Preprint",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19326v1",
      "title": "MonoMSK: Monocular 3D Musculoskeletal Dynamics Estimation",
      "authors": [
        "Farnoosh Koleini",
        "Hongfei Xue",
        "Ahmed Helmy",
        "Pu Wang"
      ],
      "abstract": "Reconstructing biomechanically realistic 3D human motion - recovering both kinematics (motion) and kinetics (forces) - is a critical challenge. While marker-based systems are lab-bound and slow, popular monocular methods use oversimplified, anatomically inaccurate models (e.g., SMPL) and ignore physics, fundamentally limiting their biomechanical fidelity. In this work, we introduce MonoMSK, a hybrid framework that bridges data-driven learning and physics-based simulation for biomechanically realistic 3D human motion estimation from monocular video. MonoMSK jointly recovers both kinematics (motions) and kinetics (forces and torques) through an anatomically accurate musculoskeletal model. By integrating transformer-based inverse dynamics with differentiable forward kinematics and dynamics layers governed by ODE-based simulation, MonoMSK establishes a physics-regulated inverse-forward loop that enforces biomechanical causality and physical plausibility. A novel forward-inverse consistency loss further aligns motion reconstruction with the underlying kinetic reasoning. Experiments on BML-MoVi, BEDLAM, and OpenCap show that MonoMSK significantly outperforms state-of-the-art methods in kinematic accuracy, while for the first time enabling precise monocular kinetics estimation.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19326v1",
        "pdf": "https://arxiv.org/pdf/2511.19326v1"
      },
      "arxiv_id": "2511.19326v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19325v1",
      "title": "Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval",
      "authors": [
        "Olivia Macmillan-Scott",
        "Roksana Goworek",
        "Eda B. Özyiğit"
      ],
      "abstract": "Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19325v1",
        "pdf": "https://arxiv.org/pdf/2511.19325v1"
      },
      "arxiv_id": "2511.19325v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19324v1",
      "title": "What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models",
      "authors": [
        "Roksana Goworek",
        "Olivia Macmillan-Scott",
        "Eda B. Özyiğit"
      ],
      "abstract": "Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19324v1",
        "pdf": "https://arxiv.org/pdf/2511.19324v1"
      },
      "arxiv_id": "2511.19324v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19320v1",
      "title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation",
      "authors": [
        "Jiaming Zhang",
        "Shengming Cao",
        "Rui Li",
        "Xiaotong Zhao",
        "Yutao Cui",
        "Xinglin Hou",
        "Gangshan Wu",
        "Haolan Chen",
        "Yu Xu",
        "Limin Wang",
        "Kai Ma"
      ],
      "abstract": "Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19320v1",
        "pdf": "https://arxiv.org/pdf/2511.19320v1"
      },
      "arxiv_id": "2511.19320v1",
      "comment": "10 pages, with supp",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19319v1",
      "title": "SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis",
      "authors": [
        "Lingwei Dang",
        "Zonghan Li",
        "Juntong Li",
        "Hongwen Zhang",
        "Liang An",
        "Yebin Liu",
        "Qingyao Wu"
      ],
      "abstract": "Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19319v1",
        "pdf": "https://arxiv.org/pdf/2511.19319v1"
      },
      "arxiv_id": "2511.19319v1",
      "comment": "Project Page: https://droliven.github.io/SyncMV4D",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.19316v1",
      "title": "Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach",
      "authors": [
        "Xincheng Wang",
        "Hanchi Sun",
        "Wenjun Sun",
        "Kejun Xue",
        "Wangqiu Zhou",
        "Jianbo Zhang",
        "Wei Sun",
        "Dandan Zhu",
        "Xiongkuo Min",
        "Jun Jia",
        "Zhijun Fang"
      ],
      "abstract": "Recent fine-tuning techniques for diffusion models enable them to reproduce specific image sets, such as particular faces or artistic styles, but also introduce copyright and security risks. Dataset watermarking has been proposed to ensure traceability by embedding imperceptible watermarks into training images, which remain detectable in outputs even after fine-tuning. However, current methods lack a unified evaluation framework. To address this, this paper establishes a general threat model and introduces a comprehensive evaluation framework encompassing Universality, Transmissibility, and Robustness. Experiments show that existing methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, yet still fall short under real-world threat scenarios. To reveal these vulnerabilities, the paper further proposes a practical watermark removal method that fully eliminates dataset watermarks without affecting fine-tuning, highlighting a key challenge for future research.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19316v1",
        "pdf": "https://arxiv.org/pdf/2511.19316v1"
      },
      "arxiv_id": "2511.19316v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19314v1",
      "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking",
      "authors": [
        "Jaewoo Lee",
        "Archiki Prasad",
        "Justin Chih-Yao Chen",
        "Zaid Khan",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19314v1",
        "pdf": "https://arxiv.org/pdf/2511.19314v1"
      },
      "arxiv_id": "2511.19314v1",
      "comment": "18 pages, code: https://github.com/G-JWLee/PRInTS",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.19306v1",
      "title": "Dual-Granularity Semantic Prompting for Language Guidance Infrared Small Target Detection",
      "authors": [
        "Zixuan Wang",
        "Haoran Sun",
        "Jiaming Lu",
        "Wenxuan Wang",
        "Zhongling Huang",
        "Dingwen Zhang",
        "Xuelin Qian",
        "Junwei Han"
      ],
      "abstract": "Infrared small target detection remains challenging due to limited feature representation and severe background interference, resulting in sub-optimal performance. While recent CLIP-inspired methods attempt to leverage textual guidance for detection, they are hindered by inaccurate text descriptions and reliance on manual annotations. To overcome these limitations, we propose DGSPNet, an end-to-end language prompt-driven framework. Our approach integrates dual-granularity semantic prompts: coarse-grained textual priors (e.g., 'infrared image', 'small target') and fine-grained personalized semantic descriptions derived through visual-to-textual mapping within the image space. This design not only facilitates learning fine-grained semantic information but also can inherently leverage language prompts during inference without relying on any annotation requirements. By fully leveraging the precision and conciseness of text descriptions, we further introduce a text-guide channel attention (TGCA) mechanism and text-guide spatial attention (TGSA) mechanism that enhances the model's sensitivity to potential targets across both low- and high-level feature spaces. Extensive experiments demonstrate that our method significantly improves detection accuracy and achieves state-of-the-art performance on three benchmark datasets.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19306v1",
        "pdf": "https://arxiv.org/pdf/2511.19306v1"
      },
      "arxiv_id": "2511.19306v1",
      "comment": "10 pages, 2 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19304v1",
      "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning",
      "authors": [
        "Jiayi Zhang",
        "Yiran Peng",
        "Fanqi Kong",
        "Yang Cheng",
        "Yifan Wu",
        "Zhaoyang Yu",
        "Jinyu Xiang",
        "Jianhao Ruan",
        "Jinlin Wang",
        "Maojia Song",
        "HongZhang Liu",
        "Xiangru Tang",
        "Bang Liu",
        "Chenglin Wu",
        "Yuyu Luo"
      ],
      "abstract": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19304v1",
        "pdf": "https://arxiv.org/pdf/2511.19304v1"
      },
      "arxiv_id": "2511.19304v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19301v1",
      "title": "IDEAL-M3D: Instance Diversity-Enriched Active Learning for Monocular 3D Detection",
      "authors": [
        "Johannes Meier",
        "Florian Günther",
        "Riccardo Marin",
        "Oussema Dhaouadi",
        "Jacques Kaiser",
        "Daniel Cremers"
      ],
      "abstract": "Monocular 3D detection relies on just a single camera and is therefore easy to deploy. Yet, achieving reliable 3D understanding from monocular images requires substantial annotation, and 3D labels are especially costly. To maximize performance under constrained labeling budgets, it is essential to prioritize annotating samples expected to deliver the largest performance gains. This prioritization is the focus of active learning. Curiously, we observed two significant limitations in active learning algorithms for 3D monocular object detection. First, previous approaches select entire images, which is inefficient, as non-informative instances contained in the same image also need to be labeled. Secondly, existing methods rely on uncertainty-based selection, which in monocular 3D object detection creates a bias toward depth ambiguity. Consequently, distant objects are selected, while nearby objects are overlooked.\n  To address these limitations, we propose IDEAL-M3D, the first instance-level pipeline for monocular 3D detection. For the first time, we demonstrate that an explicitly diverse, fast-to-train ensemble improves diversity-driven active learning for monocular 3D. We induce diversity with heterogeneous backbones and task-agnostic features, loss weight perturbation, and time-dependent bagging. IDEAL-M3D shows superior performance and significant resource savings: with just 60% of the annotations, we achieve similar or better AP3D on KITTI validation and test set results compared to training the same detector on the whole dataset.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19301v1",
        "pdf": "https://arxiv.org/pdf/2511.19301v1"
      },
      "arxiv_id": "2511.19301v1",
      "comment": "",
      "journal_ref": "WACV 2026",
      "has_code": false
    },
    {
      "id": "2511.19299v1",
      "title": "Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning",
      "authors": [
        "James R. M. Black",
        "Moritz S. Hanke",
        "Aaron Maiwald",
        "Tina Hernandez-Boussard",
        "Oliver M. Crook",
        "Jaspreet Pannu"
      ],
      "abstract": "Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretraining data (i.e., removing viral genomic sequences from training datasets) in order to limit gLM performance on virus-related tasks. However, it is not currently known how robust this approach is for securing open-source models that can be fine-tuned using sensitive pathogen data. Here, we evaluate a state-of-the-art gLM, Evo 2, and perform fine-tuning using sequences from 110 harmful human-infecting viruses to assess the rescue of misuse-relevant predictive capabilities. The fine- tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences. The model fine-tuned on human-infecting viruses also identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no expo- sure to SARS-CoV-2 sequences during fine-tuning. This work demonstrates that data exclusion might be circumvented by fine-tuning approaches that can, to some degree, rescue misuse-relevant capabilities of gLMs. We highlight the need for safety frameworks for gLMs and outline further work needed on evaluations and mitigation measures to enable the safe deployment of gLMs.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19299v1",
        "pdf": "https://arxiv.org/pdf/2511.19299v1"
      },
      "arxiv_id": "2511.19299v1",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Biosecurity Safeguards for Generative AI",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19294v1",
      "title": "DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting",
      "authors": [
        "Phurtivilai Patt",
        "Leyang Huang",
        "Yinqiang Zhang",
        "Yang Lei"
      ],
      "abstract": "This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19294v1",
        "pdf": "https://arxiv.org/pdf/2511.19294v1"
      },
      "arxiv_id": "2511.19294v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19291v1",
      "title": "TorchQuantumDistributed",
      "authors": [
        "Oliver Knitter",
        "Jonathan Mei",
        "Masako Yamada",
        "Martin Roetteler"
      ],
      "abstract": "TorchQuantumDistributed (tqd) is a PyTorch-based [Paszke et al., 2019] library for accelerator-agnostic differentiable quantum state vector simulation at scale. This enables studying the behavior of learnable parameterized near-term and fault- tolerant quantum circuits with high qubit counts.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "quant-ph",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19291v1",
        "pdf": "https://arxiv.org/pdf/2511.19291v1"
      },
      "arxiv_id": "2511.19291v1",
      "comment": "12 pages, 4 figures, to appear in the AI for Science Workshop at NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19289v1",
      "title": "Performance Guarantees for Quantum Neural Estimation of Entropies",
      "authors": [
        "Sreejith Sreekumar",
        "Ziv Goldfeld",
        "Mark M. Wilde"
      ],
      "abstract": "Estimating quantum entropies and divergences is an important problem in quantum physics, information theory, and machine learning. Quantum neural estimators (QNEs), which utilize a hybrid classical-quantum architecture, have recently emerged as an appealing computational framework for estimating these measures. Such estimators combine classical neural networks with parametrized quantum circuits, and their deployment typically entails tedious tuning of hyperparameters controlling the sample size, network architecture, and circuit topology. This work initiates the study of formal guarantees for QNEs of measured (Rényi) relative entropies in the form of non-asymptotic error risk bounds. We further establish exponential tail bounds showing that the error is sub-Gaussian, and thus sharply concentrates about the ground truth value. For an appropriate sub-class of density operator pairs on a space of dimension $d$ with bounded Thompson metric, our theory establishes a copy complexity of $O(|Θ(\\mathcal{U})|d/ε^2)$ for QNE with a quantum circuit parameter set $Θ(\\mathcal{U})$, which has minimax optimal dependence on the accuracy $ε$. Additionally, if the density operator pairs are permutation invariant, we improve the dimension dependence above to $O(|Θ(\\mathcal{U})|\\mathrm{polylog}(d)/ε^2)$. Our theory aims to facilitate principled implementation of QNEs for measured relative entropies and guide hyperparameter tuning in practice.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "quant-ph",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19289v1",
        "pdf": "https://arxiv.org/pdf/2511.19289v1"
      },
      "arxiv_id": "2511.19289v1",
      "comment": "42+4 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19284v1",
      "title": "The Unified Non-Convex Framework for Robust Causal Inference: Overcoming the Gaussian Barrier and Optimization Fragility",
      "authors": [
        "Eichi Uehara"
      ],
      "abstract": "This document proposes a Unified Robust Framework that re-engineers the estimation of the Average Treatment Effect on the Overlap (ATO). It synthesizes gamma-Divergence for outlier robustness, Graduated Non-Convexity (GNC) for global optimization, and a \"Gatekeeper\" mechanism to address the impossibility of higher-order orthogonality in Gaussian regimes.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19284v1",
        "pdf": "https://arxiv.org/pdf/2511.19284v1"
      },
      "arxiv_id": "2511.19284v1",
      "comment": "10 pages, 1 table",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19283v1",
      "title": "Data Flows and Colonial Regimes in Africa: A Critical Analysis of the Colonial Futurities Embedded in AI Ecosystems",
      "authors": [
        "Ndaka. A",
        "Avila-Acosta. F",
        "Mbula-Ndaka. H",
        "Amera. C",
        "Chauke. S",
        "Majiwa. E"
      ],
      "abstract": "This chapter seeks to frame the elemental and invisible problems of AI and big data in the African context by examining digital sites and infrastructure through the lens of power and interests. It will present reflections on how these sites are using AI recommendation algorithms to recreate new digital societies in the region, how they have the potential to propagate algorithmic colonialism and negative gender norms, and what this means for the regional sustainable development agenda. The chapter proposes adopting business models that embrace response-ability and consider the existence of alternative socio-material worlds of AI. These reflections will mainly come from ongoing discussions with Kenyan social media users in this authors' user space talks, personal experiences and six months of active participant observations done by the authors.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19283v1",
        "pdf": "https://arxiv.org/pdf/2511.19283v1"
      },
      "arxiv_id": "2511.19283v1",
      "comment": "12 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19279v1",
      "title": "MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings",
      "authors": [
        "Victor Rambaud",
        "Salvador Mascarenhas",
        "Yair Lakretz"
      ],
      "abstract": "A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19279v1",
        "pdf": "https://arxiv.org/pdf/2511.19279v1"
      },
      "arxiv_id": "2511.19279v1",
      "comment": "19 pages (29 with appendix), 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19278v1",
      "title": "ReMatch: Boosting Representation through Matching for Multimodal Retrieval",
      "authors": [
        "Qianying Liu",
        "Xiao Liang",
        "Zhiqiang Zhang",
        "Yibo Chen",
        "Xu Tang",
        "Zhongfei Qing",
        "Fengfan Zhou",
        "Yao Hu",
        "Paul Henderson"
      ],
      "abstract": "We present ReMatch, a framework that leverages the generative strength of MLLMs for multimodal retrieval. Previous approaches treated an MLLM as a simple encoder, ignoring its generative nature, and under-utilising its compositional reasoning and world knowledge. We instead train the embedding MLLM end-to-end with a chat-style generative matching stage. The matching stage uses the same MLLM to autoregressively decide relevance from multi-view inputs, including both raw data and its own projected embeddings for each query and document. It provides instance-wise discrimination supervision that complements a standard contrastive loss, offering stronger gradients on hard negatives and preserving the compositional strengths of the original MLLM. To obtain semantically richer multimodal embeddings, we use multiple learnable tokens to augment each input, generating fine-grained contextual, mutually orthogonal embeddings with low inference cost. Leveraging our established high-performance baseline,we assemble the ideas mentioned above into a powerful training recipe and achieve a new state-of-the-art on the Massive Multimodal Embedding Benchmark (MMEB). Our experiments show particularly strong zero-shot generalization results on five datasets, highlighting the robustness and transferability of ReMatch.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19278v1",
        "pdf": "https://arxiv.org/pdf/2511.19278v1"
      },
      "arxiv_id": "2511.19278v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19277v1",
      "title": "Closing Gaps in Emissions Monitoring with Climate TRACE",
      "authors": [
        "Brittany V. Lancellotti",
        "Jordan M. Malof",
        "Aaron Davitt",
        "Gavin McCormick",
        "Shelby Anderson",
        "Pol Carbó-Mestre",
        "Gary Collins",
        "Verity Crane",
        "Zoheyr Doctor",
        "George Ebri",
        "Kevin Foster",
        "Trey M. Gowdy",
        "Michael Guzzardi",
        "John Heal",
        "Heather Hunter",
        "David Kroodsma",
        "Khandekar Mahammad Galib",
        "Paul J. Markakis",
        "Gavin McDonald",
        "Daniel P. Moore",
        "Eric D. Nguyen",
        "Sabina Parvu",
        "Michael Pekala",
        "Christine D. Piatko",
        "Amy Piscopo",
        "Mark Powell",
        "Krsna Raniga",
        "Elizabeth P. Reilly",
        "Michael Robinette",
        "Ishan Saraswat",
        "Patrick Sicurello",
        "Isabella Söldner-Rembold",
        "Raymond Song",
        "Charlotte Underwood",
        "Kyle Bradbury"
      ],
      "abstract": "Global greenhouse gas emissions estimates are essential for monitoring and mitigation planning. Yet most datasets lack one or more characteristics that enhance their actionability, such as accuracy, global coverage, high spatial and temporal resolution, and frequent updates. To address these gaps, we present Climate TRACE (climatetrace.org), an open-access platform delivering global emissions estimates with enhanced detail, coverage, and timeliness. Climate TRACE synthesizes existing emissions data, prioritizing accuracy, coverage, and resolution, and fills gaps using sector-specific estimation approaches. The dataset is the first to provide globally comprehensive emissions estimates for individual sources (e.g., individual power plants) for all anthropogenic emitting sectors. The dataset spans January 1, 2021, to the present, with a two-month reporting lag and monthly updates. The open-access platform enables non-technical audiences to engage with detailed emissions datasets for most subnational governments worldwide. Climate TRACE supports data-driven climate action at scales where decisions are made, representing a major breakthrough for emissions accounting and mitigation.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19277v1",
        "pdf": "https://arxiv.org/pdf/2511.19277v1"
      },
      "arxiv_id": "2511.19277v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19275v1",
      "title": "Dynamic Multi-Species Bird Soundscape Generation with Acoustic Patterning and 3D Spatialization",
      "authors": [
        "Ellie L. Zhang",
        "Duoduo Liao",
        "Callie C. Liao"
      ],
      "abstract": "Generation of dynamic, scalable multi-species bird soundscapes remains a significant challenge in computer music and algorithmic sound design. Birdsongs involve rapid frequency-modulated chirps, complex amplitude envelopes, distinctive acoustic patterns, overlapping calls, and dynamic inter-bird interactions, all of which require precise temporal and spatial control in 3D environments. Existing approaches, whether Digital Signal Processing (DSP)-based or data-driven, typically focus only on single species modeling, static call structures, or synthesis directly from recordings, and often suffer from noise, limited flexibility, or large data needs. To address these challenges, we present a novel, fully algorithm-driven framework that generates dynamic multi-species bird soundscapes using DSP-based chirp generation and 3D spatialization, without relying on recordings or training data. Our approach simulates multiple independently-moving birds per species along different moving 3D trajectories, supporting controllable chirp sequences, overlapping choruses, and realistic 3D motion in scalable soundscapes while preserving species-specific acoustic patterns. A visualization interface provides bird trajectories, spectrograms, activity timelines, and sound waves for analytical and creative purposes. Both visual and audio evaluations demonstrate the ability of the system to generate dense, immersive, and ecologically inspired soundscapes, highlighting its potential for computer music, interactive virtual environments, and computational bioacoustics research.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19275v1",
        "pdf": "https://arxiv.org/pdf/2511.19275v1"
      },
      "arxiv_id": "2511.19275v1",
      "comment": "Accepted by IEEE Big Data 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19274v1",
      "title": "Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection",
      "authors": [
        "Mingyang Chen",
        "Jiawei Du",
        "Bo Huang",
        "Yi Wang",
        "Xiaobo Zhang",
        "Wei Wang"
      ],
      "abstract": "Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19274v1",
        "pdf": "https://arxiv.org/pdf/2511.19274v1"
      },
      "arxiv_id": "2511.19274v1",
      "comment": "Accepted by AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19273v1",
      "title": "Scalable Bayesian Network Structure Learning Using Tsetlin Machine to Constrain the Search Space",
      "authors": [
        "Kunal Dumbre",
        "Lei Jiao",
        "Ole-Christoffer Granmo"
      ],
      "abstract": "The PC algorithm is a widely used method in causal inference for learning the structure of Bayesian networks. Despite its popularity, the PC algorithm suffers from significant time complexity, particularly as the size of the dataset increases, which limits its applicability in large-scale real-world problems. In this study, we propose a novel approach that utilises the Tsetlin Machine (TM) to construct Bayesian structures more efficiently. Our method leverages the most significant literals extracted from the TM and performs conditional independence (CI) tests on these selected literals instead of the full set of variables, resulting in a considerable reduction in computational time. We implemented our approach and compared it with various state-of-the-art methods. Our evaluation includes categorical datasets from the bnlearn repository, such as Munin1, Hepar2. The findings indicate that the proposed TM-based method not only reduces computational complexity but also maintains competitive accuracy in causal discovery, making it a viable alternative to traditional PC algorithm implementations by offering improved efficiency without compromising performance.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19273v1",
        "pdf": "https://arxiv.org/pdf/2511.19273v1"
      },
      "arxiv_id": "2511.19273v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19272v1",
      "title": "Tiny-TSM: Efficiently Training a Lightweight SOTA Time Series Foundation Model",
      "authors": [
        "Felix Birkel"
      ],
      "abstract": "We present Tiny-TSM, a time series foundation model characterized by small scale, economical training, and state-of-the-art performance. It comprises 23M total parameters, trained on a single A100 GPU in less than a week using a new synthetic data generation and data augmentation pipeline (SynthTS). Without any neural architecture search, hyperparameter tuning, or scaling up model size, Tiny-TSM achieves state-of-the-art performance on a wide range of time series benchmark datasets, often outperforming much larger models and even matching the performance of much larger, industrial-scale, likely highly tuned foundation models. Specifically, Tiny-TSM outperforms all other time series foundation models we evaluated on medium- and long-term forecasting tasks under MSE loss, while short-term accuracy is still competitive with state-of-the-art models.\n  We also introduce a causal input normalization scheme that enables time series models to be trained with dense next-token prediction loss, significantly accelerating convergence speed and reducing training time.\n  All experiments were conducted on a single A100 GPU, illustrating the practicality of the proposed approach in a resource-constrained setting.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19272v1",
        "pdf": "https://arxiv.org/pdf/2511.19272v1"
      },
      "arxiv_id": "2511.19272v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19269v1",
      "title": "CDLM: Consistency Diffusion Language Models For Faster Sampling",
      "authors": [
        "Minseo Kim",
        "Chenfeng Xu",
        "Coleman Hooper",
        "Harman Singh",
        "Ben Athiwaratkun",
        "Ce Zhang",
        "Kurt Keutzer",
        "Amir Gholami"
      ],
      "abstract": "Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19269v1",
        "pdf": "https://arxiv.org/pdf/2511.19269v1"
      },
      "arxiv_id": "2511.19269v1",
      "comment": "18 pages, 6 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19268v1",
      "title": "BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment",
      "authors": [
        "Dewei Zhou",
        "Mingwei Li",
        "Zongxin Yang",
        "Yu Lu",
        "Yunqiu Xu",
        "Zhizhong Wang",
        "Zeyi Huang",
        "Yi Yang"
      ],
      "abstract": "Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources. These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text. Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide. Preference-based optimization techniques like Direct Preference Optimization (DPO) show promise but are limited by gradient entanglement between text and condition signals and lack disentangled training data for multi-constraint tasks. To overcome this, we propose a bidirectionally decoupled DPO framework (BideDPO). Our method creates two disentangled preference pairs-one for the condition and one for the text-to reduce gradient entanglement. The influence of pairs is managed using an Adaptive Loss Balancing strategy for balanced optimization. We introduce an automated data pipeline to sample model outputs and generate conflict-aware data. This process is embedded in an iterative optimization strategy that refines both the model and the data. We construct a DualAlign benchmark to evaluate conflict resolution between text and condition. Experiments show BideDPO significantly improves text success rates (e.g., +35%) and condition adherence. We also validate our approach using the COCO dataset. Project Pages: https://limuloo.github.io/BideDPO/.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19268v1",
        "pdf": "https://arxiv.org/pdf/2511.19268v1"
      },
      "arxiv_id": "2511.19268v1",
      "comment": "29 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19267v1",
      "title": "Leveraging Spatiotemporal Graph Neural Networks for Multi-Store Sales Forecasting",
      "authors": [
        "Manish Singh",
        "Arpita Dayama"
      ],
      "abstract": "This work evaluates the effectiveness of spatiotemporal Graph Neural Networks (GNNs) for multi-store retail sales forecasting and compares their performance against ARIMA, LSTM, and XGBoost baselines. Using weekly sales data from 45 Walmart stores, we construct a relational forecasting framework that models inter-store dependencies through a learned adaptive graph. The proposed STGNN predicts log-differenced sales and reconstructs final values through a residual path, enabling stable training and improved generalisation. Experiments show that STGNN achieves the lowest overall forecasting error, outperforming all baselines in Normalised Total Absolute Error, P90 MAPE, and variance of MAPE across stores. Analysis of the learned adjacency matrix reveals meaningful functional store clusters and high-influence nodes that emerge without geographic metadata. These results demonstrate that relational structure significantly improves forecast quality in interconnected retail environments and establishes STGNNs as a robust modelling choice for multi-store demand prediction.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19267v1",
        "pdf": "https://arxiv.org/pdf/2511.19267v1"
      },
      "arxiv_id": "2511.19267v1",
      "comment": "6 pages, 4 figures, 1 table",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19265v1",
      "title": "Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks",
      "authors": [
        "Bianka Kowalska",
        "Halina Kwaśnicka"
      ],
      "abstract": "The black box nature of deep neural networks poses a significant challenge for the deployment of transparent and trustworthy artificial intelligence (AI) systems. With the growing presence of AI in society, it becomes increasingly important to develop methods that can explain and interpret the decisions made by these systems. To address this, mechanistic interpretability (MI) emerged as a promising and distinctive research program within the broader field of explainable artificial intelligence (XAI). MI is the process of studying the inner computations of neural networks and translating them into human-understandable algorithms. It encompasses reverse engineering techniques aimed at uncovering the computational algorithms implemented by neural networks. In this article, we propose a unified taxonomy of MI approaches and provide a detailed analysis of key techniques, illustrated with concrete examples and pseudo-code. We contextualize MI within the broader interpretability landscape, comparing its goals, methods, and insights to other strands of XAI. Additionally, we trace the development of MI as a research area, highlighting its conceptual roots and the accelerating pace of recent work. We argue that MI holds significant potential to support a more scientific understanding of machine learning systems -- treating models not only as tools for solving tasks, but also as systems to be studied and understood. We hope to invite new researchers into the field of mechanistic interpretability.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19265v1",
        "pdf": "https://arxiv.org/pdf/2511.19265v1"
      },
      "arxiv_id": "2511.19265v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19264v1",
      "title": "Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry",
      "authors": [
        "Amirtha Varshini A S",
        "Duminda S. Ranasinghe",
        "Hok Hei Tam"
      ],
      "abstract": "Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19264v1",
        "pdf": "https://arxiv.org/pdf/2511.19264v1"
      },
      "arxiv_id": "2511.19264v1",
      "comment": "13 pages, 7 figures. Accepted for presentation at NeurIPS 2025 WiML Workshop and Molecular Machine Learning Conference (MoML) 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19263v1",
      "title": "Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention",
      "authors": [
        "Lucas Li",
        "Jean-Baptiste Puel",
        "Florence Carton",
        "Dounya Barrit",
        "Jhony H. Giraldo"
      ],
      "abstract": "Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19263v1",
        "pdf": "https://arxiv.org/pdf/2511.19263v1"
      },
      "arxiv_id": "2511.19263v1",
      "comment": "Accepted at the AI for Accelerated Materials Design (AI4Mat) Workshop at NeurIPS 2025. 14 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19262v1",
      "title": "Psychometric Tests for AI Agents and Their Moduli Space",
      "authors": [
        "Przemyslaw Chojecki"
      ],
      "abstract": "We develop a moduli-theoretic view of psychometric test batteries for AI agents and connect it explicitly to the AAI score developed previously. First, we make precise the notion of an AAI functional on a battery and set out axioms that any reasonable autonomy/general intelligence score should satisfy. Second, we show that the composite index ('AAI-Index') defined previously is a special case of our AAI functional. Third, we introduce the notion of a cognitive core of an agent relative to a battery and define the associated AAI$_{\\textrm{core}}$ score as the restriction of an AAI functional to that core. Finally, we use these notions to describe invariants of batteries under evaluation-preserving symmetries and outline how moduli of equivalent batteries are organized.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19262v1",
        "pdf": "https://arxiv.org/pdf/2511.19262v1"
      },
      "arxiv_id": "2511.19262v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19261v1",
      "title": "LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models",
      "authors": [
        "Shuai Wang",
        "Daoan Zhang",
        "Tianyi Bai",
        "Shitong Shao",
        "Jiebo Luo",
        "Jiaheng Wei"
      ],
      "abstract": "Humans can perceive and understand 3D space and long videos from sequential visual observations. But do vision-language models (VLMs) can? Recent work demonstrates that even state-of-the-art VLMs still struggle to understand 3D space and long videos, although they are powerful in typical vision-language tasks. Current methods often rely on specialized architectural designs to improve performance for 3D tasks and video understanding tasks separately. In contrast, we propose LAST, short for LeArn to Think in Space and Time, to jointly improve 3D spatial and long video understanding for general VLMs with only a set of 2D images as inputs. LAST makes VLMs think in space and time rather than only with text before giving the final answer, building visual thinking trajectories in 3D space and temporal dimension. We demonstrate the effectiveness of LAST in two scenarios: 1) zero-shot, where we directly prompt proprietary models; and 2) fine-tuning general VLMs with data that include thinking trajectories in 3D space and time. We show that LAST brings substantial gains in various benchmarks, including 3 spatial understanding, 4 video understanding, and 3 image understanding tasks. Notably, 15.8% gains on EgoSchema with GPT-4o in a zero-shot manner and 8.3 gains on VSI-Bench compared with Qwen2.5-VL-7B.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19261v1",
        "pdf": "https://arxiv.org/pdf/2511.19261v1"
      },
      "arxiv_id": "2511.19261v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19260v1",
      "title": "A Nutrition Multimodal Photoplethysmography Language Model",
      "authors": [
        "Kyle Verrier",
        "Achille Nazaret",
        "Joseph Futoma",
        "Andrew C. Miller",
        "Guillermo Sapiro"
      ],
      "abstract": "Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19260v1",
        "pdf": "https://arxiv.org/pdf/2511.19260v1"
      },
      "arxiv_id": "2511.19260v1",
      "comment": "21 pages, 2 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19257v1",
      "title": "Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation",
      "authors": [
        "Yingjia Shang",
        "Yi Liu",
        "Huimin Wang",
        "Furong Li",
        "Wenfang Sun",
        "Wu Chengyu",
        "Yefeng Zheng"
      ],
      "abstract": "With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at https://anonymous.4open.science/r/MMed-RAG-Attack-F05A.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19257v1",
        "pdf": "https://arxiv.org/pdf/2511.19257v1"
      },
      "arxiv_id": "2511.19257v1",
      "comment": "Accepted at KDD 2026 First Cycle (full version). Authors marked with * contributed equally. Yi Liu is the lead author",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19256v1",
      "title": "SimDiff: Simpler Yet Better Diffusion Model for Time Series Point Forecasting",
      "authors": [
        "Hang Ding",
        "Xue Wang",
        "Tian Zhou",
        "Tao Yao"
      ],
      "abstract": "Diffusion models have recently shown promise in time series forecasting, particularly for probabilistic predictions. However, they often fail to achieve state-of-the-art point estimation performance compared to regression-based methods. This limitation stems from difficulties in providing sufficient contextual bias to track distribution shifts and in balancing output diversity with the stability and precision required for point forecasts. Existing diffusion-based approaches mainly focus on full-distribution modeling under probabilistic frameworks, often with likelihood maximization objectives, while paying little attention to dedicated strategies for high-accuracy point estimation. Moreover, other existing point prediction diffusion methods frequently rely on pre-trained or jointly trained mature models for contextual bias, sacrificing the generative flexibility of diffusion models.\n  To address these challenges, we propose SimDiff, a single-stage, end-to-end framework. SimDiff employs a single unified Transformer network carefully tailored to serve as both denoiser and predictor, eliminating the need for external pre-trained or jointly trained regressors. It achieves state-of-the-art point estimation performance by leveraging intrinsic output diversity and improving mean squared error accuracy through multiple inference ensembling. Key innovations, including normalization independence and the median-of-means estimator, further enhance adaptability and stability. Extensive experiments demonstrate that SimDiff significantly outperforms existing methods in time series point forecasting.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19256v1",
        "pdf": "https://arxiv.org/pdf/2511.19256v1"
      },
      "arxiv_id": "2511.19256v1",
      "comment": "Accepted by AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19254v1",
      "title": "Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation",
      "authors": [
        "Mohamed Rissal Hedna",
        "Sesugh Samuel Nder"
      ],
      "abstract": "Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19254v1",
        "pdf": "https://arxiv.org/pdf/2511.19254v1"
      },
      "arxiv_id": "2511.19254v1",
      "comment": "9 pages, 5 figures, 1 algorithm",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19253v1",
      "title": "MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization",
      "authors": [
        "Boyuan Wu"
      ],
      "abstract": "Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19253v1",
        "pdf": "https://arxiv.org/pdf/2511.19253v1"
      },
      "arxiv_id": "2511.19253v1",
      "comment": "Preprint. 16 pages, 6 figures. Preliminary version; extended experiments and analysis forthcoming",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19248v1",
      "title": "FedPoisonTTP: A Threat Model and Poisoning Attack for Federated Test-Time Personalization",
      "authors": [
        "Md Akil Raihan Iftee",
        "Syed Md. Ahnaf Hasan",
        "Amin Ahsan Ali",
        "AKM Mahbubur Rahman",
        "Sajib Mistry",
        "Aneesh Krishna"
      ],
      "abstract": "Test-time personalization in federated learning enables models at clients to adjust online to local domain shifts, enhancing robustness and personalization in deployment. Yet, existing federated learning work largely overlooks the security risks that arise when local adaptation occurs at test time. Heterogeneous domain arrivals, diverse adaptation algorithms, and limited cross-client visibility create vulnerabilities where compromised participants can craft poisoned inputs and submit adversarial updates that undermine both global and per-client performance. To address this threat, we introduce FedPoisonTTP, a realistic grey-box attack framework that explores test-time data poisoning in the federated adaptation setting. FedPoisonTTP distills a surrogate model from adversarial queries, synthesizes in-distribution poisons using feature-consistency, and optimizes attack objectives to generate high-entropy or class-confident poisons that evade common adaptation filters. These poisons are injected during local adaptation and spread through collaborative updates, leading to broad degradation. Extensive experiments on corrupted vision benchmarks show that compromised participants can substantially diminish overall test-time performance.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19248v1",
        "pdf": "https://arxiv.org/pdf/2511.19248v1"
      },
      "arxiv_id": "2511.19248v1",
      "comment": "13 pages, 3 figures, 2 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19246v1",
      "title": "Neural Architecture Search for Quantum Autoencoders",
      "authors": [
        "Hibah Agha",
        "Samuel Yen-Chi Chen",
        "Huan-Hsin Tseng",
        "Shinjae Yoo"
      ],
      "abstract": "In recent years, machine learning and deep learning have driven advances in domains such as image classification, speech recognition, and anomaly detection by leveraging multi-layer neural networks to model complex data. Simultaneously, quantum computing (QC) promises to address classically intractable problems via quantum parallelism, motivating research in quantum machine learning (QML). Among QML techniques, quantum autoencoders show promise for compressing high-dimensional quantum and classical data. However, designing effective quantum circuit architectures for quantum autoencoders remains challenging due to the complexity of selecting gates, arranging circuit layers, and tuning parameters.\n  This paper proposes a neural architecture search (NAS) framework that automates the design of quantum autoencoders using a genetic algorithm (GA). By systematically evolving variational quantum circuit (VQC) configurations, our method seeks to identify high-performing hybrid quantum-classical autoencoders for data reconstruction without becoming trapped in local minima. We demonstrate effectiveness on image datasets, highlighting the potential of quantum autoencoders for efficient feature extraction within a noise-prone, near-term quantum era. Our approach lays a foundation for broader application of genetic algorithms to quantum architecture search, aiming for a robust, automated method that can adapt to varied data and hardware constraints.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19246v1",
        "pdf": "https://arxiv.org/pdf/2511.19246v1"
      },
      "arxiv_id": "2511.19246v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19241v1",
      "title": "Local Entropy Search over Descent Sequences for Bayesian Optimization",
      "authors": [
        "David Stenger",
        "Armin Lindicke",
        "Alexander von Rohr",
        "Sebastian Trimpe"
      ],
      "abstract": "Searching large and complex design spaces for a global optimum can be infeasible and unnecessary. A practical alternative is to iteratively refine the neighborhood of an initial design using local optimization methods such as gradient descent. We propose local entropy search (LES), a Bayesian optimization paradigm that explicitly targets the solutions reachable by the descent sequences of iterative optimizers. The algorithm propagates the posterior belief over the objective through the optimizer, resulting in a probability distribution over descent sequences. It then selects the next evaluation by maximizing mutual information with that distribution, using a combination of analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results on high-complexity synthetic objectives and benchmark problems show that LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19241v1",
        "pdf": "https://arxiv.org/pdf/2511.19241v1"
      },
      "arxiv_id": "2511.19241v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19240v1",
      "title": "Empirical Comparison of Forgetting Mechanisms for UCB-based Algorithms on a Data-Driven Simulation Platform",
      "authors": [
        "Minxin Chen"
      ],
      "abstract": "Many real-world bandit problems involve non-stationary reward distributions, where the optimal decision may shift due to evolving environments. However, the performance of some typical Multi-Armed Bandit (MAB) models such as Upper Confidence Bound (UCB) algorithms degrades significantly in non-stationary environments where reward distributions change over time. To address this limitation, this paper introduces and evaluates FDSW-UCB, a novel dual-view algorithm that integrates a discount-based long-term perspective with a sliding-window-based short-term view. A data-driven semi-synthetic simulation platform, built upon the MovieLens-1M and Open Bandit datasets, is developed to test algorithm adaptability under abrupt and gradual drift scenarios. Experimental results demonstrate that a well-configured sliding-window mechanism (SW-UCB) is robust, while the widely used discounting method (D-UCB) suffers from a fundamental learning failure, leading to linear regret. Crucially, the proposed FDSW-UCB, when employing an optimistic aggregation strategy, achieves superior performance in dynamic settings, highlighting that the ensemble strategy itself is a decisive factor for success.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19240v1",
        "pdf": "https://arxiv.org/pdf/2511.19240v1"
      },
      "arxiv_id": "2511.19240v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19236v1",
      "title": "SENTINEL: A Fully End-to-End Language-Action Model for Humanoid Whole Body Control",
      "authors": [
        "Yuxuan Wang",
        "Haobin Jiang",
        "Shiqing Yao",
        "Ziluo Ding",
        "Zongqing Lu"
      ],
      "abstract": "Existing humanoid control systems often rely on teleoperation or modular generation pipelines that separate language understanding from physical execution. However, the former is entirely human-driven, and the latter lacks tight alignment between language commands and physical behaviors. In this paper, we present SENTINEL, a fully end-to-end language-action model for humanoid whole-body control. We construct a large-scale dataset by tracking human motions in simulation using a pretrained whole body controller, combined with their text annotations. The model directly maps language commands and proprioceptive inputs to low-level actions without any intermediate representation. The model generates action chunks using flow matching, which can be subsequently refined by a residual action head for real-world deployment. Our method exhibits strong semantic understanding and stable execution on humanoid robots in both simulation and real-world deployment, and also supports multi-modal extensions by converting inputs into texts.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19236v1",
        "pdf": "https://arxiv.org/pdf/2511.19236v1"
      },
      "arxiv_id": "2511.19236v1",
      "comment": "23 pages, 8 figures, 11 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19235v1",
      "title": "IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes",
      "authors": [
        "Carl Lindström",
        "Mahan Rafidashti",
        "Maryam Fatemi",
        "Lars Hammarstrand",
        "Martin R. Oswald",
        "Lennart Svensson"
      ],
      "abstract": "Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19235v1",
        "pdf": "https://arxiv.org/pdf/2511.19235v1"
      },
      "arxiv_id": "2511.19235v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19232v1",
      "title": "In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations",
      "authors": [
        "Christos-Nikolaos Zacharopoulos",
        "Revekka Kyriakoglou"
      ],
      "abstract": "How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19232v1",
        "pdf": "https://arxiv.org/pdf/2511.19232v1"
      },
      "arxiv_id": "2511.19232v1",
      "comment": "Accepted at AICS2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19229v1",
      "title": "Learning Plug-and-play Memory for Guiding Video Diffusion Models",
      "authors": [
        "Selena Song",
        "Ziming Xu",
        "Zijun Zhang",
        "Kun Zhou",
        "Jiaxian Guo",
        "Lianhui Qin",
        "Biwei Huang"
      ],
      "abstract": "Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19229v1",
        "pdf": "https://arxiv.org/pdf/2511.19229v1"
      },
      "arxiv_id": "2511.19229v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19221v1",
      "title": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving",
      "authors": [
        "Jianhua Han",
        "Meng Tian",
        "Jiangtong Zhu",
        "Fan He",
        "Huixin Zhang",
        "Sitong Guo",
        "Dechang Zhu",
        "Hao Tang",
        "Pei Xu",
        "Yuze Guo",
        "Minzhe Niu",
        "Haojie Zhu",
        "Qichao Dong",
        "Xuechao Yan",
        "Siyuan Dong",
        "Lu Hou",
        "Qingqiu Huang",
        "Xiaosong Jia",
        "Hang Xu"
      ],
      "abstract": "Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19221v1",
        "pdf": "https://arxiv.org/pdf/2511.19221v1"
      },
      "arxiv_id": "2511.19221v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19220v1",
      "title": "Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering",
      "authors": [
        "Federico Felizzi",
        "Olivia Riccomi",
        "Michele Ferramola",
        "Francesco Andrea Causio",
        "Manuel Del Medico",
        "Vittorio De Vita",
        "Lorenzo De Mori",
        "Alessandra Piscitelli Pietro Eric Risuleo",
        "Bianca Destro Castaniti",
        "Antonio Cristiano Alessia Longo",
        "Luigi De Angelis",
        "Mariapia Vassalli",
        "Marcello Di Pumpo"
      ],
      "abstract": "Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19220v1",
        "pdf": "https://arxiv.org/pdf/2511.19220v1"
      },
      "arxiv_id": "2511.19220v1",
      "comment": "Accepted at the Workshop on Multimodal Representation Learning for Healthcare (MMRL4H), EurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19218v1",
      "title": "Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization",
      "authors": [
        "Xurui Li",
        "Kaisong Song",
        "Rui Zhu",
        "Pin-Yu Chen",
        "Haixu Tang"
      ],
      "abstract": "Large Language Models (LLMs) have developed rapidly in web services, delivering unprecedented capabilities while amplifying societal risks. Existing works tend to focus on either isolated jailbreak attacks or static defenses, neglecting the dynamic interplay between evolving threats and safeguards in real-world web contexts. To mitigate these challenges, we propose ACE-Safety (Adversarial Co-Evolution for LLM Safety), a novel framework that jointly optimize attack and defense models by seamlessly integrating two key innovative procedures: (1) Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS), which efficiently explores jailbreak strategies to uncover vulnerabilities and generate diverse adversarial samples; (2) Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO), which jointly trains attack and defense LLMs with challenging samples via curriculum reinforcement learning, enabling robust mutual improvement. Evaluations across multiple benchmarks demonstrate that our method outperforms existing attack and defense approaches, and provides a feasible pathway for developing LLMs that can sustainably support responsible AI ecosystems.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19218v1",
        "pdf": "https://arxiv.org/pdf/2511.19218v1"
      },
      "arxiv_id": "2511.19218v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19217v1",
      "title": "ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment",
      "authors": [
        "Wanjiang Weng",
        "Xiaofeng Tan",
        "Junbo Wang",
        "Guo-Sen Xie",
        "Pan Zhou",
        "Hongsong Wang"
      ],
      "abstract": "Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19217v1",
        "pdf": "https://arxiv.org/pdf/2511.19217v1"
      },
      "arxiv_id": "2511.19217v1",
      "comment": "Accepted by AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19202v1",
      "title": "NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting",
      "authors": [
        "Brent Zoomers",
        "Florian Hahlbohm",
        "Joni Vanherck",
        "Lode Jorissen",
        "Marcus Magnor",
        "Nick Michiels"
      ],
      "abstract": "3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19202v1",
        "pdf": "https://arxiv.org/pdf/2511.19202v1"
      },
      "arxiv_id": "2511.19202v1",
      "comment": "15 pages, 13 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19200v1",
      "title": "Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?",
      "authors": [
        "Itay Cohen",
        "Ethan Fetaya",
        "Amir Rosenfeld"
      ],
      "abstract": "Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired \"real\"/\"lookalike\" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19200v1",
        "pdf": "https://arxiv.org/pdf/2511.19200v1"
      },
      "arxiv_id": "2511.19200v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19199v1",
      "title": "CLASH: A Benchmark for Cross-Modal Contradiction Detection",
      "authors": [
        "Teodora Popordanoska",
        "Jiameng Li",
        "Matthew B. Blaschko"
      ],
      "abstract": "Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19199v1",
        "pdf": "https://arxiv.org/pdf/2511.19199v1"
      },
      "arxiv_id": "2511.19199v1",
      "comment": "First two authors contributed equally",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19198v1",
      "title": "Three-Dimensional Anatomical Data Generation Based on Artificial Neural Networks",
      "authors": [
        "Ann-Sophia Müller",
        "Moonkwang Jeong",
        "Meng Zhang",
        "Jiyuan Tian",
        "Arkadiusz Miernik",
        "Stefanie Speidel",
        "Tian Qiu"
      ],
      "abstract": "Surgical planning and training based on machine learning requires a large amount of 3D anatomical models reconstructed from medical imaging, which is currently one of the major bottlenecks. Obtaining these data from real patients and during surgery is very demanding, if even possible, due to legal, ethical, and technical challenges. It is especially difficult for soft tissue organs with poor imaging contrast, such as the prostate. To overcome these challenges, we present a novel workflow for automated 3D anatomical data generation using data obtained from physical organ models. We additionally use a 3D Generative Adversarial Network (GAN) to obtain a manifold of 3D models useful for other downstream machine learning tasks that rely on 3D data. We demonstrate our workflow using an artificial prostate model made of biomimetic hydrogels with imaging contrast in multiple zones. This is used to physically simulate endoscopic surgery. For evaluation and 3D data generation, we place it into a customized ultrasound scanner that records the prostate before and after the procedure. A neural network is trained to segment the recorded ultrasound images, which outperforms conventional, non-learning-based computer vision techniques in terms of intersection over union (IoU). Based on the segmentations, a 3D mesh model is reconstructed, and performance feedback is provided.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19198v1",
        "pdf": "https://arxiv.org/pdf/2511.19198v1"
      },
      "arxiv_id": "2511.19198v1",
      "comment": "6 pages, 4 figures, 1 table, IEEE International Conference on Intelligent Robots and Systems (IROS)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19187v1",
      "title": "SpectraNet: FFT-assisted Deep Learning Classifier for Deepfake Face Detection",
      "authors": [
        "Nithira Jayarathne",
        "Naveen Basnayake",
        "Keshawa Jayasundara",
        "Pasindu Dodampegama",
        "Praveen Wijesinghe",
        "Hirushika Pelagewatta",
        "Kavishka Abeywardana",
        "Sandushan Ranaweera",
        "Chamira Edussooriya"
      ],
      "abstract": "Detecting deepfake images is crucial in combating misinformation. We present a lightweight, generalizable binary classification model based on EfficientNet-B6, fine-tuned with transformation techniques to address severe class imbalances. By leveraging robust preprocessing, oversampling, and optimization strategies, our model achieves high accuracy, stability, and generalization. While incorporating Fourier transform-based phase and amplitude features showed minimal impact, our proposed framework helps non-experts to effectively identify deepfake images, making significant strides toward accessible and reliable deepfake detection.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19187v1",
        "pdf": "https://arxiv.org/pdf/2511.19187v1"
      },
      "arxiv_id": "2511.19187v1",
      "comment": "4 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19184v1",
      "title": "Torsion-Space Diffusion for Protein Backbone Generation with Geometric Refinement",
      "authors": [
        "Lakshaditya Singh",
        "Adwait Shelke",
        "Divyansh Agrawal"
      ],
      "abstract": "Designing new protein structures is fundamental to computational biology, enabling advances in therapeutic molecule discovery and enzyme engineering. Existing diffusion-based generative models typically operate in Cartesian coordinate space, where adding noise disrupts strict geometric constraints such as fixed bond lengths and angles, often producing physically invalid structures. To address this limitation, we propose a Torsion-Space Diffusion Model that generates protein backbones by denoising torsion angles, ensuring perfect local geometry by construction. A differentiable forward-kinematics module reconstructs 3D coordinates with fixed 3.8 Angstrom backbone bond lengths while a constrained post-processing refinement optimizes global compactness via Radius of Gyration (Rg) correction, without violating bond constraints. Experiments on standard PDB proteins demonstrate 100% bond-length accuracy and significantly improved structural compactness, reducing Rg error from 70% to 18.6% compared to Cartesian diffusion baselines. Overall, this hybrid torsion-diffusion plus geometric-refinement framework generates physically valid and compact protein backbones, providing a promising path toward full-atom protein generation.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19184v1",
        "pdf": "https://arxiv.org/pdf/2511.19184v1"
      },
      "arxiv_id": "2511.19184v1",
      "comment": "5 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19183v1",
      "title": "nnActive: A Framework for Evaluation of Active Learning in 3D Biomedical Segmentation",
      "authors": [
        "Carsten T. Lüth",
        "Jeremias Traub",
        "Kim-Celine Kahl",
        "Till J. Bungert",
        "Lukas Klein",
        "Lars Krämer",
        "Paul F. Jaeger",
        "Fabian Isensee",
        "Klaus Maier-Hein"
      ],
      "abstract": "Semantic segmentation is crucial for various biomedical applications, yet its reliance on large annotated datasets presents a bottleneck due to the high cost and specialized expertise required for manual labeling. Active Learning (AL) aims to mitigate this challenge by querying only the most informative samples, thereby reducing annotation effort. However, in the domain of 3D biomedical imaging, there is no consensus on whether AL consistently outperforms Random sampling. Four evaluation pitfalls hinder the current methodological assessment. These are (1) restriction to too few datasets and annotation budgets, (2) using 2D models on 3D images without partial annotations, (3) Random baseline not being adapted to the task, and (4) measuring annotation cost only in voxels. In this work, we introduce nnActive, an open-source AL framework that overcomes these pitfalls by (1) means of a large scale study spanning four biomedical imaging datasets and three label regimes, (2) extending nnU-Net by using partial annotations for training with 3D patch-based query selection, (3) proposing Foreground Aware Random sampling strategies tackling the foreground-background class imbalance of medical images and (4) propose the foreground efficiency metric, which captures the low annotation cost of background-regions. We reveal the following findings: (A) while all AL methods outperform standard Random sampling, none reliably surpasses an improved Foreground Aware Random sampling; (B) benefits of AL depend on task specific parameters; (C) Predictive Entropy is overall the best performing AL method, but likely requires the most annotation effort; (D) AL performance can be improved with more compute intensive design choices. As a holistic, open-source framework, nnActive can serve as a catalyst for research and application of AL in 3D biomedical imaging. Code is at: https://github.com/MIC-DKFZ/nnActive",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19183v1",
        "pdf": "https://arxiv.org/pdf/2511.19183v1"
      },
      "arxiv_id": "2511.19183v1",
      "comment": "Accepted at TMLR",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19180v1",
      "title": "Evaluating Deep Learning and Traditional Approaches Used in Source Camera Identification",
      "authors": [
        "Mansur Ozaman"
      ],
      "abstract": "One of the most important tasks in computer vision is identifying the device using which the image was taken, useful for facilitating further comprehensive analysis of the image. This paper presents comparative analysis of three techniques used in source camera identification (SCI): Photo Response Non-Uniformity (PRNU), JPEG compression artifact analysis, and convolutional neural networks (CNNs). It evaluates each method in terms of device classification accuracy. Furthermore, the research discusses the possible scientific development needed for the implementation of the methods in real-life scenarios.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19180v1",
        "pdf": "https://arxiv.org/pdf/2511.19180v1"
      },
      "arxiv_id": "2511.19180v1",
      "comment": "4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.19176v1",
      "title": "From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation",
      "authors": [
        "Jeeho Shin",
        "Kyungho Kim",
        "Kijung Shin"
      ],
      "abstract": "Recipe recommendation has become an essential task in web-based food platforms. A central challenge is effectively leveraging rich multimodal features beyond user-recipe interactions. Our analysis shows that even simple uses of multimodal signals yield competitive performance, suggesting that systematic enhancement of these signals is highly promising. We propose TESMR, a 3-stage framework for recipe recommendation that progressively refines raw multimodal features into effective embeddings through: (1) content-based enhancement using foundation models with multimodal comprehension, (2) relation-based enhancement via message propagation over user-recipe interactions, and (3) learning-based enhancement through contrastive learning with learnable embeddings. Experiments on two real-world datasets show that TESMR outperforms existing methods, achieving 7-15% higher Recall@10.",
      "published": "2025-11-24",
      "updated": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.19176v1",
        "pdf": "https://arxiv.org/pdf/2511.19176v1"
      },
      "arxiv_id": "2511.19176v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    }
  ]
}