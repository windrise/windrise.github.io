{
  "fetched_at": "2025-11-27T00:25:08.842664",
  "total_papers": 100,
  "papers": [
    {
      "id": "2511.20651v1",
      "title": "RubricRL: Simple Generalizable Rewards for Text-to-Image Generation",
      "authors": [
        "Xuelu Feng",
        "Yunsheng Li",
        "Ziyu Wan",
        "Zixuan Gao",
        "Junsong Yuan",
        "Dongdong Chen",
        "Chunming Qiao"
      ],
      "abstract": "Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20651v1",
        "pdf": "https://arxiv.org/pdf/2511.20651v1"
      },
      "arxiv_id": "2511.20651v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20650v1",
      "title": "MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities",
      "authors": [
        "Tooba Tehreem Sheikh",
        "Jean Lahoud",
        "Rao Muhammad Anwer",
        "Fahad Shahbaz Khan",
        "Salman Khan",
        "Hisham Cholakkal"
      ],
      "abstract": "Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at https://github.com/toobatehreem/MedROV.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20650v1",
        "pdf": "https://arxiv.org/pdf/2511.20650v1"
      },
      "arxiv_id": "2511.20650v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20649v1",
      "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
      "authors": [
        "Hidir Yesiltepe",
        "Tuna Han Salih Meral",
        "Adil Kaan Akan",
        "Kaan Oktay",
        "Pinar Yanardag"
      ],
      "abstract": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20649v1",
        "pdf": "https://arxiv.org/pdf/2511.20649v1"
      },
      "arxiv_id": "2511.20649v1",
      "comment": "Project Page: https://infinity-rope.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.20647v1",
      "title": "Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization",
      "authors": [
        "Tahira Kazimi",
        "Connor Dunlop",
        "Pinar Yanardag"
      ],
      "abstract": "While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20647v1",
        "pdf": "https://arxiv.org/pdf/2511.20647v1"
      },
      "arxiv_id": "2511.20647v1",
      "comment": "Project webpage: https://diverse-video.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.20648v1",
      "title": "LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight",
      "authors": [
        "Yunze Man",
        "Shihao Wang",
        "Guowen Zhang",
        "Johan Bjorck",
        "Zhiqi Li",
        "Liang-Yan Gui",
        "Jim Fan",
        "Jan Kautz",
        "Yu-Xiong Wang",
        "Zhiding Yu"
      ],
      "abstract": "To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20648v1",
        "pdf": "https://arxiv.org/pdf/2511.20648v1"
      },
      "arxiv_id": "2511.20648v1",
      "comment": "Tech report. Project page: https://nvlabs.github.io/LocateAnything3D/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.20646v1",
      "title": "3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding",
      "authors": [
        "Xiaoye Wang",
        "Chen Tang",
        "Xiangyu Yue",
        "Wei-Hong Li"
      ],
      "abstract": "This paper addresses the challenge of training a single network to jointly perform multiple dense prediction tasks, such as segmentation and depth estimation, i.e., multi-task learning (MTL). Current approaches mainly capture cross-task relations in the 2D image space, often leading to unstructured features lacking 3D-awareness. We argue that 3D-awareness is vital for modeling cross-task correlations essential for comprehensive scene understanding. We propose to address this problem by integrating correlations across views, i.e., cost volume, as geometric consistency in the MTL network. Specifically, we introduce a lightweight Cross-view Module (CvM), shared across tasks, to exchange information across views and capture cross-view correlations, integrated with a feature from MTL encoder for multi-task predictions. This module is architecture-agnostic and can be applied to both single and multi-view data. Extensive results on NYUv2 and PASCAL-Context demonstrate that our method effectively injects geometric consistency into existing MTL methods to improve performance.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20646v1",
        "pdf": "https://arxiv.org/pdf/2511.20646v1"
      },
      "arxiv_id": "2511.20646v1",
      "comment": "3D-aware Multi-task Learning, Cross-view Correlations, Code will be available at https://github.com/WeiHongLee/CrossView3DMTL",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.20645v1",
      "title": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
      "authors": [
        "Yongsheng Yu",
        "Wei Xiong",
        "Weili Nie",
        "Yichen Sheng",
        "Shiqiu Liu",
        "Jiebo Luo"
      ],
      "abstract": "Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20645v1",
        "pdf": "https://arxiv.org/pdf/2511.20645v1"
      },
      "arxiv_id": "2511.20645v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20644v1",
      "title": "Vision-Language Memory for Spatial Reasoning",
      "authors": [
        "Zuntao Liu",
        "Yi Du",
        "Taimeng Fu",
        "Shaoshu Su",
        "Cherie Ho",
        "Chen Wang"
      ],
      "abstract": "Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM$^2$, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM$^2$ achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20644v1",
        "pdf": "https://arxiv.org/pdf/2511.20644v1"
      },
      "arxiv_id": "2511.20644v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20643v1",
      "title": "Concept-Aware Batch Sampling Improves Language-Image Pretraining",
      "authors": [
        "Adhiraj Ghosh",
        "Vishaal Udandarao",
        "Thao Nguyen",
        "Matteo Farina",
        "Mehdi Cherti",
        "Jenia Jitsev",
        "Sewoong Oh",
        "Elisa Ricci",
        "Ludwig Schmidt",
        "Matthias Bethge"
      ],
      "abstract": "What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20643v1",
        "pdf": "https://arxiv.org/pdf/2511.20643v1"
      },
      "arxiv_id": "2511.20643v1",
      "comment": "Tech Report",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20641v1",
      "title": "Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition",
      "authors": [
        "Wei Tang",
        "Zuo-Zheng Wang",
        "Kun Zhang",
        "Tong Wei",
        "Min-Ling Zhang"
      ],
      "abstract": "Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes. Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance. However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity. Moreover, CLIP's zero-shot paradigm is optimized for single-label image-text matching, making it suboptimal for multi-label tasks. To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder. The framework incorporates a graph convolutional network for label-aware propagation and learnable soft prompts for refined embeddings. It utilizes a distribution-balanced Focal loss with class-aware re-weighting for optimized training under imbalance. Moreover, it improves generalization through test-time ensembling and realigns visual-textual modalities using parameter-efficient fine-tuning to avert overfitting on tail classes without compromising head class performance. Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20641v1",
        "pdf": "https://arxiv.org/pdf/2511.20641v1"
      },
      "arxiv_id": "2511.20641v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20640v1",
      "title": "MotionV2V: Editing Motion in a Video",
      "authors": [
        "Ryan Burgert",
        "Charles Herrmann",
        "Forrester Cole",
        "Michael S Ryoo",
        "Neal Wadhwa",
        "Andrey Voynov",
        "Nataniel Ruiz"
      ],
      "abstract": "While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a \"motion edit\" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating \"motion counterfactuals\", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20640v1",
        "pdf": "https://arxiv.org/pdf/2511.20640v1"
      },
      "arxiv_id": "2511.20640v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20639v1",
      "title": "Latent Collaboration in Multi-Agent Systems",
      "authors": [
        "Jiaru Zou",
        "Xiyuan Yang",
        "Ruizhong Qiu",
        "Gaotang Li",
        "Katherine Tieu",
        "Pan Lu",
        "Ke Shen",
        "Hanghang Tong",
        "Yejin Choi",
        "Jingrui He",
        "James Zou",
        "Mengdi Wang",
        "Ling Yang"
      ],
      "abstract": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20639v1",
        "pdf": "https://arxiv.org/pdf/2511.20639v1"
      },
      "arxiv_id": "2511.20639v1",
      "comment": "Project: https://github.com/Gen-Verse/LatentMAS",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.20636v1",
      "title": "Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model",
      "authors": [
        "Ziyue Wang",
        "Yayati Jadhav",
        "Peter Pak",
        "Amir Barati Farimani"
      ],
      "abstract": "Mechanical design and manufacturing workflows conventionally begin with conceptual design, followed by the creation of a computer-aided design (CAD) model and fabrication through material-extrusion (MEX) printing. This process requires converting CAD geometry into machine-readable G-code through slicing and path planning. While each step is well established, dependence on CAD modeling remains a major bottleneck: constructing object-specific 3D geometry is slow and poorly suited to rapid prototyping. Even minor design variations typically necessitate manual updates in CAD software, making iteration time-consuming and difficult to scale. To address this limitation, we introduce Image2Gcode, an end-to-end data-driven framework that bypasses the CAD stage and generates printer-ready G-code directly from images and part drawings. Instead of relying on an explicit 3D model, a hand-drawn or captured 2D image serves as the sole input. The framework first extracts slice-wise structural cues from the image and then employs a denoising diffusion probabilistic model (DDPM) over G-code sequences. Through iterative denoising, the model transforms Gaussian noise into executable print-move trajectories with corresponding extrusion parameters, establishing a direct mapping from visual input to native toolpaths. By producing structured G-code directly from 2D imagery, Image2Gcode eliminates the need for CAD or STL intermediates, lowering the entry barrier for additive manufacturing and accelerating the design-to-fabrication cycle. This approach supports on-demand prototyping from simple sketches or visual references and integrates with upstream 2D-to-3D reconstruction modules to enable an automated pipeline from concept to physical artifact. The result is a flexible, computationally efficient framework that advances accessibility in design iteration, repair workflows, and distributed manufacturing.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20636v1",
        "pdf": "https://arxiv.org/pdf/2511.20636v1"
      },
      "arxiv_id": "2511.20636v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20635v1",
      "title": "iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation",
      "authors": [
        "Zhoujie Fu",
        "Xianfang Zeng",
        "Jinghong Lan",
        "Xinyao Liao",
        "Cheng Chen",
        "Junyi Chen",
        "Jiacheng Wei",
        "Wei Cheng",
        "Shiyu Liu",
        "Yunuo Chen",
        "Gang Yu",
        "Guosheng Lin"
      ],
      "abstract": "Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20635v1",
        "pdf": "https://arxiv.org/pdf/2511.20635v1"
      },
      "arxiv_id": "2511.20635v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20629v1",
      "title": "MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models",
      "authors": [
        "Chieh-Yun Chen",
        "Zhonghao Wang",
        "Qi Chen",
        "Zhifan Ye",
        "Min Shi",
        "Yue Zhao",
        "Yinan Zhao",
        "Hui Qu",
        "Wei-An Lin",
        "Yiru Shen",
        "Ajinkya Kale",
        "Irfan Essa",
        "Humphrey Shi"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20629v1",
        "pdf": "https://arxiv.org/pdf/2511.20629v1"
      },
      "arxiv_id": "2511.20629v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20627v1",
      "title": "Fighting AI with AI: Leveraging Foundation Models for Assuring AI-Enabled Safety-Critical Systems",
      "authors": [
        "Anastasia Mavridou",
        "Divya Gopinath",
        "Corina S. Păsăreanu"
      ],
      "abstract": "The integration of AI components, particularly Deep Neural Networks (DNNs), into safety-critical systems such as aerospace and autonomous vehicles presents fundamental challenges for assurance. The opacity of AI systems, combined with the semantic gap between high-level requirements and low-level network representations, creates barriers to traditional verification approaches. These AI-specific challenges are amplified by longstanding issues in Requirements Engineering, including ambiguity in natural language specifications and scalability bottlenecks in formalization. We propose an approach that leverages AI itself to address these challenges through two complementary components. REACT (Requirements Engineering with AI for Consistency and Testing) employs Large Language Models (LLMs) to bridge the gap between informal natural language requirements and formal specifications, enabling early verification and validation. SemaLens (Semantic Analysis of Visual Perception using large Multi-modal models) utilizes Vision Language Models (VLMs) to reason about, test, and monitor DNN-based perception systems using human-understandable concepts. Together, these components provide a comprehensive pipeline from informal requirements to validated implementations.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20627v1",
        "pdf": "https://arxiv.org/pdf/2511.20627v1"
      },
      "arxiv_id": "2511.20627v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20626v1",
      "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training",
      "authors": [
        "Wei He",
        "Kai Han",
        "Hang Zhou",
        "Hanting Chen",
        "Zhicheng Liu",
        "Xinghao Chen",
        "Yunhe Wang"
      ],
      "abstract": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20626v1",
        "pdf": "https://arxiv.org/pdf/2511.20626v1"
      },
      "arxiv_id": "2511.20626v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20624v1",
      "title": "ShapeGen: Towards High-Quality 3D Shape Synthesis",
      "authors": [
        "Yangguang Li",
        "Xianglong He",
        "Zi-Xin Zou",
        "Zexiang Liu",
        "Wanli Ouyang",
        "Ding Liang",
        "Yan-Pei Cao"
      ],
      "abstract": "Inspired by generative paradigms in image and video, 3D shape generation has made notable progress, enabling the rapid synthesis of high-fidelity 3D assets from a single image. However, current methods still face challenges, including the lack of intricate details, overly smoothed surfaces, and fragmented thin-shell structures. These limitations leave the generated 3D assets still one step short of meeting the standards favored by artists. In this paper, we present ShapeGen, which achieves high-quality image-to-3D shape generation through 3D representation and supervision improvements, resolution scaling up, and the advantages of linear transformers. These advancements allow the generated assets to be seamlessly integrated into 3D pipelines, facilitating their widespread adoption across various applications. Through extensive experiments, we validate the impact of these improvements on overall performance. Ultimately, thanks to the synergistic effects of these enhancements, ShapeGen achieves a significant leap in image-to-3D generation, establishing a new state-of-the-art performance.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20624v1",
        "pdf": "https://arxiv.org/pdf/2511.20624v1"
      },
      "arxiv_id": "2511.20624v1",
      "comment": "Accepted to SIGGRAPH Asia 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20623v1",
      "title": "Copyright Detection in Large Language Models: An Ethical Approach to Generative AI Development",
      "authors": [
        "David Szczecina",
        "Senan Gaffori",
        "Edmond Li"
      ],
      "abstract": "The widespread use of Large Language Models (LLMs) raises critical concerns regarding the unauthorized inclusion of copyrighted content in training data. Existing detection frameworks, such as DE-COP, are computationally intensive, and largely inaccessible to independent creators. As legal scrutiny increases, there is a pressing need for a scalable, transparent, and user-friendly solution. This paper introduce an open-source copyright detection platform that enables content creators to verify whether their work was used in LLM training datasets. Our approach enhances existing methodologies by facilitating ease of use, improving similarity detection, optimizing dataset validation, and reducing computational overhead by 10-30% with efficient API calls. With an intuitive user interface and scalable backend, this framework contributes to increasing transparency in AI development and ethical compliance, facilitating the foundation for further research in responsible AI development and copyright enforcement.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20623v1",
        "pdf": "https://arxiv.org/pdf/2511.20623v1"
      },
      "arxiv_id": "2511.20623v1",
      "comment": "4 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20621v1",
      "title": "DiFR: Inference Verification Despite Nondeterminism",
      "authors": [
        "Adam Karvonen",
        "Daniel Reuter",
        "Roy Rinberg",
        "Luke Marks",
        "Adrià Garriga-Alonso",
        "Keri Warr"
      ],
      "abstract": "As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20621v1",
        "pdf": "https://arxiv.org/pdf/2511.20621v1"
      },
      "arxiv_id": "2511.20621v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20620v1",
      "title": "Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI",
      "authors": [
        "Xinhao Liu",
        "Jiaqi Li",
        "Youming Deng",
        "Ruxin Chen",
        "Yingjia Zhang",
        "Yifei Ma",
        "Li Guo",
        "Yiming Li",
        "Jing Zhang",
        "Chen Feng"
      ],
      "abstract": "Reproducible closed-loop evaluation remains a major bottleneck in Embodied AI such as visual navigation. A promising path forward is high-fidelity simulation that combines photorealistic sensor rendering with geometrically grounded interaction in complex, open-world urban environments. Although recent video-3DGS methods ease open-world scene capturing, they are still unsuitable for benchmarking due to large visual and geometric sim-to-real gaps. To address these challenges, we introduce Wanderland, a real-to-sim framework that features multi-sensor capture, reliable reconstruction, accurate geometry, and robust view synthesis. Using this pipeline, we curate a diverse dataset of indoor-outdoor urban scenes and systematically demonstrate how image-only pipelines scale poorly, how geometry quality impacts novel view synthesis, and how all of these adversely affect navigation policy learning and evaluation reliability. Beyond serving as a trusted testbed for embodied navigation, Wanderland's rich raw sensor data further allows benchmarking of 3D reconstruction and novel view synthesis models. Our work establishes a new foundation for reproducible research in open-world embodied AI. Project website is at https://ai4ce.github.io/wanderland/.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20620v1",
        "pdf": "https://arxiv.org/pdf/2511.20620v1"
      },
      "arxiv_id": "2511.20620v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20615v1",
      "title": "Evaluating the Performance of Deep Learning Models in Whole-body Dynamic 3D Posture Prediction During Load-reaching Activities",
      "authors": [
        "Seyede Niloofar Hosseini",
        "Ali Mojibi",
        "Mahdi Mohseni",
        "Navid Arjmand",
        "Alireza Taheri"
      ],
      "abstract": "This study aimed to explore the application of deep neural networks for whole-body human posture prediction during dynamic load-reaching activities. Two time-series models were trained using bidirectional long short-term memory (BLSTM) and transformer architectures. The dataset consisted of 3D full-body plug-in gait dynamic coordinates from 20 normal-weight healthy male individuals each performing 204 load-reaching tasks from different load positions while adapting various lifting and handling techniques. The model inputs consisted of the 3D position of the hand-load position, lifting (stoop, full-squat and semi-squat) and handling (one- and two-handed) techniques, body weight and height, and the 3D coordinate data of the body posture from the first 25% of the task duration. These inputs were used by the models to predict body coordinates during the remaining 75% of the task period. Moreover, a novel method was proposed to improve the accuracy of the previous and present posture prediction networks by enforcing constant body segment lengths through the optimization of a new cost function. The results indicated that the new cost function decreased the prediction error of the models by approximately 8% and 21% for the arm and leg models, respectively. We indicated that utilizing the transformer architecture, with a root-mean-square-error of 47.0 mm, exhibited ~58% more accurate long-term performance than the BLSTM-based model. This study merits the use of neural networks that capture time series dependencies in 3D motion frames, providing a unique approach for understanding and predict motion dynamics during manual material handling activities.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20615v1",
        "pdf": "https://arxiv.org/pdf/2511.20615v1"
      },
      "arxiv_id": "2511.20615v1",
      "comment": "10 pages, 6 figures, 7 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20614v1",
      "title": "The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment",
      "authors": [
        "Ziheng Ouyang",
        "Yiren Song",
        "Yaoli Liu",
        "Shihao Zhu",
        "Qibin Hou",
        "Ming-Ming Cheng",
        "Mike Zheng Shou"
      ],
      "abstract": "Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20614v1",
        "pdf": "https://arxiv.org/pdf/2511.20614v1"
      },
      "arxiv_id": "2511.20614v1",
      "comment": "Project page: https://ouyangziheng.github.io/ImageCritic-Page/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.20613v1",
      "title": "Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning",
      "authors": [
        "Panayiotis Danassis",
        "Naman Goel"
      ],
      "abstract": "The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and $\\sim 40$k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20613v1",
        "pdf": "https://arxiv.org/pdf/2511.20613v1"
      },
      "arxiv_id": "2511.20613v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20612v1",
      "title": "Sparse-to-Field Reconstruction via Stochastic Neural Dynamic Mode Decomposition",
      "authors": [
        "Yujin Kim",
        "Sarah Dean"
      ],
      "abstract": "Many consequential real-world systems, like wind fields and ocean currents, are dynamic and hard to model. Learning their governing dynamics remains a central challenge in scientific machine learning. Dynamic Mode Decomposition (DMD) provides a simple, data-driven approximation, but practical use is limited by sparse/noisy observations from continuous fields, reliance on linear approximations, and the lack of principled uncertainty quantification. To address these issues, we introduce Stochastic NODE-DMD, a probabilistic extension of DMD that models continuous-time, nonlinear dynamics while remaining interpretable. Our approach enables continuous spatiotemporal reconstruction at arbitrary coordinates and quantifies predictive uncertainty. Across four benchmarks, a synthetic setting and three physics-based flows, it surpasses a baseline in reconstruction accuracy when trained from only 10% observation density. It further recovers the dynamical structure by aligning learned modes and continuous-time eigenvalues with ground truth. Finally, on datasets with multiple realizations, our method learns a calibrated distribution over latent dynamics that preserves ensemble variability rather than averaging across regimes. Our code is available at: https://github.com/sedan-group/Stochastic-NODE-DMD",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20612v1",
        "pdf": "https://arxiv.org/pdf/2511.20612v1"
      },
      "arxiv_id": "2511.20612v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20610v1",
      "title": "Building a Foundation Model for Trajectory from Scratch",
      "authors": [
        "Gaspard Merten",
        "Mahmoud Sakr",
        "Gilles Dejaegere"
      ],
      "abstract": "Foundation models are transformative in artificial intelligence, but building them from scratch, especially for mobility trajectories, is not yet clear or documented. This tutorial bridges this gap by demonstrating the steps and code of a minimal implementation of a trajectory-focused foundation model starting from GPT-2. Through a concise, step-by-step, code-driven process, we demonstrate adapting GPT-2 for spatiotemporal data. We then review and compare representative trajectory foundation models, such as TrajFM and TrajGPT, highlighting their architectural innovations and differences. Additionally, we introduce complementary techniques from related domains, like TimesFM's patching approach. Targeted at researchers and practitioners, this tutorial aims to explain the concepts and terminology of foundation models, at the implementation level. We find it timely and indispensable to create this educational material in order to support the SIGSPATIAL community in building and evaluating mobility foundation models, enhancing both research clarity and peer-review effectiveness in mobility AI.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20610v1",
        "pdf": "https://arxiv.org/pdf/2511.20610v1"
      },
      "arxiv_id": "2511.20610v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20609v1",
      "title": "Adaptive Hopfield Network: Rethinking Similarities in Associative Memory",
      "authors": [
        "Shurong Wang",
        "Yuqi Pan",
        "Zhuoyang Shen",
        "Meng Zhang",
        "Hongwei Wang",
        "Guoqi Li"
      ],
      "abstract": "Associative memory models are content-addressable memory systems fundamental to biological intelligence and are notable for their high interpretability. However, existing models evaluate the quality of retrieval based on proximity, which cannot guarantee that the retrieved pattern has the strongest association with the query, failing correctness. We reframe this problem by proposing that a query is a generative variant of a stored memory pattern, and define a variant distribution to model this subtle context-dependent generative process. Consequently, correct retrieval should return the memory pattern with the maximum a posteriori probability of being the query's origin. This perspective reveals that an ideal similarity measure should approximate the likelihood of each stored pattern generating the query in accordance with variant distribution, which is impossible for fixed and pre-defined similarities used by existing associative memories. To this end, we develop adaptive similarity, a novel mechanism that learns to approximate this insightful but unknown likelihood from samples drawn from context, aiming for correct retrieval. We theoretically prove that our proposed adaptive similarity achieves optimal correct retrieval under three canonical and widely applicable types of variants: noisy, masked, and biased. We integrate this mechanism into a novel adaptive Hopfield network (A-Hop), and empirical results show that it achieves state-of-the-art performance across diverse tasks, including memory retrieval, tabular classification, image classification, and multiple instance learning.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20609v1",
        "pdf": "https://arxiv.org/pdf/2511.20609v1"
      },
      "arxiv_id": "2511.20609v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20607v1",
      "title": "Optimization of Sums of Bivariate Functions: An Introduction to Relaxation-Based Methods for the Case of Finite Domains",
      "authors": [
        "Nils Müller"
      ],
      "abstract": "We study the optimization of functions with $n>2$ arguments that have a representation as a sum of several functions that have only $2$ of the $n$ arguments each, termed sums of bivariates, on finite domains. The complexity of optimizing sums of bivariates is shown to be NP-equivalent and it is shown that there exists free lunch in the optimization of sums of bivariates. Based on measure-valued extensions of the objective function, so-called relaxations, $\\ell^2$-approximation, and entropy-regularization, we derive several tractable problem formulations solvable with linear programming, coordinate ascent as well as with closed-form solutions. The limits of applying tractable versions of such relaxations to sums of bivariates are investigated using general results for reconstructing measures from their bivariate marginals. Experiments in which the derived algorithms are applied to random functions, vertex coloring, and signal reconstruction problems provide insights into qualitatively different function classes that can be modeled as sums of bivariates.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "math.OC",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "math.OC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20607v1",
        "pdf": "https://arxiv.org/pdf/2511.20607v1"
      },
      "arxiv_id": "2511.20607v1",
      "comment": "59 pages, 7 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20605v1",
      "title": "How to Purchase Labels? A Cost-Effective Approach Using Active Learning Markets",
      "authors": [
        "Xiwen Huang",
        "Pierre Pinson"
      ],
      "abstract": "We introduce and analyse active learning markets as a way to purchase labels, in situations where analysts aim to acquire additional data to improve model fitting, or to better train models for predictive analytics applications. This comes in contrast to the many proposals that already exist to purchase features and examples. By originally formalising the market clearing as an optimisation problem, we integrate budget constraints and improvement thresholds into the label acquisition process. We focus on a single-buyer-multiple-seller setup and propose the use of two active learning strategies (variance based and query-by-committee based), paired with distinct pricing mechanisms. They are compared to a benchmark random sampling approach. The proposed strategies are validated on real-world datasets from two critical application domains: real estate pricing and energy forecasting. Results demonstrate the robustness of our approach, consistently achieving superior performance with fewer labels acquired compared to conventional methods. Our proposal comprises an easy-to-implement practical solution for optimising data acquisition in resource-constrained environments.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20605v1",
        "pdf": "https://arxiv.org/pdf/2511.20605v1"
      },
      "arxiv_id": "2511.20605v1",
      "comment": "Submitted as a preprint. 34 pages, 14 figures, 4 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20604v1",
      "title": "On Evaluating LLM Alignment by Evaluating LLMs as Judges",
      "authors": [
        "Yixin Liu",
        "Pengfei Liu",
        "Arman Cohan"
      ],
      "abstract": "Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20604v1",
        "pdf": "https://arxiv.org/pdf/2511.20604v1"
      },
      "arxiv_id": "2511.20604v1",
      "comment": "NeurIPS 2025 Camera Ready",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20601v1",
      "title": "The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting",
      "authors": [
        "Heman Shakeri"
      ],
      "abstract": "Deep sequence models for blood glucose forecasting consistently fail to leverage clinically informative drivers--insulin, meals, and activity--despite well-understood physiological mechanisms. We term this Driver-Blindness and formalize it via $Δ_{\\text{drivers}}$, the performance gain of multivariate models over matched univariate baselines. Across the literature, $Δ_{\\text{drivers}}$ is typically near zero. We attribute this to three interacting factors: architectural biases favoring autocorrelation (C1), data fidelity gaps that render drivers noisy and confounded (C2), and physiological heterogeneity that undermines population-level models (C3). We synthesize strategies that partially mitigate Driver-Blindness--including physiological feature encoders, causal regularization, and personalization--and recommend that future work routinely report $Δ_{\\text{drivers}}$ to prevent driver-blind models from being considered state-of-the-art.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20601v1",
        "pdf": "https://arxiv.org/pdf/2511.20601v1"
      },
      "arxiv_id": "2511.20601v1",
      "comment": "7 pages, 1 figure",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20597v1",
      "title": "BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents",
      "authors": [
        "Kaiyuan Zhang",
        "Mark Tenenholtz",
        "Kyle Polley",
        "Jerry Ma",
        "Denis Yarats",
        "Ninghui Li"
      ],
      "abstract": "The integration of artificial intelligence (AI) agents into web browsers introduces security challenges that go beyond traditional web application threat models. Prior work has identified prompt injection as a new attack vector for web agents, yet the resulting impact within real-world environments remains insufficiently understood.\n  In this work, we examine the landscape of prompt injection attacks and synthesize a benchmark of attacks embedded in realistic HTML payloads. Our benchmark goes beyond prior work by emphasizing injections that can influence real-world actions rather than mere text outputs, and by presenting attack payloads with complexity and distractor frequency similar to what real-world agents encounter. We leverage this benchmark to conduct a comprehensive empirical evaluation of existing defenses, assessing their effectiveness across a suite of frontier AI models. We propose a multi-layered defense strategy comprising both architectural and model-based defenses to protect against evolving prompt injection attacks. Our work offers a blueprint for designing practical, secure web agents through a defense-in-depth approach.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20597v1",
        "pdf": "https://arxiv.org/pdf/2511.20597v1"
      },
      "arxiv_id": "2511.20597v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20592v1",
      "title": "Latent Diffusion Inversion Requires Understanding the Latent Space",
      "authors": [
        "Mingxing Rao",
        "Bowen Qu",
        "Daniel Moyer"
      ],
      "abstract": "The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\\% and substantial increases in TPR@1\\%FPR (6.42\\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pokémon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20592v1",
        "pdf": "https://arxiv.org/pdf/2511.20592v1"
      },
      "arxiv_id": "2511.20592v1",
      "comment": "14 pages, 4 figures, 4 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20591v1",
      "title": "Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning",
      "authors": [
        "Charlotte Beylier",
        "Hannah Selder",
        "Arthur Fleig",
        "Simon M. Hofmann",
        "Nico Scherf"
      ],
      "abstract": "The learning process of a reinforcement learning (RL) agent remains poorly understood beyond the mathematical formulation of its learning algorithm. To address this gap, we introduce attention-oriented metrics (ATOMs) to investigate the development of an RL agent's attention during training. In a controlled experiment, we tested ATOMs on three variations of a Pong game, each designed to teach the agent distinct behaviours, complemented by a behavioural assessment. ATOMs successfully delineate the attention patterns of an agent trained on each game variation, and that these differences in attention patterns translate into differences in the agent's behaviour. Through continuous monitoring of ATOMs during training, we observed that the agent's attention developed in phases, and that these phases were consistent across game variations. Overall, we believe that ATOM could help improve our understanding of the learning processes of RL agents and better understand the relationship between attention and learning.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20591v1",
        "pdf": "https://arxiv.org/pdf/2511.20591v1"
      },
      "arxiv_id": "2511.20591v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20590v1",
      "title": "EnergyTwin: A Multi-Agent System for Simulating and Coordinating Energy Microgrids",
      "authors": [
        "Jakub Muszyński",
        "Ignacy Walużenicz",
        "Patryk Zan",
        "Zofia Wrona",
        "Maria Ganzha",
        "Marcin Paprzycki",
        "Costin Bădică"
      ],
      "abstract": "Microgrids are deployed to reduce purchased grid energy, limit exposure to volatile tariffs, and ensure service continuity during disturbances. This requires coordinating heterogeneous distributed energy resources across multiple time scales and under variable conditions. Among existing tools, typically, power-system simulators capture physical behaviour but assume centralized control, while multi-agent frameworks model decentralized decision-making but represent energy with no physical grounding. In this context, the EnergyTwin is introduced, an agent-based microgrid simulation environment that couples physically grounded models with forecast-informed, rolling-horizon planning, and negotiations. Each asset is modeled as an agent, interacting with a central agent that obtains forecasts, formulates predictions, and allocates energy through contract-based interactions. EnergyTwin targets tertiary-layer decision making and is extensible for digital-twin use. Its feasibility was evaluated in a university campus microgrid scenario where multiple planning strategies were compared. Achieved results show that forecast-driven rolling-horizon planning increases local energy self-sufficiency, maintains higher battery reserves, and reduces exposure to low-resilience operating states. They demonstrate also potential of EnergyTwin as platform supporting research on resilient, negotiation-driven microgrids.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.MA",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20590v1",
        "pdf": "https://arxiv.org/pdf/2511.20590v1"
      },
      "arxiv_id": "2511.20590v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20587v1",
      "title": "Anatomica: Localized Control over Geometric and Topological Properties for Anatomical Diffusion Models",
      "authors": [
        "Karim Kadry",
        "Abdallah Abdelwahed",
        "Shoaib Goraya",
        "Ajay Manicka",
        "Naravich Chutisilp",
        "Farhad Nezami",
        "Elazer Edelman"
      ],
      "abstract": "We present Anatomica: an inference-time framework for generating multi-class anatomical voxel maps with localized geo-topological control. During generation, we use cuboidal control domains of varying dimensionality, location, and shape to slice out relevant substructures. These local substructures are used to compute differentiable penalty functions that steer the sample towards target constraints. We control geometric features such as size, shape, and position through voxel-wise moments, while topological features such as connected components, loops, and voids are enforced through persistent homology. Lastly, we implement Anatomica for latent diffusion models, where neural field decoders partially extract substructures, enabling the efficient control of anatomical properties. Anatomica applies flexibly across diverse anatomical systems, composing constraints to control complex structures over arbitrary dimensions and coordinate systems, thereby enabling the rational design of synthetic datasets for virtual trials or machine learning workflows.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20587v1",
        "pdf": "https://arxiv.org/pdf/2511.20587v1"
      },
      "arxiv_id": "2511.20587v1",
      "comment": "8 pages, 10 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20586v1",
      "title": "PaTAS: A Parallel System for Trust Propagation in Neural Networks Using Subjective Logic",
      "authors": [
        "Koffi Ismael Ouattara",
        "Ioannis Krontiris",
        "Theo Dimitrakos",
        "Dennis Eisermann",
        "Frank Kargl"
      ],
      "abstract": "Trustworthiness has become a key requirement for the deployment of artificial intelligence systems in safety-critical applications. Conventional evaluation metrics such as accuracy and precision fail to capture uncertainty or the reliability of model predictions, particularly under adversarial or degraded conditions. This paper introduces the \\emph{Parallel Trust Assessment System (PaTAS)}, a framework for modeling and propagating trust in neural networks using Subjective Logic (SL). PaTAS operates in parallel with standard neural computation through \\emph{Trust Nodes} and \\emph{Trust Functions} that propagate input, parameter, and activation trust across the network. The framework defines a \\emph{Parameter Trust Update} mechanism to refine parameter reliability during training and an \\emph{Inference-Path Trust Assessment (IPTA)} method to compute instance-specific trust at inference. Experiments on real-world and adversarial datasets demonstrate that PaTAS produces interpretable, symmetric, and convergent trust estimates that complement accuracy and expose reliability gaps in poisoned, biased, or uncertain data scenarios. The results show that PaTAS effectively distinguishes between benign and adversarial inputs and identifies cases where model confidence diverges from actual reliability. By enabling transparent and quantifiable trust reasoning within neural architectures, PaTAS provides a principled foundation for evaluating model reliability across the AI lifecycle.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20586v1",
        "pdf": "https://arxiv.org/pdf/2511.20586v1"
      },
      "arxiv_id": "2511.20586v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20584v1",
      "title": "A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent",
      "authors": [
        "Shuo Xie",
        "Tianhao Wang",
        "Beining Wu",
        "Zhiyuan Li"
      ],
      "abstract": "Adaptive optimizers can reduce to normalized steepest descent (NSD) when only adapting to the current gradient, suggesting a close connection between the two algorithmic families. A key distinction between their analyses, however, lies in the geometries, e.g., smoothness notions, they rely on. In the convex setting, adaptive optimizers are governed by a stronger adaptive smoothness condition, while NSD relies on the standard notion of smoothness. We extend the theory of adaptive smoothness to the nonconvex setting and show that it precisely characterizes the convergence of adaptive optimizers. Moreover, we establish that adaptive smoothness enables acceleration of adaptive optimizers with Nesterov momentum in the convex setting, a guarantee unattainable under standard smoothness for certain non-Euclidean geometry. We further develop an analogous comparison for stochastic optimization by introducing adaptive gradient variance, which parallels adaptive smoothness and leads to dimension-free convergence guarantees that cannot be achieved under standard gradient variance for certain non-Euclidean geometry.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20584v1",
        "pdf": "https://arxiv.org/pdf/2511.20584v1"
      },
      "arxiv_id": "2511.20584v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20577v1",
      "title": "MSTN: Fast and Efficient Multivariate Time Series Model",
      "authors": [
        "Sumit S Shevtekar",
        "Chandresh K Maurya",
        "Gourab Sil"
      ],
      "abstract": "Real-world time-series data is highly non stationary and complex in dynamics that operate across multiple timescales, ranging from fast, short-term changes to slow, long-term trends. Most existing models rely on fixed-scale structural priors, such as patch-based tokenization, fixed frequency transformations, or frozen backbone architectures. This often leads to over-regularization of temporal dynamics, which limits their ability to adaptively model the full spectrum of temporal variations and impairs their performance on unpredictable, Sudden, high-magnitude events. To address this, we introduce the Multi-scale Temporal Network (MSTN), a novel deep learning architecture founded on a hierarchical multi-scale and sequence modeling principle. The MSTN framework integrates: (i) a multi-scale convolutional encoder that constructs a hierarchical feature pyramid for local patterns (ii) a sequence modeling component for long-range temporal dependencies. We empirically validate this with BiLSTM and Transformer variants, establishing a flexible foundation for future architectural advancements. and (iii) a gated fusion mechanism augmented with squeeze-and-excitation (SE) and multi-head temporal attention (MHTA) for dynamic, context-aware feature integration. This design enables MSTN to adaptively model temporal patterns from milliseconds to long-range dependencies within a unified framework. Extensive evaluations across time-series long-horizon forecasting, imputation, classification and generalizability study demonstrate that MSTN achieves competitive state-of-the-art (SOTA) performance, showing improvements over contemporary approaches including EMTSF, LLM4TS, HiMTM, TIME-LLM, MTST, SOFTS, iTransformer, TimesNet, and PatchTST. In total, MSTN establishes new SOTA performance on 24 of 32 benchmark datasets, demonstrating its consistent performance across diverse temporal tasks.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20577v1",
        "pdf": "https://arxiv.org/pdf/2511.20577v1"
      },
      "arxiv_id": "2511.20577v1",
      "comment": "21 pages, 1 figure, 5 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20573v1",
      "title": "VQ-VA World: Towards High-Quality Visual Question-Visual Answering",
      "authors": [
        "Chenhui Gou",
        "Zilong Chen",
        "Zeyu Wang",
        "Feng Li",
        "Deyao Zhu",
        "Zicheng Duan",
        "Kunchang Li",
        "Chaorui Deng",
        "Hongyi Yuan",
        "Haoqi Fan",
        "Cihang Xie",
        "Jianfei Cai",
        "Hamid Rezatofighi"
      ],
      "abstract": "This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20573v1",
        "pdf": "https://arxiv.org/pdf/2511.20573v1"
      },
      "arxiv_id": "2511.20573v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20570v1",
      "title": "Gated Uncertainty-Aware Runtime Dual Invariants for Neural Signal-Controlled Robotics",
      "authors": [
        "Tasha Kim",
        "Oiwi Parker Jones"
      ],
      "abstract": "Safety-critical assistive systems that directly decode user intent from neural signals require rigorous guarantees of reliability and trust. We present GUARDIAN (Gated Uncertainty-Aware Runtime Dual Invariants), a framework for real-time neuro-symbolic verification for neural signal-controlled robotics. GUARDIAN enforces both logical safety and physiological trust by coupling confidence-calibrated brain signal decoding with symbolic goal grounding and dual-layer runtime monitoring. On the BNCI2014 motor imagery electroencephalogram (EEG) dataset with 9 subjects and 5,184 trials, the system performs at a high safety rate of 94-97% even with lightweight decoder architectures with low test accuracies (27-46%) and high ECE confidence miscalibration (0.22-0.41). We demonstrate 1.7x correct interventions in simulated noise testing versus at baseline. The monitor operates at 100Hz and sub-millisecond decision latency, making it practically viable for closed-loop neural signal-based systems. Across 21 ablation results, GUARDIAN exhibits a graduated response to signal degradation, and produces auditable traces from intent, plan to action, helping to link neural evidence to verifiable robot action.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20570v1",
        "pdf": "https://arxiv.org/pdf/2511.20570v1"
      },
      "arxiv_id": "2511.20570v1",
      "comment": "Embodied and Safe-Assured Robotic Systems workshop at NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20565v1",
      "title": "DINO-Tok: Adapting DINO for Visual Tokenizers",
      "authors": [
        "Mingkai Jia",
        "Mingxiao Li",
        "Liaoyuan Fan",
        "Tianxing Shi",
        "Jiaxin Guo",
        "Zeming Li",
        "Xiaoyang Guo",
        "Xiao-Xiao Long",
        "Qian Zhang",
        "Ping Tan",
        "Wei Yin"
      ],
      "abstract": "Recent advances in visual generation have highlighted the rise of Latent Generative Models (LGMs), which rely on effective visual tokenizers to bridge pixels and semantics. However, existing tokenizers are typically trained from scratch and struggle to balance semantic representation and reconstruction fidelity, particularly in high-dimensional latent spaces. In this work, we introduce DINO-Tok, a DINO-based visual tokenizer that unifies hierarchical representations into an information-complete latent space. By integrating shallow features that retain fine-grained details with deep features encoding global semantics, DINO-Tok effectively bridges pretrained representations and visual generation. We further analyze the challenges of vector quantization (VQ) in this high-dimensional space, where key information is often lost and codebook collapse occurs. We thus propose a global PCA reweighting mechanism to stabilize VQ and preserve essential information across dimensions. On ImageNet 256$\\times$256, DINO-Tok achieves state-of-the-art reconstruction performance, reaching 28.54 PSNR for autoencoding and 23.98 PSNR for VQ-based modeling, significantly outperforming prior tokenizers and comparable to billion-level data trained models (such as Hunyuan and Wan). These results demonstrate that adapting powerful pretrained vision models like DINO for tokenization enables semantically aligned and high-fidelity latent representations, enabling next-generation visual generative models. Code will be publicly available at https://github.com/MKJia/DINO-Tok.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20565v1",
        "pdf": "https://arxiv.org/pdf/2511.20565v1"
      },
      "arxiv_id": "2511.20565v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20564v1",
      "title": "E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems",
      "authors": [
        "Rui Xue",
        "Shichao Zhu",
        "Liang Qin",
        "Guangmou Pan",
        "Yang Song",
        "Tianfu Wu"
      ],
      "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for modeling graph-structured data and have been widely used in recommender systems, such as for capturing complex user-item and item-item relations. However, most industrial deployments adopt a two-stage pipeline: GNNs are first pre-trained offline to generate node embeddings, which are then used as static features for downstream recommender systems. This decoupled paradigm leads to two key limitations: (1) high computational overhead, since large-scale GNN inference must be repeatedly executed to refresh embeddings; and (2) lack of joint optimization, as the gradient from the recommender system cannot directly influence the GNN learning process, causing the GNN to be suboptimally informative for the recommendation task. In this paper, we propose E2E-GRec, a novel end-to-end training framework that unifies GNN training with the recommender system. Our framework is characterized by three key components: (i) efficient subgraph sampling from a large-scale cross-domain heterogeneous graph to ensure training scalability and efficiency; (ii) a Graph Feature Auto-Encoder (GFAE) serving as an auxiliary self-supervised task to guide the GNN to learn structurally meaningful embeddings; and (iii) a two-level feature fusion mechanism combined with Gradnorm-based dynamic loss balancing, which stabilizes graph-aware multi-task end-to-end training. Extensive offline evaluations, online A/B tests (e.g., a +0.133% relative improvement in stay duration, a 0.3171% reduction in the average number of videos a user skips) on large-scale production data, together with theoretical analysis, demonstrate that E2E-GRec consistently surpasses traditional approaches, yielding significant gains across multiple recommendation metrics.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20564v1",
        "pdf": "https://arxiv.org/pdf/2511.20564v1"
      },
      "arxiv_id": "2511.20564v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20563v1",
      "title": "A Reason-then-Describe Instruction Interpreter for Controllable Video Generation",
      "authors": [
        "Shengqiong Wu",
        "Weicai Ye",
        "Yuanxing Zhang",
        "Jiahao Wang",
        "Quande Liu",
        "Xintao Wang",
        "Pengfei Wan",
        "Kun Gai",
        "Hao Fei",
        "Tat-Seng Chua"
      ],
      "abstract": "Diffusion Transformers have significantly improved video fidelity and temporal coherence, however, practical controllability remains limited. Concise, ambiguous, and compositionally complex user inputs contrast with the detailed prompts used in training, yielding an intent-output mismatch. We propose ReaDe, a universal, model-agnostic interpreter that converts raw instructions into precise, actionable specifications for downstream video generators. ReaDe follows a reason-then-describe paradigm: it first analyzes the user request to identify core requirements and resolve ambiguities, then produces detailed guidance that enables faithful, controllable generation. We train ReaDe via a two-stage optimization: (i) reasoning-augmented supervision imparts analytic parsing with stepwise traces and dense captions, and (ii) a multi-dimensional reward assigner enables stable, feedback-driven refinement for natural-style captions. Experiments across single- and multi-condition scenarios show consistent gains in instruction fidelity, caption accuracy, and downstream video quality, with strong generalization to reasoning-intensive and unseen inputs. ReaDe offers a practical route to aligning controllable video generation with accurately interpreted user intent. Project Page: https://sqwu.top/ReaDe/.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20563v1",
        "pdf": "https://arxiv.org/pdf/2511.20563v1"
      },
      "arxiv_id": "2511.20563v1",
      "comment": "27 pages, 13 figures, 13 tables, Project Page: https://sqwu.top/ReaDe/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20562v1",
      "title": "PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding",
      "authors": [
        "Haoze Zhang",
        "Tianyu Huang",
        "Zichen Wan",
        "Xiaowei Jin",
        "Hongzhi Zhang",
        "Hui Li",
        "Wangmeng Zuo"
      ],
      "abstract": "While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20562v1",
        "pdf": "https://arxiv.org/pdf/2511.20562v1"
      },
      "arxiv_id": "2511.20562v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20561v1",
      "title": "Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward",
      "authors": [
        "Yuwei Niu",
        "Weiyang Jin",
        "Jiaqi Liao",
        "Chaoran Feng",
        "Peng Jin",
        "Bin Lin",
        "Zongjian Li",
        "Bin Zhu",
        "Weihao Yu",
        "Li Yuan"
      ],
      "abstract": "Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20561v1",
        "pdf": "https://arxiv.org/pdf/2511.20561v1"
      },
      "arxiv_id": "2511.20561v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20558v1",
      "title": "Spatio-Temporal Hierarchical Causal Models",
      "authors": [
        "Xintong Li",
        "Haoran Zhang",
        "Xiao Zhou"
      ],
      "abstract": "The abundance of fine-grained spatio-temporal data, such as traffic sensor networks, offers vast opportunities for scientific discovery. However, inferring causal relationships from such observational data remains challenging, particularly due to unobserved confounders that are specific to units (e.g., geographical locations) yet influence outcomes over time. Most existing methods for spatio-temporal causal inference assume that all confounders are observed, an assumption that is often violated in practice. In this paper, we introduce Spatio-Temporal Hierarchical Causal Models (ST-HCMs), a novel graphical framework that extends hierarchical causal modeling to the spatio-temporal domain. At the core of our approach is the Spatio-Temporal Collapse Theorem, which shows that a complex ST-HCM converges to a simpler flat causal model as the amount of subunit data increases. This theoretical result enables a general procedure for causal identification, allowing ST-HCMs to recover causal effects even in the presence of unobserved, time-invariant unit-level confounders, a scenario where standard non-hierarchical models fail. We validate the effectiveness of our framework on both synthetic and real-world datasets, demonstrating its potential for robust causal inference in complex dynamic systems.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20558v1",
        "pdf": "https://arxiv.org/pdf/2511.20558v1"
      },
      "arxiv_id": "2511.20558v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20551v1",
      "title": "Time-Domain Linear Model-based Framework for Passive Acoustic Mapping of Cavitation Activity",
      "authors": [
        "Tatiana Gelvez-Barrera",
        "Barbara Nicolas",
        "Denis Kouamé",
        "Bruno Gilles",
        "Adrian Basarab"
      ],
      "abstract": "Passive acoustic mapping enables the spatial mapping and temporal monitoring of cavitation activity, playing a crucial role in therapeutic ultrasound applications. Most conventional beamforming methods, whether implemented in the time or frequency domains, suffer from limited axial resolution due to the absence of a reference emission onset time. While frequency-domain methods, the most efficient of which are based on the cross-spectral matrix, require long signals for accurate estimation, time-domain methods typically achieve lower spatial resolution. To address these limitations, we propose a linear model-based beamforming framework fully formulated in the time domain. The linear forward model relates a discretized spatiotemporal distribution of cavitation activity to the temporal signals recorded by a probe, explicitly accounting for time-of-flight delays dictated by the acquisition geometry. This model is then inverted using regularization techniques that exploit prior knowledge of cavitation activity in both spatial and temporal domains. Experimental results show that the proposed framework achieves enhanced or competitive cavitation map quality while using only 20\\% of the data typically required by frequency-domain methods. This highlights the substantial gain in data efficiency and the flexibility of our spatiotemporal regularization to adapt to diverse passive cavitation scenarios, outperforming state-of-the-art techniques.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "eess.SP",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "eess.SP",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20551v1",
        "pdf": "https://arxiv.org/pdf/2511.20551v1"
      },
      "arxiv_id": "2511.20551v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20549v1",
      "title": "Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning",
      "authors": [
        "Guanjie Chen",
        "Shirui Huang",
        "Kai Liu",
        "Jianchen Zhu",
        "Xiaoye Qu",
        "Peng Chen",
        "Yu Cheng",
        "Yifu Sun"
      ],
      "abstract": "Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only $2.1\\%$ its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20549v1",
        "pdf": "https://arxiv.org/pdf/2511.20549v1"
      },
      "arxiv_id": "2511.20549v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20544v1",
      "title": "New York Smells: A Large Multimodal Dataset for Olfaction",
      "authors": [
        "Ege Ozguroglu",
        "Junbang Liang",
        "Ruoshi Liu",
        "Mia Chiquier",
        "Michael DeTienne",
        "Wesley Wei Qian",
        "Alexandra Horowitz",
        "Andrew Owens",
        "Carl Vondrick"
      ],
      "abstract": "While olfaction is central to how animals perceive the world, this rich chemical sensory modality remains largely inaccessible to machines. One key bottleneck is the lack of diverse, multimodal olfactory training data collected in natural settings. We present New York Smells, a large dataset of paired image and olfactory signals captured ``in the wild.'' Our dataset contains 7,000 smell-image pairs from 3,500 distinct objects across indoor and outdoor environments, with approximately 70$\\times$ more objects than existing olfactory datasets. Our benchmark has three tasks: cross-modal smell-to-image retrieval, recognizing scenes, objects, and materials from smell alone, and fine-grained discrimination between grass species. Through experiments on our dataset, we find that visual data enables cross-modal olfactory representation learning, and that our learned olfactory representations outperform widely-used hand-crafted features.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20544v1",
        "pdf": "https://arxiv.org/pdf/2511.20544v1"
      },
      "arxiv_id": "2511.20544v1",
      "comment": "Project website at https://smell.cs.columbia.edu",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20543v1",
      "title": "Feature-Modulated UFNO for Improved Prediction of Multiphase Flow in Porous Media",
      "authors": [
        "Alhasan Abdellatif",
        "Hannah P. Menke",
        "Ahmed H. Elsheikh",
        "Florian Doster",
        "Kamaljit Singh"
      ],
      "abstract": "The UNet-enhanced Fourier Neural Operator (UFNO) extends the Fourier Neural Operator (FNO) by incorporating a parallel UNet pathway, enabling the retention of both high- and low-frequency components. While UFNO improves predictive accuracy over FNO, it inefficiently treats scalar inputs (e.g., temperature, injection rate) as spatially distributed fields by duplicating their values across the domain. This forces the model to process redundant constant signals within the frequency domain. Additionally, its standard loss function does not account for spatial variations in error sensitivity, limiting performance in regions of high physical importance. We introduce UFNO-FiLM, an enhanced architecture that incorporates two key innovations. First, we decouple scalar inputs from spatial features using a Feature-wise Linear Modulation (FiLM) layer, allowing the model to modulate spatial feature maps without introducing constant signals into the Fourier transform. Second, we employ a spatially weighted loss function that prioritizes learning in critical regions. Our experiments on subsurface multiphase flow demonstrate a 21\\% reduction in gas saturation Mean Absolute Error (MAE) compared to UFNO, highlighting the effectiveness of our approach in improving predictive accuracy.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20543v1",
        "pdf": "https://arxiv.org/pdf/2511.20543v1"
      },
      "arxiv_id": "2511.20543v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20541v1",
      "title": "Automated Monitoring of Cultural Heritage Artifacts Using Semantic Segmentation",
      "authors": [
        "Andrea Ranieri",
        "Giorgio Palmieri",
        "Silvia Biasotti"
      ],
      "abstract": "This paper addresses the critical need for automated crack detection in the preservation of cultural heritage through semantic segmentation. We present a comparative study of U-Net architectures, using various convolutional neural network (CNN) encoders, for pixel-level crack identification on statues and monuments. A comparative quantitative evaluation is performed on the test set of the OmniCrack30k dataset [1] using popular segmentation metrics including Mean Intersection over Union (mIoU), Dice coefficient, and Jaccard index. This is complemented by an out-of-distribution qualitative evaluation on an unlabeled test set of real-world cracked statues and monuments. Our findings provide valuable insights into the capabilities of different CNN- based encoders for fine-grained crack segmentation. We show that the models exhibit promising generalization capabilities to unseen cultural heritage contexts, despite never having been explicitly trained on images of statues or monuments.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20541v1",
        "pdf": "https://arxiv.org/pdf/2511.20541v1"
      },
      "arxiv_id": "2511.20541v1",
      "comment": "Keywords: Cultural Heritage, Monitoring, Deep Learning, U-Nets, Semantic Segmentation",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20540v1",
      "title": "Proceedings Twentieth Conference on Theoretical Aspects of Rationality and Knowledge",
      "authors": [
        "Adam Bjorndahl"
      ],
      "abstract": "The TARK conference (Theoretical Aspects of Rationality and Knowledge) is a conference that aims to bring together researchers from a wide variety of fields, including computer science, artificial intelligence, game theory, decision theory, philosophy, logic, linguistics, and cognitive science. Its goal is to further our understanding of interdisciplinary issues involving reasoning about rationality and knowledge.\n  Previous conferences have been held biennially around the world since 1986, on the initiative of Joe Halpern (Cornell University). Topics of interest include, but are not limited to, semantic models for knowledge, belief, uncertainty, awareness, bounded rationality, common sense epistemic reasoning, epistemic logic, epistemic game theory, knowledge and action, applications of reasoning about knowledge and other mental states, belief revision, computational social choice, algorithmic game theory, and foundations of multi-agent systems.\n  Information about TARK is available at http://www.tark.org/.\n  These proceedings contain the papers that have been accepted for presentation at the Twentieth Conference on Theoretical Aspects of Rationality and Knowledge (TARK 2025), held July 14--16, 2025, at Heinrich-Heine-Universität, Düsseldorf, Germany. The conference website can be found at https://ccc.cs.uni-duesseldorf.de/tark-2025/.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.GT",
        "cs.MA"
      ],
      "primary_category": "cs.LO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20540v1",
        "pdf": "https://arxiv.org/pdf/2511.20540v1"
      },
      "arxiv_id": "2511.20540v1",
      "comment": "",
      "journal_ref": "EPTCS 437, 2025",
      "has_code": false
    },
    {
      "id": "2511.20532v1",
      "title": "MIMIC-MJX: Neuromechanical Emulation of Animal Behavior",
      "authors": [
        "Charles Y. Zhang",
        "Yuanjia Yang",
        "Aidan Sirbu",
        "Elliott T. T. Abe",
        "Emil Wärnberg",
        "Eric J. Leonardis",
        "Diego E. Aldarondo",
        "Adam Lee",
        "Aaditya Prasad",
        "Jason Foat",
        "Kaiwen Bian",
        "Joshua Park",
        "Rusham Bhatt",
        "Hutton Saunders",
        "Akira Nagamori",
        "Ayesha R. Thanawalla",
        "Kee Wui Huang",
        "Fabian Plum",
        "Hendrik K. Beck",
        "Steven W. Flavell",
        "David Labonte",
        "Blake A. Richards",
        "Bingni W. Brunton",
        "Eiman Azim",
        "Bence P. Ölveczky",
        "Talmo D. Pereira"
      ],
      "abstract": "The primary output of the nervous system is movement and behavior. While recent advances have democratized pose tracking during complex behavior, kinematic trajectories alone provide only indirect access to the underlying control processes. Here we present MIMIC-MJX, a framework for learning biologically-plausible neural control policies from kinematics. MIMIC-MJX models the generative process of motor control by training neural controllers that learn to actuate biomechanically-realistic body models in physics simulation to reproduce real kinematic trajectories. We demonstrate that our implementation is accurate, fast, data-efficient, and generalizable to diverse animal body models. Policies trained with MIMIC-MJX can be utilized to both analyze neural control strategies and simulate behavioral experiments, illustrating its potential as an integrative modeling framework for neuroscience.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "q-bio.NC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20532v1",
        "pdf": "https://arxiv.org/pdf/2511.20532v1"
      },
      "arxiv_id": "2511.20532v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20531v1",
      "title": "Beyond Generation: Multi-Hop Reasoning for Factual Accuracy in Vision-Language Models",
      "authors": [
        "Shamima Hossain"
      ],
      "abstract": "Visual Language Models (VLMs) are powerful generative tools but often produce factually inaccurate outputs due to a lack of robust reasoning capabilities. While extensive research has been conducted on integrating external knowledge for reasoning in large language models (LLMs), such efforts remain underexplored in VLMs, where the challenge is compounded by the need to bridge multiple modalities seamlessly. This work introduces a framework for knowledge-guided reasoning in VLMs, leveraging structured knowledge graphs for multi-hop verification using image-captioning task to illustrate our framework. Our approach enables systematic reasoning across multiple steps, including visual entity recognition, knowledge graph traversal, and fact-based caption refinement. We evaluate the framework using hierarchical, triple-based and bullet-point based knowledge representations, analyzing their effectiveness in factual accuracy and logical inference. Empirical results show that our approach improves factual accuracy by approximately 31% on preliminary experiments on a curated dataset of mixtures from Google Landmarks v2, Conceptual captions and Coco captions revealing key insights into reasoning patterns and failure modes. This work demonstrates the potential of integrating external knowledge for advancing reasoning in VLMs, paving the way for more reliable and knowledgable multimodal systems.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20531v1",
        "pdf": "https://arxiv.org/pdf/2511.20531v1"
      },
      "arxiv_id": "2511.20531v1",
      "comment": "Accepted as poster at NewInML Workshop ICML, 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20526v1",
      "title": "Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam",
      "authors": [
        "Xinran Wang",
        "Boran Zhu",
        "Shujuan Zhou",
        "Ziwen Long",
        "Dehua Zhou",
        "Shu Zhang"
      ],
      "abstract": "Background: As large language models (LLMs) become increasingly integrated into digital health education and assessment workflows, their capabilities in supporting high-stakes, domain-specific certification tasks remain underexplored.In China, the national pharmacist licensure exam serves as a standardized benchmark for evaluating pharmacists' clinical and theoretical competencies. Objective: This study aimed to compare the performance of two LLMs: ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021), and to discuss the implications of these performance differences for AI-enabled formative evaluation. Methods: A total of 2,306 multiple-choice (text-only) questions were compiled from official exams, training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Pearson's Chi-squared test was used to compare overall performance, and Fisher's exact test was applied to year-wise multiple-choice accuracy. Results: DeepSeek-R1 outperformed ChatGPT-4o with a significantly higher overall accuracy (90.0% vs. 76.1%, p < 0.001). Unit-level analyses revealed consistent advantages for DeepSeek-R1, particularly in foundational and clinical synthesis modules. While year-by-year multiple-choice performance also favored DeepSeek-R1, this performance gap did not reach statistical significance in any specific unit-year (all p > 0.05). Conclusion: DeepSeek-R1 demonstrated robust alignment with the structural and semantic demands of the pharmacist licensure exam. These findings suggest that domain-specific models warrant further investigation for this context, while also reinforcing the necessity of human oversight in legally and ethically sensitive contexts.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20526v1",
        "pdf": "https://arxiv.org/pdf/2511.20526v1"
      },
      "arxiv_id": "2511.20526v1",
      "comment": "15 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20525v1",
      "title": "Mistake Attribution: Fine-Grained Mistake Understanding in Egocentric Videos",
      "authors": [
        "Yayuan Li",
        "Aadit Jain",
        "Filippos Bellos",
        "Jason J. Corso"
      ],
      "abstract": "We introduce Mistake Attribution (MATT), a task for fine-grained understanding of human mistakes in egocentric video. Unlike prior mistake understanding work, which lacks fine-grained output, MATT concretely attributes mistakes to the input instruction text or the attempt video. MATT determines what part of the instruction is violated (semantic role), when the deviation becomes irreversible (the Point-of-No-Return, PNR), and where the mistake appears in the PNR frame. We develop MisEngine, a data engine that automatically constructs attribution-rich mistake samples from existing datasets and inherits their annotations. Applied to large egocentric corpora, MisEngine yields EPIC-KITCHENS-M and Ego4D-M, two datasets that are up to two orders of magnitude larger than prior mistake datasets. We then present MisFormer, a unified attention-based model for mistake attribution across semantic (what), temporal (when), and spatial (where) dimensions, trained using MisEngine supervision. Experiments on our new datasets and prior benchmarks show that MisFormer outperforms strong video-language, temporal localization, hand-object interaction, and mistake-detection baselines.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20525v1",
        "pdf": "https://arxiv.org/pdf/2511.20525v1"
      },
      "arxiv_id": "2511.20525v1",
      "comment": "11 pages, 4 figures, 6 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20520v1",
      "title": "HBridge: H-Shape Bridging of Heterogeneous Experts for Unified Multimodal Understanding and Generation",
      "authors": [
        "Xiang Wang",
        "Zhifei Zhang",
        "He Zhang",
        "Zhe Lin",
        "Yuqian Zhou",
        "Qing Liu",
        "Shiwei Zhang",
        "Yijun Li",
        "Shaoteng Liu",
        "Haitian Zheng",
        "Jason Kuen",
        "Yuehuan Wang",
        "Changxin Gao",
        "Nong Sang"
      ],
      "abstract": "Recent unified models integrate understanding experts (e.g., LLMs) with generative experts (e.g., diffusion models), achieving strong multimodal performance. However, recent advanced methods such as BAGEL and LMFusion follow the Mixture-of-Transformers (MoT) paradigm, adopting a symmetric design that mirrors one expert to another for convenient initialization and fusion, which remains suboptimal due to inherent modality discrepancies. In this work, we propose HBridge, an asymmetric H-shaped architecture that enables heterogeneous experts to optimally leverage pretrained priors from their respective modality domains. Unlike prior dense fusion strategies that straightforwardly connect all layers between experts via shared attention, HBridge selectively bridges intermediate layers, reducing over 40% attention sharing, which improves efficiency and enhances generation quality. Shallow and deep layers, which capture modality-specific representations, are decoupled, while mid-layer bridging promotes semantic alignment. To further strengthen cross-modal coherence, we introduce semantic reconstruction tokens that explicitly guide the generative expert to reconstruct visual semantic tokens of the target image. Extensive experiments across multiple benchmarks demonstrate the effectiveness and superior performance of HBridge, establishing a new paradigm for unified multimodal generation.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20520v1",
        "pdf": "https://arxiv.org/pdf/2511.20520v1"
      },
      "arxiv_id": "2511.20520v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20516v1",
      "title": "Adam Simplified: Bias Correction Simplified",
      "authors": [
        "Sam Laing",
        "Antonio Orvieto"
      ],
      "abstract": "The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $β_1, β_2 \\in [0,1)$. Our findings challenge the universal inclusion of this component.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20516v1",
        "pdf": "https://arxiv.org/pdf/2511.20516v1"
      },
      "arxiv_id": "2511.20516v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20515v1",
      "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
      "authors": [
        "Kuniaki Saito",
        "Risa Shinoda",
        "Shohei Tanaka",
        "Tosho Hirasawa",
        "Fumio Okura",
        "Yoshitaka Ushiku"
      ],
      "abstract": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20515v1",
        "pdf": "https://arxiv.org/pdf/2511.20515v1"
      },
      "arxiv_id": "2511.20515v1",
      "comment": "Project Page: https://dahlian00.github.io/AlignBench/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.20513v1",
      "title": "DesignPref: Capturing Personal Preferences in Visual Design Generation",
      "authors": [
        "Yi-Hao Peng",
        "Jeffrey P. Bigham",
        "Jason Wu"
      ],
      "abstract": "Generative models, such as large language models and text-to-image diffusion models, are increasingly used to create visual designs like user interfaces (UIs) and presentation slides. Finetuning and benchmarking these generative models have often relied on datasets of human-annotated design preferences. Yet, due to the subjective and highly personalized nature of visual design, preference varies widely among individuals. In this paper, we study this problem by introducing DesignPref, a dataset of 12k pairwise comparisons of UI design generation annotated by 20 professional designers with multi-level preference ratings. We found that among trained designers, substantial levels of disagreement exist (Krippendorff's alpha = 0.25 for binary preferences). Natural language rationales provided by these designers indicate that disagreements stem from differing perceptions of various design aspect importance and individual preferences. With DesignPref, we demonstrate that traditional majority-voting methods for training aggregated judge models often do not accurately reflect individual preferences. To address this challenge, we investigate multiple personalization strategies, particularly fine-tuning or incorporating designer-specific annotations into RAG pipelines. Our results show that personalized models consistently outperform aggregated baseline models in predicting individual designers' preferences, even when using 20 times fewer examples. Our work provides the first dataset to study personalized visual design evaluation and support future research into modeling individual design taste.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20513v1",
        "pdf": "https://arxiv.org/pdf/2511.20513v1"
      },
      "arxiv_id": "2511.20513v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20510v1",
      "title": "FRAGMENTA: End-to-end Fragmentation-based Generative Model with Agentic Tuning for Drug Lead Optimization",
      "authors": [
        "Yuto Suzuki",
        "Paul Awolade",
        "Daniel V. LaBarbera",
        "Farnoush Banaei-Kashani"
      ],
      "abstract": "Molecule generation using generative AI is vital for drug discovery, yet class-specific datasets often contain fewer than 100 training examples. While fragment-based models handle limited data better than atom-based approaches, existing heuristic fragmentation limits diversity and misses key fragments. Additionally, model tuning typically requires slow, indirect collaboration between medicinal chemists and AI engineers. We introduce FRAGMENTA, an end-to-end framework for drug lead optimization comprising: 1) a novel generative model that reframes fragmentation as a \"vocabulary selection\" problem, using dynamic Q-learning to jointly optimize fragmentation and generation; and 2) an agentic AI system that refines objectives via conversational feedback from domain experts. This system removes the AI engineer from the loop and progressively learns domain knowledge to eventually automate tuning. In real-world cancer drug discovery experiments, FRAGMENTA's Human-Agent configuration identified nearly twice as many high-scoring molecules as baselines. Furthermore, the fully autonomous Agent-Agent system outperformed traditional Human-Human tuning, demonstrating the efficacy of agentic tuning in capturing expert intent.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20510v1",
        "pdf": "https://arxiv.org/pdf/2511.20510v1"
      },
      "arxiv_id": "2511.20510v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20509v1",
      "title": "DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning",
      "authors": [
        "Mihaela Hudişteanu",
        "Edwige Cyffers",
        "Nikita P. Kalinin"
      ],
      "abstract": "Adaptive optimizers are the de facto standard in non-private training as they often enable faster convergence and improved performance. In contrast, differentially private (DP) training is still predominantly performed with DP-SGD, typically requiring extensive compute and hyperparameter tuning. We propose DP-MicroAdam, a memory-efficient and sparsity-aware adaptive DP optimizer. We prove that DP-MicroAdam converges in stochastic non-convex optimization at the optimal $\\mathcal{O}(1/\\sqrt{T})$ rate, up to privacy-dependent constants. Empirically, DP-MicroAdam outperforms existing adaptive DP optimizers and achieves competitive or superior accuracy compared to DP-SGD across a range of benchmarks, including CIFAR-10, large-scale ImageNet training, and private fine-tuning of pretrained transformers. These results demonstrate that adaptive optimization can improve both performance and stability under differential privacy.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20509v1",
        "pdf": "https://arxiv.org/pdf/2511.20509v1"
      },
      "arxiv_id": "2511.20509v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20507v1",
      "title": "The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models",
      "authors": [
        "Nathan Roll",
        "Jill Kries",
        "Flora Jin",
        "Catherine Wang",
        "Ann Marie Finley",
        "Meghan Sumner",
        "Cory Shain",
        "Laura Gwilliams"
      ],
      "abstract": "Large language models (LLMs) have emerged as a candidate \"model organism\" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20507v1",
        "pdf": "https://arxiv.org/pdf/2511.20507v1"
      },
      "arxiv_id": "2511.20507v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20503v1",
      "title": "Generative Modeling with Manifold Percolation",
      "authors": [
        "Rui Tong"
      ],
      "abstract": "Generative modeling is typically framed as learning mapping rules, but from an observer's perspective without access to these rules, the task manifests as disentangling the geometric support from the probability distribution. We propose that Continuum Percolation is uniquely suited for this support analysis, as the sampling process effectively projects high-dimensional density estimation onto a geometric counting problem on the support. In this work, we establish a rigorous isomorphism between the topological phase transitions of Random Geometric Graphs and the underlying data manifold in high-dimensional space. By analyzing the relationship between our proposed Percolation Shift metric and FID, we demonstrate that our metric captures structural pathologies (such as implicit mode collapse) where statistical metrics fail. Finally, we translate this topological phenomenon into a differentiable loss function to guide training. Experimental results confirm that this approach not only prevents manifold shrinkage but drives the model toward a state of \"Hyper-Generalization,\" achieving good fidelity and verified topological expansion.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20503v1",
        "pdf": "https://arxiv.org/pdf/2511.20503v1"
      },
      "arxiv_id": "2511.20503v1",
      "comment": "13 pages, 7 figures. Correspondence: Rui.Tong@warwick.ac.uk",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20501v1",
      "title": "A Physics-Informed Loss Function for Boundary-Consistent and Robust Artery Segmentation in DSA Sequences",
      "authors": [
        "Muhammad Irfan",
        "Nasir Rahim",
        "Khalid Mahmood Malik"
      ],
      "abstract": "Accurate extraction and segmentation of the cerebral arteries from digital subtraction angiography (DSA) sequences is essential for developing reliable clinical management models of complex cerebrovascular diseases. Conventional loss functions often rely solely on pixel-wise overlap, overlooking the geometric and physical consistency of vascular boundaries, which can lead to fragmented or unstable vessel predictions. To overcome this limitation, we propose a novel \\textit{Physics-Informed Loss} (PIL) that models the interaction between the predicted and ground-truth boundaries as an elastic process inspired by dislocation theory in materials physics. This formulation introduces a physics-based regularization term that enforces smooth contour evolution and structural consistency, allowing the network to better capture fine vascular geometry. The proposed loss is integrated into several segmentation architectures, including U-Net, U-Net++, SegFormer, and MedFormer, and evaluated on two public benchmarks: DIAS and DSCA. Experimental results demonstrate that PIL consistently outperforms conventional loss functions such as Cross-Entropy, Dice, Active Contour, and Surface losses, achieving superior sensitivity, F1 score, and boundary coherence. These findings confirm that the incorporation of physics-based boundary interactions into deep neural networks improves both the precision and robustness of vascular segmentation in dynamic angiographic imaging. The implementation of the proposed method is publicly available at https://github.com/irfantahir301/Physicsis_loss.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20501v1",
        "pdf": "https://arxiv.org/pdf/2511.20501v1"
      },
      "arxiv_id": "2511.20501v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20500v1",
      "title": "From One Attack Domain to Another: Contrastive Transfer Learning with Siamese Networks for APT Detection",
      "authors": [
        "Sidahmed Benabderrahmane",
        "Talal Rahwan"
      ],
      "abstract": "Advanced Persistent Threats (APT) pose a major cybersecurity challenge due to their stealth, persistence, and adaptability. Traditional machine learning detectors struggle with class imbalance, high dimensional features, and scarce real world traces. They often lack transferability-performing well in the training domain but degrading in novel attack scenarios. We propose a hybrid transfer framework that integrates Transfer Learning, Explainable AI (XAI), contrastive learning, and Siamese networks to improve cross-domain generalization. An attention-based autoencoder supports knowledge transfer across domains, while Shapley Additive exPlanations (SHAP) select stable, informative features to reduce dimensionality and computational cost. A Siamese encoder trained with a contrastive objective aligns source and target representations, increasing anomaly separability and mitigating feature drift. We evaluate on real-world traces from the DARPA Transparent Computing (TC) program and augment with synthetic attack scenarios to test robustness. Across source to target transfers, the approach delivers improved detection scores with classical and deep baselines, demonstrating a scalable, explainable, and transferable solution for APT detection.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20500v1",
        "pdf": "https://arxiv.org/pdf/2511.20500v1"
      },
      "arxiv_id": "2511.20500v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20497v1",
      "title": "Quantifying the Privacy Implications of High-Fidelity Synthetic Network Traffic",
      "authors": [
        "Van Tran",
        "Shinan Liu",
        "Tian Li",
        "Nick Feamster"
      ],
      "abstract": "To address the scarcity and privacy concerns of network traffic data, various generative models have been developed to produce synthetic traffic. However, synthetic traffic is not inherently privacy-preserving, and the extent to which it leaks sensitive information, and how to measure such leakage, remain largely unexplored. This challenge is further compounded by the diversity of model architectures, which shape how traffic is represented and synthesized. We introduce a comprehensive set of privacy metrics for synthetic network traffic, combining standard approaches like membership inference attacks (MIA) and data extraction attacks with network-specific identifiers and attributes. Using these metrics, we systematically evaluate the vulnerability of different representative generative models and examine the factors that influence attack success. Our results reveal substantial variability in privacy risks across models and datasets. MIA success ranges from 0% to 88%, and up to 100% of network identifiers can be recovered from generated traffic, highlighting serious privacy vulnerabilities. We further identify key factors that significantly affect attack outcomes, including training data diversity and how well the generative model fits the training data. These findings provide actionable guidance for designing and deploying generative models that minimize privacy leakage, establishing a foundation for safer synthetic network traffic generation.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20497v1",
        "pdf": "https://arxiv.org/pdf/2511.20497v1"
      },
      "arxiv_id": "2511.20497v1",
      "comment": "14 pages, 13 Figures, 6 Tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20490v1",
      "title": "MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology",
      "authors": [
        "Kiril Vasilev",
        "Alexandre Misrahi",
        "Eeshaan Jain",
        "Phil F Cheng",
        "Petros Liakopoulos",
        "Olivier Michielin",
        "Michael Moor",
        "Charlotte Bunne"
      ],
      "abstract": "Multimodal Large Language Models (LLMs) hold promise for biomedical reasoning, but current benchmarks fail to capture the complexity of real-world clinical workflows. Existing evaluations primarily assess unimodal, decontextualized question-answering, overlooking multi-agent decision-making environments such as Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in oncology, where diagnostic and prognostic tasks require integrating heterogeneous data and evolving insights over time. Current benchmarks lack this longitudinal and multimodal complexity. We introduce MTBBench, an agentic benchmark simulating MTB-style decision-making through clinically challenging, multimodal, and longitudinal oncology questions. Ground truth annotations are validated by clinicians via a co-developed app, ensuring clinical relevance. We benchmark multiple open and closed-source LLMs and show that, even at scale, they lack reliability -- frequently hallucinating, struggling with reasoning from time-resolved data, and failing to reconcile conflicting evidence or different modalities. To address these limitations, MTBBench goes beyond benchmarking by providing an agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively. Overall, MTBBench offers a challenging and realistic testbed for advancing multimodal LLM reasoning, reliability, and tool-use with a focus on MTB environments in precision oncology.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20490v1",
        "pdf": "https://arxiv.org/pdf/2511.20490v1"
      },
      "arxiv_id": "2511.20490v1",
      "comment": "Accepted to NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20489v1",
      "title": "InferF: Declarative Factorization of AI/ML Inferences over Joins",
      "authors": [
        "Kanchan Chowdhury",
        "Lixi Zhou",
        "Lulu Xie",
        "Xinwei Fu",
        "Jia Zou"
      ],
      "abstract": "Real-world AI/ML workflows often apply inference computations to feature vectors joined from multiple datasets. To avoid the redundant AI/ML computations caused by repeated data records in the join's output, factorized ML has been proposed to decompose ML computations into sub-computations to be executed on each normalized dataset. However, there is insufficient discussion on how factorized ML could impact AI/ML inference over multi-way joins. To address the limitations, we propose a novel declarative InferF system, focusing on the factorization of arbitrary inference workflows represented as analyzable expressions over the multi-way joins. We formalize our problem to flexibly push down partial factorized computations to qualified nodes in the join tree to minimize the overall inference computation and join costs and propose two algorithms to resolve the problem: (1) a greedy algorithm based on a per-node cost function that estimates the influence on overall latency if a subset of factorized computations is pushed to a node, and (2) a genetic algorithm for iteratively enumerating and evaluating promising factorization plans. We implement InferF on Velox, an open-sourced database engine from Meta, evaluate it on real-world datasets, observed up to 11.3x speedups, and systematically summarized the factors that determine when factorized ML can benefit AI/ML inference workflows.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20489v1",
        "pdf": "https://arxiv.org/pdf/2511.20489v1"
      },
      "arxiv_id": "2511.20489v1",
      "comment": "Accepted to SIGMOD 2026 as full research paper. This archived version has a full appendix",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20480v1",
      "title": "Ranking-Enhanced Anomaly Detection Using Active Learning-Assisted Attention Adversarial Dual AutoEncoders",
      "authors": [
        "Sidahmed Benabderrahmane",
        "James Cheney",
        "Talal Rahwan"
      ],
      "abstract": "Advanced Persistent Threats (APTs) pose a significant challenge in cybersecurity due to their stealthy and long-term nature. Modern supervised learning methods require extensive labeled data, which is often scarce in real-world cybersecurity environments. In this paper, we propose an innovative approach that leverages AutoEncoders for unsupervised anomaly detection, augmented by active learning to iteratively improve the detection of APT anomalies. By selectively querying an oracle for labels on uncertain or ambiguous samples, we minimize labeling costs while improving detection rates, enabling the model to improve its detection accuracy with minimal data while reducing the need for extensive manual labeling. We provide a detailed formulation of the proposed Attention Adversarial Dual AutoEncoder-based anomaly detection framework and show how the active learning loop iteratively enhances the model. The framework is evaluated on real-world imbalanced provenance trace databases produced by the DARPA Transparent Computing program, where APT-like attacks constitute as little as 0.004\\% of the data. The datasets span multiple operating systems, including Android, Linux, BSD, and Windows, and cover two attack scenarios. The results have shown significant improvements in detection rates during active learning and better performance compared to other existing approaches.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20480v1",
        "pdf": "https://arxiv.org/pdf/2511.20480v1"
      },
      "arxiv_id": "2511.20480v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20478v1",
      "title": "NVIDIA Nemotron Parse 1.1",
      "authors": [
        "Kateryna Chumachenko",
        "Amala Sanjay Deshmukh",
        "Jarno Seppanen",
        "Ilia Karmanov",
        "Chia-Chih Chen",
        "Lukas Voegtle",
        "Philipp Fischer",
        "Marek Wawrzos",
        "Saeid Motiian",
        "Roman Ageev",
        "Kedi Wu",
        "Alexandre Milesi",
        "Maryam Moosaei",
        "Krzysztof Pawelec",
        "Padmavathy Subramanian",
        "Mehrzad Samadi",
        "Xin Yu",
        "Celina Dear",
        "Sarah Stoddard",
        "Jenna Diamond",
        "Jesse Oliver",
        "Leanna Chraghchian",
        "Patrick Skelly",
        "Tom Balough",
        "Yao Xu",
        "Jane Polak Scowcroft",
        "Daniel Korzekwa",
        "Darragh Hanley",
        "Sandip Bhaskar",
        "Timo Roman",
        "Karan Sapra",
        "Andrew Tao",
        "Bryan Catanzaro"
      ],
      "abstract": "We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20478v1",
        "pdf": "https://arxiv.org/pdf/2511.20478v1"
      },
      "arxiv_id": "2511.20478v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20474v1",
      "title": "Modular Deep Learning Framework for Assistive Perception: Gaze, Affect, and Speaker Identification",
      "authors": [
        "Akshit Pramod Anchan",
        "Jewelith Thomas",
        "Sritama Roy"
      ],
      "abstract": "Developing comprehensive assistive technologies requires the seamless integration of visual and auditory perception. This research evaluates the feasibility of a modular architecture inspired by core functionalities of perceptive systems like 'Smart Eye.' We propose and benchmark three independent sensing modules: a Convolutional Neural Network (CNN) for eye state detection (drowsiness/attention), a deep CNN for facial expression recognition, and a Long Short-Term Memory (LSTM) network for voice-based speaker identification. Utilizing the Eyes Image, FER2013, and customized audio datasets, our models achieved accuracies of 93.0%, 97.8%, and 96.89%, respectively. This study demonstrates that lightweight, domain-specific models can achieve high fidelity on discrete tasks, establishing a validated foundation for future real-time, multimodal integration in resource-constrained assistive devices.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20474v1",
        "pdf": "https://arxiv.org/pdf/2511.20474v1"
      },
      "arxiv_id": "2511.20474v1",
      "comment": "10 pages, 9 figures, and 3 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20471v1",
      "title": "Universe of Thoughts: Enabling Creative Reasoning with Large Language Models",
      "authors": [
        "Yuto Suzuki",
        "Farnoush Banaei-Kashani"
      ],
      "abstract": "Reasoning based on Large Language Models (LLMs) has garnered increasing attention due to outstanding performance of these models in mathematical and complex logical tasks. Beginning with the Chain-of-Thought (CoT) prompting technique, numerous reasoning methods have emerged that decompose problems into smaller, sequential steps (or thoughts). However, existing reasoning models focus on conventional problem-solving and do not necessarily generate creative solutions by ``creative reasoning''. In domains where the solution space is expansive and conventional solutions are suboptimal, such as drug discovery or business strategization, creative reasoning to discover innovative solutions is crucial. To address this gap, first we introduce a computational framework for creative reasoning inspired by established cognitive science principles. With this framework, we propose three core creative reasoning paradigms, namely, \\textit{combinational}, \\textit{exploratory}, and \\textit{transformative} reasoning, where each offers specific directions for systematic exploration of the universe of thoughts to generate creative solutions. Next, to materialize this framework using LLMs, we introduce the \\textit{Universe of Thoughts} (or \\textit{UoT}, for short), a novel set of methods to implement the aforementioned three creative processes. Finally, we introduce three novel tasks that necessitate creative problem-solving, along with an evaluation benchmark to assess creativity from three orthogonal perspectives: feasibility as constraint, and utility and novelty as metrics. With a comparative analysis against the state-of-the-art (SOTA) reasoning techniques as well as representative commercial models with reasoning capability, we show that UoT demonstrates superior performance in creative reasoning.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20471v1",
        "pdf": "https://arxiv.org/pdf/2511.20471v1"
      },
      "arxiv_id": "2511.20471v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20470v1",
      "title": "Efficient and Fast Generative-Based Singing Voice Separation using a Latent Diffusion Model",
      "authors": [
        "Genís Plaja-Roglans",
        "Yun-Ning Hung",
        "Xavier Serra",
        "Igor Pereira"
      ],
      "abstract": "Extracting individual elements from music mixtures is a valuable tool for music production and practice. While neural networks optimized to mask or transform mixture spectrograms into the individual source(s) have been the leading approach, the source overlap and correlation in music signals poses an inherent challenge. Also, accessing all sources in the mixture is crucial to train these systems, while complicated. Attempts to address these challenges in a generative fashion exist, however, the separation performance and inference efficiency remain limited. In this work, we study the potential of diffusion models to advance toward bridging this gap, focusing on generative singing voice separation relying only on corresponding pairs of isolated vocals and mixtures for training. To align with creative workflows, we leverage latent diffusion: the system generates samples encoded in a compact latent space, and subsequently decodes these into audio. This enables efficient optimization and faster inference. Our system is trained using only open data. We outperform existing generative separation systems, and level the compared non-generative systems on a list of signal quality measures and on interference removal. We provide a noise robustness study on the latent encoder, providing insights on its potential for the task. We release a modular toolkit for further research on the topic.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20470v1",
        "pdf": "https://arxiv.org/pdf/2511.20470v1"
      },
      "arxiv_id": "2511.20470v1",
      "comment": "Accepted for oral presentation at IJCNN 2025",
      "journal_ref": "2025 International Joint Conference on Neural Networks (IJCNN), Rome, Italy, 2025, pp. 1-8",
      "has_code": false
    },
    {
      "id": "2511.20469v1",
      "title": "Dance Style Classification using Laban-Inspired and Frequency-Domain Motion Features",
      "authors": [
        "Ben Hamscher",
        "Arnold Brosch",
        "Nicolas Binninger",
        "Maksymilian Jan Dejna",
        "Kira Maag"
      ],
      "abstract": "Dance is an essential component of human culture and serves as a tool for conveying emotions and telling stories. Identifying and distinguishing dance genres based on motion data is a complex problem in human activity recognition, as many styles share similar poses, gestures, and temporal motion patterns. This work presents a lightweight framework for classifying dance styles that determines motion characteristics based on pose estimates extracted from videos. We propose temporal-spatial descriptors inspired by Laban Movement Analysis. These features capture local joint dynamics such as velocity, acceleration, and angular movement of the upper body, enabling a structured representation of spatial coordination. To further encode rhythmic and periodic aspects of movement, we integrate Fast Fourier Transform features that characterize movement patterns in the frequency domain. The proposed approach achieves robust classification of different dance styles with low computational effort, as complex model architectures are not required, and shows that interpretable motion representations can effectively capture stylistic nuances.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20469v1",
        "pdf": "https://arxiv.org/pdf/2511.20469v1"
      },
      "arxiv_id": "2511.20469v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20468v1",
      "title": "DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs",
      "authors": [
        "Yuanhao Li",
        "Mingshan Liu",
        "Hongbo Wang",
        "Yiding Zhang",
        "Yifei Ma",
        "Wei Tan"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20468v1",
        "pdf": "https://arxiv.org/pdf/2511.20468v1"
      },
      "arxiv_id": "2511.20468v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20462v1",
      "title": "STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow",
      "authors": [
        "Jiatao Gu",
        "Ying Shen",
        "Tianrong Chen",
        "Laurent Dinh",
        "Yuyang Wang",
        "Miguel Angel Bautista",
        "David Berthelot",
        "Josh Susskind",
        "Shuangfei Zhai"
      ],
      "abstract": "Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20462v1",
        "pdf": "https://arxiv.org/pdf/2511.20462v1"
      },
      "arxiv_id": "2511.20462v1",
      "comment": "21 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20460v1",
      "title": "Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search",
      "authors": [
        "Yunqi Zhou",
        "Chengjie Jiang",
        "Chun Yuan",
        "Jing Li"
      ],
      "abstract": "With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model look where it matters before prediction becomes crucial. Therefore, we present ZoomSearch, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines Adaptive Multi-Branch Zoom Search, which performs a hierarchical search over image patches to localize query-relevant regions, with Layout-Aware Patch Reassembly, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, ZoomSearch attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by 26.3% on LRS-VQA and 114.8\\% on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by 20%~44% in speed.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20460v1",
        "pdf": "https://arxiv.org/pdf/2511.20460v1"
      },
      "arxiv_id": "2511.20460v1",
      "comment": "17 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20459v1",
      "title": "Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts",
      "authors": [
        "Mosab Rezaei",
        "Mina Rajaei Moghadam",
        "Abdul Rahman Shaikh",
        "Hamed Alhoori",
        "Reva Freedman"
      ],
      "abstract": "Recent advances in large language models have created new opportunities for stylometry, the study of writing styles and authorship. Two challenges, however, remain central: training generative models when no paired data exist, and evaluating stylistic text without relying only on human judgment. In this work, we present a framework for both generating and evaluating sentences in the style of 19th-century novelists. Large language models are fine-tuned with minimal, single-token prompts to produce text in the voices of authors such as Dickens, Austen, Twain, Alcott, and Melville. To assess these generative models, we employ a transformer-based detector trained on authentic sentences, using it both as a classifier and as a tool for stylistic explanation. We complement this with syntactic comparisons and explainable AI methods, including attention-based and gradient-based analyses, to identify the linguistic cues that drive stylistic imitation. Our findings show that the generated text reflects the authors' distinctive patterns and that AI-based evaluation offers a reliable alternative to human assessment. All artifacts of this work are published online.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20459v1",
        "pdf": "https://arxiv.org/pdf/2511.20459v1"
      },
      "arxiv_id": "2511.20459v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20457v1",
      "title": "A Fully Probabilistic Tensor Network for Regularized Volterra System Identification",
      "authors": [
        "Afra Kilic",
        "Kim Batselier"
      ],
      "abstract": "Modeling nonlinear systems with Volterra series is challenging because the number of kernel coefficients grows exponentially with the model order. This work introduces Bayesian Tensor Network Volterra kernel machines (BTN-V), extending the Bayesian Tensor Network framework to Volterra system identification. BTN-V represents Volterra kernels using canonical polyadic decomposition, reducing model complexity from O(I^D) to O(DIR). By treating all tensor components and hyperparameters as random variables, BTN-V provides predictive uncertainty estimation at no additional computational cost. Sparsity-inducing hierarchical priors enable automatic rank determination and the learning of fading-memory behavior directly from data, improving interpretability and preventing overfitting. Empirical results demonstrate competitive accuracy, enhanced uncertainty quantification, and reduced computational cost.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20457v1",
        "pdf": "https://arxiv.org/pdf/2511.20457v1"
      },
      "arxiv_id": "2511.20457v1",
      "comment": "6 pages, 3 figures, 1 table. Submitted to IFAC 2026. Code available at: https://github.com/afrakilic/BTN_Volterra_Sys_ID",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.20456v1",
      "title": "Towards Trustworthy Wi-Fi Sensing: Systematic Evaluation of Deep Learning Model Robustness to Adversarial Attacks",
      "authors": [
        "Shreevanth Krishnaa Gopalakrishnan",
        "Stephen Hailes"
      ],
      "abstract": "Machine learning has become integral to Channel State Information (CSI)-based human sensing systems and is expected to power applications such as device-free activity recognition and identity detection in future cellular and Wi-Fi generations. However, these systems rely on models whose decisions can be subtly perturbed, raising concerns for security and reliability in ubiquitous sensing. Quantifying and understanding the robustness of such models, defined as their ability to maintain accurate predictions under adversarial perturbations, is therefore critical before wireless sensing can be safely deployed in real-world environments.\n  This work presents a systematic evaluation of the robustness of CSI deep learning models under diverse threat models (white-box, black-box/transfer, and universal perturbations) and varying degrees of attack realism. We establish a framework to compare compact temporal autoencoder models with larger deep architectures across three public datasets, quantifying how model scale, training regime, and physical constraints influence robustness. Our experiments show that smaller models, while efficient and equally performant on clean data, are markedly less robust. We further confirm that physically realizable signal-space perturbations, designed to be feasible in real wireless channels, significantly reduce attack success compared to unconstrained feature-space attacks. Adversarial training mitigates these vulnerabilities, improving mean robust accuracy with only moderate degradation in clean performance across both model classes. As wireless sensing advances towards reliable, cross-domain operation, these findings provide quantitative baselines for robustness estimation and inform design principles for secure and trustworthy human-centered sensing systems.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20456v1",
        "pdf": "https://arxiv.org/pdf/2511.20456v1"
      },
      "arxiv_id": "2511.20456v1",
      "comment": "19 pages, 8 figures, 7 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20446v1",
      "title": "Learning to Generate Human-Human-Object Interactions from Textual Descriptions",
      "authors": [
        "Jeonghyeon Na",
        "Sangwon Baik",
        "Inhee Lee",
        "Junyoung Lee",
        "Hanbyul Joo"
      ],
      "abstract": "The way humans interact with each other, including interpersonal distances, spatial configuration, and motion, varies significantly across different situations. To enable machines to understand such complex, context-dependent behaviors, it is essential to model multiple people in relation to the surrounding scene context. In this paper, we present a novel research problem to model the correlations between two people engaged in a shared interaction involving an object. We refer to this formulation as Human-Human-Object Interactions (HHOIs). To overcome the lack of dedicated datasets for HHOIs, we present a newly captured HHOIs dataset and a method to synthesize HHOI data by leveraging image generative models. As an intermediary, we obtain individual human-object interaction (HOIs) and human-human interaction (HHIs) from the HHOIs, and with these data, we train an text-to-HOI and text-to-HHI model using score-based diffusion model. Finally, we present a unified generative framework that integrates the two individual model, capable of synthesizing complete HHOIs in a single advanced sampling process. Our method extends HHOI generation to multi-human settings, enabling interactions involving more than two individuals. Experimental results show that our method generates realistic HHOIs conditioned on textual descriptions, outperforming previous approaches that focus only on single-human HOIs. Furthermore, we introduce multi-human motion generation involving objects as an application of our framework.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20446v1",
        "pdf": "https://arxiv.org/pdf/2511.20446v1"
      },
      "arxiv_id": "2511.20446v1",
      "comment": "Project Page: https://tlb-miss.github.io/hhoi/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.20445v1",
      "title": "Diffusion for Fusion: Designing Stellarators with Generative AI",
      "authors": [
        "Misha Padidar",
        "Teresa Huang",
        "Andrew Giuliani",
        "Marina Spivak"
      ],
      "abstract": "Stellarators are a prospective class of fusion-based power plants that confine a hot plasma with three-dimensional magnetic fields. Typically framed as a PDE-constrained optimization problem, stellarator design is a time-consuming process that can take hours to solve on a computing cluster. Developing fast methods for designing stellarators is crucial for advancing fusion research. Given the recent development of large datasets of optimized stellarators, machine learning approaches have emerged as a potential candidate. Motivated by this, we present an open inverse problem to the machine learning community: to rapidly generate high-quality stellarator designs which have a set of desirable characteristics. As a case study in the problem space, we train a conditional diffusion model on data from the QUASR database to generate quasisymmetric stellarator designs with desirable characteristics (aspect ratio and mean rotational transform). The diffusion model is applied to design stellarators with characteristics not seen during training. We provide evaluation protocols and show that many of the generated stellarators exhibit solid performance: less than 5% deviation from quasisymmetry and the target characteristics. The modest deviation from quasisymmetry highlights an opportunity to reach the sub 1% target. Beyond the case study, we share multiple promising avenues for generative modeling to advance stellarator design.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "physics.plasm-ph"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20445v1",
        "pdf": "https://arxiv.org/pdf/2511.20445v1"
      },
      "arxiv_id": "2511.20445v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20439v1",
      "title": "Object-Centric Vision Token Pruning for Vision Language Models",
      "authors": [
        "Guangyuan Li",
        "Rongzhen Zhao",
        "Jinhong Deng",
        "Yanbo Wang",
        "Joni Pajarinen"
      ],
      "abstract": "In Vision Language Models (VLMs), vision tokens are quantity-heavy yet information-dispersed compared with language tokens, thus consume too much unnecessary computation. Pruning redundant vision tokens for high VLM inference efficiency has been continuously studied but all existing methods resort to indirect and non-guaranteed ways. We propose OC-VTP, a direct and guaranteed approach to select the most representative vision tokens for high-efficiency yet accuracy-preserving VLM inference. Our OC-VTP requires merely light-weight pre-training of a small object-centric vision token pruner, which can then be inserted into existing VLMs, without fine-tuning of any models on any datasets. It is gauranteed that the most representative vision tokens are kept by minimizing the error in reconstructing the original unpruned tokens from the selected ones. Across any vision pruning ratios, i.e., inference efficiency, our OC-VTP consistently helps mainstream VLMs to preserve the highest inference accuracy. Our pruning also demonstrates interesting interpretability. Our codes are available at https://github.com/GarryLarry010131/OC-VTP.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20439v1",
        "pdf": "https://arxiv.org/pdf/2511.20439v1"
      },
      "arxiv_id": "2511.20439v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20431v1",
      "title": "BRIC: Bridging Kinematic Plans and Physical Control at Test Time",
      "authors": [
        "Dohun Lim",
        "Minji Kim",
        "Jaewoon Lim",
        "Sungchan Kim"
      ],
      "abstract": "We propose BRIC, a novel test-time adaptation (TTA) framework that enables long-term human motion generation by resolving execution discrepancies between diffusion-based kinematic motion planners and reinforcement learning-based physics controllers. While diffusion models can generate diverse and expressive motions conditioned on text and scene context, they often produce physically implausible outputs, leading to execution drift during simulation. To address this, BRIC dynamically adapts the physics controller to noisy motion plans at test time, while preserving pre-trained skills via a loss function that mitigates catastrophic forgetting. In addition, BRIC introduces a lightweight test-time guidance mechanism that steers the diffusion model in the signal space without updating its parameters. By combining both adaptation strategies, BRIC ensures consistent and physically plausible long-term executions across diverse environments in an effective and efficient manner. We validate the effectiveness of BRIC on a variety of long-term tasks, including motion composition, obstacle avoidance, and human-scene interaction, achieving state-of-the-art performance across all tasks.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20431v1",
        "pdf": "https://arxiv.org/pdf/2511.20431v1"
      },
      "arxiv_id": "2511.20431v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20426v1",
      "title": "Block Cascading: Training Free Acceleration of Block-Causal Video Models",
      "authors": [
        "Hmrishav Bandyopadhyay",
        "Nikhil Pinnaparaju",
        "Rahim Entezari",
        "Jim Scott",
        "Yi-Zhe Song",
        "Varun Jampani"
      ],
      "abstract": "Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20426v1",
        "pdf": "https://arxiv.org/pdf/2511.20426v1"
      },
      "arxiv_id": "2511.20426v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20422v1",
      "title": "VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning",
      "authors": [
        "Bo Pang",
        "Chenxi Xu",
        "Jierui Ren",
        "Guoping Wang",
        "Sheng Li"
      ],
      "abstract": "Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.GR",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20422v1",
        "pdf": "https://arxiv.org/pdf/2511.20422v1"
      },
      "arxiv_id": "2511.20422v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20418v1",
      "title": "StableTrack: Stabilizing Multi-Object Tracking on Low-Frequency Detections",
      "authors": [
        "Matvei Shelukhan",
        "Timur Mamedov",
        "Karina Kvanchiani"
      ],
      "abstract": "Multi-object tracking (MOT) is one of the most challenging tasks in computer vision, where it is important to correctly detect objects and associate these detections across frames. Current approaches mainly focus on tracking objects in each frame of a video stream, making it almost impossible to run the model under conditions of limited computing resources. To address this issue, we propose StableTrack, a novel approach that stabilizes the quality of tracking on low-frequency detections. Our method introduces a new two-stage matching strategy to improve the cross-frame association between low-frequency detections. We propose a novel Bbox-Based Distance instead of the conventional Mahalanobis distance, which allows us to effectively match objects using the Re-ID model. Furthermore, we integrate visual tracking into the Kalman Filter and the overall tracking pipeline. Our method outperforms current state-of-the-art trackers in the case of low-frequency detections, achieving $\\textit{11.6%}$ HOTA improvement at $\\textit{1}$ Hz on MOT17-val, while keeping up with the best approaches on the standard MOT17, MOT20, and DanceTrack benchmarks with full-frequency detections.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20418v1",
        "pdf": "https://arxiv.org/pdf/2511.20418v1"
      },
      "arxiv_id": "2511.20418v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20415v1",
      "title": "MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts",
      "authors": [
        "Zilong Huang",
        "Jun He",
        "Xiaobin Huang",
        "Ziyi Xiong",
        "Yang Luo",
        "Junyan Ye",
        "Weijia Li",
        "Yiping Chen",
        "Ting Han"
      ],
      "abstract": "Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20415v1",
        "pdf": "https://arxiv.org/pdf/2511.20415v1"
      },
      "arxiv_id": "2511.20415v1",
      "comment": "13 pages, 6 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20410v1",
      "title": "Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs",
      "authors": [
        "Bao Tang",
        "Shuai Zhang",
        "Yueting Zhu",
        "Jijun Xiang",
        "Xin Yang",
        "Li Yu",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "abstract": "Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20410v1",
        "pdf": "https://arxiv.org/pdf/2511.20410v1"
      },
      "arxiv_id": "2511.20410v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20407v1",
      "title": "Tight Margin-Based Generalization Bounds for Voting Classifiers over Finite Hypothesis Sets",
      "authors": [
        "Kasper Green Larsen",
        "Natascha Schalburg"
      ],
      "abstract": "We prove the first margin-based generalization bound for voting classifiers, that is asymptotically tight in the tradeoff between the size of the hypothesis set, the margin, the fraction of training points with the given margin, the number of training samples and the failure probability.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20407v1",
        "pdf": "https://arxiv.org/pdf/2511.20407v1"
      },
      "arxiv_id": "2511.20407v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20406v1",
      "title": "Short-Range Oversquashing",
      "authors": [
        "Yaaqov Mishayev",
        "Yonatan Sverdlov",
        "Tal Amir",
        "Nadav Dym"
      ],
      "abstract": "Message Passing Neural Networks (MPNNs) are widely used for learning on graphs, but their ability to process long-range information is limited by the phenomenon of oversquashing. This limitation has led some researchers to advocate Graph Transformers as a better alternative, whereas others suggest that it can be mitigated within the MPNN framework, using virtual nodes or other rewiring techniques.\n  In this work, we demonstrate that oversquashing is not limited to long-range tasks, but can also arise in short-range problems. This observation allows us to disentangle two distinct mechanisms underlying oversquashing: (1) the bottleneck phenomenon, which can arise even in low-range settings, and (2) the vanishing gradient phenomenon, which is closely associated with long-range tasks.\n  We further show that the short-range bottleneck effect is not captured by existing explanations for oversquashing, and that adding virtual nodes does not resolve it. In contrast, transformers do succeed in such tasks, positioning them as the more compelling solution to oversquashing, compared to specialized MPNNs.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20406v1",
        "pdf": "https://arxiv.org/pdf/2511.20406v1"
      },
      "arxiv_id": "2511.20406v1",
      "comment": "Accepted to Learning on Graphs (LoG) 2025. Version identical to the camera-ready paper",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20403v1",
      "title": "LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework",
      "authors": [
        "Andrea Lops",
        "Fedelucio Narducci",
        "Azzurra Ragone",
        "Michelantonio Trizio",
        "Claudio Barto"
      ],
      "abstract": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20403v1",
        "pdf": "https://arxiv.org/pdf/2511.20403v1"
      },
      "arxiv_id": "2511.20403v1",
      "comment": "Accepted at 40th IEEE/ACM International Conference on Automated Software Engineering",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20401v1",
      "title": "A Training-Free Approach for Multi-ID Customization via Attention Adjustment and Spatial Control",
      "authors": [
        "Jiawei Lin",
        "Guanlong Jiao",
        "Jianjin Xu"
      ],
      "abstract": "Multi-ID customization is an interesting topic in computer vision and attracts considerable attention recently. Given the ID images of multiple individuals, its purpose is to generate a customized image that seamlessly integrates them while preserving their respective identities. Compared to single-ID customization, multi-ID customization is much more difficult and poses two major challenges. First, since the multi-ID customization model is trained to reconstruct an image from the cropped person regions, it often encounters the copy-paste issue during inference, leading to lower quality. Second, the model also suffers from inferior text controllability. The generated result simply combines multiple persons into one image, regardless of whether it is aligned with the input text. In this work, we propose MultiID to tackle this challenging task in a training-free manner. Since the existing single-ID customization models have less copy-paste issue, our key idea is to adapt these models to achieve multi-ID customization. To this end, we present an ID-decoupled cross-attention mechanism, injecting distinct ID embeddings into the corresponding image regions and thus generating multi-ID outputs. To enhance the generation controllability, we introduce three critical strategies, namely the local prompt, depth-guided spatial control, and extended self-attention, making the results more consistent with the text prompts and ID images. We also carefully build a benchmark, called IDBench, for evaluation. The extensive qualitative and quantitative results demonstrate the effectiveness of MultiID in solving the aforementioned two challenges. Its performance is comparable or even better than the training-based multi-ID customization methods.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20401v1",
        "pdf": "https://arxiv.org/pdf/2511.20401v1"
      },
      "arxiv_id": "2511.20401v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20399v1",
      "title": "BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali",
      "authors": [
        "Abdullah Al Sefat"
      ],
      "abstract": "Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20399v1",
        "pdf": "https://arxiv.org/pdf/2511.20399v1"
      },
      "arxiv_id": "2511.20399v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20397v1",
      "title": "Model-Based Learning of Whittle indices",
      "authors": [
        "Joël Charles-Rebuffé",
        "Nicolas Gast",
        "Bruno Gaujal"
      ],
      "abstract": "We present BLINQ, a new model-based algorithm that learns the Whittle indices of an indexable, communicating and unichain Markov Decision Process (MDP). Our approach relies on building an empirical estimate of the MDP and then computing its Whittle indices using an extended version of a state-of-the-art existing algorithm. We provide a proof of convergence to the Whittle indices we want to learn as well as a bound on the time needed to learn them with arbitrary precision. Moreover, we investigate its computational complexity. Our numerical experiments suggest that BLINQ significantly outperforms existing Q-learning approaches in terms of the number of samples needed to get an accurate approximation. In addition, it has a total computational cost even lower than Q-learning for any reasonably high number of samples. These observations persist even when the Q-learning algorithms are speeded up using pre-trained neural networks to predict Q-values.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "cs.DS",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20397v1",
        "pdf": "https://arxiv.org/pdf/2511.20397v1"
      },
      "arxiv_id": "2511.20397v1",
      "comment": "31 pages, 8 figures, submitted to TOMPECS",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20395v1",
      "title": "Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI",
      "authors": [
        "M. C. Schoppema",
        "B. H. M. van der Velden",
        "A. Hürriyetoğlu",
        "M. D. Klijnstra",
        "E. J. Faassen",
        "A. Gerssen",
        "H. J. van der Fels-Klerx"
      ],
      "abstract": "Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.\n  We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.\n  Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.\n  To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20395v1",
        "pdf": "https://arxiv.org/pdf/2511.20395v1"
      },
      "arxiv_id": "2511.20395v1",
      "comment": "18 pages, 6 figures, submitted to Nature Food",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20390v1",
      "title": "FREE: Uncertainty-Aware Autoregression for Parallel Diffusion Transformers",
      "authors": [
        "Xinwan Wen",
        "Bowen Li",
        "Jiajun Luo",
        "Ye Li",
        "Zhi Wang"
      ],
      "abstract": "Diffusion Transformers (DiTs) achieve state-of-the-art generation quality but require long sequential denoising trajectories, leading to high inference latency. Recent speculative inference methods enable lossless parallel sampling in U-Net-based diffusion models via a drafter-verifier scheme, but their acceleration is limited on DiTs due to insufficient draft accuracy during verification. To address this limitation, we analyze the DiTs' feature dynamics and find the features of the final transformer layer (top-block) exhibit strong temporal consistency and rich semantic abstraction. Based on this insight, we propose FREE, a novel framework that employs a lightweight drafter to perform feature-level autoregression with parallel verification, guaranteeing lossless acceleration with theoretical and empirical support. Meanwhile, prediction variance (uncertainty) of DiTs naturally increases in later denoising steps, reducing acceptance rates under speculative sampling. To mitigate this effect, we further introduce an uncertainty-guided relaxation strategy, forming FREE (relax), which dynamically adjusts the acceptance probability in response to uncertainty levels. Experiments on ImageNet-$512^2$ show that FREE achieves up to $1.86 \\times$ acceleration, and FREE (relax) further reaches $2.25 \\times$ speedup while maintaining high perceptual and quantitative fidelity in generation quality.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20390v1",
        "pdf": "https://arxiv.org/pdf/2511.20390v1"
      },
      "arxiv_id": "2511.20390v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.20382v1",
      "title": "MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers",
      "authors": [
        "Audrey Pei-Hsuan Chen"
      ],
      "abstract": "Representation learning on multi-omics data is challenging due to extreme dimensionality, modality heterogeneity, and cohort-specific batch effects. While pre-trained transformer backbones have shown broad generalization capabilities in biological sequence modeling, their application to multi-omics integration remains underexplored. We present MoRE (Multi-Omics Representation Embedding), a framework that repurposes frozen pre-trained transformers to align heterogeneous assays into a shared latent space. Unlike purely generative approaches, MoRE employs a parameter-efficient fine-tuning (PEFT) strategy, prioritizing cross-sample and cross-modality alignment over simple sequence reconstruction. Specifically, MoRE attaches lightweight, modality-specific adapters and a task-adaptive fusion layer to the frozen backbone. It optimizes a masked modeling objective jointly with supervised contrastive and batch-invariant alignment losses, yielding structure-preserving embeddings that generalize across unseen cell types and platforms. We benchmark MoRE against established baselines, including scGPT, scVI, and Harmony with scArches, evaluating integration fidelity, rare population detection, and modality transfer. Our results demonstrate that MoRE achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters compared to fully fine-tuned models. This work positions MoRE as a practical step toward general-purpose omics foundation models.",
      "published": "2025-11-25",
      "updated": "2025-11-25",
      "categories": [
        "cs.LG",
        "q-bio.GN"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.20382v1",
        "pdf": "https://arxiv.org/pdf/2511.20382v1"
      },
      "arxiv_id": "2511.20382v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    }
  ]
}