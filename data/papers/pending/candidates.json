{
  "fetched_at": "2025-11-18T00:24:59.043693",
  "total_papers": 100,
  "papers": [
    {
      "id": "2511.11571v1",
      "title": "Optimizing Mixture of Block Attention",
      "authors": [
        "Guangxuan Xiao",
        "Junxian Guo",
        "Kasra Mazaheri",
        "Song Han"
      ],
      "abstract": "Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11571v1",
        "pdf": "https://arxiv.org/pdf/2511.11571v1"
      },
      "arxiv_id": "2511.11571v1",
      "comment": "The first two authors contributed equally to this work",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11569v1",
      "title": "Private Frequency Estimation Via Residue Number Systems",
      "authors": [
        "Héber H. Arcolezi"
      ],
      "abstract": "We present \\textsf{ModularSubsetSelection} (MSS), a new algorithm for locally differentially private (LDP) frequency estimation. Given a universe of size $k$ and $n$ users, our $\\varepsilon$-LDP mechanism encodes each input via a Residue Number System (RNS) over $\\ell$ pairwise-coprime moduli $m_0, \\ldots, m_{\\ell-1}$, and reports a randomly chosen index $j \\in [\\ell]$ along with the perturbed residue using the statistically optimal \\textsf{SubsetSelection}~(SS) (Wang et al. 2016). This design reduces the user communication cost from $Θ\\bigl(ω\\log_2(k/ω)\\bigr)$ bits required by standard SS (with $ω\\approx k/(e^\\varepsilon+1)$) down to $\\lceil \\log_2 \\ell \\rceil + \\lceil \\log_2 m_j \\rceil$ bits, where $m_j < k$. Server-side decoding runs in $Θ(n + r k \\ell)$ time, where $r$ is the number of LSMR (Fong and Saunders 2011) iterations. In practice, with well-conditioned moduli (\\textit{i.e.}, constant $r$ and $\\ell = Θ(\\log k)$), this becomes $Θ(n + k \\log k)$. We prove that MSS achieves worst-case MSE within a constant factor of state-of-the-art protocols such as SS and \\textsf{ProjectiveGeometryResponse} (PGR) (Feldman et al. 2022), while avoiding the algebraic prerequisites and dynamic-programming decoder required by PGR. Empirically, MSS matches the estimation accuracy of SS, PGR, and \\textsf{RAPPOR} (Erlingsson, Pihur, and Korolova 2014) across realistic $(k, \\varepsilon)$ settings, while offering faster decoding than PGR and shorter user messages than SS. Lastly, by sampling from multiple moduli and reporting only a single perturbed residue, MSS achieves the lowest reconstruction-attack success rate among all evaluated LDP protocols.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11569v1",
        "pdf": "https://arxiv.org/pdf/2511.11569v1"
      },
      "arxiv_id": "2511.11569v1",
      "comment": "AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11564v1",
      "title": "Estimating Total Effects in Bipartite Experiments with Spillovers and Partial Eligibility",
      "authors": [
        "Albert Tan",
        "Mohsen Bayati",
        "James Nordlund",
        "Roman Istomin"
      ],
      "abstract": "We study randomized experiments in bipartite systems where only a subset of treatment-side units are eligible for assignment while all units continue to interact, generating interference. We formalize eligibility-constrained bipartite experiments and define estimands aligned with full deployment: the Primary Total Treatment Effect (PTTE) on eligible units and the Secondary Total Treatment Effect (STTE) on ineligible units. Under randomization within the eligible set, we give identification conditions and develop interference-aware ensemble estimators that combine exposure mappings, generalized propensity scores, and flexible machine learning. We further introduce a projection that links treatment- and outcome-level estimands; this mapping is exact under a Linear Additive Edges condition and enables estimation on the (typically much smaller) treatment side with deterministic aggregation to outcomes. In simulations with known ground truth across realistic exposure regimes, the proposed estimators recover PTTE and STTE with low bias and variance and reduce the bias that could arise when interference is ignored. Two field experiments illustrate practical relevance: our method corrects the direction of expected interference bias for a pre-specified metric in both studies and reverses the sign and significance of the primary decision metric in one case.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "stat.ME",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11564v1",
        "pdf": "https://arxiv.org/pdf/2511.11564v1"
      },
      "arxiv_id": "2511.11564v1",
      "comment": "21 pages, 6 figures, Appeared as Oral Presentation in 2025 Conference on Digital Experimentation (CODE) at MIT",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.11563v1",
      "title": "LARM: A Large Articulated-Object Reconstruction Model",
      "authors": [
        "Sylvia Yuan",
        "Ruoxi Shi",
        "Xinyue Wei",
        "Xiaoshuai Zhang",
        "Hao Su",
        "Minghua Liu"
      ],
      "abstract": "Modeling 3D articulated objects with realistic geometry, textures, and kinematics is essential for a wide range of applications. However, existing optimization-based reconstruction methods often require dense multi-view inputs and expensive per-instance optimization, limiting their scalability. Recent feedforward approaches offer faster alternatives but frequently produce coarse geometry, lack texture reconstruction, and rely on brittle, complex multi-stage pipelines. We introduce LARM, a unified feedforward framework that reconstructs 3D articulated objects from sparse-view images by jointly recovering detailed geometry, realistic textures, and accurate joint structures. LARM extends LVSM a recent novel view synthesis (NVS) approach for static 3D objects into the articulated setting by jointly reasoning over camera pose and articulation variation using a transformer-based architecture, enabling scalable and accurate novel view synthesis. In addition, LARM generates auxiliary outputs such as depth maps and part masks to facilitate explicit 3D mesh extraction and joint estimation. Our pipeline eliminates the need for dense supervision and supports high-fidelity reconstruction across diverse object categories. Extensive experiments demonstrate that LARM outperforms state-of-the-art methods in both novel view and state synthesis as well as 3D articulated object reconstruction, generating high-quality meshes that closely adhere to the input images. project page: https://sylviayuan-sy.github.io/larm-site/",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11563v1",
        "pdf": "https://arxiv.org/pdf/2511.11563v1"
      },
      "arxiv_id": "2511.11563v1",
      "comment": "project page: https://sylviayuan-sy.github.io/larm-site/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.11560v1",
      "title": "A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication",
      "authors": [
        "Angelo Rodio",
        "Giovanni Neglia",
        "Zheng Chen",
        "Erik G. Larsson"
      ],
      "abstract": "In semi-decentralized federated learning, devices primarily rely on device-to-device communication but occasionally interact with a central server. Periodically, a sampled subset of devices uploads their local models to the server, which computes an aggregate model. The server can then either (i) share this aggregate model only with the sampled clients (sampled-to-sampled, S2S) or (ii) broadcast it to all clients (sampled-to-all, S2A). Despite their practical significance, a rigorous theoretical and empirical comparison of these two strategies remains absent. We address this gap by analyzing S2S and S2A within a unified convergence framework that accounts for key system parameters: sampling rate, server aggregation frequency, and network connectivity. Our results, both analytical and experimental, reveal distinct regimes where one strategy outperforms the other, depending primarily on the degree of data heterogeneity across devices. These insights lead to concrete design guidelines for practical semi-decentralized FL deployments.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11560v1",
        "pdf": "https://arxiv.org/pdf/2511.11560v1"
      },
      "arxiv_id": "2511.11560v1",
      "comment": "Accepted as a conference paper at AAAI 2026 (oral presentation). This is the extended version including the appendix",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11558v1",
      "title": "Human-AI collaborative autonomous synthesis with pulsed laser deposition for remote epitaxy",
      "authors": [
        "Asraful Haque",
        "Daniel T. Yimam",
        "Jawad Chowdhury",
        "Ralph Bulanadi",
        "Ivan Vlassiouk",
        "John Lasseter",
        "Sujoy Ghosh",
        "Christopher M. Rouleau",
        "Kai Xiao",
        "Yongtao Liu",
        "Eva Zarkadoula",
        "Rama K. Vasudevan",
        "Sumner B. Harris"
      ],
      "abstract": "Autonomous laboratories typically rely on data-driven decision-making, occasionally with human-in-the-loop oversight to inject domain expertise. Fully leveraging AI agents, however, requires tightly coupled, collaborative workflows spanning hypothesis generation, experimental planning, execution, and interpretation. To address this, we develop and deploy a human-AI collaborative (HAIC) workflow that integrates large language models for hypothesis generation and analysis, with collaborative policy updates driving autonomous pulsed laser deposition (PLD) experiments for remote epitaxy of BaTiO$_3$/graphene. HAIC accelerated the hypothesis formation and experimental design and efficiently mapped the growth space to graphene-damage. In situ Raman spectroscopy reveals that chemistry drives degradation while the highest energy plume components seed defects, identifying a low-O$_2$ pressure low-temperature synthesis window that preserves graphene but is incompatible with optimal BaTiO$_3$ growth. Thus, we show a two-step Ar/O$_2$ deposition is required to exfoliate ferroelectric BaTiO$_3$ while maintaining a monolayer graphene interlayer. HAIC stages human insight with AI reasoning between autonomous batches to drive rapid scientific progress, providing an evolution to many existing human-in-the-loop autonomous workflows.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11558v1",
        "pdf": "https://arxiv.org/pdf/2511.11558v1"
      },
      "arxiv_id": "2511.11558v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11553v1",
      "title": "Multistability of Self-Attention Dynamics in Transformers",
      "authors": [
        "Claudio Altafini"
      ],
      "abstract": "In machine learning, a self-attention dynamics is a continuous-time multiagent-like model of the attention mechanisms of transformers. In this paper we show that such dynamics is related to a multiagent version of the Oja flow, a dynamical system that computes the principal eigenvector of a matrix corresponding for transformers to the value matrix. We classify the equilibria of the ``single-head'' self-attention system into four classes: consensus, bipartite consensus, clustering and polygonal equilibria. Multiple asymptotically stable equilibria from the first three classes often coexist in the self-attention dynamics. Interestingly, equilibria from the first two classes are always aligned with the eigenvectors of the value matrix, often but not exclusively with the principal eigenvector.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG",
        "eess.SY",
        "math.DS"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11553v1",
        "pdf": "https://arxiv.org/pdf/2511.11553v1"
      },
      "arxiv_id": "2511.11553v1",
      "comment": "8 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11552v1",
      "title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding",
      "authors": [
        "Dawei Zhu",
        "Rui Meng",
        "Jiefeng Chen",
        "Sujian Li",
        "Tomas Pfister",
        "Jinsung Yoon"
      ],
      "abstract": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11552v1",
        "pdf": "https://arxiv.org/pdf/2511.11552v1"
      },
      "arxiv_id": "2511.11552v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11551v1",
      "title": "Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping",
      "authors": [
        "Dena Mujtaba",
        "Brian Hu",
        "Anthony Hoogs",
        "Arslan Basharat"
      ],
      "abstract": "The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11551v1",
        "pdf": "https://arxiv.org/pdf/2511.11551v1"
      },
      "arxiv_id": "2511.11551v1",
      "comment": "Accepted to AAAI 2026 AI Alignment Track",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11539v1",
      "title": "Generalizing Fair Clustering to Multiple Groups: Algorithms and Applications",
      "authors": [
        "Diptarka Chakraborty",
        "Kushagra Chatterjee",
        "Debarati Das",
        "Tien-Long Nguyen"
      ],
      "abstract": "Clustering is a fundamental task in machine learning and data analysis, but it frequently fails to provide fair representation for various marginalized communities defined by multiple protected attributes -- a shortcoming often caused by biases in the training data. As a result, there is a growing need to enhance the fairness of clustering outcomes, ideally by making minimal modifications, possibly as a post-processing step after conventional clustering. Recently, Chakraborty et al. [COLT'25] initiated the study of \\emph{closest fair clustering}, though in a restricted scenario where data points belong to only two groups. In practice, however, data points are typically characterized by many groups, reflecting diverse protected attributes such as age, ethnicity, gender, etc.\n  In this work, we generalize the study of the \\emph{closest fair clustering} problem to settings with an arbitrary number (more than two) of groups. We begin by showing that the problem is NP-hard even when all groups are of equal size -- a stark contrast with the two-group case, for which an exact algorithm exists. Next, we propose near-linear time approximation algorithms that efficiently handle arbitrary-sized multiple groups, thereby answering an open question posed by Chakraborty et al. [COLT'25].\n  Leveraging our closest fair clustering algorithms, we further achieve improved approximation guarantees for the \\emph{fair correlation clustering} problem, advancing the state-of-the-art results established by Ahmadian et al. [AISTATS'20] and Ahmadi et al. [2020]. Additionally, we are the first to provide approximation algorithms for the \\emph{fair consensus clustering} problem involving multiple (more than two) groups, thus addressing another open direction highlighted by Chakraborty et al. [COLT'25].",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11539v1",
        "pdf": "https://arxiv.org/pdf/2511.11539v1"
      },
      "arxiv_id": "2511.11539v1",
      "comment": "Accepted in AAAI 2026 for Oral Representation",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11533v1",
      "title": "Volumetric Ergodic Control",
      "authors": [
        "Jueun Kwon",
        "Max M. Sun",
        "Todd Murphey"
      ],
      "abstract": "Ergodic control synthesizes optimal coverage behaviors over spatial distributions for nonlinear systems. However, existing formulations model the robot as a non-volumetric point, but in practice a robot interacts with the environment through its body and sensors with physical volume. In this work, we introduce a new ergodic control formulation that optimizes spatial coverage using a volumetric state representation. Our method preserves the asymptotic coverage guarantees of ergodic control, adds minimal computational overhead for real-time control, and supports arbitrary sample-based volumetric models. We evaluate our method across search and manipulation tasks -- with multiple robot dynamics and end-effector geometries or sensor models -- and show that it improves coverage efficiency by more than a factor of two while maintaining a 100% task completion rate across all experiments, outperforming the standard ergodic control method. Finally, we demonstrate the effectiveness of our method on a robot arm performing mechanical erasing tasks.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11533v1",
        "pdf": "https://arxiv.org/pdf/2511.11533v1"
      },
      "arxiv_id": "2511.11533v1",
      "comment": "8 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11526v1",
      "title": "Bridging Hidden States in Vision-Language Models",
      "authors": [
        "Benjamin Fein-Ashley",
        "Jacob Fein-Ashley"
      ],
      "abstract": "Vision-Language Models (VLMs) are a new family of models that align image content with natural language. Existing approaches typically fuse either (a) early: by mixing tokens/features inside the encoders, or (b) late: by comparing pooled embeddings. Many methods also tie fusion to an autoregressive decoder. However, the hidden states of both modalities already carry rich, modality-specific structure (spatial layout in vision; syntax and semantics in text), so directly aligning these states is a natural way to match what the two modalities \"think\". We propose a lightweight fusion module: a few cross-only, bidirectional attention layers placed near the top of both encoders. Each layer projects the vision and text encoder hidden-state sequences into a shared space, attends across modalities, and sends gated residual updates back, with simple stabilizers to improve alignment. The encoders remain non-causal and strong for understanding, while generation stays cleanly decoupled via an optional decoder. Across standard retrieval, VQA, and visual reasoning benchmarks, BRIDGE outperforms comparable VLMs while preserving the bi-encoder efficiency of contrastive models. We make our code publicly available at https://github.com/jfeinashley/BRIDGE.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11526v1",
        "pdf": "https://arxiv.org/pdf/2511.11526v1"
      },
      "arxiv_id": "2511.11526v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11522v1",
      "title": "CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation",
      "authors": [
        "Luthira Abeykoon",
        "Ved Patel",
        "Gawthaman Senthilvelan",
        "Darshan Kasundra"
      ],
      "abstract": "Chess has experienced a large increase in viewership since the pandemic, driven largely by the accessibility of online learning platforms. However, no equivalent assistance exists for physical chess games, creating a divide between analog and digital chess experiences. This paper presents CVChess, a deep learning framework for converting chessboard images to Forsyth-Edwards Notation (FEN), which is later input into online chess engines to provide you with the best next move. Our approach employs a convolutional neural network (CNN) with residual layers to perform piece recognition from smartphone camera images. The system processes RGB images of a physical chess board through a multistep process: image preprocessing using the Hough Line Transform for edge detection, projective transform to achieve a top-down board alignment, segmentation into 64 individual squares, and piece classification into 13 classes (6 unique white pieces, 6 unique black pieces and an empty square) using the residual CNN. Residual connections help retain low-level visual features while enabling deeper feature extraction, improving accuracy and stability during training. We train and evaluate our model using the Chess Recognition Dataset (ChessReD), containing 10,800 annotated smartphone images captured under diverse lighting conditions and angles. The resulting classifications are encoded as an FEN string, which can be fed into a chess engine to generate the most optimal move",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11522v1",
        "pdf": "https://arxiv.org/pdf/2511.11522v1"
      },
      "arxiv_id": "2511.11522v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11519v1",
      "title": "Experience-Guided Adaptation of Inference-Time Reasoning Strategies",
      "authors": [
        "Adam Stein",
        "Matthew Trager",
        "Benjamin Bowman",
        "Michael Kleinman",
        "Aditya Chattopadhyay",
        "Wei Xia",
        "Stefano Soatto"
      ],
      "abstract": "Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11519v1",
        "pdf": "https://arxiv.org/pdf/2511.11519v1"
      },
      "arxiv_id": "2511.11519v1",
      "comment": "29 pages, 5 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11512v1",
      "title": "Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities",
      "authors": [
        "Yiyun Zhou",
        "Mingjing Xu",
        "Jingwei Shi",
        "Quanjiang Li",
        "Jingyuan Chen"
      ],
      "abstract": "Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11512v1",
        "pdf": "https://arxiv.org/pdf/2511.11512v1"
      },
      "arxiv_id": "2511.11512v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11510v1",
      "title": "OpenUS: A Fully Open-Source Foundation Model for Ultrasound Image Analysis via Self-Adaptive Masked Contrastive Learning",
      "authors": [
        "Xiaoyu Zheng",
        "Xu Chen",
        "Awais Rauf",
        "Qifan Fu",
        "Benedetta Monosi",
        "Felice Rivellese",
        "Myles J. Lewis",
        "Shaogang Gong",
        "Gregory Slabaugh"
      ],
      "abstract": "Ultrasound (US) is one of the most widely used medical imaging modalities, thanks to its low cost, portability, real-time feedback, and absence of ionizing radiation. However, US image interpretation remains highly operator-dependent and varies significantly across anatomical regions, acquisition protocols, and device types. These variations, along with unique challenges such as speckle, low contrast, and limited standardized annotations, hinder the development of generalizable, label-efficient ultrasound AI models. In this paper, we propose OpenUS, the first reproducible, open-source ultrasound foundation model built on a large collection of public data. OpenUS employs a vision Mamba backbone, capturing both local and global long-range dependencies across the image. To extract rich features during pre-training, we introduce a novel self-adaptive masking framework that combines contrastive learning with masked image modeling. This strategy integrates the teacher's attention map with student reconstruction loss, adaptively refining clinically-relevant masking to enhance pre-training effectiveness. OpenUS also applies a dynamic learning schedule to progressively adjust the difficulty of the pre-training process. To develop the foundation model, we compile the largest to-date public ultrasound dataset comprising over 308K images from 42 publicly available datasets, covering diverse anatomical regions, institutions, imaging devices, and disease types. Our pre-trained OpenUS model can be easily adapted to specific downstream tasks by serving as a backbone for label-efficient fine-tuning. Code is available at https://github.com/XZheng0427/OpenUS.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11510v1",
        "pdf": "https://arxiv.org/pdf/2511.11510v1"
      },
      "arxiv_id": "2511.11510v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11505v1",
      "title": "FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models",
      "authors": [
        "Yonatan Dukler",
        "Guihong Li",
        "Deval Shah",
        "Vikram Appia",
        "Emad Barsoum"
      ],
      "abstract": "Blocking communication presents a major hurdle in running MoEs efficiently in distributed settings. To address this, we present FarSkip-Collective which modifies the architecture of modern models to enable overlapping of their computation with communication. Our approach modifies the architecture to skip connections in the model and it is unclear a priori whether the modified model architecture can remain as capable, especially for large state-of-the-art models and while modifying all of the model layers. We answer this question in the affirmative and fully convert a series of state-of-the-art models varying from 16B to 109B parameters to enable overlapping of their communication while achieving accuracy on par with their original open-source releases. For example, we convert Llama 4 Scout (109B) via self-distillation and achieve average accuracy within 1% of its instruction tuned release averaged across a wide range of downstream evaluations. In addition to demonstrating retained accuracy of the large modified models, we realize the benefits of FarSkip-Collective through optimized implementations that explicitly overlap communication with computation, accelerating both training and inference in existing frameworks.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11505v1",
        "pdf": "https://arxiv.org/pdf/2511.11505v1"
      },
      "arxiv_id": "2511.11505v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11502v1",
      "title": "PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models",
      "authors": [
        "Nhat Hoang-Xuan",
        "Minh Vu",
        "My T. Thai",
        "Manish Bhattarai"
      ],
      "abstract": "Large vision-language models (LVLMs) are powerful, yet they remain unreliable due to object hallucinations. In this work, we show that in many hallucinatory predictions the LVLM effectively ignores the image and instead relies on previously generated output (prelim) tokens to infer new objects. We quantify this behavior via the mutual information between the image and the predicted object conditioned on the prelim, demonstrating that weak image dependence strongly correlates with hallucination. Building on this finding, we introduce the Prelim Attention Score (PAS), a lightweight, training-free signal computed from attention weights over prelim tokens. PAS requires no additional forward passes and can be computed on the fly during inference. Exploiting this previously overlooked signal, PAS achieves state-of-the-art object-hallucination detection across multiple models and datasets, enabling real-time filtering and intervention.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11502v1",
        "pdf": "https://arxiv.org/pdf/2511.11502v1"
      },
      "arxiv_id": "2511.11502v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11500v1",
      "title": "Honesty over Accuracy: Trustworthy Language Models through Reinforced Hesitation",
      "authors": [
        "Mohamad Amin Mohamadi",
        "Tianhao Wang",
        "Zhiyuan Li"
      ],
      "abstract": "Modern language models fail a fundamental requirement of trustworthy intelligence: knowing when not to answer. Despite achieving impressive accuracy on benchmarks, these models produce confident hallucinations, even when wrong answers carry catastrophic consequences. Our evaluations on GSM8K, MedQA and GPQA show frontier models almost never abstain despite explicit warnings of severe penalties, suggesting that prompts cannot override training that rewards any answer over no answer. As a remedy, we propose Reinforced Hesitation (RH): a modification to Reinforcement Learning from Verifiable Rewards (RLVR) to use ternary rewards (+1 correct, 0 abstention, -$λ$ error) instead of binary. Controlled experiments on logic puzzles reveal that varying $λ$ produces distinct models along a Pareto frontier, where each training penalty yields the optimal model for its corresponding risk regime: low penalties produce aggressive answerers, high penalties conservative abstainers. We then introduce two inference strategies that exploit trained abstention as a coordination signal: cascading routes queries through models with decreasing risk tolerance, while self-cascading re-queries the same model on abstention. Both outperform majority voting with lower computational cost. These results establish abstention as a first-class training objective that transforms ``I don't know'' from failure into a coordination signal, enabling models to earn trust through calibrated honesty about their limits.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11500v1",
        "pdf": "https://arxiv.org/pdf/2511.11500v1"
      },
      "arxiv_id": "2511.11500v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11498v1",
      "title": "Learning and Testing Convex Functions",
      "authors": [
        "Renato Ferreira Pinto",
        "Cassandra Marcussen",
        "Elchanan Mossel",
        "Shivam Nadimpalli"
      ],
      "abstract": "We consider the problems of \\emph{learning} and \\emph{testing} real-valued convex functions over Gaussian space. Despite the extensive study of function convexity across mathematics, statistics, and computer science, its learnability and testability have largely been examined only in discrete or restricted settings -- typically with respect to the Hamming distance, which is ill-suited for real-valued functions.\n  In contrast, we study these problems in high dimensions under the standard Gaussian measure, assuming sample access to the function and a mild smoothness condition, namely Lipschitzness. A smoothness assumption is natural and, in fact, necessary even in one dimension: without it, convexity cannot be inferred from finitely many samples. As our main results, we give:\n  - Learning Convex Functions: An agnostic proper learning algorithm for Lipschitz convex functions that achieves error $\\varepsilon$ using $n^{O(1/\\varepsilon^2)}$ samples, together with a complementary lower bound of $n^{\\mathrm{poly}(1/\\varepsilon)}$ samples in the \\emph{correlational statistical query (CSQ)} model.\n  - Testing Convex Functions: A tolerant (two-sided) tester for convexity of Lipschitz functions with the same sample complexity (as a corollary of our learning result), and a one-sided tester (which never rejects convex functions) using $O(\\sqrt{n}/\\varepsilon)^n$ samples.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.DS",
        "cs.LG"
      ],
      "primary_category": "cs.DS",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11498v1",
        "pdf": "https://arxiv.org/pdf/2511.11498v1"
      },
      "arxiv_id": "2511.11498v1",
      "comment": "43 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11490v1",
      "title": "Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models",
      "authors": [
        "Joan Font-Quer Roset",
        "Devina Mohan",
        "Anna Scaife"
      ],
      "abstract": "In this work, we estimate the intrinsic dimension (iD) of the Radio Galaxy Zoo (RGZ) dataset using a score-based diffusion model. We examine how the iD estimates vary as a function of Bayesian neural network (BNN) energy scores, which measure how similar the radio sources are to the MiraBest subset of the RGZ dataset. We find that out-of-distribution sources exhibit higher iD values, and that the overall iD for RGZ exceeds those typically reported for natural image datasets. Furthermore, we analyse how iD varies across Fanaroff-Riley (FR) morphological classes and as a function of the signal-to-noise ratio (SNR). While no relationship is found between FR I and FR II classes, a weak trend toward higher SNR at lower iD. Future work using the RGZ dataset could make use of the relationship between iD and energy scores to quantitatively study and improve the representations learned by various self-supervised learning algorithms.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG",
        "astro-ph.IM",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11490v1",
        "pdf": "https://arxiv.org/pdf/2511.11490v1"
      },
      "arxiv_id": "2511.11490v1",
      "comment": "9 pages, 5 figures, 2 tables, submitted to NeurIPS 2025 ML4PS Workshop",
      "journal_ref": "NeurIPS 2025 Workshop on ML for Physical Sciences",
      "has_code": false
    },
    {
      "id": "2511.11486v1",
      "title": "Multimodal Posterior Sampling-based Uncertainty in PD-L1 Segmentation from H&E Images",
      "authors": [
        "Roman Kinakh",
        "Gonzalo R. Ríos-Muñoz",
        "Arrate Muñoz-Barrutia"
      ],
      "abstract": "Accurate assessment of PD-L1 expression is critical for guiding immunotherapy, yet current immunohistochemistry (IHC) based methods are resource-intensive. We present nnUNet-B: a Bayesian segmentation framework that infers PD-L1 expression directly from H&E-stained histology images using Multimodal Posterior Sampling (MPS). Built upon nnUNet-v2, our method samples diverse model checkpoints during cyclic training to approximate the posterior, enabling both accurate segmentation and epistemic uncertainty estimation via entropy and standard deviation. Evaluated on a dataset of lung squamous cell carcinoma, our approach achieves competitive performance against established baselines with mean Dice Score and mean IoU of 0.805 and 0.709, respectively, while providing pixel-wise uncertainty maps. Uncertainty estimates show strong correlation with segmentation error, though calibration remains imperfect. These results suggest that uncertainty-aware H&E-based PD-L1 prediction is a promising step toward scalable, interpretable biomarker assessment in clinical workflows.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "q-bio.QM"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11486v1",
        "pdf": "https://arxiv.org/pdf/2511.11486v1"
      },
      "arxiv_id": "2511.11486v1",
      "comment": "Preprint (pre-review). Accepted for publication in Lecture Notes in Bioinformatics (Springer, 2025). The final authenticated version will be available on SpringerLink once published",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11485v1",
      "title": "Data-efficient U-Net for Segmentation of Carbide Microstructures in SEM Images of Steel Alloys",
      "authors": [
        "Alinda Ezgi Gerçek",
        "Till Korten",
        "Paul Chekhonin",
        "Maleeha Hassan",
        "Peter Steinbach"
      ],
      "abstract": "Understanding reactor-pressure-vessel steel microstructure is crucial for predicting mechanical properties, as carbide precipitates both strengthen the alloy and can initiate cracks. In scanning electron microscopy images, gray-value overlap between carbides and matrix makes simple thresholding ineffective. We present a data-efficient segmentation pipeline using a lightweight U-Net (30.7~M parameters) trained on just \\textbf{10 annotated scanning electron microscopy images}. Despite limited data, our model achieves a \\textbf{Dice-Sørensen coefficient of 0.98}, significantly outperforming the state-of-the-art in the field of metallurgy (classical image analysis: 0.85), while reducing annotation effort by one order of magnitude compared to the state-of-the-art data efficient segmentation model. This approach enables rapid, automated carbide quantification for alloy design and generalizes to other steel types, demonstrating the potential of data-efficient deep learning in reactor-pressure-vessel steel analysis.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11485v1",
        "pdf": "https://arxiv.org/pdf/2511.11485v1"
      },
      "arxiv_id": "2511.11485v1",
      "comment": "",
      "journal_ref": "Machine Learning and the Physical Sciences Workshop @ NeurIPS 2025 https://openreview.net/forum?id=xYY5pn4f8N",
      "has_code": false
    },
    {
      "id": "2511.11483v1",
      "title": "ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation",
      "authors": [
        "Kaishen Wang",
        "Ruibo Chen",
        "Tong Zheng",
        "Heng Huang"
      ],
      "abstract": "Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing approaches, such as prompt rewriting, best-of-N sampling, and self-refinement, can mitigate these issues but usually require additional modules and operate independently, hindering test-time scaling efficiency and increasing computational overhead. In this paper, we introduce ImAgent, a training-free unified multimodal agent that integrates reasoning, generation, and self-evaluation within a single framework for efficient test-time scaling. Guided by a policy controller, multiple generation actions dynamically interact and self-organize to enhance image fidelity and semantic alignment without relying on external models. Extensive experiments on image generation and editing tasks demonstrate that ImAgent consistently improves over the backbone and even surpasses other strong baselines where the backbone model fails, highlighting the potential of unified multimodal agents for adaptive and efficient image generation under test-time scaling.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11483v1",
        "pdf": "https://arxiv.org/pdf/2511.11483v1"
      },
      "arxiv_id": "2511.11483v1",
      "comment": "12 pages, 5 tables, 6 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11480v1",
      "title": "Inferring response times of perceptual decisions with Poisson variational autoencoders",
      "authors": [
        "Hayden R. Johnson",
        "Anastasia N. Krouglova",
        "Hadi Vafaii",
        "Jacob L. Yates",
        "Pedro J. Gonçalves"
      ],
      "abstract": "Many properties of perceptual decision making are well-modeled by deep neural networks. However, such architectures typically treat decisions as instantaneous readouts, overlooking the temporal dynamics of the decision process. We present an image-computable model of perceptual decision making in which choices and response times arise from efficient sensory encoding and Bayesian decoding of neural spiking activity. We use a Poisson variational autoencoder to learn unsupervised representations of visual stimuli in a population of rate-coded neurons, modeled as independent homogeneous Poisson processes. A task-optimized decoder then continually infers an approximate posterior over actions conditioned on incoming spiking activity. Combining these components with an entropy-based stopping rule yields a principled and image-computable model of perceptual decisions capable of generating trial-by-trial patterns of choices and response times. Applied to MNIST digit classification, the model reproduces key empirical signatures of perceptual decision making, including stochastic variability, right-skewed response time distributions, logarithmic scaling of response times with the number of alternatives (Hick's law), and speed-accuracy trade-offs.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11480v1",
        "pdf": "https://arxiv.org/pdf/2511.11480v1"
      },
      "arxiv_id": "2511.11480v1",
      "comment": "To appear at the NeurIPS 2025 Workshop on Data on the Mind and Brain",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11478v1",
      "title": "Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective",
      "authors": [
        "Nhat Chung",
        "Taisei Hanyu",
        "Toan Nguyen",
        "Huy Le",
        "Frederick Bumgarner",
        "Duy Minh Ho Nguyen",
        "Khoa Vo",
        "Kashu Yamazaki",
        "Chase Rainwater",
        "Tung Kieu",
        "Anh Nguyen",
        "Ngan Le"
      ],
      "abstract": "As embodied agents operate in increasingly complex environments, the ability to perceive, track, and reason about individual object instances over time becomes essential, especially in tasks requiring sequenced interactions with visually similar objects. In these non-Markovian settings, key decision cues are often hidden in object-specific histories rather than the current scene. Without persistent memory of prior interactions (what has been interacted with, where it has been, or how it has changed) visuomotor policies may fail, repeat past actions, or overlook completed ones. To surface this challenge, we introduce LIBERO-Mem, a non-Markovian task suite for stress-testing robotic manipulation under object-level partial observability. It combines short- and long-horizon object tracking with temporally sequenced subgoals, requiring reasoning beyond the current frame. However, vision-language-action (VLA) models often struggle in such settings, with token scaling quickly becoming intractable even for tasks spanning just a few hundred frames. We propose Embodied-SlotSSM, a slot-centric VLA framework built for temporal scalability. It maintains spatio-temporally consistent slot identities and leverages them through two mechanisms: (1) slot-state-space modeling for reconstructing short-term history, and (2) a relational encoder to align the input tokens with action decoding. Together, these components enable temporally grounded, context-aware action prediction. Experiments show Embodied-SlotSSM's baseline performance on LIBERO-Mem and general tasks, offering a scalable solution for non-Markovian reasoning in object-centric robotic policies.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11478v1",
        "pdf": "https://arxiv.org/pdf/2511.11478v1"
      },
      "arxiv_id": "2511.11478v1",
      "comment": "Accepted at AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11476v1",
      "title": "Context-aware Adaptive Visualizations for Critical Decision Making",
      "authors": [
        "Angela Lopez-Cardona",
        "Mireia Masias Bruns",
        "Nuwan T. Attygalle",
        "Sebastian Idesis",
        "Matteo Salvatori",
        "Konstantinos Raftopoulos",
        "Konstantinos Oikonomou",
        "Saravanakumar Duraisamy",
        "Parvin Emami",
        "Nacera Latreche",
        "Alaa Eddine Anis Sahraoui",
        "Michalis Vakallelis",
        "Jean Vanderdonckt",
        "Ioannis Arapakis",
        "Luis A. Leiva"
      ],
      "abstract": "Effective decision-making often relies on timely insights from complex visual data. While Information Visualization (InfoVis) dashboards can support this process, they rarely adapt to users' cognitive state, and less so in real time. We present Symbiotik, an intelligent, context-aware adaptive visualization system that leverages neurophysiological signals to estimate mental workload (MWL) and dynamically adapt visual dashboards using reinforcement learning (RL). Through a user study with 120 participants and three visualization types, we demonstrate that our approach improves task performance and engagement. Symbiotik offers a scalable, real-time adaptation architecture, and a validated methodology for neuroadaptive user interfaces.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11476v1",
        "pdf": "https://arxiv.org/pdf/2511.11476v1"
      },
      "arxiv_id": "2511.11476v1",
      "comment": "",
      "journal_ref": "ISBN978-1-64368-631-8 2025",
      "has_code": false
    },
    {
      "id": "2511.11472v1",
      "title": "Quantifying and Improving Adaptivity in Conformal Prediction through Input Transformations",
      "authors": [
        "Sooyong Jang",
        "Insup Lee"
      ],
      "abstract": "Conformal prediction constructs a set of labels instead of a single point prediction, while providing a probabilistic coverage guarantee. Beyond the coverage guarantee, adaptiveness to example difficulty is an important property. It means that the method should produce larger prediction sets for more difficult examples, and smaller ones for easier examples. Existing evaluation methods for adaptiveness typically analyze coverage rate violation or average set size across bins of examples grouped by difficulty. However, these approaches often suffer from imbalanced binning, which can lead to inaccurate estimates of coverage or set size. To address this issue, we propose a binning method that leverages input transformations to sort examples by difficulty, followed by uniform-mass binning. Building on this binning, we introduce two metrics to better evaluate adaptiveness. These metrics provide more reliable estimates of coverage rate violation and average set size due to balanced binning, leading to more accurate adaptivity assessment. Through experiments, we demonstrate that our proposed metric correlates more strongly with the desired adaptiveness property compared to existing ones. Furthermore, motivated by our findings, we propose a new adaptive prediction set algorithm that groups examples by estimated difficulty and applies group-conditional conformal prediction. This allows us to determine appropriate thresholds for each group. Experimental results on both (a) an Image Classification (ImageNet) (b) a medical task (visual acuity prediction) show that our method outperforms existing approaches according to the new metrics.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11472v1",
        "pdf": "https://arxiv.org/pdf/2511.11472v1"
      },
      "arxiv_id": "2511.11472v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11470v1",
      "title": "Sat2RealCity: Geometry-Aware and Appearance-Controllable 3D Urban Generation from Satellite Imagery",
      "authors": [
        "Yijie Kang",
        "Xinliang Wang",
        "Zhenyu Wu",
        "Yifeng Shi",
        "Hailong Zhu"
      ],
      "abstract": "Recent advances in generative modeling have substantially enhanced 3D urban generation, enabling applications in digital twins, virtual cities, and large-scale simulations. However, existing methods face two key challenges: (1) the need for large-scale 3D city assets for supervised training, which are difficult and costly to obtain, and (2) reliance on semantic or height maps, which are used exclusively for generating buildings in virtual worlds and lack connection to real-world appearance, limiting the realism and generalizability of generated cities. To address these limitations, we propose Sat2RealCity, a geometry-aware and appearance-controllable framework for 3D urban generation from real-world satellite imagery. Unlike previous city-level generation methods, Sat2RealCity builds generation upon individual building entities, enabling the use of rich priors and pretrained knowledge from 3D object generation while substantially reducing dependence on large-scale 3D city assets. Specifically, (1) we introduce the OSM-based spatial priors strategy to achieve interpretable geometric generation from spatial topology to building instances; (2) we design an appearance-guided controllable modeling mechanism for fine-grained appearance realism and style control; and (3) we construct an MLLM-powered semantic-guided generation pipeline, bridging semantic interpretation and geometric reconstruction. Extensive quantitative and qualitative experiments demonstrate that Sat2RealCity significantly surpasses existing baselines in structural consistency and appearance realism, establishing a strong foundation for real-world aligned 3D urban content creation. The code will be released soon.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11470v1",
        "pdf": "https://arxiv.org/pdf/2511.11470v1"
      },
      "arxiv_id": "2511.11470v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11468v1",
      "title": "Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents",
      "authors": [
        "Davide Napolitano",
        "Luca Cagliero",
        "Fabrizio Battiloro"
      ],
      "abstract": "The evolution of Visual Large Language Models (VLLMs) has revolutionized the automatic understanding of Visually Rich Documents (VRDs), which contain both textual and visual elements. Although VLLMs excel in Visual Question Answering (VQA) on multi-page VRDs, their ability to detect unanswerable questions is still an open research question. Our research delves into the robustness of the VLLMs to plausible yet unanswerable questions, i.e., questions that appear valid but cannot be answered due to subtle corruptions caused by swaps between related concepts or plausible question formulations. Corruptions are generated by replacing the original natural language entities with other ones of the same type, belonging to different document elements, and in different layout positions or pages of the related document. To this end, we present VRD-UQA (VISUALLY RICH DOCUMENT UNANSWERABLE QUESTION ANSWERING), a benchmark for evaluating VLLMs' resilience to plausible yet unanswerable questions across multiple dimensions. It automatically alters the questions of existing VQA datasets consisting of multi-page VRDs, verifies their unanswerability using a VLLM-as-a-judge approach, and then thoroughly evaluates VLLMs' performance. Experiments, run on 12 models, analyze: (1) The VLLMs' accuracy in detecting unanswerable questions at both page and document levels; (2) The effect of different types of corruption (NLP entity, document element, layout); (3) The effectiveness of different knowledge injection strategies based on in-context learning (OCR, multi-page selection, or the possibility of unanswerability). Our findings reveal VLLMs' limitations and demonstrate that VRD-UQA can serve as an evaluation framework for developing resilient document VQA systems.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11468v1",
        "pdf": "https://arxiv.org/pdf/2511.11468v1"
      },
      "arxiv_id": "2511.11468v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11466v1",
      "title": "Non-Euclidean SGD for Structured Optimization: Unified Analysis and Improved Rates",
      "authors": [
        "Dmitry Kovalev",
        "Ekaterina Borodich"
      ],
      "abstract": "Recently, several instances of non-Euclidean SGD, including SignSGD, Lion, and Muon, have attracted significant interest from the optimization community due to their practical success in training deep neural networks. Consequently, a number of works have attempted to explain this success by developing theoretical convergence analyses. Unfortunately, these results cannot properly justify the superior performance of these methods, as they could not beat the convergence rate of vanilla Euclidean SGD. We resolve this important open problem by developing a new unified convergence analysis under the structured smoothness and gradient noise assumption. In particular, our results indicate that non-Euclidean SGD (i) can exploit the sparsity or low-rank structure of the upper bounds on the Hessian and gradient noise, (ii) can provably benefit from popular algorithmic tools such as extrapolation or momentum variance reduction, and (iii) can match the state-of-the-art convergence rates of adaptive and more complex optimization algorithms such as AdaGrad and Shampoo.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11466v1",
        "pdf": "https://arxiv.org/pdf/2511.11466v1"
      },
      "arxiv_id": "2511.11466v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11464v1",
      "title": "Adaptive Intrusion Detection for Evolving RPL IoT Attacks Using Incremental Learning",
      "authors": [
        "Sumeyye Bas",
        "Kiymet Kaya",
        "Elif Ak",
        "Sule Gunduz Oguducu"
      ],
      "abstract": "The routing protocol for low-power and lossy networks (RPL) has become the de facto routing standard for resource-constrained IoT systems, but its lightweight design exposes critical vulnerabilities to a wide range of routing-layer attacks such as hello flood, decreased rank, and version number manipulation. Traditional countermeasures, including protocol-level modifications and machine learning classifiers, can achieve high accuracy against known threats, yet they fail when confronted with novel or zero-day attacks unless fully retrained, an approach that is impractical for dynamic IoT environments. In this paper, we investigate incremental learning as a practical and adaptive strategy for intrusion detection in RPL-based networks. We systematically evaluate five model families, including ensemble models and deep learning models. Our analysis highlights that incremental learning not only restores detection performance on new attack classes but also mitigates catastrophic forgetting of previously learned threats, all while reducing training time compared to full retraining. By combining five diverse models with attack-specific analysis, forgetting behavior, and time efficiency, this study provides systematic evidence that incremental learning offers a scalable pathway to maintain resilient intrusion detection in evolving RPL-based IoT networks.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11464v1",
        "pdf": "https://arxiv.org/pdf/2511.11464v1"
      },
      "arxiv_id": "2511.11464v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11462v1",
      "title": "MoCap2Radar: A Spatiotemporal Transformer for Synthesizing Micro-Doppler Radar Signatures from Motion Capture",
      "authors": [
        "Kevin Chen",
        "Kenneth W. Parker",
        "Anish Arora"
      ],
      "abstract": "We present a pure machine learning process for synthesizing radar spectrograms from Motion-Capture (MoCap) data. We formulate MoCap-to-spectrogram translation as a windowed sequence-to-sequence task using a transformer-based model that jointly captures spatial relations among MoCap markers and temporal dynamics across frames. Real-world experiments show that the proposed approach produces visually and quantitatively plausible doppler radar spectrograms and achieves good generalizability. Ablation experiments show that the learned model includes both the ability to convert multi-part motion into doppler signatures and an understanding of the spatial relations between different parts of the human body.\n  The result is an interesting example of using transformers for time-series signal processing. It is especially applicable to edge computing and Internet of Things (IoT) radars. It also suggests the ability to augment scarce radar datasets using more abundant MoCap data for training higher-level applications. Finally, it requires far less computation than physics-based methods for generating radar data.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11462v1",
        "pdf": "https://arxiv.org/pdf/2511.11462v1"
      },
      "arxiv_id": "2511.11462v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11461v1",
      "title": "Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking Bias-Variance in Recursive and Direct Strategies",
      "authors": [
        "Riku Green",
        "Huw Day",
        "Zahraa S. Abdallah",
        "Telmo M. Silva Filho"
      ],
      "abstract": "Multi-step forecasting is often described through a simple rule of thumb: recursive strategies are said to have high bias and low variance, while direct strategies are said to have low bias and high variance. We revisit this belief by decomposing the expected multi-step forecast error into three parts: irreducible noise, a structural approximation gap, and an estimation-variance term. For linear predictors we show that the structural gap is identically zero for any dataset. For nonlinear predictors, however, the repeated composition used in recursion can increase model expressivity, making the structural gap depend on both the model and the data. We further show that the estimation variance of the recursive strategy at any horizon can be written as the one-step variance multiplied by a Jacobian-based amplification factor that measures how sensitive the composed predictor is to parameter error. This perspective explains when recursive forecasting may simultaneously have lower bias and higher variance than direct forecasting. Experiments with multilayer perceptrons on the ETTm1 dataset confirm these findings. The results offer practical guidance for choosing between recursive and direct strategies based on model nonlinearity and noise characteristics, rather than relying on traditional bias-variance intuition.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11461v1",
        "pdf": "https://arxiv.org/pdf/2511.11461v1"
      },
      "arxiv_id": "2511.11461v1",
      "comment": "2025 EIML Eurips Workshop, 6 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11460v1",
      "title": "Rethinking Efficient Mixture-of-Experts for Remote Sensing Modality-Missing Classification",
      "authors": [
        "Qinghao Gao",
        "Jianhai Qu",
        "Yunsong Li",
        "Weiqiang Dong"
      ],
      "abstract": "Multimodal classification in remote sensing often suffers from missing modalities caused by environmental interference, sensor failures, or atmospheric effects, which severely degrade classification performance. Existing two-stage adaptation methods are computationally expensive and assume complete multimodal data during training, limiting their generalization to real-world incompleteness. To overcome these issues, we propose a Missing-aware Mixture-of-Loras (MaMOL) framework that reformulates modality missing as a multi-task learning problem. MaMOL introduces a dual-routing mechanism: a task-oriented dynamic router that adaptively activates experts for different missing patterns, and a modality-specific-shared static router that maintains stable cross-modal knowledge sharing. Unlike prior methods that train separate networks for each missing configuration, MaMOL achieves parameter-efficient adaptation via lightweight expert updates and shared expert reuse. Experiments on multiple remote sensing benchmarks demonstrate superior robustness and generalization under varying missing rates, with minimal computational overhead. Moreover, transfer experiments on natural image datasets validate its scalability and cross-domain applicability, highlighting MaMOL as a general and efficient solution for incomplete multimodal learning.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11460v1",
        "pdf": "https://arxiv.org/pdf/2511.11460v1"
      },
      "arxiv_id": "2511.11460v1",
      "comment": "11 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11459v1",
      "title": "FairReweighing: Density Estimation-Based Reweighing Framework for Improving Separation in Fair Regression",
      "authors": [
        "Xiaoyin Xi",
        "Zhe Yu"
      ],
      "abstract": "There has been a prevalence of applying AI software in both high-stakes public-sector and industrial contexts. However, the lack of transparency has raised concerns about whether these data-informed AI software decisions secure fairness against people of all racial, gender, or age groups. Despite extensive research on emerging fairness-aware AI software, up to now most efforts to solve this issue have been dedicated to binary classification tasks. Fairness in regression is relatively underexplored. In this work, we adopted a mutual information-based metric to assess separation violations. The metric is also extended so that it can be directly applied to both classification and regression problems with both binary and continuous sensitive attributes. Inspired by the Reweighing algorithm in fair classification, we proposed a FairReweighing pre-processing algorithm based on density estimation to ensure that the learned model satisfies the separation criterion. Theoretically, we show that the proposed FairReweighing algorithm can guarantee separation in the training data under a data independence assumption. Empirically, on both synthetic and real-world data, we show that FairReweighing outperforms existing state-of-the-art regression fairness solutions in terms of improving separation while maintaining high accuracy.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11459v1",
        "pdf": "https://arxiv.org/pdf/2511.11459v1"
      },
      "arxiv_id": "2511.11459v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11452v1",
      "title": "Synergy vs. Noise: Performance-Guided Multimodal Fusion For Biochemical Recurrence-Free Survival in Prostate Cancer",
      "authors": [
        "Seth Alain Chang",
        "Muhammad Mueez Amjad",
        "Noorul Wahab",
        "Ethar Alzaid",
        "Nasir Rajpoot",
        "Adam Shephard"
      ],
      "abstract": "Multimodal deep learning (MDL) has emerged as a transformative approach in computational pathology. By integrating complementary information from multiple data sources, MDL models have demonstrated superior predictive performance across diverse clinical tasks compared to unimodal models. However, the assumption that combining modalities inherently improves performance remains largely unexamined. We hypothesise that multimodal gains depend critically on the predictive quality of individual modalities, and that integrating weak modalities may introduce noise rather than complementary information. We test this hypothesis on a prostate cancer dataset with histopathology, radiology, and clinical data to predict time-to-biochemical recurrence. Our results confirm that combining high-performing modalities yield superior performance compared to unimodal approaches. However, integrating a poor-performing modality with other higher-performing modalities degrades predictive accuracy. These findings demonstrate that multimodal benefit requires selective, performance-guided integration rather than indiscriminate modality combination, with implications for MDL design across computational pathology and medical imaging.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "q-bio.QM",
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "q-bio.QM",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11452v1",
        "pdf": "https://arxiv.org/pdf/2511.11452v1"
      },
      "arxiv_id": "2511.11452v1",
      "comment": "5 pages, 1 figure, 4 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11450v1",
      "title": "VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation",
      "authors": [
        "Maximilian Rokuss",
        "Moritz Langenberg",
        "Yannick Kirchhoff",
        "Fabian Isensee",
        "Benjamin Hamm",
        "Constantin Ulrich",
        "Sebastian Regnery",
        "Lukas Bauer",
        "Efthimios Katsigiannopulos",
        "Tobias Norajitra",
        "Klaus Maier-Hein"
      ],
      "abstract": "We introduce VoxTell, a vision-language model for text-prompted volumetric medical image segmentation. It maps free-form descriptions, from single words to full clinical sentences, to 3D masks. Trained on 62K+ CT, MRI, and PET volumes spanning over 1K anatomical and pathological classes, VoxTell uses multi-stage vision-language fusion across decoder layers to align textual and visual features at multiple scales. It achieves state-of-the-art zero-shot performance across modalities on unseen datasets, excelling on familiar concepts while generalizing to related unseen classes. Extensive experiments further demonstrate strong cross-modality transfer, robustness to linguistic variations and clinical language, as well as accurate instance-specific segmentation from real-world text. Code is available at: https://www.github.com/MIC-DKFZ/VoxTell",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11450v1",
        "pdf": "https://arxiv.org/pdf/2511.11450v1"
      },
      "arxiv_id": "2511.11450v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11446v1",
      "title": "DiffPro: Joint Timestep and Layer-Wise Precision Optimization for Efficient Diffusion Inference",
      "authors": [
        "Farhana Amin",
        "Sabiha Afroz",
        "Kanchon Gharami",
        "Mona Moghadampanah",
        "Dimitrios S. Nikolopoulos"
      ],
      "abstract": "Diffusion models produce high quality images but inference is costly due to many denoising steps and heavy matrix operations. We present DiffPro, a post-training, hardware-faithful framework that works with the exact integer kernels used in deployment and jointly tunes timesteps and per-layer precision in Diffusion Transformers (DiTs) to reduce latency and memory without any training. DiffPro combines three parts: a manifold-aware sensitivity metric to allocate weight bits, dynamic activation quantization to stabilize activations across timesteps, and a budgeted timestep selector guided by teacher-student drift. In experiments DiffPro achieves up to 6.25x model compression, fifty percent fewer timesteps, and 2.8x faster inference with Delta FID <= 10 on standard benchmarks, demonstrating practical efficiency gains. DiffPro unifies step reduction and precision planning into a single budgeted deployable plan for real-time energy-aware diffusion inference.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11446v1",
        "pdf": "https://arxiv.org/pdf/2511.11446v1"
      },
      "arxiv_id": "2511.11446v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11440v1",
      "title": "From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs",
      "authors": [
        "Massimo Rizzoli",
        "Simone Alghisi",
        "Seyed Mahed Mousavi",
        "Giuseppe Riccardi"
      ],
      "abstract": "Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11440v1",
        "pdf": "https://arxiv.org/pdf/2511.11440v1"
      },
      "arxiv_id": "2511.11440v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11439v1",
      "title": "Retrofit: Continual Learning with Bounded Forgetting for Security Applications",
      "authors": [
        "Yiling He",
        "Junchi Lei",
        "Hongyu She",
        "Shuo Shao",
        "Xinran Zheng",
        "Yiping Liu",
        "Zhan Qin",
        "Lorenzo Cavallaro"
      ],
      "abstract": "Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.\n  We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11439v1",
        "pdf": "https://arxiv.org/pdf/2511.11439v1"
      },
      "arxiv_id": "2511.11439v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11438v1",
      "title": "VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models",
      "authors": [
        "Mingjie Xu",
        "Jinpeng Chen",
        "Yuzhi Zhao",
        "Jason Chun Lok Li",
        "Yue Qiu",
        "Zekang Du",
        "Mengyang Wu",
        "Pingping Zhang",
        "Kun Li",
        "Hongzheng Yang",
        "Wenao Ma",
        "Jiaheng Wei",
        "Qinbin Li",
        "Kangcheng Liu",
        "Wenqiang Lei"
      ],
      "abstract": "Multimodal large language models (MLLMs) have enabled a wide range of advanced vision-language applications, including fine-grained object recognition and contextual understanding. When querying specific regions or objects in an image, human users naturally use \"visual prompts\" (VPs), such as bounding boxes, to provide reference. However, no existing benchmark systematically evaluates the ability of MLLMs to interpret such VPs. This gap leaves it unclear whether current MLLMs can effectively recognize VPs, an intuitive prompting method for humans, and use them to solve problems. To address this limitation, we introduce VP-Bench, a benchmark for assessing MLLMs' capability in VP perception and utilization. VP-Bench employs a two-stage evaluation framework: Stage 1 examines models' ability to perceive VPs in natural scenes, using 30k visualized prompts spanning eight shapes and 355 attribute combinations. Stage 2 investigates the impact of VPs on downstream tasks, measuring their effectiveness in real-world problem-solving scenarios. Using VP-Bench, we evaluate 28 MLLMs, including proprietary systems (e.g., GPT-4o) and open-source models (e.g., InternVL3 and Qwen2.5-VL), and provide a comprehensive analysis of factors that affect VP understanding, such as variations in VP attributes, question arrangement, and model scale. VP-Bench establishes a new reference framework for studying how MLLMs comprehend and resolve grounded referring questions.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11438v1",
        "pdf": "https://arxiv.org/pdf/2511.11438v1"
      },
      "arxiv_id": "2511.11438v1",
      "comment": "This is the extended version of the paper accepted at AAAI 2026, which includes all technical appendices and additional experimental details",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11437v1",
      "title": "Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping",
      "authors": [
        "Guowei Zhang",
        "Yun Zhao",
        "Moein Khajehnejad",
        "Adeel Razi",
        "Levin Kuhlmann"
      ],
      "abstract": "Mapping human brain activity to natural images offers a new window into vision and cognition, yet current diffusion-based decoders face a core difficulty: most condition directly on fMRI features without analyzing how visual information is organized across the cortex. This overlooks the brain's hierarchical processing and blurs the roles of early, middle, and late visual areas. We propose Hi-DREAM, a brain-inspired conditional diffusion framework that makes the cortical organization explicit. A region-of-interest (ROI) adapter groups fMRI into early/mid/late streams and converts them into a multi-scale cortical pyramid aligned with the U-Net depth (shallow scales preserve layout and edges; deeper scales emphasize objects and semantics). A lightweight, depth-matched ControlNet injects these scale-specific hints during denoising. The result is an efficient and interpretable decoder in which each signal plays a brain-like role, allowing the model not only to reconstruct images but also to illuminate functional contributions of different visual areas. Experiments on the Natural Scenes Dataset (NSD) show that Hi-DREAM attains state-of-the-art performance on high-level semantic metrics while maintaining competitive low-level fidelity. These findings suggest that structuring conditioning by cortical hierarchy is a powerful alternative to purely data-driven embeddings and provides a useful lens for studying the visual cortex.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11437v1",
        "pdf": "https://arxiv.org/pdf/2511.11437v1"
      },
      "arxiv_id": "2511.11437v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11436v1",
      "title": "Unsupervised Motion-Compensated Decomposition for Cardiac MRI Reconstruction via Neural Representation",
      "authors": [
        "Xuanyu Tian",
        "Lixuan Chen",
        "Qing Wu",
        "Xiao Wang",
        "Jie Feng",
        "Yuyao Zhang",
        "Hongjiang Wei"
      ],
      "abstract": "Cardiac magnetic resonance (CMR) imaging is widely used to characterize cardiac morphology and function. To accelerate CMR imaging, various methods have been proposed to recover high-quality spatiotemporal CMR images from highly undersampled k-t space data. However, current CMR reconstruction techniques either fail to achieve satisfactory image quality or are restricted by the scarcity of ground truth data, leading to limited applicability in clinical scenarios. In this work, we proposed MoCo-INR, a new unsupervised method that integrates implicit neural representations (INR) with the conventional motion-compensated (MoCo) framework. Using explicit motion modeling and the continuous prior of INRs, MoCo-INR can produce accurate cardiac motion decomposition and high-quality CMR reconstruction. Furthermore, we introduce a new INR network architecture tailored to the CMR problem, which significantly stabilizes model optimization. Experiments on retrospective (simulated) datasets demonstrate the superiority of MoCo-INR over state-of-the-art methods, achieving fast convergence and fine-detailed reconstructions at ultra-high acceleration factors (e.g., 20x in VISTA sampling). Additionally, evaluations on prospective (real-acquired) free-breathing CMR scans highlight the clinical practicality of MoCo-INR for real-time imaging. Several ablation studies further confirm the effectiveness of the critical components of MoCo-INR.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11436v1",
        "pdf": "https://arxiv.org/pdf/2511.11436v1"
      },
      "arxiv_id": "2511.11436v1",
      "comment": "Accepted by AAAI-26",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11435v1",
      "title": "The Persistence of Cultural Memory: Investigating Multimodal Iconicity in Diffusion Models",
      "authors": [
        "Maria-Teresa De Rosa Palmini",
        "Eva Cetinic"
      ],
      "abstract": "Our work addresses the ambiguity between generalization and memorization in text-to-image diffusion models, focusing on a specific case we term multimodal iconicity. This refers to instances where images and texts evoke culturally shared associations, such as when a title recalls a familiar artwork or film scene. While prior research on memorization and unlearning emphasizes forgetting, we examine what is remembered and how, focusing on the balance between recognizing cultural references and reproducing them. We introduce an evaluation framework that separates recognition, whether a model identifies a reference, from realization, how it depicts it through replication or reinterpretation, quantified through measures capturing both dimensions. By evaluating five diffusion models across 767 Wikidata-derived cultural references spanning static and dynamic imagery, we show that our framework distinguishes replication from transformation more effectively than existing similarity-based methods. To assess linguistic sensitivity, we conduct prompt perturbation experiments using synonym substitutions and literal image descriptions, finding that models often reproduce iconic visual structures even when textual cues are altered. Finally, our analysis shows that cultural alignment correlates not only with training data frequency, but also textual uniqueness, reference popularity, and creation date. Our work reveals that the value of diffusion models lies not only in what they reproduce but in how they transform and recontextualize cultural knowledge, advancing evaluation beyond simple text-image matching toward richer contextual understanding.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11435v1",
        "pdf": "https://arxiv.org/pdf/2511.11435v1"
      },
      "arxiv_id": "2511.11435v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11434v1",
      "title": "WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation",
      "authors": [
        "Wei Chow",
        "Jiachun Pan",
        "Yongyuan Liang",
        "Mingze Zhou",
        "Xue Song",
        "Liyu Jia",
        "Saining Zhang",
        "Siliang Tang",
        "Juncheng Li",
        "Fengda Zhang",
        "Weijia Wu",
        "Hanwang Zhang",
        "Tat-Seng Chua"
      ],
      "abstract": "Recent advances in unified multimodal models (UMMs) have enabled impressive progress in visual comprehension and generation. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation and editing. To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation. Our suite consists of two complementary parts. WEAVE-100k is a large-scale dataset of 100K interleaved samples spanning over 370K dialogue turns and 500K images, covering comprehension, editing, and generation tasks that require reasoning over historical context. WEAVEBench is a human-annotated benchmark with 100 tasks based on 480 images, featuring a hybrid VLM judger evaluation framework based on both the reference image and the combination of the original image with editing instructions that assesses models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains. Experiments demonstrate that training on WEAVE-100k enables vision comprehension, image editing, and comprehension-generation collaboration capabilities. Furthermore, it facilitates UMMs to develop emergent visual-memory capabilities, while extensive evaluations on WEAVEBench expose the persistent limitations and challenges of current approaches in multi-turn, context-aware image generation and editing. We believe WEAVE provides a view and foundation for studying in-context interleaved comprehension and generation for multi-modal community.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11434v1",
        "pdf": "https://arxiv.org/pdf/2511.11434v1"
      },
      "arxiv_id": "2511.11434v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11427v1",
      "title": "Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs",
      "authors": [
        "Francisco Nogueira",
        "Alexandre Bernardino",
        "Bruno Martins"
      ],
      "abstract": "Referring Expression Comprehension (REC) requires models to localize objects in images based on natural language descriptions. Research on the area remains predominantly English-centric, despite increasing global deployment demands. This work addresses multilingual REC through two main contributions. First, we construct a unified multilingual dataset spanning 10 languages, by systematically expanding 12 existing English REC benchmarks through machine translation and context-based translation enhancement. The resulting dataset comprises approximately 8 million multilingual referring expressions across 177,620 images, with 336,882 annotated objects. Second, we introduce an attention-anchored neural architecture that uses multilingual SigLIP2 encoders. Our attention-based approach generates coarse spatial anchors from attention distributions, which are subsequently refined through learned residuals. Experimental evaluation demonstrates competitive performance on standard benchmarks, e.g. achieving 86.9% accuracy at IoU@50 on RefCOCO aggregate multilingual evaluation, compared to an English-only result of 91.3%. Multilingual evaluation shows consistent capabilities across languages, establishing the practical feasibility of multilingual visual grounding systems. The dataset and model are available at $\\href{https://multilingual.franreno.com}{multilingual.franreno.com}$.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11427v1",
        "pdf": "https://arxiv.org/pdf/2511.11427v1"
      },
      "arxiv_id": "2511.11427v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11423v1",
      "title": "CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction",
      "authors": [
        "Cong-Tinh Dao",
        "Nguyen Minh Thao Phan",
        "Jun-En Ding",
        "Chenwei Wu",
        "David Restrepo",
        "Dongsheng Luo",
        "Fanyi Zhao",
        "Chun-Chieh Liao",
        "Wen-Chih Peng",
        "Chi-Te Wang",
        "Pei-Fu Chen",
        "Ling Chen",
        "Xinglong Ju",
        "Feng Liu",
        "Fang-Ming Hung"
      ],
      "abstract": "Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11423v1",
        "pdf": "https://arxiv.org/pdf/2511.11423v1"
      },
      "arxiv_id": "2511.11423v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11422v1",
      "title": "Shrinking the Teacher: An Adaptive Teaching Paradigm for Asymmetric EEG-Vision Alignment",
      "authors": [
        "Lukun Wu",
        "Jie Li",
        "Ziqi Ren",
        "Kaifan Zhang",
        "Xinbo Gao"
      ],
      "abstract": "Decoding visual features from EEG signals is a central challenge in neuroscience, with cross-modal alignment as the dominant approach. We argue that the relationship between visual and brain modalities is fundamentally asymmetric, characterized by two critical gaps: a Fidelity Gap (stemming from EEG's inherent noise and signal degradation, vs. vision's high-fidelity features) and a Semantic Gap (arising from EEG's shallow conceptual representation, vs. vision's rich semantic depth). Previous methods often overlook this asymmetry, forcing alignment between the two modalities as if they were equal partners and thereby leading to poor generalization. To address this, we propose the adaptive teaching paradigm. This paradigm empowers the ``teacher\" modality (vision) to dynamically shrink and adjust its knowledge structure under task guidance, tailoring its semantically dense features to match the ``student\" modality (EEG)'s capacity. We implement this paradigm with the ShrinkAdapter, a simple yet effective module featuring a residual-free design and a bottleneck structure. Through extensive experiments, we validate the underlying rationale and effectiveness of our paradigm. Our method achieves a top-1 accuracy of 60.2\\% on the zero-shot brain-to-image retrieval task, surpassing previous state-of-the-art methods by a margin of 9.8\\%. Our work introduces a new perspective for asymmetric alignment: the teacher must shrink and adapt to bridge the vision-brain gap.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11422v1",
        "pdf": "https://arxiv.org/pdf/2511.11422v1"
      },
      "arxiv_id": "2511.11422v1",
      "comment": "21pages,12 figures,published to AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11421v1",
      "title": "BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning",
      "authors": [
        "Lan Li",
        "Tao Hu",
        "Da-Wei Zhou",
        "Han-Jia Ye",
        "De-Chuan Zhan"
      ],
      "abstract": "Class-Incremental Learning (CIL) aims to continually learn new categories without forgetting previously acquired knowledge. Vision-language models such as CLIP offer strong transferable representations via multi-modal supervision, making them promising for CIL. However, applying CLIP to CIL poses two major challenges: (1) adapting to downstream tasks often requires additional learnable modules, increasing model complexity and susceptibility to forgetting; and (2) while multi-modal representations offer complementary strengths, existing methods have yet to fully realize their potential in effectively integrating visual and textual modalities. To address these issues, we propose BOFA (Bridge-layer Orthogonal Fusion for Adaptation), a novel framework for CIL. BOFA confines all model adaptation exclusively to CLIP's existing cross-modal bridge-layer, thereby adding no extra parameters or inference cost. To prevent forgetting within this layer, it leverages Orthogonal Low-Rank Fusion, a mechanism that constrains parameter updates to a low-rank ``safe subspace\" mathematically constructed to be orthogonal to past task features. This ensures stable knowledge accumulation without data replay. Furthermore, BOFA employs a cross-modal hybrid prototype that synergizes stable textual prototypes with visual counterparts derived from our stably adapted bridge-layer, enhancing classification performance. Extensive experiments on standard benchmarks show that BOFA achieves superior accuracy and efficiency compared to existing methods.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11421v1",
        "pdf": "https://arxiv.org/pdf/2511.11421v1"
      },
      "arxiv_id": "2511.11421v1",
      "comment": "Accepted by AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11418v1",
      "title": "Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching",
      "authors": [
        "Dara Varam",
        "Diaa A. Abuhani",
        "Imran Zualkernan",
        "Raghad AlDamani",
        "Lujain Khalil"
      ],
      "abstract": "Flow Matching (FM) generative models offer efficient simulation-free training and deterministic sampling, but their practical deployment is challenged by high-precision parameter requirements. We adapt optimal transport (OT)-based post-training quantization to FM models, minimizing the 2-Wasserstein distance between quantized and original weights, and systematically compare its effectiveness against uniform, piecewise, and logarithmic quantization schemes. Our theoretical analysis provides upper bounds on generative degradation under quantization, and empirical results across five benchmark datasets of varying complexity show that OT-based quantization preserves both visual generation quality and latent space stability down to 2-3 bits per parameter, where alternative methods fail. This establishes OT-based quantization as a principled, effective approach to compress FM generative models for edge and embedded AI applications.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11418v1",
        "pdf": "https://arxiv.org/pdf/2511.11418v1"
      },
      "arxiv_id": "2511.11418v1",
      "comment": "12 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11415v1",
      "title": "Differentiation Strategies for Acoustic Inverse Problems: Admittance Estimation and Shape Optimization",
      "authors": [
        "Nikolas Borrel-Jensen",
        "Josiah Bjorgaard"
      ],
      "abstract": "We demonstrate a practical differentiable programming approach for acoustic inverse problems through two applications: admittance estimation and shape optimization for resonance damping. First, we show that JAX-FEM's automatic differentiation (AD) enables direct gradient-based estimation of complex boundary admittance from sparse pressure measurements, achieving 3-digit precision without requiring manual derivation of adjoint equations. Second, we apply randomized finite differences to acoustic shape optimization, combining JAX-FEM for forward simulation with PyTorch3D for mesh manipulation through AD. By separating physics-driven boundary optimization from geometry-driven interior mesh adaptation, we achieve 48.1% energy reduction at target frequencies with 30-fold fewer FEM solutions compared to standard finite difference on the full mesh. This work showcases how modern differentiable software stacks enable rapid prototyping of optimization workflows for physics-based inverse problems, with automatic differentiation for parameter estimation and a combination of finite differences and AD for geometric design.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG",
        "cs.CE",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11415v1",
        "pdf": "https://arxiv.org/pdf/2511.11415v1"
      },
      "arxiv_id": "2511.11415v1",
      "comment": "4 pages, 2 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11413v1",
      "title": "Multicalibration yields better matchings",
      "authors": [
        "Riccardo Colini Baldeschi",
        "Simone Di Gregorio",
        "Simone Fioravanti",
        "Federico Fusco",
        "Ido Guy",
        "Daniel Haimovich",
        "Stefano Leonardi",
        "Fridolin Linder",
        "Lorenzo Perini",
        "Matteo Russo",
        "Niek Tax"
      ],
      "abstract": "Consider the problem of finding the best matching in a weighted graph where we only have access to predictions of the actual stochastic weights, based on an underlying context. If the predictor is the Bayes optimal one, then computing the best matching based on the predicted weights is optimal. However, in practice, this perfect information scenario is not realistic. Given an imperfect predictor, a suboptimal decision rule may compensate for the induced error and thus outperform the standard optimal rule.\n  In this paper, we propose multicalibration as a way to address this problem. This fairness notion requires a predictor to be unbiased on each element of a family of protected sets of contexts. Given a class of matching algorithms $\\mathcal C$ and any predictor $γ$ of the edge-weights, we show how to construct a specific multicalibrated predictor $\\hat γ$, with the following property. Picking the best matching based on the output of $\\hat γ$ is competitive with the best decision rule in $\\mathcal C$ applied onto the original predictor $γ$. We complement this result by providing sample complexity bounds.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11413v1",
        "pdf": "https://arxiv.org/pdf/2511.11413v1"
      },
      "arxiv_id": "2511.11413v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11410v1",
      "title": "Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models",
      "authors": [
        "Jiaxi Huang",
        "Dongxu Wu",
        "Hanwei Zhu",
        "Lingyu Zhu",
        "Jun Xing",
        "Xu Wang",
        "Baoliang Chen"
      ],
      "abstract": "The rapid advancement of Multi-modal Large Language Models (MLLMs) has expanded their capabilities beyond high-level vision tasks. Nevertheless, their potential for Document Image Quality Assessment (DIQA) remains underexplored. To bridge this gap, we propose Q-Doc, a three-tiered evaluation framework for systematically probing DIQA capabilities of MLLMs at coarse, middle, and fine granularity levels. a) At the coarse level, we instruct MLLMs to assign quality scores to document images and analyze their correlation with Quality Annotations. b) At the middle level, we design distortion-type identification tasks, including single-choice and multi-choice tests for multi-distortion scenarios. c) At the fine level, we introduce distortion-severity assessment where MLLMs classify distortion intensity against human-annotated references. Our evaluation demonstrates that while MLLMs possess nascent DIQA abilities, they exhibit critical limitations: inconsistent scoring, distortion misidentification, and severity misjudgment. Significantly, we show that Chain-of-Thought (CoT) prompting substantially enhances performance across all levels. Our work provides a benchmark for DIQA capabilities in MLLMs, revealing pronounced deficiencies in their quality perception and promising pathways for enhancement. The benchmark and code are publicly available at:\n  https://github.com/cydxf/Q-Doc.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11410v1",
        "pdf": "https://arxiv.org/pdf/2511.11410v1"
      },
      "arxiv_id": "2511.11410v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11407v1",
      "title": "MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model",
      "authors": [
        "Manyu Li",
        "Ruian He",
        "Chenxi Ma",
        "Weimin Tan",
        "Bo Yan"
      ],
      "abstract": "Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11407v1",
        "pdf": "https://arxiv.org/pdf/2511.11407v1"
      },
      "arxiv_id": "2511.11407v1",
      "comment": "11 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11406v1",
      "title": "Disentangling Emotional Bases and Transient Fluctuations: A Low-Rank Sparse Decomposition Approach for Video Affective Analysis",
      "authors": [
        "Feng-Qi Cui",
        "Jinyang Huang",
        "Ziyu Jia",
        "Xinyu Li",
        "Xin Yan",
        "Xiaokang Zhou",
        "Meng Wang"
      ],
      "abstract": "Video-based Affective Computing (VAC), vital for emotion analysis and human-computer interaction, suffers from model instability and representational degradation due to complex emotional dynamics. Since the meaning of different emotional fluctuations may differ under different emotional contexts, the core limitation is the lack of a hierarchical structural mechanism to disentangle distinct affective components, i.e., emotional bases (the long-term emotional tone), and transient fluctuations (the short-term emotional fluctuations). To address this, we propose the Low-Rank Sparse Emotion Understanding Framework (LSEF), a unified model grounded in the Low-Rank Sparse Principle, which theoretically reframes affective dynamics as a hierarchical low-rank sparse compositional process. LSEF employs three plug-and-play modules, i.e., the Stability Encoding Module (SEM) captures low-rank emotional bases; the Dynamic Decoupling Module (DDM) isolates sparse transient signals; and the Consistency Integration Module (CIM) reconstructs multi-scale stability and reactivity coherence. This framework is optimized by a Rank Aware Optimization (RAO) strategy that adaptively balances gradient smoothness and sensitivity. Extensive experiments across multiple datasets confirm that LSEF significantly enhances robustness and dynamic discrimination, which further validates the effectiveness and generality of hierarchical low-rank sparse modeling for understanding affective dynamics.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11406v1",
        "pdf": "https://arxiv.org/pdf/2511.11406v1"
      },
      "arxiv_id": "2511.11406v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11402v1",
      "title": "Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning",
      "authors": [
        "Amit Jain",
        "Victor Rodriguez-Fernandez",
        "Richard Linares"
      ],
      "abstract": "Autonomous spacecraft control for mission phases such as launch, ascent, stage separation, and orbit insertion remains a critical challenge due to the need for adaptive policies that generalize across dynamically distinct regimes. While reinforcement learning (RL) has shown promise in individual astrodynamics tasks, existing approaches often require separate policies for distinct mission phases, limiting adaptability and increasing operational complexity. This work introduces a transformer-based RL framework that unifies multi-phase trajectory optimization through a single policy architecture, leveraging the transformer's inherent capacity to model extended temporal contexts. Building on proximal policy optimization (PPO), our framework replaces conventional recurrent networks with a transformer encoder-decoder structure, enabling the agent to maintain coherent memory across mission phases spanning seconds to minutes during critical operations. By integrating a Gated Transformer-XL (GTrXL) architecture, the framework eliminates manual phase transitions while maintaining stability in control decisions. We validate our approach progressively: first demonstrating near-optimal performance on single-phase benchmarks (double integrator and Van der Pol oscillator), then extending to multiphase waypoint navigation variants, and finally tackling a complex multiphase rocket ascent problem that includes atmospheric flight, stage separation, and vacuum operations. Results demonstrate that the transformer-based framework not only matches analytical solutions in simple cases but also effectively learns coherent control policies across dynamically distinct regimes, establishing a foundation for scalable autonomous mission planning that reduces reliance on phase-specific controllers while maintaining compatibility with safety-critical verification protocols.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11402v1",
        "pdf": "https://arxiv.org/pdf/2511.11402v1"
      },
      "arxiv_id": "2511.11402v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11397v1",
      "title": "Variational Quantum Algorithms for Particle Track Reconstruction",
      "authors": [
        "Vincenzo Lipardi",
        "Xenofon Chiotopoulos",
        "Jacco A. de Vries",
        "Domenica Dibenedetto",
        "Kurt Driessens",
        "Marcel Merk",
        "Mark H. M. Winands"
      ],
      "abstract": "Quantum Computing is a rapidly developing field with the potential to tackle the increasing computational challenges faced in high-energy physics. In this work, we explore the potential and limitations of variational quantum algorithms in solving the particle track reconstruction problem. We present an analysis of two distinct formulations for identifying straight-line tracks in a multilayer detection system, inspired by the LHCb vertex detector. The first approach is formulated as a ground-state energy problem, while the second approach is formulated as a system of linear equations. This work addresses one of the main challenges when dealing with variational quantum algorithms on general problems, namely designing an expressive and efficient quantum ansatz working on tracking events with fixed detector geometry. For this purpose, we employed a quantum architecture search method based on Monte Carlo Tree Search to design the quantum circuits for different problem sizes. We provide experimental results to test our approach on both formulations for different problem sizes in terms of performance and computational cost.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11397v1",
        "pdf": "https://arxiv.org/pdf/2511.11397v1"
      },
      "arxiv_id": "2511.11397v1",
      "comment": "17 pages, 5 figures, 2 tables, pre-proceedings BNAIC 2024",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11393v1",
      "title": "Robust and Efficient Communication in Multi-Agent Reinforcement Learning",
      "authors": [
        "Zejiao Liu",
        "Yi Li",
        "Jiali Wang",
        "Junqi Tu",
        "Yitian Hong",
        "Fangfei Li",
        "Yang Liu",
        "Toshiharu Sugawara",
        "Yang Tang"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11393v1",
        "pdf": "https://arxiv.org/pdf/2511.11393v1"
      },
      "arxiv_id": "2511.11393v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11391v1",
      "title": "SPOT: Single-Shot Positioning via Trainable Near-Field Rainbow Beamforming",
      "authors": [
        "Yeyue Cai",
        "Jianhua Mo",
        "Meixia Tao"
      ],
      "abstract": "Phase-time arrays, which integrate phase shifters (PSs) and true-time delays (TTDs), have emerged as a cost-effective architecture for generating frequency-dependent rainbow beams in wideband sensing and localization. This paper proposes an end-to-end deep learning-based scheme that simultaneously designs the rainbow beams and estimates user positions. Treating the PS and TTD coefficients as trainable variables allows the network to synthesize task-oriented beams that maximize localization accuracy. A lightweight fully connected module then recovers the user's angle-range coordinates from its feedback of the maximum quantized received power and its corresponding subcarrier index after a single downlink transmission. Compared with existing analytical and learning-based schemes, the proposed method reduces overhead by an order of magnitude and delivers consistently lower two-dimensional positioning error.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11391v1",
        "pdf": "https://arxiv.org/pdf/2511.11391v1"
      },
      "arxiv_id": "2511.11391v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11388v1",
      "title": "Robust inverse material design with physical guarantees using the Voigt-Reuss Net",
      "authors": [
        "Sanath Keshav",
        "Felix Fritzen"
      ],
      "abstract": "We propose a spectrally normalized surrogate for forward and inverse mechanical homogenization with hard physical guarantees. Leveraging the Voigt-Reuss bounds, we factor their difference via a Cholesky-like operator and learn a dimensionless, symmetric positive semi-definite representation with eigenvalues in $[0,1]$; the inverse map returns symmetric positive-definite predictions that lie between the bounds in the Löwner sense. In 3D linear elasticity on an open dataset of stochastic biphasic microstructures, a fully connected Voigt-Reuss net trained on $>\\!7.5\\times 10^{5}$ FFT-based labels with 236 isotropy-invariant descriptors and three contrast parameters recovers the isotropic projection with near-perfect fidelity (isotropy-related entries: $R^2 \\ge 0.998$), while anisotropy-revealing couplings are unidentifiable from $SO(3)$-invariant inputs. Tensor-level relative Frobenius errors have median $\\approx 1.7\\%$ and mean $\\approx 3.4\\%$ across splits. For 2D plane strain on thresholded trigonometric microstructures, coupling spectral normalization with a differentiable renderer and a CNN yields $R^2>0.99$ on all components, subpercent normalized losses, accurate tracking of percolation-induced eigenvalue jumps, and robust generalization to out-of-distribution images. Treating the parametric microstructure as design variables, batched first-order optimization with a single surrogate matches target tensors within a few percent and returns diverse near-optimal designs. Overall, the Voigt-Reuss net unifies accurate, physically admissible forward prediction with large-batch, constraint-consistent inverse design, and is generic to elliptic operators and coupled-physics settings.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11388v1",
        "pdf": "https://arxiv.org/pdf/2511.11388v1"
      },
      "arxiv_id": "2511.11388v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11381v1",
      "title": "SoK: Security Evaluation of Wi-Fi CSI Biometrics: Attacks, Metrics, and Systemic Weaknesses",
      "authors": [
        "Gioliano de Oliveira Braga",
        "Pedro Henrique dos Santos Rocha",
        "Rafael Pimenta de Mattos Paixão",
        "Giovani Hoff da Costa",
        "Gustavo Cavalcanti Morais",
        "Lourenço Alves Pereira Júnior"
      ],
      "abstract": "Wi-Fi Channel State Information (CSI) has been repeatedly proposed as a biometric modality, often with reports of high accuracy and operational feasibility. However, the field lacks a consolidated understanding of its security properties, adversarial resilience, and methodological consistency. This Systematization of Knowledge (SoK) examines CSI-based biometric authentication through a security perspective, analyzing how existing work differs across sensing infrastructure, signal representations, feature pipelines, learning models, and evaluation methodologies. Our synthesis reveals systemic inconsistencies: reliance on aggregate accuracy metrics, limited reporting of FAR/FRR/EER, absence of per-user risk analysis, and scarce consideration of threat models or adversarial feasibility. We construct a unified evaluation framework to empirically expose these issues and demonstrate how security-relevant metrics, such as per-class EER, FCS, and the Gini Coefficient, uncover risk concentration that remains hidden under traditional reporting practices. Our analysis highlights concrete attack surfaces and shows how methodological choices materially influence vulnerability profiles, which include replay, geometric mimicry, and environmental perturbation. Based on these findings, we articulate the security boundaries of current CSI biometrics and provide guidelines for rigorous evaluation, reproducible experimentation, and future research directions. This SoK offers the security community a structured, evidence-driven reassessment of Wi-Fi CSI biometrics and their suitability as an authentication primitive.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CR",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11381v1",
        "pdf": "https://arxiv.org/pdf/2511.11381v1"
      },
      "arxiv_id": "2511.11381v1",
      "comment": "An improved version will be submitted to Euro S&P 2026, and this paper will be updated in the near future",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11380v1",
      "title": "When Genes Speak: A Semantic-Guided Framework for Spatially Resolved Transcriptomics Data Clustering",
      "authors": [
        "Jiangkai Long",
        "Yanran Zhu",
        "Chang Tang",
        "Kun Sun",
        "Yuanyuan Liu",
        "Xuesong Yan"
      ],
      "abstract": "Spatial transcriptomics enables gene expression profiling with spatial context, offering unprecedented insights into the tissue microenvironment. However, most computational models treat genes as isolated numerical features, ignoring the rich biological semantics encoded in their symbols. This prevents a truly deep understanding of critical biological characteristics. To overcome this limitation, we present SemST, a semantic-guided deep learning framework for spatial transcriptomics data clustering. SemST leverages Large Language Models (LLMs) to enable genes to \"speak\" through their symbolic meanings, transforming gene sets within each tissue spot into biologically informed embeddings. These embeddings are then fused with the spatial neighborhood relationships captured by Graph Neural Networks (GNNs), achieving a coherent integration of biological function and spatial structure. We further introduce the Fine-grained Semantic Modulation (FSM) module to optimally exploit these biological priors. The FSM module learns spot-specific affine transformations that empower the semantic embeddings to perform an element-wise calibration of the spatial features, thus dynamically injecting high-order biological knowledge into the spatial context. Extensive experiments on public spatial transcriptomics datasets show that SemST achieves state-of-the-art clustering performance. Crucially, the FSM module exhibits plug-and-play versatility, consistently improving the performance when integrated into other baseline methods.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11380v1",
        "pdf": "https://arxiv.org/pdf/2511.11380v1"
      },
      "arxiv_id": "2511.11380v1",
      "comment": "AAAI'2026 poster paper. 12 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11378v1",
      "title": "Unsupervised Segmentation of Micro-CT Scans of Polyurethane Structures By Combining Hidden-Markov-Random Fields and a U-Net",
      "authors": [
        "Julian Grolig",
        "Lars Griem",
        "Michael Selzer",
        "Hans-Ulrich Kauczor",
        "Simon M. F. Triphan",
        "Britta Nestler",
        "Arnd Koeppe"
      ],
      "abstract": "Extracting digital material representations from images is a necessary prerequisite for a quantitative analysis of material properties. Different segmentation approaches have been extensively studied in the past to achieve this task, but were often lacking accuracy or speed. With the advent of machine learning, supervised convolutional neural networks (CNNs) have achieved state-of-the-art performance for different segmentation tasks. However, these models are often trained in a supervised manner, which requires large labeled datasets. Unsupervised approaches do not require ground-truth data for learning, but suffer from long segmentation times and often worse segmentation accuracy. Hidden Markov Random Fields (HMRF) are an unsupervised segmentation approach that incorporates concepts of neighborhood and class distributions. We present a method that integrates HMRF theory and CNN segmentation, leveraging the advantages of both areas: unsupervised learning and fast segmentation times. We investigate the contribution of different neighborhood terms and components for the unsupervised HMRF loss. We demonstrate that the HMRF-UNet enables high segmentation accuracy without ground truth on a Micro-Computed Tomography ($μ$CT) image dataset of Polyurethane (PU) foam structures. Finally, we propose and demonstrate a pre-training strategy that considerably reduces the required amount of ground-truth data when training a segmentation model.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11378v1",
        "pdf": "https://arxiv.org/pdf/2511.11378v1"
      },
      "arxiv_id": "2511.11378v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11373v1",
      "title": "MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism",
      "authors": [
        "Shulin Liu",
        "Dong Du",
        "Tao Yang",
        "Yang Li",
        "Boyu Qiu"
      ],
      "abstract": "Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11373v1",
        "pdf": "https://arxiv.org/pdf/2511.11373v1"
      },
      "arxiv_id": "2511.11373v1",
      "comment": "10 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11368v1",
      "title": "Free3D: 3D Human Motion Emerges from Single-View 2D Supervision",
      "authors": [
        "Sheng Liu",
        "Yuanzhi Liang",
        "Sidan Du"
      ],
      "abstract": "Recent 3D human motion generation models demonstrate remarkable reconstruction accuracy yet struggle to generalize beyond training distributions. This limitation arises partly from the use of precise 3D supervision, which encourages models to fit fixed coordinate patterns instead of learning the essential 3D structure and motion semantic cues required for robust generalization.To overcome this limitation, we propose Free3D, a framework that synthesizes realistic 3D motions without any 3D motion annotations. Free3D introduces a Motion-Lifting Residual Quantized VAE (ML-RQ) that maps 2D motion sequences into 3D-consistent latent spaces, and a suite of 3D-free regularization objectives enforcing view consistency, orientation coherence, and physical plausibility. Trained entirely on 2D motion data, Free3D generates diverse, temporally coherent, and semantically aligned 3D motions, achieving performance comparable to or even surpassing fully 3D-supervised counterparts. These results suggest that relaxing explicit 3D supervision encourages stronger structural reasoning and generalization, offering a scalable and data-efficient paradigm for 3D motion generation.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11368v1",
        "pdf": "https://arxiv.org/pdf/2511.11368v1"
      },
      "arxiv_id": "2511.11368v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11362v1",
      "title": "On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization",
      "authors": [
        "Prabodh Katti",
        "Sangwoo Park",
        "Bipin Rajendran",
        "Osvaldo Simeone"
      ],
      "abstract": "On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11362v1",
        "pdf": "https://arxiv.org/pdf/2511.11362v1"
      },
      "arxiv_id": "2511.11362v1",
      "comment": "Conference submission; Under review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11361v1",
      "title": "Toward Multi-Fidelity Machine Learning Force Field for Cathode Materials",
      "authors": [
        "Guangyi Dong",
        "Zhihui Wang"
      ],
      "abstract": "Machine learning force fields (MLFFs), which employ neural networks to map atomic structures to system energies, effectively combine the high accuracy of first-principles calculation with the computational efficiency of empirical force fields. They are widely used in computational materials simulations. However, the development and application of MLFFs for lithium-ion battery cathode materials remain relatively limited. This is primarily due to the complex electronic structure characteristics of cathode materials and the resulting scarcity of high-quality computational datasets available for force field training. In this work, we develop a multi-fidelity machine learning force field framework to enhance the data efficiency of computational results, which can simultaneously utilize both low-fidelity non-magnetic and high-fidelity magnetic computational datasets of cathode materials for training. Tests conducted on the lithium manganese iron phosphate (LMFP) cathode material system demonstrate the effectiveness of this multi-fidelity approach. This work helps to achieve high-accuracy MLFF training for cathode materials at a lower training dataset cost, and offers new perspectives for applying MLFFs to computational simulations of cathode materials.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11361v1",
        "pdf": "https://arxiv.org/pdf/2511.11361v1"
      },
      "arxiv_id": "2511.11361v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11357v1",
      "title": "KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics",
      "authors": [
        "Haixin Li",
        "Yanke Li",
        "Diego Paez-Granados"
      ],
      "abstract": "We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11357v1",
        "pdf": "https://arxiv.org/pdf/2511.11357v1"
      },
      "arxiv_id": "2511.11357v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11347v1",
      "title": "Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions",
      "authors": [
        "Shaowei Guan",
        "Hin Chi Kwok",
        "Ngai Fong Law",
        "Gregor Stiglic",
        "Vivian Hui"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has rapidly emerged as a transformative approach for integrating large language models into clinical and biomedical workflows. However, privacy risks, such as protected health information (PHI) exposure, remain inconsistently mitigated. This review provides a thorough analysis of the current landscape of RAG applications in healthcare, including (i) sensitive data type across clinical scenarios, (ii) the associated privacy risks, (iii) current and emerging data-privacy protection mechanisms and (iv) future direction for patient data privacy protection. We synthesize 23 articles on RAG applications in healthcare and systematically analyze privacy challenges through a pipeline-structured framework encompassing data storage, transmission, retrieval and generation stages, delineating potential failure modes, their underlying causes in threat models and system mechanisms, and their practical implications. Building on this analysis, we critically review 17 articles on privacy-preserving strategies for RAG systems. Our evaluation reveals critical gaps, including insufficient clinical validation, absence of standardized evaluation frameworks, and lack of automated assessment tools. We propose actionable directions based on these limitations and conclude with a call to action. This review provides researchers and practitioners with a structured framework for understanding privacy vulnerabilities in healthcare RAG and offers a roadmap toward developing systems that achieve both clinical effectiveness and robust privacy preservation.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11347v1",
        "pdf": "https://arxiv.org/pdf/2511.11347v1"
      },
      "arxiv_id": "2511.11347v1",
      "comment": "23 pages, 2 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11346v1",
      "title": "Fast and Expressive Multi-Token Prediction with Probabilistic Circuits",
      "authors": [
        "Andreas Grivas",
        "Lorenzo Loconte",
        "Emile van Krieken",
        "Piotr Nawrot",
        "Yu Zhao",
        "Euan Wielewski",
        "Pasquale Minervini",
        "Edoardo Ponti",
        "Antonio Vergari"
      ],
      "abstract": "Multi-token prediction (MTP) is a prominent strategy to significantly speed up generation in large language models (LLMs), including byte-level LLMs, which are tokeniser-free but prohibitively slow. However, existing MTP methods often sacrifice expressiveness by assuming independence between future tokens. In this work, we investigate the trade-off between expressiveness and latency in MTP within the framework of probabilistic circuits (PCs). Our framework, named MTPC, allows one to explore different ways to encode the joint distributions over future tokens by selecting different circuit architectures, generalising classical models such as (hierarchical) mixture models, hidden Markov models and tensor networks. We show the efficacy of MTPC by retrofitting existing byte-level LLMs, such as EvaByte. Our experiments show that, when combined with speculative decoding, MTPC significantly speeds up generation compared to MTP with independence assumptions, while guaranteeing to retain the performance of the original verifier LLM. We also rigorously study the optimal trade-off between expressiveness and latency when exploring the possible parameterisations of MTPC, such as PC architectures and partial layer sharing between the verifier and draft LLMs.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11346v1",
        "pdf": "https://arxiv.org/pdf/2511.11346v1"
      },
      "arxiv_id": "2511.11346v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11344v1",
      "title": "YCB-Ev SD: Synthetic event-vision dataset for 6DoF object pose estimation",
      "authors": [
        "Pavel Rojtberg",
        "Julius Kühn"
      ],
      "abstract": "We introduce YCB-Ev SD, a synthetic dataset of event-camera data at standard definition (SD) resolution for 6DoF object pose estimation. While synthetic data has become fundamental in frame-based computer vision, event-based vision lacks comparable comprehensive resources. Addressing this gap, we present 50,000 event sequences of 34 ms duration each, synthesized from Physically Based Rendering (PBR) scenes of YCB-Video objects following the Benchmark for 6D Object Pose (BOP) methodology. Our generation framework employs simulated linear camera motion to ensure complete scene coverage, including background activity.\n  Through systematic evaluation of event representations for CNN-based inference, we demonstrate that time-surfaces with linear decay and dual-channel polarity encoding achieve superior pose estimation performance, outperforming exponential decay and single-channel alternatives by significant margins. Our analysis reveals that polarity information contributes most substantially to performance gains, while linear temporal encoding preserves critical motion information more effectively than exponential decay. The dataset is provided in a structured format with both raw event streams and precomputed optimal representations to facilitate immediate research use and reproducible benchmarking.\n  The dataset is publicly available at https://huggingface.co/datasets/paroj/ycbev_sd.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11344v1",
        "pdf": "https://arxiv.org/pdf/2511.11344v1"
      },
      "arxiv_id": "2511.11344v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11340v1",
      "title": "M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text",
      "authors": [
        "Salima Lamsiyah",
        "Saad Ezzini",
        "Abdelkader El Mahdaouy",
        "Hamza Alami",
        "Abdessamad Benlahbib",
        "Samir El Amrany",
        "Salmane Chafik",
        "Hicham Hammouchi"
      ],
      "abstract": "The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11340v1",
        "pdf": "https://arxiv.org/pdf/2511.11340v1"
      },
      "arxiv_id": "2511.11340v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11324v1",
      "title": "NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery",
      "authors": [
        "Anurag J. Vaidya",
        "Felix Meissen",
        "Daniel C. Castro",
        "Shruthi Bannur",
        "Tristan Lazard",
        "Drew F. K. Williamson",
        "Faisal Mahmood",
        "Javier Alvarez-Valle",
        "Stephanie L. Hyland",
        "Kenza Bouzid"
      ],
      "abstract": "Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11324v1",
        "pdf": "https://arxiv.org/pdf/2511.11324v1"
      },
      "arxiv_id": "2511.11324v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11323v1",
      "title": "RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms",
      "authors": [
        "Yitian Kou",
        "Yihe Gu",
        "Chen Zhou",
        "DanDan Zhu",
        "Shuguang Kuai"
      ],
      "abstract": "Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11323v1",
        "pdf": "https://arxiv.org/pdf/2511.11323v1"
      },
      "arxiv_id": "2511.11323v1",
      "comment": "AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11320v1",
      "title": "StochEP: Stochastic Equilibrium Propagation for Spiking Convergent Recurrent Neural Networks",
      "authors": [
        "Jiaqi Lin",
        "Yi Jiang",
        "Abhronil Sengupta"
      ],
      "abstract": "Spiking Neural Networks (SNNs) promise energy-efficient, sparse, biologically inspired computation. Training them with Backpropagation Through Time (BPTT) and surrogate gradients achieves strong performance but remains biologically implausible. Equilibrium Propagation (EP) provides a more local and biologically grounded alternative. However, existing EP frameworks, primarily based on deterministic neurons, either require complex mechanisms to handle discontinuities in spiking dynamics or fail to scale beyond simple visual tasks. Inspired by the stochastic nature of biological spiking mechanism and recent hardware trends, we propose a stochastic EP framework that integrates probabilistic spiking neurons into the EP paradigm. This formulation smoothens the optimization landscape, stabilizes training, and enables scalable learning in deep convolutional spiking convergent recurrent neural networks (CRNNs). We provide theoretical guarantees showing that the proposed stochastic EP dynamics approximate deterministic EP under mean-field theory, thereby inheriting its underlying theoretical guarantees. The proposed framework narrows the gap to both BPTT-trained SNNs and EP-trained non-spiking CRNNs in vision benchmarks while preserving locality, highlighting stochastic EP as a promising direction for neuromorphic and on-chip learning.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.ET",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11320v1",
        "pdf": "https://arxiv.org/pdf/2511.11320v1"
      },
      "arxiv_id": "2511.11320v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11315v1",
      "title": "LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models",
      "authors": [
        "Jawad Ibn Ahad",
        "Muhammad Rafsan Kabir",
        "Robin Krambroeckers",
        "Sifat Momen",
        "Nabeel Mohammed",
        "Shafin Rahman"
      ],
      "abstract": "Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11315v1",
        "pdf": "https://arxiv.org/pdf/2511.11315v1"
      },
      "arxiv_id": "2511.11315v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11313v1",
      "title": "DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding",
      "authors": [
        "Tanveer Hannan",
        "Dimitrios Mallios",
        "Parth Pathak",
        "Faegheh Sardari",
        "Thomas Seidl",
        "Gedas Bertasius",
        "Mohsen Fayyaz",
        "Sunando Sengupta"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents. However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices. We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources. DocSLM incorporates a Hierarchical Multimodal Compressor that jointly encodes visual, textual, and layout information from each page into a fixed-length sequence, greatly reducing memory consumption while preserving both local and global semantics. To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator. Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\\% fewer visual tokens, 75\\% fewer parameters, and 71\\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices. Code is available in the supplementary material.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11313v1",
        "pdf": "https://arxiv.org/pdf/2511.11313v1"
      },
      "arxiv_id": "2511.11313v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11311v1",
      "title": "Large-scale modality-invariant foundation models for brain MRI analysis: Application to lesion segmentation",
      "authors": [
        "Petros Koutsouvelis",
        "Matej Gazda",
        "Leroy Volmer",
        "Sina Amirrajab",
        "Kamil Barbierik",
        "Branislav Setlak",
        "Jakub Gazda",
        "Peter Drotar"
      ],
      "abstract": "The field of computer vision is undergoing a paradigm shift toward large-scale foundation model pre-training via self-supervised learning (SSL). Leveraging large volumes of unlabeled brain MRI data, such models can learn anatomical priors that improve few-shot performance in diverse neuroimaging tasks. However, most SSL frameworks are tailored to natural images, and their adaptation to capture multi-modal MRI information remains underexplored. This work proposes a modality-invariant representation learning setup and evaluates its effectiveness in stroke and epilepsy lesion segmentation, following large-scale pre-training. Experimental results suggest that despite successful cross-modality alignment, lesion segmentation primarily benefits from preserving fine-grained modality-specific features. Model checkpoints and code are made publicly available.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11311v1",
        "pdf": "https://arxiv.org/pdf/2511.11311v1"
      },
      "arxiv_id": "2511.11311v1",
      "comment": "Submitted to IEEE ISBI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11307v1",
      "title": "6D Strawberry Pose Estimation: Real-time and Edge AI Solutions Using Purely Synthetic Training Data",
      "authors": [
        "Saptarshi Neil Sinha",
        "Julius Kühn",
        "Mika Silvan Goschke",
        "Michael Weinmann"
      ],
      "abstract": "Automated and selective harvesting of fruits has become an important area of research, particularly due to challenges such as high costs and a shortage of seasonal labor in advanced economies. This paper focuses on 6D pose estimation of strawberries using purely synthetic data generated through a procedural pipeline for photorealistic rendering. We employ the YOLOX-6D-Pose algorithm, a single-shot approach that leverages the YOLOX backbone, known for its balance between speed and accuracy, and its support for edge inference. To address the lacking availability of training data, we introduce a robust and flexible pipeline for generating synthetic strawberry data from various 3D models via a procedural Blender pipeline, where we focus on enhancing the realism of the synthesized data in comparison to previous work to make it a valuable resource for training pose estimation algorithms. Quantitative evaluations indicate that our models achieve comparable accuracy on both the NVIDIA RTX 3090 and Jetson Orin Nano across several ADD-S metrics, with the RTX 3090 demonstrating superior processing speed. However, the Jetson Orin Nano is particularly suited for resource-constrained environments, making it an excellent choice for deployment in agricultural robotics. Qualitative assessments further confirm the model's performance, demonstrating its capability to accurately infer the poses of ripe and partially ripe strawberries, while facing challenges in detecting unripe specimens. This suggests opportunities for future improvements, especially in enhancing detection capabilities for unripe strawberries (if desired) by exploring variations in color. Furthermore, the methodology presented could be adapted easily for other fruits such as apples, peaches, and plums, thereby expanding its applicability and impact in the field of agricultural automation.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11307v1",
        "pdf": "https://arxiv.org/pdf/2511.11307v1"
      },
      "arxiv_id": "2511.11307v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11306v1",
      "title": "iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference",
      "authors": [
        "Wei Fan",
        "JinYi Yoon",
        "Bo Ji"
      ],
      "abstract": "Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11306v1",
        "pdf": "https://arxiv.org/pdf/2511.11306v1"
      },
      "arxiv_id": "2511.11306v1",
      "comment": "Accepted in AAAI 2026 (Oral)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11305v1",
      "title": "MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising",
      "authors": [
        "Chenghan Fu",
        "Daoze Zhang",
        "Yukang Lin",
        "Zhanheng Nie",
        "Xiang Zhang",
        "Jianyu Liu",
        "Yueran Liu",
        "Wanxian Guan",
        "Pengjie Wang",
        "Jian Xu",
        "Bo Zheng"
      ],
      "abstract": "We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of \"Pretraining, Post-training, and Application\", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11305v1",
        "pdf": "https://arxiv.org/pdf/2511.11305v1"
      },
      "arxiv_id": "2511.11305v1",
      "comment": "31 pages, 12 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11301v1",
      "title": "EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment",
      "authors": [
        "Ruoxi Cheng",
        "Haoxuan Ma",
        "Teng Ma",
        "Hongyi Zhang"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11301v1",
        "pdf": "https://arxiv.org/pdf/2511.11301v1"
      },
      "arxiv_id": "2511.11301v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11299v1",
      "title": "AUVIC: Adversarial Unlearning of Visual Concepts for Multi-modal Large Language Models",
      "authors": [
        "Haokun Chen",
        "Jianing Li",
        "Yao Zhang",
        "Jinhe Bi",
        "Yan Xia",
        "Jindong Gu",
        "Volker Tresp"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) achieve impressive performance once optimized on massive datasets. Such datasets often contain sensitive or copyrighted content, raising significant data privacy concerns. Regulatory frameworks mandating the 'right to be forgotten' drive the need for machine unlearning. This technique allows for the removal of target data without resource-consuming retraining. However, while well-studied for text, visual concept unlearning in MLLMs remains underexplored. A primary challenge is precisely removing a target visual concept without disrupting model performance on related entities. To address this, we introduce AUVIC, a novel visual concept unlearning framework for MLLMs. AUVIC applies adversarial perturbations to enable precise forgetting. This approach effectively isolates the target concept while avoiding unintended effects on similar entities. To evaluate our method, we construct VCUBench. It is the first benchmark designed to assess visual concept unlearning in group contexts. Experimental results demonstrate that AUVIC achieves state-of-the-art target forgetting rates while incurs minimal performance degradation on non-target concepts.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11299v1",
        "pdf": "https://arxiv.org/pdf/2511.11299v1"
      },
      "arxiv_id": "2511.11299v1",
      "comment": "AAAI 2026. Code: https://github.com/HaokunChen245/AUVIC",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.11298v1",
      "title": "Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation",
      "authors": [
        "Yihao Zhang",
        "Yuankai Qi",
        "Xi Zheng"
      ],
      "abstract": "Foundation models applied in robotics, particularly \\textbf{Vision--Language--Action (VLA)} models, hold great promise for achieving general-purpose manipulation. Yet, systematic real-world evaluations and cross-model comparisons remain scarce. This paper reports our \\textbf{empirical experiences} from benchmarking four representative VLAs -- \\textbf{ACT}, \\textbf{OpenVLA--OFT}, \\textbf{RDT-1B}, and \\boldmath{$π_0$} -- across four manipulation tasks conducted in both simulation and on the \\textbf{ALOHA Mobile} platform. We establish a \\textbf{standardized evaluation framework} that measures performance along three key dimensions: (1) \\textit{accuracy and efficiency} (success rate and time-to-success), (2) \\textit{adaptability} across in-distribution, spatial out-of-distribution, and instance-plus-spatial out-of-distribution settings, and (3) \\textit{language instruction-following accuracy}. Through this process, we observe that \\boldmath{$π_0$} demonstrates superior adaptability in out-of-distribution scenarios, while \\textbf{ACT} provides the highest stability in-distribution. Further analysis highlights differences in computational demands, data-scaling behavior, and recurring failure modes such as near-miss grasps, premature releases, and long-horizon state drift. These findings reveal practical trade-offs among VLA model architectures in balancing precision, generalization, and deployment cost, offering actionable insights for selecting and deploying VLAs in real-world robotic manipulation tasks.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11298v1",
        "pdf": "https://arxiv.org/pdf/2511.11298v1"
      },
      "arxiv_id": "2511.11298v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11295v1",
      "title": "SimuFreeMark: A Noise-Simulation-Free Robust Watermarking Against Image Editing",
      "authors": [
        "Yichao Tang",
        "Mingyang Li",
        "Di Miao",
        "Sheng Li",
        "Zhenxing Qian",
        "Xinpeng Zhang"
      ],
      "abstract": "The advancement of artificial intelligence generated content (AIGC) has created a pressing need for robust image watermarking that can withstand both conventional signal processing and novel semantic editing attacks. Current deep learning-based methods rely on training with hand-crafted noise simulation layers, which inherently limit their generalization to unforeseen distortions. In this work, we propose $\\textbf{SimuFreeMark}$, a noise-$\\underline{\\text{simu}}$lation-$\\underline{\\text{free}}$ water$\\underline{\\text{mark}}$ing framework that circumvents this limitation by exploiting the inherent stability of image low-frequency components. We first systematically establish that low-frequency components exhibit significant robustness against a wide range of attacks. Building on this foundation, SimuFreeMark embeds watermarks directly into the deep feature space of the low-frequency components, leveraging a pre-trained variational autoencoder (VAE) to bind the watermark with structurally stable image representations. This design completely eliminates the need for noise simulation during training. Extensive experiments demonstrate that SimuFreeMark outperforms state-of-the-art methods across a wide range of conventional and semantic attacks, while maintaining superior visual quality.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11295v1",
        "pdf": "https://arxiv.org/pdf/2511.11295v1"
      },
      "arxiv_id": "2511.11295v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11294v1",
      "title": "Decomposing Direct and Indirect Biases in Linear Models under Demographic Parity Constraint",
      "authors": [
        "Bertille Tierny",
        "Arthur Charpentier",
        "François Hu"
      ],
      "abstract": "Linear models are widely used in high-stakes decision-making due to their simplicity and interpretability. Yet when fairness constraints such as demographic parity are introduced, their effects on model coefficients, and thus on how predictive bias is distributed across features, remain opaque. Existing approaches on linear models often rely on strong and unrealistic assumptions, or overlook the explicit role of the sensitive attribute, limiting their practical utility for fairness assessment. We extend the work of (Chzhen and Schreuder, 2022) and (Fukuchi and Sakuma, 2023) by proposing a post-processing framework that can be applied on top of any linear model to decompose the resulting bias into direct (sensitive-attribute) and indirect (correlated-features) components. Our method analytically characterizes how demographic parity reshapes each model coefficient, including those of both sensitive and non-sensitive features. This enables a transparent, feature-level interpretation of fairness interventions and reveals how bias may persist or shift through correlated variables. Our framework requires no retraining and provides actionable insights for model auditing and mitigation. Experiments on both synthetic and real-world datasets demonstrate that our method captures fairness dynamics missed by prior work, offering a practical and interpretable tool for responsible deployment of linear models.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11294v1",
        "pdf": "https://arxiv.org/pdf/2511.11294v1"
      },
      "arxiv_id": "2511.11294v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11293v1",
      "title": "Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria",
      "authors": [
        "Jiheum Park",
        "Chao Pang",
        "Tristan Y. Lee",
        "Jeong Yun Yang",
        "Jacob Berkowitz",
        "Alexander Z. Wei",
        "Nicholas Tatonetti"
      ],
      "abstract": "Current cancer screening guidelines cover only a few cancer types and rely on narrowly defined criteria such as age or a single risk factor like smoking history, to identify high-risk individuals. Predictive models using electronic health records (EHRs), which capture large-scale longitudinal patient-level health information, may provide a more effective tool for identifying high-risk groups by detecting subtle prediagnostic signals of cancer. Recent advances in large language and foundation models have further expanded this potential, yet evidence remains limited on how useful HER-based models are compared with traditional risk factors currently used in screening guidelines. We systematically evaluated the clinical utility of EHR-based predictive models against traditional risk factors, including gene mutations and family history of cancer, for identifying high-risk individuals across eight major cancers (breast, lung, colorectal, prostate, ovarian, liver, pancreatic, and stomach), using data from the All of Us Research Program, which integrates EHR, genomic, and survey data from over 865,000 participants. Even with a baseline modeling approach, EHR-based models achieved a 3- to 6-fold higher enrichment of true cancer cases among individuals identified as high risk compared with traditional risk factors alone, whether used as a standalone or complementary tool. The EHR foundation model, a state-of-the-art approach trained on comprehensive patient trajectories, further improved predictive performance across 26 cancer types, demonstrating the clinical potential of EHR-based predictive modeling to support more precise and scalable early detection strategies.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11293v1",
        "pdf": "https://arxiv.org/pdf/2511.11293v1"
      },
      "arxiv_id": "2511.11293v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11289v1",
      "title": "RTGaze: Real-Time 3D-Aware Gaze Redirection from a Single Image",
      "authors": [
        "Hengfei Wang",
        "Zhongqun Zhang",
        "Yihua Cheng",
        "Hyung Jin Chang"
      ],
      "abstract": "Gaze redirection methods aim to generate realistic human face images with controllable eye movement. However, recent methods often struggle with 3D consistency, efficiency, or quality, limiting their practical applications. In this work, we propose RTGaze, a real-time and high-quality gaze redirection method. Our approach learns a gaze-controllable facial representation from face images and gaze prompts, then decodes this representation via neural rendering for gaze redirection. Additionally, we distill face geometric priors from a pretrained 3D portrait generator to enhance generation quality. We evaluate RTGaze both qualitatively and quantitatively, demonstrating state-of-the-art performance in efficiency, redirection accuracy, and image quality across multiple datasets. Our system achieves real-time, 3D-aware gaze redirection with a feedforward network (~0.06 sec/image), making it 800x faster than the previous state-of-the-art 3D-aware methods.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11289v1",
        "pdf": "https://arxiv.org/pdf/2511.11289v1"
      },
      "arxiv_id": "2511.11289v1",
      "comment": "AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11287v1",
      "title": "Building the Web for Agents: A Declarative Framework for Agent-Web Interaction",
      "authors": [
        "Sven Schultze",
        "Meike Verena Kietzmann",
        "Nils-Lucas Schönfeld",
        "Ruth Stock-Homburg"
      ],
      "abstract": "The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.MA"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11287v1",
        "pdf": "https://arxiv.org/pdf/2511.11287v1"
      },
      "arxiv_id": "2511.11287v1",
      "comment": "for associated documentation, see https://svenschultze.github.io/VOIX/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.11286v1",
      "title": "D-GAP: Improving Out-of-Domain Robustness via Dataset-Agnostic and Gradient-Guided Augmentation in Amplitude and Pixel Spaces",
      "authors": [
        "Ruoqi Wang",
        "Haitao Wang",
        "Shaojie Guo",
        "Qiong Luo"
      ],
      "abstract": "Out-of-domain (OOD) robustness is challenging to achieve in real-world computer vision applications, where shifts in image background, style, and acquisition instruments always degrade model performance. Generic augmentations show inconsistent gains under such shifts, whereas dataset-specific augmentations require expert knowledge and prior analysis. Moreover, prior studies show that neural networks adapt poorly to domain shifts because they exhibit a learning bias to domain-specific frequency components. Perturbing frequency values can mitigate such bias but overlooks pixel-level details, leading to suboptimal performance. To address these problems, we propose D-GAP (Dataset-agnostic and Gradient-guided augmentation in Amplitude and Pixel spaces), improving OOD robustness by introducing targeted augmentation in both the amplitude space (frequency space) and pixel space. Unlike conventional handcrafted augmentations, D-GAP computes sensitivity maps in the frequency space from task gradients, which reflect how strongly the model responds to different frequency components, and uses the maps to adaptively interpolate amplitudes between source and target samples. This way, D-GAP reduces the learning bias in frequency space, while a complementary pixel-space blending procedure restores fine spatial details. Extensive experiments on four real-world datasets and three domain-adaptation benchmarks show that D-GAP consistently outperforms both generic and dataset-specific augmentations, improving average OOD performance by +5.3% on real-world datasets and +1.8% on benchmark datasets.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11286v1",
        "pdf": "https://arxiv.org/pdf/2511.11286v1"
      },
      "arxiv_id": "2511.11286v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11281v1",
      "title": "Can You Tell the Difference? Contrastive Explanations for ABox Entailments",
      "authors": [
        "Patrick Koopmann",
        "Yasir Mahmood",
        "Axel-Cyrille Ngonga Ngomo",
        "Balram Tiwari"
      ],
      "abstract": "We introduce the notion of contrastive ABox explanations to answer questions of the type \"Why is a an instance of C, but b is not?\". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11281v1",
        "pdf": "https://arxiv.org/pdf/2511.11281v1"
      },
      "arxiv_id": "2511.11281v1",
      "comment": "Technical report to the paper accepted at AAAI-2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11276v1",
      "title": "Coordinative Learning with Ordinal and Relational Priors for Volumetric Medical Image Segmentation",
      "authors": [
        "Haoyi Wang"
      ],
      "abstract": "Volumetric medical image segmentation presents unique challenges due to the inherent anatomical structure and limited availability of annotations. While recent methods have shown promise by contrasting spatial relationships between slices, they rely on hard binary thresholds to define positive and negative samples, thereby discarding valuable continuous information about anatomical similarity. Moreover, these methods overlook the global directional consistency of anatomical progression, resulting in distorted feature spaces that fail to capture the canonical anatomical manifold shared across patients. To address these limitations, we propose Coordinative Ordinal-Relational Anatomical Learning (CORAL) to capture both local and global structure in volumetric images. First, CORAL employs a contrastive ranking objective to leverage continuous anatomical similarity, ensuring relational feature distances between slices are proportional to their anatomical position differences. In addition, CORAL incorporates an ordinal objective to enforce global directional consistency, aligning the learned feature distribution with the canonical anatomical progression across patients. Learning these inter-slice relationships produces anatomically informed representations that benefit the downstream segmentation task. Through this coordinative learning framework, CORAL achieves state-of-the-art performance on benchmark datasets under limited-annotation settings while learning representations with meaningful anatomical structure. Code is available at https://github.com/haoyiwang25/CORAL.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11276v1",
        "pdf": "https://arxiv.org/pdf/2511.11276v1"
      },
      "arxiv_id": "2511.11276v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11275v1",
      "title": "A Workflow for Full Traceability of AI Decisions",
      "authors": [
        "Julius Wenzel",
        "Syeda Umaima Alam",
        "Andreas Schmidt",
        "Hanwei Zhang",
        "Holger Hermanns"
      ],
      "abstract": "An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.\n  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11275v1",
        "pdf": "https://arxiv.org/pdf/2511.11275v1"
      },
      "arxiv_id": "2511.11275v1",
      "comment": "10 pages, 10 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11270v1",
      "title": "Φeat: Physically-Grounded Feature Representation",
      "authors": [
        "Giuseppe Vecchio",
        "Adrien Kaiser",
        "Rouffet Romain",
        "Rosalie Martin",
        "Elena Garces",
        "Tamy Boubekeur"
      ],
      "abstract": "Foundation models have emerged as effective backbones for many vision tasks. However, current self-supervised features entangle high-level semantics with low-level physical factors, such as geometry and illumination, hindering their use in tasks requiring explicit physical reasoning. In this paper, we introduce $Φ$eat, a novel physically-grounded visual backbone that encourages a representation sensitive to material identity, including reflectance cues and geometric mesostructure. Our key idea is to employ a pretraining strategy that contrasts spatial crops and physical augmentations of the same material under varying shapes and lighting conditions. While similar data have been used in high-end supervised tasks such as intrinsic decomposition or material estimation, we demonstrate that a pure self-supervised training strategy, without explicit labels, already provides a strong prior for tasks requiring robust features invariant to external physical factors. We evaluate the learned representations through feature similarity analysis and material selection, showing that $Φ$eat captures physically-grounded structure beyond semantic grouping. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11270v1",
        "pdf": "https://arxiv.org/pdf/2511.11270v1"
      },
      "arxiv_id": "2511.11270v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11266v1",
      "title": "GraphPilot: Grounded Scene Graph Conditioning for Language-Based Autonomous Driving",
      "authors": [
        "Fabian Schmidt",
        "Markus Enzweiler",
        "Abhinav Valada"
      ],
      "abstract": "Vision-language models have recently emerged as promising planners for autonomous driving, where success hinges on topology-aware reasoning over spatial structure and dynamic interactions from multimodal input. However, existing models are typically trained without supervision that explicitly encodes these relational dependencies, limiting their ability to infer how agents and other traffic entities influence one another from raw sensor data. In this work, we bridge this gap with a novel model-agnostic method that conditions language-based driving models on structured relational context in the form of traffic scene graphs. We serialize scene graphs at various abstraction levels and formats, and incorporate them into the models via structured prompt templates, enabling a systematic analysis of when and how relational supervision is most beneficial. Extensive evaluations on the public LangAuto benchmark show that scene graph conditioning of state-of-the-art approaches yields large and persistent improvement in driving performance. Notably, we observe up to a 15.6\\% increase in driving score for LMDrive and 17.5\\% for BEVDriver, indicating that models can better internalize and ground relational priors through scene graph-conditioned training, even without requiring scene graph input at test-time. Code, fine-tuned models, and our scene graph dataset are publicly available at https://github.com/iis-esslingen/GraphPilot.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11266v1",
        "pdf": "https://arxiv.org/pdf/2511.11266v1"
      },
      "arxiv_id": "2511.11266v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11265v1",
      "title": "SQuaD: The Software Quality Dataset",
      "authors": [
        "Mikel Robredo",
        "Matteo Esposito",
        "Davide Taibi",
        "Rafael Peñaloza",
        "Valentina Lenarduzzi"
      ],
      "abstract": "Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.IR"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11265v1",
        "pdf": "https://arxiv.org/pdf/2511.11265v1"
      },
      "arxiv_id": "2511.11265v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11262v1",
      "title": "Discovering Meaningful Units with Visually Grounded Semantics from Image Captions",
      "authors": [
        "Melika Behjati",
        "James Henderson"
      ],
      "abstract": "Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11262v1",
        "pdf": "https://arxiv.org/pdf/2511.11262v1"
      },
      "arxiv_id": "2511.11262v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11258v1",
      "title": "KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement",
      "authors": [
        "Sania Nayab",
        "Marco Simoni",
        "Giulio Rossolini",
        "Andrea Saracino"
      ],
      "abstract": "The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11258v1",
        "pdf": "https://arxiv.org/pdf/2511.11258v1"
      },
      "arxiv_id": "2511.11258v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.11257v1",
      "title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery",
      "authors": [
        "Yuqi Yin",
        "Yibo Fu",
        "Siyuan Wang",
        "Peng Sun",
        "Hongyu Wang",
        "Xiaohui Wang",
        "Lei Zheng",
        "Zhiyong Li",
        "Zhirong Liu",
        "Jianji Wang",
        "Zhaoxi Sun"
      ],
      "abstract": "The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.",
      "published": "2025-11-14",
      "updated": "2025-11-14",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.11257v1",
        "pdf": "https://arxiv.org/pdf/2511.11257v1"
      },
      "arxiv_id": "2511.11257v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    }
  ]
}