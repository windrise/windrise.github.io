{
  "fetched_at": "2026-02-01T00:37:04.020571",
  "total_papers": 100,
  "papers": [
    {
      "id": "2601.22159v1",
      "title": "RedSage: A Cybersecurity Generalist LLM",
      "authors": [
        "Naufal Suryanto",
        "Muzammal Naseer",
        "Pengfei Li",
        "Syed Talal Wasim",
        "Jinhui Yi",
        "Juergen Gall",
        "Paolo Ceravolo",
        "Ernesto Damiani"
      ],
      "abstract": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22159v1",
        "pdf": "https://arxiv.org/pdf/2601.22159v1"
      },
      "arxiv_id": "2601.22159v1",
      "comment": "Accepted on ICLR 2026; Project page: https://risys-lab.github.io/RedSage/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.22158v1",
      "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
      "authors": [
        "Yiyang Lu",
        "Susie Lu",
        "Qiao Sun",
        "Hanhong Zhao",
        "Zhicheng Jiang",
        "Xianbang Wang",
        "Tianhong Li",
        "Zhengyang Geng",
        "Kaiming He"
      ],
      "abstract": "Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22158v1",
        "pdf": "https://arxiv.org/pdf/2601.22158v1"
      },
      "arxiv_id": "2601.22158v1",
      "comment": "Technical report",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22157v1",
      "title": "Discovering Hidden Gems in Model Repositories",
      "authors": [
        "Jonathan Kahana",
        "Eliahu Horwitz",
        "Yedid Hoshen"
      ],
      "abstract": "Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22157v1",
        "pdf": "https://arxiv.org/pdf/2601.22157v1"
      },
      "arxiv_id": "2601.22157v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22156v1",
      "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
      "authors": [
        "Yingfa Chen",
        "Zhen Leng Thai",
        "Zihan Zhou",
        "Zhu Zhang",
        "Xingyu Shen",
        "Shuo Wang",
        "Chaojun Xiao",
        "Xu Han",
        "Zhiyuan Liu"
      ],
      "abstract": "Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22156v1",
        "pdf": "https://arxiv.org/pdf/2601.22156v1"
      },
      "arxiv_id": "2601.22156v1",
      "comment": "20 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22155v1",
      "title": "UEval: A Benchmark for Unified Multimodal Generation",
      "authors": [
        "Bo Li",
        "Yida Yin",
        "Wenhao Chai",
        "Xingyu Fu",
        "Zhuang Liu"
      ],
      "abstract": "We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22155v1",
        "pdf": "https://arxiv.org/pdf/2601.22155v1"
      },
      "arxiv_id": "2601.22155v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22154v1",
      "title": "Exploring Reasoning Reward Model for Agents",
      "authors": [
        "Kaixuan Fan",
        "Kaituo Feng",
        "Manyuan Zhang",
        "Tianshuo Peng",
        "Zhixun Li",
        "Yilei Jiang",
        "Shuang Chen",
        "Peng Pei",
        "Xunliang Cai",
        "Xiangyu Yue"
      ],
      "abstract": "Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22154v1",
        "pdf": "https://arxiv.org/pdf/2601.22154v1"
      },
      "arxiv_id": "2601.22154v1",
      "comment": "Project page: https://github.com/kxfan2002/Reagent",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.22153v1",
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "authors": [
        "Haozhe Xie",
        "Beichen Wen",
        "Jiarui Zheng",
        "Zhaoxi Chen",
        "Fangzhou Hong",
        "Haiwen Diao",
        "Ziwei Liu"
      ],
      "abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22153v1",
        "pdf": "https://arxiv.org/pdf/2601.22153v1"
      },
      "arxiv_id": "2601.22153v1",
      "comment": "Project Page: https://www.infinitescript.com/project/dynamic-vla/ GitHub: https://github.com/hzxie/DynamicVLA",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.22151v1",
      "title": "Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing",
      "authors": [
        "Daniel Stein",
        "Shaoyi Huang",
        "Rolf Drechsler",
        "Bing Li",
        "Grace Li Zhang"
      ],
      "abstract": "Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. State-of-the-art research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any accuracy degradation.\n  The code is open source at https://github.com/TUDa-HWAI/NN2Logic",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22151v1",
        "pdf": "https://arxiv.org/pdf/2601.22151v1"
      },
      "arxiv_id": "2601.22151v1",
      "comment": "accepted by DATE2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22150v1",
      "title": "Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions",
      "authors": [
        "Xiaoxiao Sun",
        "Mingyang Li",
        "Kun yuan",
        "Min Woo Sun",
        "Mark Endo",
        "Shengguang Wu",
        "Changlin Li",
        "Yuhui Zhang",
        "Zeyu Wang",
        "Serena Yeung-Levy"
      ],
      "abstract": "Large Vision-Language Models (VLMs) often answer classic visual illusions \"correctly\" on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22150v1",
        "pdf": "https://arxiv.org/pdf/2601.22150v1"
      },
      "arxiv_id": "2601.22150v1",
      "comment": "26 pages, 31 figures, 13 tables. Project Page: https://sites.google.com/view/vi-probe/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22149v1",
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "authors": [
        "Hang Ding",
        "Peidong Liu",
        "Junqiao Wang",
        "Ziwei Ji",
        "Meng Cao",
        "Rongzhao Zhang",
        "Lynn Ai",
        "Eric Yang",
        "Tianyu Shi",
        "Lei Yu"
      ],
      "abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22149v1",
        "pdf": "https://arxiv.org/pdf/2601.22149v1"
      },
      "arxiv_id": "2601.22149v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22146v1",
      "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
      "authors": [
        "Ajay Patel",
        "Colin Raffel",
        "Chris Callison-Burch"
      ],
      "abstract": "Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised \"predict the next word\" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of \"instruction-tuning\" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With \"supervised\" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22146v1",
        "pdf": "https://arxiv.org/pdf/2601.22146v1"
      },
      "arxiv_id": "2601.22146v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22143v1",
      "title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
      "authors": [
        "Anthony Chen",
        "Naomi Ken Korem",
        "Tavi Halperin",
        "Matan Ben Yosef",
        "Urska Jelercic",
        "Ofir Bibi",
        "Or Patashnik",
        "Daniel Cohen-Or"
      ],
      "abstract": "Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22143v1",
        "pdf": "https://arxiv.org/pdf/2601.22143v1"
      },
      "arxiv_id": "2601.22143v1",
      "comment": "Project webpage available at https://justdubit.github.io",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.22141v1",
      "title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
      "authors": [
        "Grzegorz Stefanski",
        "Alberto Presta",
        "Michal Byra"
      ],
      "abstract": "In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22141v1",
        "pdf": "https://arxiv.org/pdf/2601.22141v1"
      },
      "arxiv_id": "2601.22141v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22139v1",
      "title": "Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers",
      "authors": [
        "Xin Chen",
        "Feng Jiang",
        "Yiqian Zhang",
        "Hardy Chen",
        "Shuo Yan",
        "Wenya Xie",
        "Min Yang",
        "Shujian Huang"
      ],
      "abstract": "Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \\emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\\% higher accuracy, 22.90\\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22139v1",
        "pdf": "https://arxiv.org/pdf/2601.22139v1"
      },
      "arxiv_id": "2601.22139v1",
      "comment": "The manuscript is under review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22137v1",
      "title": "PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training",
      "authors": [
        "Shenghao Yang",
        "Zhichao Wang",
        "Oleg Balabanov",
        "N. Benjamin Erichson",
        "Michael W. Mahoney"
      ],
      "abstract": "Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22137v1",
        "pdf": "https://arxiv.org/pdf/2601.22137v1"
      },
      "arxiv_id": "2601.22137v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22136v1",
      "title": "StepShield: When, Not Whether to Intervene on Rogue Agents",
      "authors": [
        "Gloria Felicia",
        "Michael Eniolade",
        "Jinfeng He",
        "Zitha Sasindran",
        "Hemant Kumar",
        "Milan Hussain Angati",
        "Sandeep Bandarupalli"
      ],
      "abstract": "Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22136v1",
        "pdf": "https://arxiv.org/pdf/2601.22136v1"
      },
      "arxiv_id": "2601.22136v1",
      "comment": "16 pages, 2 figures, 14 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22135v1",
      "title": "PI-Light: Physics-Inspired Diffusion for Full-Image Relighting",
      "authors": [
        "Zhexin Liang",
        "Zhaoxi Chen",
        "Yongwei Chen",
        "Tianyi Wei",
        "Tengfei Wang",
        "Xingang Pan"
      ],
      "abstract": "Full-image relighting remains a challenging problem due to the difficulty of collecting large-scale structured paired data, the difficulty of maintaining physical plausibility, and the limited generalizability imposed by data-driven priors. Existing attempts to bridge the synthetic-to-real gap for full-scene relighting remain suboptimal. To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight ($π$-Light, or PI-Light), a two-stage framework that leverages physics-inspired diffusion models. Our design incorporates (i) batch-aware attention, which improves the consistency of intrinsic predictions across a collection of images, (ii) a physics-guided neural rendering module that enforces physically plausible light transport, (iii) physics-inspired losses that regularize training dynamics toward a physically meaningful landscape, thereby enhancing generalizability to real-world image editing, and (iv) a carefully curated dataset of diverse objects and scenes captured under controlled lighting conditions. Together, these components enable efficient finetuning of pretrained diffusion models while also providing a solid benchmark for downstream evaluation. Experiments demonstrate that $π$-Light synthesizes specular highlights and diffuse reflections across a wide variety of materials, achieving superior generalization to real-world scenes compared with prior approaches.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22135v1",
        "pdf": "https://arxiv.org/pdf/2601.22135v1"
      },
      "arxiv_id": "2601.22135v1",
      "comment": "Accepted at ICLR 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22134v1",
      "title": "Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography",
      "authors": [
        "Wenxuan Li",
        "Pedro R. A. S. Bassi",
        "Lizhou Wu",
        "Xinze Zhou",
        "Yuxuan Zhao",
        "Qi Chen",
        "Szymon Plotka",
        "Tianyu Lin",
        "Zheren Zhu",
        "Marisa Martin",
        "Justin Caskey",
        "Shanshan Jiang",
        "Xiaoxi Chen",
        "Jaroslaw B. Ćwikla",
        "Artur Sankowski",
        "Yaping Wu",
        "Sergio Decherchi",
        "Andrea Cavalli",
        "Chandana Lall",
        "Cristian Tomasetti",
        "Yaxing Guo",
        "Xuan Yu",
        "Yuqing Cai",
        "Hualin Qiao",
        "Jie Bao",
        "Chenhan Hu",
        "Ximing Wang",
        "Arkadiusz Sitek",
        "Kai Ding",
        "Heng Li",
        "Meiyun Wang",
        "Dexin Yu",
        "Guang Zhang",
        "Yang Yang",
        "Kang Wang",
        "Alan L. Yuille",
        "Zongwei Zhou"
      ],
      "abstract": "Pancreatic ductal adenocarcinoma (PDAC), one of the deadliest solid malignancies, is often detected at a late and inoperable stage. Retrospective reviews of prediagnostic CT scans, when conducted by expert radiologists aware that the patient later developed PDAC, frequently reveal lesions that were previously overlooked. To help detecting these lesions earlier, we developed an automated system named ePAI (early Pancreatic cancer detection with Artificial Intelligence). It was trained on data from 1,598 patients from a single medical center. In the internal test involving 1,009 patients, ePAI achieved an area under the receiver operating characteristic curve (AUC) of 0.939-0.999, a sensitivity of 95.3%, and a specificity of 98.7% for detecting small PDAC less than 2 cm in diameter, precisely localizing PDAC as small as 2 mm. In an external test involving 7,158 patients across 6 centers, ePAI achieved an AUC of 0.918-0.945, a sensitivity of 91.5%, and a specificity of 88.0%, precisely localizing PDAC as small as 5 mm. Importantly, ePAI detected PDACs on prediagnostic CT scans obtained 3 to 36 months before clinical diagnosis that had originally been overlooked by radiologists. It successfully detected and localized PDACs in 75 of 159 patients, with a median lead time of 347 days before clinical diagnosis. Our multi-reader study showed that ePAI significantly outperformed 30 board-certified radiologists by 50.3% (P < 0.05) in sensitivity while maintaining a comparable specificity of 95.4% in detecting PDACs early and prediagnostic. These findings suggest its potential of ePAI as an assistive tool to improve early detection of pancreatic cancer.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22134v1",
        "pdf": "https://arxiv.org/pdf/2601.22134v1"
      },
      "arxiv_id": "2601.22134v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22132v1",
      "title": "Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference",
      "authors": [
        "Ziming Dong",
        "Hardik Sharma",
        "Evan O'Toole",
        "Jaya Prakash Champati",
        "Kui Wu"
      ],
      "abstract": "Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22132v1",
        "pdf": "https://arxiv.org/pdf/2601.22132v1"
      },
      "arxiv_id": "2601.22132v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22131v1",
      "title": "SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization",
      "authors": [
        "Leonard Papenmeier",
        "Petru Tighineanu"
      ],
      "abstract": "Multi-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has been extended to meta-learning and multi-objective optimization independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian optimization - remain largely unexplored. We propose SMOG, a scalable and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process prior across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task prior augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian optimization acquisition functions.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22131v1",
        "pdf": "https://arxiv.org/pdf/2601.22131v1"
      },
      "arxiv_id": "2601.22131v1",
      "comment": "19 pages, 15 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22130v1",
      "title": "World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems",
      "authors": [
        "Lakshya Gupta",
        "Litao Li",
        "Yizhe Liu",
        "Sriram Ganapathi Subramanian",
        "Kaheer Suleman",
        "Zichen Zhang",
        "Haoye Lu",
        "Sumit Pasupalak"
      ],
      "abstract": "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22130v1",
        "pdf": "https://arxiv.org/pdf/2601.22130v1"
      },
      "arxiv_id": "2601.22130v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22129v1",
      "title": "SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents",
      "authors": [
        "Yifeng Ding",
        "Lingming Zhang"
      ],
      "abstract": "Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22129v1",
        "pdf": "https://arxiv.org/pdf/2601.22129v1"
      },
      "arxiv_id": "2601.22129v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22128v1",
      "title": "The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR",
      "authors": [
        "Irsyad Adam",
        "Zekai Chen",
        "David Laprade",
        "Shaun Porwal",
        "David Laub",
        "Erik Reinertsen",
        "Arda Pekis",
        "Kevin Brown"
      ],
      "abstract": "Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient's trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-embedding prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in latent space from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.CE",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22128v1",
        "pdf": "https://arxiv.org/pdf/2601.22128v1"
      },
      "arxiv_id": "2601.22128v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22127v1",
      "title": "EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers",
      "authors": [
        "John Flynn",
        "Wolfgang Paier",
        "Dimitar Dinev",
        "Sam Nhut Nguyen",
        "Hayk Poghosyan",
        "Manuel Toribio",
        "Sandipan Banerjee",
        "Guy Gafni"
      ],
      "abstract": "Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22127v1",
        "pdf": "https://arxiv.org/pdf/2601.22127v1"
      },
      "arxiv_id": "2601.22127v1",
      "comment": "Project page: https://edit-yourself.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.22125v1",
      "title": "Creative Image Generation with Diffusion Model",
      "authors": [
        "Kunpeng Song",
        "Ahmed Elgammal"
      ],
      "abstract": "Creative image generation has emerged as a compelling area of research, driven by the need to produce novel and high-quality images that expand the boundaries of imagination. In this work, we propose a novel framework for creative generation using diffusion models, where creativity is associated with the inverse probability of an image's existence in the CLIP embedding space. Unlike prior approaches that rely on a manual blending of concepts or exclusion of subcategories, our method calculates the probability distribution of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs. We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation framework, showcasing its ability to produce unique, novel, and thought-provoking images. This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22125v1",
        "pdf": "https://arxiv.org/pdf/2601.22125v1"
      },
      "arxiv_id": "2601.22125v1",
      "comment": "Project page: https://creative-t2i.github.io",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.22123v1",
      "title": "Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics",
      "authors": [
        "Winfried Ripken",
        "Michael Plainer",
        "Gregor Lied",
        "Thorben Frank",
        "Oliver T. Unke",
        "Stefan Chmiela",
        "Frank Noé",
        "Klaus Robert Müller"
      ],
      "abstract": "Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Δt$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22123v1",
        "pdf": "https://arxiv.org/pdf/2601.22123v1"
      },
      "arxiv_id": "2601.22123v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22119v1",
      "title": "Alpha Discovery via Grammar-Guided Learning and Search",
      "authors": [
        "Han Yang",
        "Dong Hao",
        "Zhuohan Wang",
        "Qi Shi",
        "Xingtong Li"
      ],
      "abstract": "Automatically discovering formulaic alpha factors is a central problem in quantitative finance. Existing methods often ignore syntactic and semantic constraints, relying on exhaustive search over unstructured and unbounded spaces. We present AlphaCFG, a grammar-based framework for defining and discovering alpha factors that are syntactically valid, financially interpretable, and computationally efficient. AlphaCFG uses an alpha-oriented context-free grammar to define a tree-structured, size-controlled search space, and formulates alpha discovery as a tree-structured linguistic Markov decision process, which is then solved using a grammar-aware Monte Carlo Tree Search guided by syntax-sensitive value and policy networks. Experiments on Chinese and U.S. stock market datasets show that AlphaCFG outperforms state-of-the-art baselines in both search efficiency and trading profitability. Beyond trading strategies, AlphaCFG serves as a general framework for symbolic factor discovery and refinement across quantitative finance, including asset pricing and portfolio construction.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.CP",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22119v1",
        "pdf": "https://arxiv.org/pdf/2601.22119v1"
      },
      "arxiv_id": "2601.22119v1",
      "comment": "24 pages, 10 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22118v1",
      "title": "Defining Operational Conditions for Safety-Critical AI-Based Systems from Data",
      "authors": [
        "Johann Christensen",
        "Elena Hoemann",
        "Frank Köster",
        "Sven Hallerbach"
      ],
      "abstract": "Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22118v1",
        "pdf": "https://arxiv.org/pdf/2601.22118v1"
      },
      "arxiv_id": "2601.22118v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22114v1",
      "title": "SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence",
      "authors": [
        "Saoud Aldowaish",
        "Yashwanth Karumanchi",
        "Kai-Chen Chiang",
        "Soroosh Noorzad",
        "Morteza Fayazi"
      ],
      "abstract": "Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22114v1",
        "pdf": "https://arxiv.org/pdf/2601.22114v1"
      },
      "arxiv_id": "2601.22114v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22113v1",
      "title": "Diverse Approaches to Optimal Execution Schedule Generation",
      "authors": [
        "Robert de Witt",
        "Mikko S. Pakkanen"
      ],
      "abstract": "We present the first application of MAP-Elites, a quality-diversity algorithm, to trade execution. Rather than searching for a single optimal policy, MAP-Elites generates a diverse portfolio of regime-specialist strategies indexed by liquidity and volatility conditions. Individual specialists achieve 8-10% performance improvements within their behavioural niches, while other cells show degradation, suggesting opportunities for ensemble approaches that combine improved specialists with the baseline PPO policy. Results indicate that quality-diversity methods offer promise for regime-adaptive execution, though substantial computational resources per behavioural cell may be required for robust specialist development across all market conditions. To ensure experimental integrity, we develop a calibrated Gymnasium environment focused on order scheduling rather than tactical placement decisions. The simulator features a transient impact model with exponential decay and square-root volume scaling, fit to 400+ U.S. equities with R^2>0.02 out-of-sample. Within this environment, two Proximal Policy Optimization architectures - both MLP and CNN feature extractors - demonstrate substantial improvements over industry baselines, with the CNN variant achieving 2.13 bps arrival slippage versus 5.23 bps for VWAP on 4,900 out-of-sample orders ($21B notional). These results validate both the simulation realism and provide strong single-policy baselines for quality-diversity methods.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "q-fin.TR",
        "cs.LG"
      ],
      "primary_category": "q-fin.TR",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22113v1",
        "pdf": "https://arxiv.org/pdf/2601.22113v1"
      },
      "arxiv_id": "2601.22113v1",
      "comment": "27 pages, 15 figures, 5 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22111v1",
      "title": "Physics Informed Reconstruction of Four-Dimensional Atmospheric Wind Fields Using Multi-UAS Swarm Observations in a Synthetic Turbulent Environment",
      "authors": [
        "Abdullah Tasim",
        "Wei Sun"
      ],
      "abstract": "Accurate reconstruction of atmospheric wind fields is essential for applications such as weather forecasting, hazard prediction, and wind energy assessment, yet conventional instruments leave spatio-temporal gaps within the lower atmospheric boundary layer. Unmanned aircraft systems (UAS) provide flexible in situ measurements, but individual platforms sample wind only along their flight trajectories, limiting full wind-field recovery. This study presents a framework for reconstructing four-dimensional atmospheric wind fields using measurements obtained from a coordinated UAS swarm. A synthetic turbulence environment and high-fidelity multirotor simulation are used to generate training and evaluation data. Local wind components are estimated from UAS dynamics using a bidirectional long short-term memory network (Bi-LSTM) and assimilated into a physics-informed neural network (PINN) to reconstruct a continuous wind field in space and time. For local wind estimation, the bidirectional LSTM achieves root-mean-square errors (RMSE) of 0.064 and 0.062 m/s for the north and east components in low-wind conditions, increasing to 0.122 to 0.129 m/s under moderate winds and 0.271 to 0.273 m/s in high-wind conditions, while the vertical component exhibits higher error, with RMSE values of 0.029 to 0.091 m/s. The physics-informed reconstruction recovers the dominant spatial and temporal structure of the wind field up to 1000 m altitude while preserving mean flow direction and vertical shear. Under moderate wind conditions, the reconstructed mean wind field achieves an overall RMSE between 0.118 and 0.154 m/s across evaluated UAS configurations, with the lowest error obtained using a five-UAS swarm. These results demonstrate that coordinated UAS measurements enable accurate and scalable four-dimensional wind-field reconstruction without dedicated wind sensors or fixed infrastructure.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "eess.SY",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22111v1",
        "pdf": "https://arxiv.org/pdf/2601.22111v1"
      },
      "arxiv_id": "2601.22111v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22108v1",
      "title": "Value-Based Pre-Training with Downstream Feedback",
      "authors": [
        "Shuqi Ke",
        "Giulia Fanti"
      ],
      "abstract": "Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22108v1",
        "pdf": "https://arxiv.org/pdf/2601.22108v1"
      },
      "arxiv_id": "2601.22108v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22107v1",
      "title": "Prior-Informed Flow Matching for Graph Reconstruction",
      "authors": [
        "Harvey Chen",
        "Nicolas Zilberstein",
        "Santiago Segarra"
      ],
      "abstract": "We introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a prior, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and state-of-the-art generative baselines in reconstruction accuracy.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22107v1",
        "pdf": "https://arxiv.org/pdf/2601.22107v1"
      },
      "arxiv_id": "2601.22107v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22101v1",
      "title": "ECO: Quantized Training without Full-Precision Master Weights",
      "authors": [
        "Mahdi Nikdan",
        "Amir Zandieh",
        "Dan Alistarh",
        "Vahab Mirrokni"
      ],
      "abstract": "Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as $\\textit{master weights}$. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22101v1",
        "pdf": "https://arxiv.org/pdf/2601.22101v1"
      },
      "arxiv_id": "2601.22101v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22100v1",
      "title": "Boosting CVaR Policy Optimization with Quantile Gradients",
      "authors": [
        "Yudong Luo",
        "Erick Delage"
      ],
      "abstract": "Optimizing Conditional Value-at-risk (CVaR) using policy gradient (a.k.a CVaR-PG) faces significant challenges of sample inefficiency. This inefficiency stems from the fact that it focuses on tail-end performance and overlooks many sampled trajectories. We address this problem by augmenting CVaR with an expected quantile term. Quantile optimization admits a dynamic programming formulation that leverages all sampled data, thus improves sample efficiency. This does not alter the CVaR objective since CVaR corresponds to the expectation of quantile over the tail. Empirical results in domains with verifiable risk-averse behavior show that our algorithm within the Markovian policy class substantially improves upon CVaR-PG and consistently outperforms other existing methods.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22100v1",
        "pdf": "https://arxiv.org/pdf/2601.22100v1"
      },
      "arxiv_id": "2601.22100v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22095v1",
      "title": "GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization",
      "authors": [
        "Chuanyang Zheng",
        "Jiankai Sun",
        "Yihang Gao",
        "Chi Wang",
        "Yuehao Wang",
        "Jing Xiong",
        "Liliang Ren",
        "Bo Peng",
        "Qingmei Wang",
        "Xiaoran Shang",
        "Mac Schwager",
        "Anderson Schneider",
        "Yuriy Nevmyvaka",
        "Xiaodong Liu"
      ],
      "abstract": "The placement of normalization layers, specifically Pre-Norm and Post-Norm, remains an open question in Transformer architecture design. In this work, we rethink these approaches through the lens of manifold optimization, interpreting the outputs of the Feed-Forward Network (FFN) and attention layers as update directions in optimization. Building on this perspective, we introduce GeoNorm, a novel method that replaces standard normalization with geodesic updates on the manifold. Furthermore, analogous to learning rate schedules, we propose a layer-wise update decay for the FFN and attention components. Comprehensive experiments demonstrate that GeoNorm consistently outperforms existing normalization methods in Transformer models. Crucially, GeoNorm can be seamlessly integrated into standard Transformer architectures, achieving performance improvements with negligible additional computational cost.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22095v1",
        "pdf": "https://arxiv.org/pdf/2601.22095v1"
      },
      "arxiv_id": "2601.22095v1",
      "comment": "Tech Report",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22094v1",
      "title": "RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation",
      "authors": [
        "Hanzhuo Huang",
        "Qingyang Bao",
        "Zekai Gu",
        "Zhongshuo Du",
        "Cheng Lin",
        "Yuan Liu",
        "Sibei Yang"
      ],
      "abstract": "In this paper, we propose a 3D asset-referenced diffusion model for image generation, exploring how to integrate 3D assets into image diffusion models. Existing reference-based image generation methods leverage large-scale pretrained diffusion models and demonstrate strong capability in generating diverse images conditioned on a single reference image. However, these methods are limited to single-image references and cannot leverage 3D assets, constraining their practical versatility. To address this gap, we present a cross-domain diffusion model with dual-branch perception that leverages multi-view RGB images and point maps of 3D assets to jointly model their colors and canonical-space coordinates, achieving precise consistency between generated images and the 3D references. Our spatially aligned dual-branch generation architecture and domain-decoupled generation mechanism ensure the simultaneous generation of two spatially aligned but content-disentangled outputs, RGB images and point maps, linking 2D image attributes with 3D asset attributes. Experiments show that our approach effectively uses 3D assets as references to produce images consistent with the given assets, opening new possibilities for combining diffusion models with 3D content creation.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22094v1",
        "pdf": "https://arxiv.org/pdf/2601.22094v1"
      },
      "arxiv_id": "2601.22094v1",
      "comment": "ICLR 2026. Project page: https://judgementh.github.io/RefAny3D Codes: https://github.com/JudgementH/RefAny3D",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.22093v1",
      "title": "Investigating Associational Biases in Inter-Model Communication of Large Generative Models",
      "authors": [
        "Fethiye Irmak Dogan",
        "Yuval Weiss",
        "Kajal Patel",
        "Jiaee Cheong",
        "Hatice Gunes"
      ],
      "abstract": "Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model's output becomes another's input. This is especially salient for human-centred perception tasks, such as human activity recognition and affect prediction, where inferences about behaviour and internal states can lead to errors or stereotypical associations that propagate into unequal treatment. In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication pipeline that alternates between image generation and image description. Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability pipeline. Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions. We further find evidence that some predictions are supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). We also examine whether these demographic drifts translate into measurable differences in downstream behaviour, i.e., while predicting activity and emotion labels. Finally, we outline mitigation strategies spanning data-centric, training and deployment interventions, and emphasise the need for careful safeguards when deploying interconnected models in human-centred AI systems.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22093v1",
        "pdf": "https://arxiv.org/pdf/2601.22093v1"
      },
      "arxiv_id": "2601.22093v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22086v1",
      "title": "Learning Transient Convective Heat Transfer with Geometry Aware World Models",
      "authors": [
        "Onur T. Doganay",
        "Alexander Klawonn",
        "Martin Eigel",
        "Hanno Gottschalk"
      ],
      "abstract": "Partial differential equation (PDE) simulations are fundamental to engineering and physics but are often computationally prohibitive for real-time applications. While generative AI offers a promising avenue for surrogate modeling, standard video generation architectures lack the specific control and data compatibility required for physical simulations. This paper introduces a geometry aware world model architecture, derived from a video generation architecture (LongVideoGAN), designed to learn transient physics. We introduce two key architecture elements: (1) a twofold conditioning mechanism incorporating global physical parameters and local geometric masks, and (2) an architectural adaptation to support arbitrary channel dimensions, moving beyond standard RGB constraints. We evaluate this approach on a 2D transient computational fluid dynamics (CFD) problem involving convective heat transfer from buoyancy-driven flow coupled to a heat flow in a solid structure. We demonstrate that the conditioned model successfully reproduces complex temporal dynamics and spatial correlations of the training data. Furthermore, we assess the model's generalization capabilities on unseen geometric configurations, highlighting both its potential for controlled simulation synthesis and current limitations in spatial precision for out-of-distribution samples.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "physics.flu-dyn",
        "cs.CV"
      ],
      "primary_category": "physics.flu-dyn",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22086v1",
        "pdf": "https://arxiv.org/pdf/2601.22086v1"
      },
      "arxiv_id": "2601.22086v1",
      "comment": "36 pages, 18 figures, 2 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22083v1",
      "title": "Latent Adversarial Regularization for Offline Preference Optimization",
      "authors": [
        "Enyi Jiang",
        "Yibo Jacky Zhang",
        "Yinglun Xu",
        "Andreas Haupt",
        "Nancy Amato",
        "Sanmi Koyejo"
      ],
      "abstract": "Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22083v1",
        "pdf": "https://arxiv.org/pdf/2601.22083v1"
      },
      "arxiv_id": "2601.22083v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22076v1",
      "title": "Where Do the Joules Go? Diagnosing Inference Energy Consumption",
      "authors": [
        "Jae-Won Chung",
        "Ruofan Wu",
        "Jeff J. Ma",
        "Mosharaf Chowdhury"
      ],
      "abstract": "Energy is now a critical ML computing resource. While measuring energy consumption and observing trends is a valuable first step, accurately understanding and diagnosing why those differences occur is crucial for optimization. To that end, we begin by presenting a large-scale measurement study of inference time and energy across the generative AI landscape with 46 models, 7 tasks, and 1,858 different configurations on NVIDIA H100 and B200 GPUs. Our empirical findings span order-of-magnitude variations: LLM task type can lead to 25$\\times$ energy differences, video generation sometimes consumes more than 100$\\times$ the energy of images, and GPU utilization differences can result in 3--5$\\times$ energy differences. Based on our observations, we present a framework for reasoning about the underlying mechanisms that govern time and energy consumption. The essence is that time and energy are determined by latent metrics like memory and utilization, which are in turn affected by various factors across the algorithm, software, and hardware layers. Our framework also extends directly to throughput per watt, a critical metric for power-constrained datacenters.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22076v1",
        "pdf": "https://arxiv.org/pdf/2601.22076v1"
      },
      "arxiv_id": "2601.22076v1",
      "comment": "The ML.ENERGY Leaderboard v3.0 is open https://ml.energy/leaderboard",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22070v1",
      "title": "Wrapper-Aware Rate-Distortion Optimization in Feature Coding for Machines",
      "authors": [
        "Samuel Fernández-Menduiña",
        "Hyomin Choi",
        "Fabien Racapé",
        "Eduardo Pavez",
        "Antonio Ortega"
      ],
      "abstract": "Feature coding for machines (FCM) is a lossy compression paradigm for split-inference. The transmitter encodes the outputs of the first part of a neural network before sending them to the receiver for completing the inference. Practical FCM methods ``sandwich'' a traditional codec between pre- and post-processing neural networks, called wrappers, to make features easier to compress using video codecs. Since traditional codecs are non-differentiable, the wrappers are trained using a proxy codec, which is later replaced by a standard codec after training. These codecs perform rate-distortion optimization (RDO) based on the sum of squared errors (SSE). Because the RDO does not consider the post-processing wrapper, the inner codec can invest bits in preserving information that the post-processing later discards. In this paper, we modify the bit-allocation in the inner codec via a wrapper-aware weighted SSE metric. To make wrapper-aware RDO (WA-RDO) practical for FCM, we propose: 1) temporal reuse of weights across a group of pictures and 2) fixed, architecture- and task-dependent weights trained offline. Under MPEG test conditions, our methods implemented on HEVC match the VVC-based FCM state-of-the-art, effectively bridging a codec generation gap with minimal runtime overhead relative to SSE-RDO HEVC.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "eess.IV",
        "eess.SP"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22070v1",
        "pdf": "https://arxiv.org/pdf/2601.22070v1"
      },
      "arxiv_id": "2601.22070v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22068v1",
      "title": "Making Foundation Models Probabilistic via Singular Value Ensembles",
      "authors": [
        "Mehmet Ozgur Turkoglu",
        "Dominik J. Mühlematter",
        "Alexander Becker",
        "Konrad Schindler",
        "Helge Aasen"
      ],
      "abstract": "Foundation models have become a dominant paradigm in machine learning, achieving remarkable performance across diverse tasks through large-scale pretraining. However, these models often yield overconfident, uncalibrated predictions. The standard approach to quantifying epistemic uncertainty, training an ensemble of independent models, incurs prohibitive computational costs that scale linearly with ensemble size, making it impractical for large foundation models. We propose Singular Value Ensemble (SVE), a parameter-efficient implicit ensemble method that builds on a simple, but powerful core assumption: namely, that the singular vectors of the weight matrices constitute meaningful subspaces of the model's knowledge. Pretrained foundation models encode rich, transferable information in their weight matrices. If the singular vectors are indeed meaningful (orthogonal) \"knowledge directions\". To obtain a model ensemble, we modulate only how strongly each direction contributes to the output. Rather than learning entirely new parameters, we freeze the singular vectors and only train per-member singular values that rescale the contribution of each direction in that shared knowledge basis. Ensemble diversity emerges naturally as stochastic initialization and random sampling of mini-batches during joint training cause different members to converge to different combinations of the same underlying knowledge. SVE achieves uncertainty quantification comparable to explicit deep ensembles while increasing the parameter count of the base model by less than 1%, making principled uncertainty estimation accessible in resource-constrained settings. We validate SVE on NLP and vision tasks with various different backbones and show that it improves calibration while maintaining predictive accuracy.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22068v1",
        "pdf": "https://arxiv.org/pdf/2601.22068v1"
      },
      "arxiv_id": "2601.22068v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22061v1",
      "title": "BLO-Inst: Bi-Level Optimization Based Alignment of YOLO and SAM for Robust Instance Segmentation",
      "authors": [
        "Li Zhang",
        "Pengtao Xie"
      ],
      "abstract": "The Segment Anything Model has revolutionized image segmentation with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do not correspond to the optimal prompting context required by SAM, and alignment overfitting in standard joint training, where the detector simply memorizes specific prompt adjustments for training samples rather than learning a generalizable policy. To bridge this gap, we introduce BLO-Inst, a unified framework that aligns detection and segmentation objectives by bi-level optimization. We formulate the alignment as a nested optimization problem over disjoint data splits. In the lower level, the SAM is fine-tuned to maximize segmentation fidelity given the current detection proposals on a subset ($D_1$). In the upper level, the detector is updated to generate bounding boxes that explicitly minimize the validation loss of the fine-tuned SAM on a separate subset ($D_2$). This effectively transforms the detector into a segmentation-aware prompt generator, optimizing the bounding boxes not just for localization accuracy, but for downstream mask quality. Extensive experiments demonstrate that BLO-Inst achieves superior performance, outperforming standard baselines on tasks in general and biomedical domains.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22061v1",
        "pdf": "https://arxiv.org/pdf/2601.22061v1"
      },
      "arxiv_id": "2601.22061v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22060v1",
      "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models",
      "authors": [
        "Wenxuan Huang",
        "Yu Zeng",
        "Qiuchen Wang",
        "Zhen Fang",
        "Shaosheng Cao",
        "Zheng Chu",
        "Qingyu Yin",
        "Shuang Chen",
        "Zhenfei Yin",
        "Lin Chen",
        "Zehui Chen",
        "Yao Hu",
        "Philip Torr",
        "Feng Zhao",
        "Wanli Ouyang"
      ],
      "abstract": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22060v1",
        "pdf": "https://arxiv.org/pdf/2601.22060v1"
      },
      "arxiv_id": "2601.22060v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22057v1",
      "title": "Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models",
      "authors": [
        "Archer Wang",
        "Emile Anand",
        "Yilun Du",
        "Marin Soljačić"
      ],
      "abstract": "Decomposing complex data into factorized representations can reveal reusable components and enable synthesizing new samples via component recombination. We investigate this in the context of diffusion-based models that learn factorized latent spaces without factor-level supervision. In images, factors can capture background, illumination, and object attributes; in robotic videos, they can capture reusable motion components. To improve both latent factor discovery and quality of compositional generation, we introduce an adversarial training signal via a discriminator trained to distinguish between single-source samples and those generated by recombining factors across sources. By optimizing the generator to fool this discriminator, we encourage physical and semantic consistency in the resulting recombinations. Our method outperforms implementations of prior baselines on CelebA-HQ, Virtual KITTI, CLEVR, and Falcor3D, achieving lower FID scores and better disentanglement as measured by MIG and MCC. Furthermore, we demonstrate a novel application to robotic video trajectories: by recombining learned action components, we generate diverse sequences that significantly increase state-space coverage for exploration on the LIBERO benchmark.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22057v1",
        "pdf": "https://arxiv.org/pdf/2601.22057v1"
      },
      "arxiv_id": "2601.22057v1",
      "comment": "28 pages, 16 figures, 4 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22054v1",
      "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
      "authors": [
        "Baorui Ma",
        "Jiahui Yang",
        "Donglin Di",
        "Xuancheng Zhang",
        "Jianxun Cui",
        "Hao Li",
        "Yan Xie",
        "Wei Chen"
      ],
      "abstract": "Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22054v1",
        "pdf": "https://arxiv.org/pdf/2601.22054v1"
      },
      "arxiv_id": "2601.22054v1",
      "comment": "Project Page: https://metric-anything.github.io/metric-anything-io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.22046v1",
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "authors": [
        "Changjian Jiang",
        "Kerui Ren",
        "Xudong Li",
        "Kaiwen Song",
        "Linning Xu",
        "Tao Lu",
        "Junting Dong",
        "Yu Zhang",
        "Bo Dai",
        "Mulin Yu"
      ],
      "abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22046v1",
        "pdf": "https://arxiv.org/pdf/2601.22046v1"
      },
      "arxiv_id": "2601.22046v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22045v1",
      "title": "Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion",
      "authors": [
        "Da Li",
        "Chen Yao",
        "Tong Mao",
        "Jiacheng Bao",
        "Houjun Sun"
      ],
      "abstract": "Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22045v1",
        "pdf": "https://arxiv.org/pdf/2601.22045v1"
      },
      "arxiv_id": "2601.22045v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22044v1",
      "title": "SIA: Symbolic Interpretability for Anticipatory Deep Reinforcement Learning in Network Control",
      "authors": [
        "MohammadErfan Jabbari",
        "Abhishek Duttagupta",
        "Claudio Fiandrino",
        "Leonardo Bonati",
        "Salvatore D'Oro",
        "Michele Polese",
        "Marco Fiore",
        "Tommaso Melodia"
      ],
      "abstract": "Deep reinforcement learning (DRL) promises adaptive control for future mobile networks but conventional agents remain reactive: they act on past and current measurements and cannot leverage short-term forecasts of exogenous KPIs such as bandwidth. Augmenting agents with predictions can overcome this temporal myopia, yet uptake in networking is scarce because forecast-aware agents act as closed-boxes; operators cannot tell whether predictions guide decisions or justify the added complexity. We propose SIA, the first interpreter that exposes in real time how forecast-augmented DRL agents operate. SIA fuses Symbolic AI abstractions with per-KPI Knowledge Graphs to produce explanations, and includes a new Influence Score metric. SIA achieves sub-millisecond speed, over 200x faster than existing XAI methods. We evaluate SIA on three diverse networking use cases, uncovering hidden issues, including temporal misalignment in forecast integration and reward-design biases that trigger counter-productive policies. These insights enable targeted fixes: a redesigned agent achieves a 9% higher average bitrate in video streaming, and SIA's online Action-Refinement module improves RAN-slicing reward by 25% without retraining. By making anticipatory DRL transparent and tunable, SIA lowers the barrier to proactive control in next-generation mobile networks.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22044v1",
        "pdf": "https://arxiv.org/pdf/2601.22044v1"
      },
      "arxiv_id": "2601.22044v1",
      "comment": "10 pages, 12 figures, accepted at IEEE INFOCOM 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22041v1",
      "title": "Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems",
      "authors": [
        "Naomi Pitzer",
        "Daniela Mihai"
      ],
      "abstract": "Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignment, multimodal systems converge to class-consistent messages grounded in perceptual input. Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification entropy, while multimodal agents require greater information exchange and exhibit higher uncertainty. Bit perturbation experiments provide strong evidence that meaning is encoded in a distributional rather than compositional manner, as each bit's contribution depends on its surrounding pattern. Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited fine-tuning enables successful cross-system communication. This work positions emergent communication as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22041v1",
        "pdf": "https://arxiv.org/pdf/2601.22041v1"
      },
      "arxiv_id": "2601.22041v1",
      "comment": "To be published in EvoLang XVI proceedings. 15 pages, 17 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22040v1",
      "title": "A Separable Architecture for Continuous Token Representation in Language Models",
      "authors": [
        "Reza T. Batley",
        "Sourav Saha"
      ],
      "abstract": "Transformer scaling law analyses typically treat parameters as interchangeable; an abstraction that accurately predicts loss-compute relationships. Yet, in sub-billion-parameter small language models (SLMs), embedding matrices dominate the parameter budget. This work argues that this allocation is as suboptimal as it is counterintuitive. Leviathan is an architecture with a continuous embedding generator to replace the discrete lookup tables of canonical models. Evaluating on the Pile dataset under isoparametric settings, Leviathan consistently outperforms a standard, LLaMA-style architecture. By means of an empirical power-law fit, Leviathan exhibits a markedly superior effective parameter capacity. Across the regime studied, Leviathan behaves as a dense model with $1.47$ to $2.11 \\times$ more parameters.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22040v1",
        "pdf": "https://arxiv.org/pdf/2601.22040v1"
      },
      "arxiv_id": "2601.22040v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22039v1",
      "title": "Understanding Multimodal Complementarity for Single-Frame Action Anticipation",
      "authors": [
        "Manuel Benavent-Lledo",
        "Konstantinos Bacharidis",
        "Konstantinos Papoutsakis",
        "Antonis Argyros",
        "Jose Garcia-Rodriguez"
      ],
      "abstract": "Human action anticipation is commonly treated as a video understanding problem, implicitly assuming that dense temporal information is required to reason about future actions. In this work, we challenge this assumption by investigating what can be achieved when action anticipation is constrained to a single visual observation. We ask a fundamental question: how much information about the future is already encoded in a single frame, and how can it be effectively exploited? Building on our prior work on Action Anticipation at a Glimpse (AAG), we conduct a systematic investigation of single-frame action anticipation enriched with complementary sources of information. We analyze the contribution of RGB appearance, depth-based geometric cues, and semantic representations of past actions, and investigate how different multimodal fusion strategies, keyframe selection policies and past-action history sources influence anticipation performance. Guided by these findings, we consolidate the most effective design choices into AAG+, a refined single-frame anticipation framework. Despite operating on a single frame, AAG+ consistently improves upon the original AAG and achieves performance comparable to, or exceeding, that of state-of-the-art video-based methods on challenging anticipation benchmarks including IKEA-ASM, Meccano and Assembly101. Our results offer new insights into the limits and potential of single-frame action anticipation, and clarify when dense temporal modeling is necessary and when a carefully selected glimpse is sufficient.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22039v1",
        "pdf": "https://arxiv.org/pdf/2601.22039v1"
      },
      "arxiv_id": "2601.22039v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22037v1",
      "title": "Optimizing Agentic Workflows using Meta-tools",
      "authors": [
        "Sami Abuzakuk",
        "Anne-Marie Kermarrec",
        "Rishi Sharma",
        "Rasmus Moorits Veski",
        "Martijn de Vos"
      ],
      "abstract": "Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22037v1",
        "pdf": "https://arxiv.org/pdf/2601.22037v1"
      },
      "arxiv_id": "2601.22037v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22036v1",
      "title": "Cross-Fusion Distance: A Novel Metric for Measuring Fusion and Separability Between Data Groups in Representation Space",
      "authors": [
        "Xiaolong Zhang",
        "Jianwei Zhang",
        "Xubo Song"
      ],
      "abstract": "Quantifying degrees of fusion and separability between data groups in representation space is a fundamental problem in representation learning, particularly under domain shift. A meaningful metric should capture fusion-altering factors like geometric displacement between representation groups, whose variations change the extent of fusion, while remaining invariant to fusion-preserving factors such as global scaling and sampling-induced layout changes, whose variations do not. Existing distributional distance metrics conflate these factors, leading to measures that are not informative of the true extent of fusion between data groups. We introduce Cross-Fusion Distance (CFD), a principled measure that isolates fusion-altering geometry while remaining robust to fusion-preserving variations, with linear computational complexity. We characterize the invariance and sensitivity properties of CFD theoretically and validate them in controlled synthetic experiments. For practical utility on real-world datasets with domain shift, CFD aligns more closely with downstream generalization degradation than commonly used alternatives. Overall, CFD provides a theoretically grounded and interpretable distance measure for representation learning.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22036v1",
        "pdf": "https://arxiv.org/pdf/2601.22036v1"
      },
      "arxiv_id": "2601.22036v1",
      "comment": "19 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22035v1",
      "title": "Thinking Out of Order: When Output Order Stops Reflecting Reasoning Order in Diffusion Language Models",
      "authors": [
        "Longxuan Yu",
        "Yu Fu",
        "Shaorong Zhang",
        "Hui Liu",
        "Mukund Varma T",
        "Greg Ver Steeg",
        "Yue Dong"
      ],
      "abstract": "Autoregressive (AR) language models enforce a fixed left-to-right generation order, creating a fundamental limitation when the required output structure conflicts with natural reasoning (e.g., producing answers before explanations due to presentation or schema constraints). In such cases, AR models must commit to answers before generating intermediate reasoning, and this rigid constraint forces premature commitment. Masked diffusion language models (MDLMs), which iteratively refine all tokens in parallel, offer a way to decouple computation order from output structure. We validate this capability on GSM8K, Math500, and ReasonOrderQA, a benchmark we introduce with controlled difficulty and order-level evaluation. When prompts request answers before reasoning, AR models exhibit large accuracy gaps compared to standard chain-of-thought ordering (up to 67% relative drop), while MDLMs remain stable ($\\leq$14% relative drop), a property we term \"order robustness\". Using ReasonOrderQA, we present evidence that MDLMs achieve order robustness by stabilizing simpler tokens (e.g., reasoning steps) earlier in the diffusion process than complex ones (e.g., final answers), enabling reasoning tokens to stabilize before answer commitment. Finally, we identify failure conditions where this advantage weakens, outlining the limits required for order robustness.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22035v1",
        "pdf": "https://arxiv.org/pdf/2601.22035v1"
      },
      "arxiv_id": "2601.22035v1",
      "comment": "18 pages, 13 figures, 5 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22033v1",
      "title": "Holographic generative flows with AdS/CFT",
      "authors": [
        "Ehsan Mirafzali",
        "Sanjit Shashi",
        "Sanya Murdeshwar",
        "Edgar Shaghoulian",
        "Daniele Venturi",
        "Razvan Marinescu"
      ],
      "abstract": "We present a framework for generative machine learning that leverages the holographic principle of quantum gravity, or to be more precise its manifestation as the anti-de Sitter/conformal field theory (AdS/CFT) correspondence, with techniques for deep learning and transport theory. Our proposal is to represent the flow of data from a base distribution to some learned distribution using the bulk-to-boundary mapping of scalar fields in AdS. In the language of machine learning, we are representing and augmenting the flow-matching algorithm with AdS physics. Using a checkerboard toy dataset and MNIST, we find that our model achieves faster and higher quality convergence than comparable physics-free flow-matching models. Our method provides a physically interpretable version of flow matching. More broadly, it establishes the utility of AdS physics and geometry in the development of novel paradigms in generative modeling.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "gr-qc",
        "hep-th"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22033v1",
        "pdf": "https://arxiv.org/pdf/2601.22033v1"
      },
      "arxiv_id": "2601.22033v1",
      "comment": "v1: 13 pages, 6 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22032v1",
      "title": "Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving",
      "authors": [
        "Linhan Wang",
        "Zichong Yang",
        "Chen Bai",
        "Guoxiang Zhang",
        "Xiaotong Liu",
        "Xiaoyin Zheng",
        "Xiao-Xiao Long",
        "Chang-Tien Lu",
        "Cheng Lu"
      ],
      "abstract": "End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only a single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, a framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining a ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce a proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with a momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with a simple transformer-based decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting a new state-of-the-art.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22032v1",
        "pdf": "https://arxiv.org/pdf/2601.22032v1"
      },
      "arxiv_id": "2601.22032v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22030v1",
      "title": "Per-parameter Task Arithmetic for Unlearning in Large Language Models",
      "authors": [
        "Chengyi Cai",
        "Zesheng Ye",
        "Jiangchao Yao",
        "Jianzhong Qi",
        "Bo Han",
        "Xiaolu Zhang",
        "Feng Liu",
        "Jun Zhou"
      ],
      "abstract": "In large language model (LLM) unlearning, private information is required to be removed. Task arithmetic unlearns by subtracting a specific task vector (TV)--defined as the parameter difference between a privacy-information-tuned model and the original model. While efficient, it can cause over-forgetting by disrupting parameters essential for retaining other information. Motivated by the observation that each parameter exhibits different importance for forgetting versus retention, we propose a per-parameter task arithmetic (PerTA) mechanism to rescale the TV, allowing per-parameter adjustment. These weights quantify the relative importance of each parameter for forgetting versus retention, estimated via gradients (i.e., PerTA-grad) or the diagonal Fisher information approximation (i.e., PerTA-fisher). Moreover, we discuss the effectiveness of PerTA, extend it to a more general form, and provide further analysis. Extensive experiments demonstrate that PerTA consistently improves upon standard TV, and in many cases surpasses widely used training-based unlearning methods in both forgetting effectiveness and overall model utility. By retaining the efficiency of task arithmetic while mitigating over-forgetting, PerTA offers a principled and practical framework for LLM unlearning.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22030v1",
        "pdf": "https://arxiv.org/pdf/2601.22030v1"
      },
      "arxiv_id": "2601.22030v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22029v1",
      "title": "The Ensemble Inverse Problem: Applications and Methods",
      "authors": [
        "Zhengyan Huan",
        "Camila Pazos",
        "Martin Klassen",
        "Vincent Croft",
        "Pierre-Hugues Beauchemin",
        "Shuchin Aeron"
      ],
      "abstract": "We introduce a new multivariate statistical problem that we refer to as the Ensemble Inverse Problem (EIP). The aim of EIP is to invert for an ensemble that is distributed according to the pushforward of a prior under a forward process. In high energy physics (HEP), this is related to a widely known problem called unfolding, which aims to reconstruct the true physics distribution of quantities, such as momentum and angle, from measurements that are distorted by detector effects. In recent applications, the EIP also arises in full waveform inversion (FWI) and inverse imaging with unknown priors. We propose non-iterative inference-time methods that construct posterior samplers based on a new class of conditional generative models, which we call ensemble inverse generative models. For the posterior modeling, these models additionally use the ensemble information contained in the observation set on top of single measurements. Unlike existing methods, our proposed methods avoid explicit and iterative use of the forward model at inference time via training across several sets of truth-observation pairs that are consistent with the same forward model, but originate from a wide range of priors. We demonstrate that this training procedure implicitly encodes the likelihood model. The use of ensemble information helps posterior inference and enables generalization to unseen priors. We benchmark the proposed method on several synthetic and real datasets in inverse imaging, HEP, and FWI. The codes are available at https://github.com/ZhengyanHuan/The-Ensemble-Inverse-Problem--Applications-and-Methods.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22029v1",
        "pdf": "https://arxiv.org/pdf/2601.22029v1"
      },
      "arxiv_id": "2601.22029v1",
      "comment": "26 pages, 11 figures, in peer review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22028v1",
      "title": "From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning",
      "authors": [
        "Haoran Tang",
        "Rajiv Khanna"
      ],
      "abstract": "Most LLM unlearning methods aim to approximate retrain-from-scratch behaviors with minimal distribution shift, often via alignment-style objectives defined in the prediction space. While effective at reducing forgotten content generation, such approaches may act as suppression: forgotten concepts can persist in representations and remain entangled with retained knowledge. We introduce CLReg, a contrastive representation regularizer that identifies forget features while pushing them away from retain features, explicitly reducing forget-retain interference with minimal shifts on retain features. We provide first theoretical insights that relate representation shaping to entanglement reduction. Across unlearning benchmarks and LLMs of different sizes, CLReg decreases forget-retain representation entanglement that facilitates mainstream unlearning methods without positing extra privacy risks, inspiring future work that reshapes the representation space to remove forget concepts.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22028v1",
        "pdf": "https://arxiv.org/pdf/2601.22028v1"
      },
      "arxiv_id": "2601.22028v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22027v1",
      "title": "CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty",
      "authors": [
        "Johannes Kirmayr",
        "Lukas Stappen",
        "Elisabeth André"
      ],
      "abstract": "Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22027v1",
        "pdf": "https://arxiv.org/pdf/2601.22027v1"
      },
      "arxiv_id": "2601.22027v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22026v1",
      "title": "Hybrid Foveated Path Tracing with Peripheral Gaussians for Immersive Anatomy",
      "authors": [
        "Constantin Kleinbeck",
        "Luisa Theelke",
        "Hannah Schieber",
        "Ulrich Eck",
        "Rüdiger von Eisenhart-Rothe",
        "Daniel Roth"
      ],
      "abstract": "Volumetric medical imaging offers great potential for understanding complex pathologies. Yet, traditional 2D slices provide little support for interpreting spatial relationships, forcing users to mentally reconstruct anatomy into three dimensions. Direct volumetric path tracing and VR rendering can improve perception but are computationally expensive, while precomputed representations, like Gaussian Splatting, require planning ahead. Both approaches limit interactive use.\n  We propose a hybrid rendering approach for high-quality, interactive, and immersive anatomical visualization. Our method combines streamed foveated path tracing with a lightweight Gaussian Splatting approximation of the periphery. The peripheral model generation is optimized with volume data and continuously refined using foveal renderings, enabling interactive updates. Depth-guided reprojection further improves robustness to latency and allows users to balance fidelity with refresh rate.\n  We compare our method against direct path tracing and Gaussian Splatting. Our results highlight how their combination can preserve strengths in visual quality while re-generating the peripheral model in under a second, eliminating extensive preprocessing and approximations. This opens new options for interactive medical visualization.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22026v1",
        "pdf": "https://arxiv.org/pdf/2601.22026v1"
      },
      "arxiv_id": "2601.22026v1",
      "comment": "Scheduled for publication in the Proceedings of IEEE VR 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22025v1",
      "title": "When \"Better\" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications",
      "authors": [
        "Daniel Commey"
      ],
      "abstract": "Evaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop.\n  We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes.\n  In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic \"improved\" prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes.\n  All test suites, harnesses, and results are included for reproducibility.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22025v1",
        "pdf": "https://arxiv.org/pdf/2601.22025v1"
      },
      "arxiv_id": "2601.22025v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22024v1",
      "title": "SymbXRL: Symbolic Explainable Deep Reinforcement Learning for Mobile Networks",
      "authors": [
        "Abhishek Duttagupta",
        "MohammadErfan Jabbari",
        "Claudio Fiandrino",
        "Marco Fiore",
        "Joerg Widmer"
      ],
      "abstract": "The operation of future 6th-generation (6G) mobile networks will increasingly rely on the ability of deep reinforcement learning (DRL) to optimize network decisions in real-time. DRL yields demonstrated efficacy in various resource allocation problems, such as joint decisions on user scheduling and antenna allocation or simultaneous control of computing resources and modulation. However, trained DRL agents are closed-boxes and inherently difficult to explain, which hinders their adoption in production settings. In this paper, we make a step towards removing this critical barrier by presenting SymbXRL, a novel technique for explainable reinforcement learning (XRL) that synthesizes human-interpretable explanations for DRL agents. SymbXRL leverages symbolic AI to produce explanations where key concepts and their relationships are described via intuitive symbols and rules; coupling such a representation with logical reasoning exposes the decision process of DRL agents and offers more comprehensible descriptions of their behaviors compared to existing approaches. We validate SymbXRL in practical network management use cases supported by DRL, proving that it not only improves the semantics of the explanations but also paves the way for explicit agent control: for instance, it enables intent-based programmatic action steering that improves by 12% the median cumulative reward over a pure DRL solution.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22024v1",
        "pdf": "https://arxiv.org/pdf/2601.22024v1"
      },
      "arxiv_id": "2601.22024v1",
      "comment": "10 pages, 9 figures, published in IEEE INFOCOM 2025",
      "journal_ref": "IEEE INFOCOM 2025 - IEEE Conference on Computer Communications, London, United Kingdom, 19-22 May 2025, pp. 1-10",
      "has_code": false
    },
    {
      "id": "2601.22020v1",
      "title": "Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning",
      "authors": [
        "Chengyi Cai",
        "Zesheng Ye",
        "Peike Li",
        "Bo Han",
        "Jianzhong Qi",
        "Feng Liu"
      ],
      "abstract": "Unlearning in Multimodal Large Language Models (MLLMs) prevents the model from revealing private information when queried about target images. Existing MLLM unlearning methods largely adopt approaches developed for LLMs. They treat all answer tokens uniformly, disregarding their varying importance in the unlearning process. Moreover, these methods focus exclusively on the language modality, disregarding visual cues that indicate key tokens in answers. In this paper, after formulating the problem of unlearning in multimodal question answering for MLLMs, we propose Visual-Guided Key-Token Regularization (ViKeR). We leverage irrelevant visual inputs to predict ideal post-unlearning token-level distributions and use these distributions to regularize the unlearning process, thereby prioritizing key tokens. Further, we define key tokens in unlearning via information entropy and discuss ViKeR's effectiveness through token-level gradient reweighting, which amplifies updates on key tokens. Experiments on MLLMU and CLEAR benchmarks demonstrate that our method effectively performs unlearning while mitigating forgetting and maintaining response coherence.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22020v1",
        "pdf": "https://arxiv.org/pdf/2601.22020v1"
      },
      "arxiv_id": "2601.22020v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22016v1",
      "title": "TBDFiltering: Sample-Efficient Tree-Based Data Filtering",
      "authors": [
        "Robert Istvan Busa-Fekete",
        "Julian Zimmert",
        "Anne Xiangyi Zheng",
        "Claudio Gentile",
        "Andras Gyorgy"
      ],
      "abstract": "The quality of machine learning models depends heavily on their training data. Selecting high-quality, diverse training sets for large language models (LLMs) is a difficult task, due to the lack of cheap and reliable quality metrics. While querying existing LLMs for document quality is common, this is not scalable to the large number (billions) of documents used in training. Instead, practitioners often use classifiers trained on sparse quality signals. In this paper, we propose a text-embedding-based hierarchical clustering approach that adaptively selects the documents to be evaluated by the LLM to estimate cluster quality. We prove that our method is query efficient: under the assumption that the hierarchical clustering contains a subtree such that each leaf cluster in the tree is pure enough (i.e., it mostly contains either only good or only bad documents), with high probability, the method can correctly predict the quality of each document after querying a small number of documents. The number of such documents is proportional to the size of the smallest subtree with (almost) pure leaves, without the algorithm knowing this subtree in advance. Furthermore, in a comprehensive experimental study, we demonstrate the benefits of our algorithm compared to other classifier-based filtering methods.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22016v1",
        "pdf": "https://arxiv.org/pdf/2601.22016v1"
      },
      "arxiv_id": "2601.22016v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22013v1",
      "title": "Vidmento: Creating Video Stories Through Context-Aware Expansion With Generative Video",
      "authors": [
        "Catherine Yeh",
        "Anh Truong",
        "Mira Dontcheva",
        "Bryan Wang"
      ],
      "abstract": "Video storytelling is often constrained by available material, limiting creative expression and leaving undesired narrative gaps. Generative video offers a new way to address these limitations by augmenting captured media with tailored visuals. To explore this potential, we interviewed eight video creators to identify opportunities and challenges in integrating generative video into their workflows. Building on these insights and established filmmaking principles, we developed Vidmento, a tool for authoring hybrid video stories that combine captured and generated media through context-aware expansion. Vidmento surfaces opportunities for story development, generates clips that blend stylistically and narratively with surrounding media, and provides controls for refinement. In a study with 12 creators, Vidmento supported narrative development and exploration by systematically expanding initial materials with generative media, enabling expressive video storytelling aligned with creative intent. We highlight how creators bridge story gaps with generative content and where they find this blending capability most valuable.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22013v1",
        "pdf": "https://arxiv.org/pdf/2601.22013v1"
      },
      "arxiv_id": "2601.22013v1",
      "comment": "25 pages, 18 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22012v1",
      "title": "Putting a Face to Forgetting: Continual Learning meets Mechanistic Interpretability",
      "authors": [
        "Sergi Masip",
        "Gido M. van de Ven",
        "Javier Ferrando",
        "Tinne Tuytelaars"
      ],
      "abstract": "Catastrophic forgetting in continual learning is often measured at the performance or last-layer representation level, overlooking the underlying mechanisms. We introduce a mechanistic framework that offers a geometric interpretation of catastrophic forgetting as the result of transformations to the encoding of individual features. These transformations can lead to forgetting by reducing the allocated capacity of features (worse representation) and disrupting their readout by downstream computations. Analysis of a tractable model formalizes this view, allowing us to identify best- and worst-case scenarios. Through experiments on this model, we empirically test our formal analysis and highlight the detrimental effect of depth. Finally, we demonstrate how our framework can be used in the analysis of practical models through the use of Crosscoders. We present a case study of a Vision Transformer trained on sequential CIFAR-10. Our work provides a new, feature-centric vocabulary for continual learning.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22012v1",
        "pdf": "https://arxiv.org/pdf/2601.22012v1"
      },
      "arxiv_id": "2601.22012v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22010v1",
      "title": "Exploring Diverse Generation Paths via Inference-time Stiefel Activation Steering",
      "authors": [
        "Dongxuan Zhu",
        "Ly Tran Ho Khanh",
        "Andy Yat-Ming Cheung",
        "Man-Chung Yue",
        "Viet Anh Nguyen"
      ],
      "abstract": "Language models often default to a narrow set of high-probability outputs, leaving their generation paths homogeneous and prone to mode collapse. Sampling-based strategies inject randomness but still struggle to guarantee diversity across multiple concurrent generation runs. We address this limitation by introducing STARS ($\\textbf{St}$iefel-based $\\textbf{A}$ctivation Steering for Diverse $\\textbf{R}$ea$\\textbf{S}$oning), a training-free, inference-time intervention method that transforms activation steering into an exploration engine. At each token, STARS collects the hidden activations of concurrent generation runs and optimizes multiple additive steering directions jointly on the Stiefel manifold. STARS maximizes the geometric volume of the steered activations, while the Stiefel manifold induces orthogonality of the steering interventions. This formulation explicitly promotes divergent activation vectors of concurrent generation runs, and implicitly promotes divergent generation trajectories. This manifold optimization formulation can be solved using a Riemannian gradient descent algorithm with convergence guarantees, but this algorithm is too time-consuming for real-time inference. To guarantee low latency, we further design a lightweight one-step update with an aggressive, closed-form stepsize. For test case generation and scientific discovery benchmarks, STARS consistently outperforms standard sampling methods, achieving greater diversity without sacrificing qualitative performance.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22010v1",
        "pdf": "https://arxiv.org/pdf/2601.22010v1"
      },
      "arxiv_id": "2601.22010v1",
      "comment": "34 pages, 2 figures. Accepted for publication at ICLR 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22009v1",
      "title": "MEIDNet: Multimodal generative AI framework for inverse materials design",
      "authors": [
        "Anand Babu",
        "Rogério Almeida Gouvêa",
        "Pierre Vandergheynst",
        "Gian-Marco Rignanese"
      ],
      "abstract": "In this work, we present Multimodal Equivariant Inverse Design Network (MEIDNet), a framework that jointly learns structural information and materials properties through contrastive learning, while encoding structures via an equivariant graph neural network (EGNN). By combining generative inverse design with multimodal learning, our approach accelerates the exploration of chemical-structural space and facilitates the discovery of materials that satisfy predefined property targets. MEIDNet exhibits strong latent-space alignment with cosine similarity 0.96 by fusion of three modalities through cross-modal learning. Through implementation of curriculum learning strategies, MEIDNet achieves ~60 times higher learning efficiency than conventional training techniques. The potential of our multimodal approach is demonstrated by generating low-bandgap perovskite structures at a stable, unique, and novel (SUN) rate of 13.6 %, which are further validated by ab initio methods. Our inverse design framework demonstrates both scalability and adaptability, paving the way for the universal learning of chemical space across diverse modalities.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22009v1",
        "pdf": "https://arxiv.org/pdf/2601.22009v1"
      },
      "arxiv_id": "2601.22009v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22003v1",
      "title": "Efficient Stochastic Optimisation via Sequential Monte Carlo",
      "authors": [
        "James Cuin",
        "Davide Carbone",
        "Yanbo Tang",
        "O. Deniz Akyildiz"
      ],
      "abstract": "The problem of optimising functions with intractable gradients frequently arise in machine learning and statistics, ranging from maximum marginal likelihood estimation procedures to fine-tuning of generative models. Stochastic approximation methods for this class of problems typically require inner sampling loops to obtain (biased) stochastic gradient estimates, which rapidly becomes computationally expensive. In this work, we develop sequential Monte Carlo (SMC) samplers for optimisation of functions with intractable gradients. Our approach replaces expensive inner sampling methods with efficient SMC approximations, which can result in significant computational gains. We establish convergence results for the basic recursions defined by our methodology which SMC samplers approximate. We demonstrate the effectiveness of our approach on the reward-tuning of energy-based models within various settings.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.CO"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22003v1",
        "pdf": "https://arxiv.org/pdf/2601.22003v1"
      },
      "arxiv_id": "2601.22003v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22002v1",
      "title": "Rate-Distortion Optimization for Transformer Inference",
      "authors": [
        "Anderson de Andrade",
        "Alon Harell",
        "Ivan V. Bajić"
      ],
      "abstract": "Transformers achieve superior performance on many tasks, but impose heavy compute and memory requirements during inference. This inference can be made more efficient by partitioning the process across multiple devices, which, in turn, requires compressing its intermediate representations. In this work, we introduce a principled rate-distortion-based framework for lossy compression that learns compact encodings that explicitly trade off bitrate against accuracy. Experiments on language benchmarks show that the proposed codec achieves substantial savings with improved accuracy in some cases, outperforming more complex baseline methods. We characterize and analyze the rate-distortion performance of transformers, offering a unified lens for understanding performance in representation coding. This formulation extends information-theoretic concepts to define the gap between rate and entropy, and derive some of its bounds. We further develop probably approximately correct (PAC)-style bounds for estimating this gap. For different architectures and tasks, we empirically demonstrate that their rates are driven by these bounds, adding to the explainability of the formulation.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22002v1",
        "pdf": "https://arxiv.org/pdf/2601.22002v1"
      },
      "arxiv_id": "2601.22002v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.22001v1",
      "title": "Heterogeneous Computing: The Key to Powering the Future of AI Agent Inference",
      "authors": [
        "Yiren Zhao",
        "Junyi Liu"
      ],
      "abstract": "AI agent inference is driving an inference heavy datacenter future and exposes bottlenecks beyond compute - especially memory capacity, memory bandwidth and high-speed interconnect. We introduce two metrics - Operational Intensity (OI) and Capacity Footprint (CF) - that jointly explain regimes the classic roofline analysis misses, including the memory capacity wall. Across agentic workflows (chat, coding, web use, computer use) and base model choices (GQA/MLA, MoE, quantization), OI/CF can shift dramatically, with long context KV cache making decode highly memory bound. These observations motivate disaggregated serving and system level heterogeneity: specialized prefill and decode accelerators, broader scale up networking, and decoupled compute-memory enabled by optical I/O. We further hypothesize agent-hardware co design, multiple inference accelerators within one system, and high bandwidth, large capacity memory disaggregation as foundations for adaptation to evolving OI/CF. Together, these directions chart a path to sustain efficiency and capability for large scale agentic AI inference.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.AR",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.22001v1",
        "pdf": "https://arxiv.org/pdf/2601.22001v1"
      },
      "arxiv_id": "2601.22001v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21999v1",
      "title": "Negatives-Dominant Contrastive Learning for Generalization in Imbalanced Domains",
      "authors": [
        "Meng Cao",
        "Jiexi Liu",
        "Songcan Chen"
      ],
      "abstract": "Imbalanced Domain Generalization (IDG) focuses on mitigating both domain and label shifts, both of which fundamentally shape the model's decision boundaries, particularly under heterogeneous long-tailed distributions across domains. Despite its practical significance, it remains underexplored, primarily due to the technical complexity of handling their entanglement and the paucity of theoretical foundations. In this paper, we begin by theoretically establishing the generalization bound for IDG, highlighting the role of posterior discrepancy and decision margin. This bound motivates us to focus on directly steering decision boundaries, marking a clear departure from existing methods. Subsequently, we technically propose a novel Negative-Dominant Contrastive Learning (NDCL) for IDG to enhance discriminability while enforce posterior consistency across domains. Specifically, inter-class decision-boundary separation is enhanced by placing greater emphasis on negatives as the primary signal in our contrastive learning, naturally amplifying gradient signals for minority classes to avoid the decision boundary being biased toward majority classes. Meanwhile, intra-class compactness is encouraged through a re-weighted cross-entropy strategy, and posterior consistency across domains is enforced through a prediction-central alignment strategy. Finally, rigorous yet challenging experiments on benchmarks validate the effectiveness of our NDCL. The code is available at https://github.com/Alrash/NDCL.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21999v1",
        "pdf": "https://arxiv.org/pdf/2601.21999v1"
      },
      "arxiv_id": "2601.21999v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21998v1",
      "title": "Causal World Modeling for Robot Control",
      "authors": [
        "Lin Li",
        "Qihang Zhang",
        "Yiming Luo",
        "Shuai Yang",
        "Ruilin Wang",
        "Fei Han",
        "Mingrui Yu",
        "Zelin Gao",
        "Nan Xue",
        "Xing Zhu",
        "Yujun Shen",
        "Yinghao Xu"
      ],
      "abstract": "This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21998v1",
        "pdf": "https://arxiv.org/pdf/2601.21998v1"
      },
      "arxiv_id": "2601.21998v1",
      "comment": "Project page: https://technology.robbyant.com/lingbot-va Code: https://github.com/robbyant/lingbot-va",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.21996v1",
      "title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
      "authors": [
        "Jianhui Chen",
        "Yuzhang Luo",
        "Liangming Pan"
      ],
      "abstract": "While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21996v1",
        "pdf": "https://arxiv.org/pdf/2601.21996v1"
      },
      "arxiv_id": "2601.21996v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21993v1",
      "title": "Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems",
      "authors": [
        "Dhiogo de Sá",
        "Carlos Schmiedel",
        "Carlos Pereira Lopes"
      ],
      "abstract": "Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21993v1",
        "pdf": "https://arxiv.org/pdf/2601.21993v1"
      },
      "arxiv_id": "2601.21993v1",
      "comment": "28 pages, 2 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21991v1",
      "title": "Geometry of Drifting MDPs with Path-Integral Stability Certificates",
      "authors": [
        "Zuyuan Zhang",
        "Mahdi Imani",
        "Tian Lan"
      ],
      "abstract": "Real-world reinforcement learning is often \\emph{nonstationary}: rewards and dynamics drift, accelerate, oscillate, and trigger abrupt switches in the optimal action. Existing theory often represents nonstationarity with coarse-scale models that measure \\emph{how much} the environment changes, not \\emph{how} it changes locally -- even though acceleration and near-ties drive tracking error and policy chattering. We take a geometric view of nonstationary discounted Markov Decision Processes (MDPs) by modeling the environment as a differentiable homotopy path and tracking the induced motion of the optimal Bellman fixed point. This yields a length-curvature-kink signature of intrinsic complexity: cumulative drift, acceleration/oscillation, and action-gap-induced nonsmoothness. We prove a solver-agnostic path-integral stability bound and derive gap-safe feasible regions that certify local stability away from switch regimes. Building on these results, we introduce \\textit{Homotopy-Tracking RL (HT-RL)} and \\textit{HT-MCTS}, lightweight wrappers that estimate replay-based proxies of length, curvature, and near-tie proximity online and adapt learning or planning intensity accordingly. Experiments show improved tracking and dynamic regret over matched static baselines, with the largest gains in oscillatory and switch-prone regimes.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21991v1",
        "pdf": "https://arxiv.org/pdf/2601.21991v1"
      },
      "arxiv_id": "2601.21991v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21990v1",
      "title": "Batched First-Order Methods for Parallel LP Solving in MIP",
      "authors": [
        "Nicolas Blin",
        "Stefano Gualandi",
        "Christopher Maes",
        "Andrea Lodi",
        "Bartolomeo Stellato"
      ],
      "abstract": "We present a batched first-order method for solving multiple linear programs in parallel on GPUs. Our approach extends the primal-dual hybrid gradient algorithm to efficiently solve batches of related linear programming problems that arise in mixed-integer programming techniques such as strong branching and bound tightening. By leveraging matrix-matrix operations instead of repeated matrix-vector operations, we obtain significant computational advantages on GPU architectures. We demonstrate the effectiveness of our approach on various case studies and identify the problem sizes where first-order methods outperform traditional simplex-based solvers depending on the computational environment one can use. This is a significant step for the design and development of integer programming algorithms tightly exploiting GPU capabilities where we argue that some specific operations should be allocated to GPUs and performed in full instead of using light-weight heuristic approaches on CPUs.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21990v1",
        "pdf": "https://arxiv.org/pdf/2601.21990v1"
      },
      "arxiv_id": "2601.21990v1",
      "comment": "15 pages, 4 figures, 4 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21988v1",
      "title": "Generalized Information Gathering Under Dynamics Uncertainty",
      "authors": [
        "Fernando Palafox",
        "Jingqi Li",
        "Jesse Milzman",
        "David Fridovich-Keil"
      ],
      "abstract": "An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying framework that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this framework, we derive a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our framework to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing theoretical justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our framework through experiments spanning linear, nonlinear, and multi-agent systems.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21988v1",
        "pdf": "https://arxiv.org/pdf/2601.21988v1"
      },
      "arxiv_id": "2601.21988v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21985v1",
      "title": "Elign: Equivariant Diffusion Model Alignment from Foundational Machine Learning Force Fields",
      "authors": [
        "Yunyang Li",
        "Lin Huang",
        "Luojia Xia",
        "Wenhe Zhang",
        "Mark Gerstein"
      ],
      "abstract": "Generative models for 3D molecular conformations must respect Euclidean symmetries and concentrate probability mass on thermodynamically favorable, mechanically stable structures. However, E(3)-equivariant diffusion models often reproduce biases from semi-empirical training data rather than capturing the equilibrium distribution of a high-fidelity Hamiltonian. While physics-based guidance can correct this, it faces two computational bottlenecks: expensive quantum-chemical evaluations (e.g., DFT) and the need to repeat such queries at every sampling step. We present Elign, a post-training framework that amortizes both costs. First, we replace expensive DFT evaluations with a faster, pretrained foundational machine-learning force field (MLFF) to provide physical signals. Second, we eliminate repeated run-time queries by shifting physical steering to the training phase. To achieve the second amortization, we formulate reverse diffusion as a reinforcement learning problem and introduce Force--Energy Disentangled Group Relative Policy Optimization (FED-GRPO) to fine-tune the denoising policy. FED-GRPO includes a potential-based energy reward and a force-based stability reward, which are optimized and group-normalized independently. Experiments show that Elign generates conformations with lower gold-standard DFT energies and forces, while improving stability. Crucially, inference remains as fast as unguided sampling, since no energy evaluations are required during generation.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21985v1",
        "pdf": "https://arxiv.org/pdf/2601.21985v1"
      },
      "arxiv_id": "2601.21985v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21984v1",
      "title": "PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable Power Converters",
      "authors": [
        "Jian Gao",
        "Yiwei Zou",
        "Abhishek Pradhan",
        "Wenhao Huang",
        "Yumin Su",
        "Kaiyuan Yang",
        "Xuan Zhang"
      ],
      "abstract": "Discovering superior circuit topologies requires navigating an exponentially large design space-a challenge traditionally reserved for human experts. Existing AI methods either select from predefined templates or generate novel topologies at a limited scale without rigorous verification, leaving large-scale performance-driven discovery underexplored. We present PowerGenie, a framework for automated discovery of higher-performance reconfigurable power converters at scale. PowerGenie introduces: (1) an automated analytical framework that determines converter functionality and theoretical performance limits without component sizing or SPICE simulation, and (2) an evolutionary finetuning method that co-evolves a generative model with its training distribution through fitness selection and uniqueness verification. Unlike existing methods that suffer from mode collapse and overfitting, our approach achieves higher syntax validity, function validity, novelty rate, and figure-of-merit (FoM). PowerGenie discovers a novel 8-mode reconfigurable converter with 23% higher FoM than the best training topology. SPICE simulations confirm average absolute efficiency gains of 10% across 8 modes and up to 17% at a single mode. Code is available at https://github.com/xz-group/PowerGenie.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21984v1",
        "pdf": "https://arxiv.org/pdf/2601.21984v1"
      },
      "arxiv_id": "2601.21984v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21983v1",
      "title": "Investigating Batch Inference in a Sequential Monte Carlo Framework for Neural Networks",
      "authors": [
        "Andrew Millard",
        "Joshua Murphy",
        "Peter Green",
        "Simon Maskell"
      ],
      "abstract": "Bayesian inference allows us to define a posterior distribution over the weights of a generic neural network (NN). Exact posteriors are usually intractable, in which case approximations can be employed. One such approximation - variational inference - is computationally efficient when using mini-batch stochastic gradient descent as subsets of the data are used for likelihood and gradient evaluations, though the approach relies on the selection of a variational distribution which sufficiently matches the form of the posterior. Particle-based methods such as Markov chain Monte Carlo and Sequential Monte Carlo (SMC) do not assume a parametric family for the posterior by typically require higher computational cost. These sampling methods typically use the full-batch of data for likelihood and gradient evaluations, which contributes to this computational expense. We explore several methods of gradually introducing more mini-batches of data (data annealing) into likelihood and gradient evaluations of an SMC sampler. We find that we can achieve up to $6\\times$ faster training with minimal loss in accuracy on benchmark image classification problems using NNs.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21983v1",
        "pdf": "https://arxiv.org/pdf/2601.21983v1"
      },
      "arxiv_id": "2601.21983v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21981v1",
      "title": "VERSA: Verified Event Data Format for Reliable Soccer Analytics",
      "authors": [
        "Geonhee Jo",
        "Mingu Kang",
        "Kangmin Lee",
        "Minho Lee",
        "Pascal Bauer",
        "Sang-Ki Ko"
      ],
      "abstract": "Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21981v1",
        "pdf": "https://arxiv.org/pdf/2601.21981v1"
      },
      "arxiv_id": "2601.21981v1",
      "comment": "13 pages, 5 figures, 3 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21979v1",
      "title": "Investigation into using stochastic embedding representations for evaluating the trustworthiness of the Fréchet Inception Distance",
      "authors": [
        "Ciaran Bench",
        "Vivek Desai",
        "Carlijn Roozemond",
        "Ruben van Engen",
        "Spencer A. Thomas"
      ],
      "abstract": "Feature embeddings acquired from pretrained models are widely used in medical applications of deep learning to assess the characteristics of datasets; e.g. to determine the quality of synthetic, generated medical images. The Fréchet Inception Distance (FID) is one popular synthetic image quality metric that relies on the assumption that the characteristic features of the data can be detected and encoded by an InceptionV3 model pretrained on ImageNet1K (natural images). While it is widely known that this makes it less effective for applications involving medical images, the extent to which the metric fails to capture meaningful differences in image characteristics is not obviously known. Here, we use Monte Carlo dropout to compute the predictive variance in the FID as well as a supplemental estimate of the predictive variance in the feature embedding model's latent representations. We show that the magnitudes of the predictive variances considered exhibit varying degrees of correlation with the extent to which test inputs (ImageNet1K validation set augmented at various strengths, and other external datasets) are out-of-distribution relative to its training data, providing some insight into the effectiveness of their use as indicators of the trustworthiness of the FID.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21979v1",
        "pdf": "https://arxiv.org/pdf/2601.21979v1"
      },
      "arxiv_id": "2601.21979v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21978v1",
      "title": "Bridging Graph Structure and Knowledge-Guided Editing for Interpretable Temporal Knowledge Graph Reasoning",
      "authors": [
        "Shiqi Fan",
        "Quanming Yao",
        "Hongyi Nie",
        "Wentao Ma",
        "Zhen Wang",
        "Wen Hua"
      ],
      "abstract": "Temporal knowledge graph reasoning (TKGR) aims to predict future events by inferring missing entities with dynamic knowledge structures. Existing LLM-based reasoning methods prioritize contextual over structural relations, struggling to extract relevant subgraphs from dynamic graphs. This limits structural information understanding, leading to unstructured, hallucination-prone inferences especially with temporal inconsistencies. To address this problem, we propose IGETR (Integration of Graph and Editing-enhanced Temporal Reasoning), a hybrid reasoning framework that combines the structured temporal modeling capabilities of Graph Neural Networks (GNNs) with the contextual understanding of LLMs. IGETR operates through a three-stage pipeline. The first stage aims to ground the reasoning process in the actual data by identifying structurally and temporally coherent candidate paths through a temporal GNN, ensuring that inference starts from reliable graph-based evidence. The second stage introduces LLM-guided path editing to address logical and semantic inconsistencies, leveraging external knowledge to refine and enhance the initial paths. The final stage focuses on integrating the refined reasoning paths to produce predictions that are both accurate and interpretable. Experiments on standard TKG benchmarks show that IGETR achieves state-of-the-art performance, outperforming strong baselines with relative improvements of up to 5.6% on Hits@1 and 8.1% on Hits@3 on the challenging ICEWS datasets. Additionally, we execute ablation studies and additional analyses confirm the effectiveness of each component.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21978v1",
        "pdf": "https://arxiv.org/pdf/2601.21978v1"
      },
      "arxiv_id": "2601.21978v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21977v1",
      "title": "From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation",
      "authors": [
        "Javier Argota Sánchez-Vaquerizo",
        "Luis Borunda Monsivais"
      ],
      "abstract": "Traditional architectural simulations (e.g. Computational Fluid Dynamics, evacuation, structural analysis) model elements as deterministic physics-based \"particles\" rather than cognitive \"agents\". To bridge this, we introduce \\textbf{Agentic Environmental Simulations}, where Large Multimodal generative models actively predict the next state of spatial environments based on semantic expectation. Drawing on examples from accessibility-oriented AR pipelines and multimodal digital twins, we propose a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations advance through meaningful, surprisal-triggered events. Within this framework we posit AI hallucinations as diagnostic tools. By formalizing the \\textbf{Cognitive Friction} ($C_f$) it is possible to reveal \"Phantom Affordances\", i.e. semiotic ambiguities in built space. Finally, we challenge current HCI paradigms by treating environments as dynamic cognitive partners and propose a human-centered framework of cognitive orchestration for designing AI-driven simulations that preserve autonomy, affective clarity, and cognitive integrity.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21977v1",
        "pdf": "https://arxiv.org/pdf/2601.21977v1"
      },
      "arxiv_id": "2601.21977v1",
      "comment": "Paper selected for the workshop Human Cognition, AI, and the Future of HCI: Navigating the Disruptive and Wild Landscape of Large Language Models and Agentic AI as part of the Human-Computer Interaction (HCI) conference of the Alpine region (AlpCHI 2026) hosted at the Congressi Stefano Franscini, March 1st to March 5th, 2026 on Monte Verità in Ascona, Switzerland",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21975v1",
      "title": "Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models",
      "authors": [
        "Pranav Mahajan",
        "Ihor Kendiukhov",
        "Syed Hussain",
        "Lydia Nottingham"
      ],
      "abstract": "Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21975v1",
        "pdf": "https://arxiv.org/pdf/2601.21975v1"
      },
      "arxiv_id": "2601.21975v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21972v1",
      "title": "Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic",
      "authors": [
        "Shuo Liu",
        "Tianle Chen",
        "Ryan Amiri",
        "Christopher Amato"
      ],
      "abstract": "Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \\textbf{CoLLM-CC} with a \\textbf{C}entralized \\textbf{C}ritic and \\textbf{CoLLM-DC} with \\textbf{D}ecentralized \\textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21972v1",
        "pdf": "https://arxiv.org/pdf/2601.21972v1"
      },
      "arxiv_id": "2601.21972v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21971v1",
      "title": "MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts",
      "authors": [
        "Lorenzo Mazza",
        "Ariel Rodriguez",
        "Rayan Younis",
        "Martin Lelis",
        "Ortrun Hellig",
        "Chenpan Li",
        "Sebastian Bodenstedt",
        "Martin Wagner",
        "Stefanie Speidel"
      ],
      "abstract": "Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21971v1",
        "pdf": "https://arxiv.org/pdf/2601.21971v1"
      },
      "arxiv_id": "2601.21971v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21969v1",
      "title": "Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding",
      "authors": [
        "Yifan Zhu",
        "Huiqiang Rong",
        "Haoran Luo"
      ],
      "abstract": "Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21969v1",
        "pdf": "https://arxiv.org/pdf/2601.21969v1"
      },
      "arxiv_id": "2601.21969v1",
      "comment": "26 pages and 11 figures,this work has been accepted for presentation at ICLR 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21967v1",
      "title": "The Energy Impact of Domain Model Design in Classical Planning",
      "authors": [
        "Ilche Georgievski",
        "Serhat Tekin",
        "Marco Aiello"
      ],
      "abstract": "AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21967v1",
        "pdf": "https://arxiv.org/pdf/2601.21967v1"
      },
      "arxiv_id": "2601.21967v1",
      "comment": "2026 IEEE/ACM 5th International Conference on AI Engineering - Software Engineering for AI (CAIN '26)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21964v1",
      "title": "From Tokens to Blocks: A Block-Diffusion Perspective on Molecular Generation",
      "authors": [
        "Qianwei Yang",
        "Dong Xu",
        "Zhangfan Yang",
        "Sisi Yuan",
        "Zexuan Zhu",
        "Jianqiang Li",
        "Junkai Ji"
      ],
      "abstract": "Drug discovery can be viewed as a combinatorial search over an immense chemical space, motivating the development of deep generative models for de novo molecular design. Among these, GPT-based molecular language models (MLM) have shown strong molecular design performance by learning chemical syntax and semantics from large-scale data. However, existing MLMs face two fundamental limitations: they inadequately capture the graph-structured nature of molecules when formulated as next-token prediction problems, and they typically lack explicit mechanisms for target-aware generation. Here, we propose SoftMol, a unified framework that co-designs molecular representation, model architecture, and search strategy for target-aware molecular generation. SoftMol introduces soft fragments, a rule-free block representation of SMILES that enables diffusion-native modeling, and develops SoftBD, the first block-diffusion molecular language model that combines local bidirectional diffusion with autoregressive generation under molecular structural constraints. To favor generated molecules with high drug-likeness and synthetic accessibility, SoftBD is trained on a carefully curated dataset named ZINC-Curated. SoftMol further integrates a gated Monte Carlo tree search to assemble fragments in a target-aware manner. Experimental results show that, compared with current state-of-the-art models, SoftMol achieves 100% chemical validity, improves binding affinity by 9.7%, yields a 2-3x increase in molecular diversity, and delivers a 6.6x speedup in inference efficiency. Code is available at https://github.com/szu-aicourse/softmol",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21964v1",
        "pdf": "https://arxiv.org/pdf/2601.21964v1"
      },
      "arxiv_id": "2601.21964v1",
      "comment": "30 pages, 13 figures, 11 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21963v1",
      "title": "Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems",
      "authors": [
        "Alexander Loth",
        "Martin Kappes",
        "Marc-Oliver Pahl"
      ],
      "abstract": "Generative AI and misinformation research has evolved since our 2024 survey. This paper presents an updated perspective, transitioning from literature review to practical countermeasures. We report on changes in the threat landscape, including improved AI-generated content through Large Language Models (LLMs) and multimodal systems. Central to this work are our practical contributions: JudgeGPT, a platform for evaluating human perception of AI-generated news, and RogueGPT, a controlled stimulus generation engine for research. Together, these tools form an experimental pipeline for studying how humans perceive and detect AI-generated misinformation. Our findings show that detection capabilities have improved, but the competition between generation and detection continues. We discuss mitigation strategies including LLM-based detection, inoculation approaches, and the dual-use nature of generative AI. This work contributes to research addressing the adverse impacts of AI on information quality.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.SI"
      ],
      "primary_category": "cs.CY",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21963v1",
        "pdf": "https://arxiv.org/pdf/2601.21963v1"
      },
      "arxiv_id": "2601.21963v1",
      "comment": "Accepted at ACM TheWebConf '26 Companion",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21961v1",
      "title": "How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors",
      "authors": [
        "Kuai Yu",
        "Naicheng Yu",
        "Han Wang",
        "Rui Yang",
        "Huan Zhang"
      ],
      "abstract": "Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents' preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents' behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents' actions, whereas font styling, text color, and item image clarity exhibit minor effects.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21961v1",
        "pdf": "https://arxiv.org/pdf/2601.21961v1"
      },
      "arxiv_id": "2601.21961v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21959v1",
      "title": "Near-Optimal Private Tests for Simple and MLR Hypotheses",
      "authors": [
        "Yu-Wei Chen",
        "Raghu Pasupathy",
        "Jordan Awan"
      ],
      "abstract": "We develop a near-optimal testing procedure under the framework of Gaussian differential privacy for simple as well as one- and two-sided tests under monotone likelihood ratio conditions. Our mechanism is based on a private mean estimator with data-driven clamping bounds, whose population risk matches the private minimax rate up to logarithmic factors. Using this estimator, we construct private test statistics that achieve the same asymptotic relative efficiency as the non-private, most powerful tests while maintaining conservative type I error control. In addition to our theoretical results, our numerical experiments show that our private tests outperform competing DP methods and offer comparable power to the non-private most powerful tests, even at moderately small sample sizes and privacy loss budgets.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21959v1",
        "pdf": "https://arxiv.org/pdf/2601.21959v1"
      },
      "arxiv_id": "2601.21959v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21957v1",
      "title": "PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing",
      "authors": [
        "Cheng Cui",
        "Ting Sun",
        "Suyin Liang",
        "Tingquan Gao",
        "Zelun Zhang",
        "Jiaxuan Liu",
        "Xueqing Wang",
        "Changda Zhou",
        "Hongen Liu",
        "Manhui Lin",
        "Yue Zhang",
        "Yubo Zhang",
        "Yi Liu",
        "Dianhai Yu",
        "Yanjun Ma"
      ],
      "abstract": "We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model's capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: https://github.com/PaddlePaddle/PaddleOCR",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21957v1",
        "pdf": "https://arxiv.org/pdf/2601.21957v1"
      },
      "arxiv_id": "2601.21957v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21956v1",
      "title": "Uncertainty-Aware Data-Based Method for Fast and Reliable Shape Optimization",
      "authors": [
        "Yunjia Yang",
        "Runze Li",
        "Yufei Zhang",
        "Haixin Chen"
      ],
      "abstract": "Data-based optimization (DBO) offers a promising approach for efficiently optimizing shape for better aerodynamic performance by leveraging a pretrained surrogate model for offline evaluations during iterations. However, DBO heavily relies on the quality of the training database. Samples outside the training distribution encountered during optimization can lead to significant prediction errors, potentially misleading the optimization process. Therefore, incorporating uncertainty quantification into optimization is critical for detecting outliers and enhancing robustness. This study proposes an uncertainty-aware data-based optimization (UA-DBO) framework to monitor and minimize surrogate model uncertainty during DBO. A probabilistic encoder-decoder surrogate model is developed to predict uncertainties associated with its outputs, and these uncertainties are integrated into a model-confidence-aware objective function to penalize samples with large prediction errors during data-based optimization process. The UA-DBO framework is evaluated on two multipoint optimization problems aimed at improving airfoil drag divergence and buffet performance. Results demonstrate that UA-DBO consistently reduces prediction errors in optimized samples and achieves superior performance gains compared to original DBO. Moreover, compared to multipoint optimization based on full computational simulations, UA-DBO offers comparable optimization effectiveness while significantly accelerating optimization speed.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21956v1",
        "pdf": "https://arxiv.org/pdf/2601.21956v1"
      },
      "arxiv_id": "2601.21956v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.21951v1",
      "title": "Diffusion Path Samplers via Sequential Monte Carlo",
      "authors": [
        "James Matthew Young",
        "Paula Cordero-Encinar",
        "Sebastian Reich",
        "Andrew Duncan",
        "O. Deniz Akyildiz"
      ],
      "abstract": "We develop a diffusion-based sampler for target distributions known up to a normalising constant. To this end, we rely on the well-known diffusion path that smoothly interpolates between a (simple) base distribution and the target distribution, widely used in diffusion models. Our approach is based on a practical implementation of diffusion-annealed Langevin Monte Carlo, which approximates the diffusion path with convergence guarantees. We tackle the score estimation problem by developing an efficient sequential Monte Carlo sampler that evolves auxiliary variables from conditional distributions along the path, which provides principled score estimates for time-varying distributions. We further develop novel control variate schedules that minimise the variance of these score estimates. Finally, we provide theoretical guarantees and empirically demonstrate the effectiveness of our method on several synthetic and real-world datasets.",
      "published": "2026-01-29",
      "updated": "2026-01-29",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.CO"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2601.21951v1",
        "pdf": "https://arxiv.org/pdf/2601.21951v1"
      },
      "arxiv_id": "2601.21951v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    }
  ]
}