{
  "fetched_at": "2025-12-14T00:28:44.085670",
  "total_papers": 100,
  "papers": [
    {
      "id": "2512.10959v1",
      "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
      "authors": [
        "Tjark Behrens",
        "Anton Obukhov",
        "Bingxin Ke",
        "Fabio Tosi",
        "Matteo Poggi",
        "Konrad Schindler"
      ],
      "abstract": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10959v1",
        "pdf": "https://arxiv.org/pdf/2512.10959v1"
      },
      "arxiv_id": "2512.10959v1",
      "comment": "Project page: https://hf.co/spaces/prs-eth/stereospace_web",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10958v1",
      "title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
      "authors": [
        "Ao Liang",
        "Lingdong Kong",
        "Tianyi Yan",
        "Hongsi Liu",
        "Wesley Yang",
        "Ziqi Huang",
        "Wei Yin",
        "Jialong Zuo",
        "Yixuan Hu",
        "Dekai Zhu",
        "Dongyue Lu",
        "Youquan Liu",
        "Guangfeng Jiang",
        "Linfeng Li",
        "Xiangtai Li",
        "Long Zhuo",
        "Lai Xing Ng",
        "Benoit R. Cottereau",
        "Changxin Gao",
        "Liang Pan",
        "Wei Tsang Ooi",
        "Ziwei Liu"
      ],
      "abstract": "Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10958v1",
        "pdf": "https://arxiv.org/pdf/2512.10958v1"
      },
      "arxiv_id": "2512.10958v1",
      "comment": "Preprint; 80 pages, 37 figures, 29 tables; Project Page at https://worldbench.github.io/worldlens",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10957v1",
      "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
      "authors": [
        "Yukai Shi",
        "Weiyu Li",
        "Zihao Wang",
        "Hongyang Li",
        "Xingyu Chen",
        "Ping Tan",
        "Lei Zhang"
      ],
      "abstract": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10957v1",
        "pdf": "https://arxiv.org/pdf/2512.10957v1"
      },
      "arxiv_id": "2512.10957v1",
      "comment": "Project page: https://idea-research.github.io/SceneMaker/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10956v1",
      "title": "Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision",
      "authors": [
        "Wentao Zhou",
        "Xuweiyi Chen",
        "Vignesh Rajagopal",
        "Jeffrey Chen",
        "Rohan Chandra",
        "Zezhou Cheng"
      ],
      "abstract": "The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.\n  We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10956v1",
        "pdf": "https://arxiv.org/pdf/2512.10956v1"
      },
      "arxiv_id": "2512.10956v1",
      "comment": "Project Page: https://www.cs.virginia.edu/~tsx4zn/stereowalk/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10955v1",
      "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
      "authors": [
        "Tsai-Shien Chen",
        "Aliaksandr Siarohin",
        "Guocheng Gordon Qian",
        "Kuan-Chieh Jackson Wang",
        "Egor Nemchinov",
        "Moayed Haji-Ali",
        "Riza Alp Guler",
        "Willi Menapace",
        "Ivan Skorokhodov",
        "Anil Kag",
        "Jun-Yan Zhu",
        "Sergey Tulyakov"
      ],
      "abstract": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10955v1",
        "pdf": "https://arxiv.org/pdf/2512.10955v1"
      },
      "arxiv_id": "2512.10955v1",
      "comment": "Project page: https://snap-research.github.io/omni-attribute",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10953v1",
      "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
      "authors": [
        "Yiyang Lu",
        "Qiao Sun",
        "Xianbang Wang",
        "Zhicheng Jiang",
        "Hanhong Zhao",
        "Kaiming He"
      ],
      "abstract": "Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation (\"1-NFE\") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10953v1",
        "pdf": "https://arxiv.org/pdf/2512.10953v1"
      },
      "arxiv_id": "2512.10953v1",
      "comment": "Tech report",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10952v1",
      "title": "Hierarchical Dataset Selection for High-Quality Data Sharing",
      "authors": [
        "Xiaona Zhou",
        "Yingyan Zeng",
        "Ran Jin",
        "Ismini Lourentzou"
      ],
      "abstract": "The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10952v1",
        "pdf": "https://arxiv.org/pdf/2512.10952v1"
      },
      "arxiv_id": "2512.10952v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10954v1",
      "title": "Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration",
      "authors": [
        "Sicheng Mo",
        "Thao Nguyen",
        "Richard Zhang",
        "Nick Kolkin",
        "Siddharth Srinivasan Iyer",
        "Eli Shechtman",
        "Krishna Kumar Singh",
        "Yong Jae Lee",
        "Bolei Zhou",
        "Yuheng Li"
      ],
      "abstract": "In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect - larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256x256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10954v1",
        "pdf": "https://arxiv.org/pdf/2512.10954v1"
      },
      "arxiv_id": "2512.10954v1",
      "comment": "Project Page: https://sichengmo.github.io/GroupDiff/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10950v1",
      "title": "E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training",
      "authors": [
        "Qitao Zhao",
        "Hao Tan",
        "Qianqian Wang",
        "Sai Bi",
        "Kai Zhang",
        "Kalyan Sunkavalli",
        "Shubham Tulsiani",
        "Hanwen Jiang"
      ],
      "abstract": "Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10950v1",
        "pdf": "https://arxiv.org/pdf/2512.10950v1"
      },
      "arxiv_id": "2512.10950v1",
      "comment": "Project website: https://qitaozhao.github.io/E-RayZer",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10949v1",
      "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
      "authors": [
        "Yiwen Tang",
        "Zoey Guo",
        "Kaixin Zhu",
        "Ray Zhang",
        "Qizhi Chen",
        "Dongzhi Jiang",
        "Junli Liu",
        "Bohan Zeng",
        "Haoming Song",
        "Delin Qu",
        "Tianyi Bai",
        "Dan Xu",
        "Wentao Zhang",
        "Bin Zhao"
      ],
      "abstract": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10949v1",
        "pdf": "https://arxiv.org/pdf/2512.10949v1"
      },
      "arxiv_id": "2512.10949v1",
      "comment": "Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10948v1",
      "title": "ClusIR: Towards Cluster-Guided All-in-One Image Restoration",
      "authors": [
        "Shengkai Hu",
        "Jiaqi Ma",
        "Jun Wan",
        "Wenwen Min",
        "Yongcheng Jing",
        "Lefei Zhang",
        "Dacheng Tao"
      ],
      "abstract": "All-in-One Image Restoration (AiOIR) aims to recover high-quality images from diverse degradations within a unified framework. However, existing methods often fail to explicitly model degradation types and struggle to adapt their restoration behavior to complex or mixed degradations. To address these issues, we propose ClusIR, a Cluster-Guided Image Restoration framework that explicitly models degradation semantics through learnable clustering and propagates cluster-aware cues across spatial and frequency domains for adaptive restoration. Specifically, ClusIR comprises two key components: a Probabilistic Cluster-Guided Routing Mechanism (PCGRM) and a Degradation-Aware Frequency Modulation Module (DAFMM). The proposed PCGRM disentangles degradation recognition from expert activation, enabling discriminative degradation perception and stable expert routing. Meanwhile, DAFMM leverages the cluster-guided priors to perform adaptive frequency decomposition and targeted modulation, collaboratively refining structural and textural representations for higher restoration fidelity. The cluster-guided synergy seamlessly bridges semantic cues with frequency-domain modulation, empowering ClusIR to attain remarkable restoration results across a wide range of degradations. Extensive experiments on diverse benchmarks validate that ClusIR reaches competitive performance under several scenarios.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10948v1",
        "pdf": "https://arxiv.org/pdf/2512.10948v1"
      },
      "arxiv_id": "2512.10948v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10947v1",
      "title": "Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving",
      "authors": [
        "Jiawei Yang",
        "Ziyu Chen",
        "Yurong You",
        "Yan Wang",
        "Yiming Li",
        "Yuxiao Chen",
        "Boyi Li",
        "Boris Ivanovic",
        "Marco Pavone",
        "Yue Wang"
      ],
      "abstract": "We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10947v1",
        "pdf": "https://arxiv.org/pdf/2512.10947v1"
      },
      "arxiv_id": "2512.10947v1",
      "comment": "Project Page: https://jiawei-yang.github.io/Flex/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10946v1",
      "title": "ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning",
      "authors": [
        "Wendi Chen",
        "Han Xue",
        "Yi Wang",
        "Fangyuan Zhou",
        "Jun Lv",
        "Yang Jin",
        "Shirun Tang",
        "Chuan Wen",
        "Cewu Lu"
      ],
      "abstract": "Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10946v1",
        "pdf": "https://arxiv.org/pdf/2512.10946v1"
      },
      "arxiv_id": "2512.10946v1",
      "comment": "Project page: https://implicit-rdp.github.io",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10945v1",
      "title": "MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation",
      "authors": [
        "Henghui Ding",
        "Chang Liu",
        "Shuting He",
        "Kaining Ying",
        "Xudong Jiang",
        "Chen Change Loy",
        "Yu-Gang Jiang"
      ],
      "abstract": "This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10945v1",
        "pdf": "https://arxiv.org/pdf/2512.10945v1"
      },
      "arxiv_id": "2512.10945v1",
      "comment": "IEEE TPAMI, Project Page: https://henghuiding.com/MeViS/",
      "journal_ref": "in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 47, no. 12, pp. 11400-11416, 2025",
      "has_code": false
    },
    {
      "id": "2512.10943v1",
      "title": "AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation",
      "authors": [
        "Sharath Girish",
        "Viacheslav Ivanov",
        "Tsai-Shien Chen",
        "Hao Chen",
        "Aliaksandr Siarohin",
        "Sergey Tulyakov"
      ],
      "abstract": "Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10943v1",
        "pdf": "https://arxiv.org/pdf/2512.10943v1"
      },
      "arxiv_id": "2512.10943v1",
      "comment": "Project page: https://snap-research.github.io/Video-AlcheMinT/snap-research.github.io/Video-AlcheMinT",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10942v1",
      "title": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language",
      "authors": [
        "Delong Chen",
        "Mustafa Shukor",
        "Theo Moutakanni",
        "Willy Chung",
        "Jade Yu",
        "Tejaswi Kasarla",
        "Allen Bolourchi",
        "Yann LeCun",
        "Pascale Fung"
      ],
      "abstract": "We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10942v1",
        "pdf": "https://arxiv.org/pdf/2512.10942v1"
      },
      "arxiv_id": "2512.10942v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10941v1",
      "title": "Mull-Tokens: Modality-Agnostic Latent Thinking",
      "authors": [
        "Arijit Ray",
        "Ahmed Abdelkader",
        "Chengzhi Mao",
        "Bryan A. Plummer",
        "Kate Saenko",
        "Ranjay Krishna",
        "Leonidas Guibas",
        "Wen-Sheng Chu"
      ],
      "abstract": "Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10941v1",
        "pdf": "https://arxiv.org/pdf/2512.10941v1"
      },
      "arxiv_id": "2512.10941v1",
      "comment": "Project webpage: https://arijitray.com/multimodal_thinking/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10940v1",
      "title": "OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis",
      "authors": [
        "Xiang Fan",
        "Sharath Girish",
        "Vivek Ramanujan",
        "Chaoyang Wang",
        "Ashkan Mirzaei",
        "Petr Sushko",
        "Aliaksandr Siarohin",
        "Sergey Tulyakov",
        "Ranjay Krishna"
      ],
      "abstract": "Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\\% in multiview NVS LLFF dataset, 60\\% in dynamic NVS Neural 3D Video benchmark, 20\\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10940v1",
        "pdf": "https://arxiv.org/pdf/2512.10940v1"
      },
      "arxiv_id": "2512.10940v1",
      "comment": "Project page: https://snap-research.github.io/OmniView/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10939v1",
      "title": "GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting",
      "authors": [
        "Madhav Agarwal",
        "Mingtian Zhang",
        "Laura Sevilla-Lara",
        "Steven McDonagh"
      ],
      "abstract": "Speech-driven talking heads have recently emerged and enable interactive avatars. However, real-world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. Diffusion methods provide realistic image generation, yet struggle with oneshot settings. Gaussian Splatting approaches are real-time, yet inaccuracies in facial tracking, or inconsistent Gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. We address this problem by mapping Gaussian Splatting using 3D Morphable Models to generate person-specific avatars. We introduce transformer-based prediction of model parameters, directly from audio, to drive temporal consistency. From monocular video and independent audio speech inputs, our method enables generation of real-time talking head videos where we report competitive quantitative and qualitative performance.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10939v1",
        "pdf": "https://arxiv.org/pdf/2512.10939v1"
      },
      "arxiv_id": "2512.10939v1",
      "comment": "IEEE/CVF Winter Conference on Applications of Computer Vision 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10938v1",
      "title": "Stronger Normalization-Free Transformers",
      "authors": [
        "Mingzhi Chen",
        "Taiming Lu",
        "Jiachen Zhu",
        "Mingjie Sun",
        "Zhuang Liu"
      ],
      "abstract": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\\mathrm{Derf}(x) = \\mathrm{erf}(αx + s)$, where $\\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10938v1",
        "pdf": "https://arxiv.org/pdf/2512.10938v1"
      },
      "arxiv_id": "2512.10938v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10937v1",
      "title": "On Decision-Making Agents and Higher-Order Causal Processes",
      "authors": [
        "Matt Wilson"
      ],
      "abstract": "We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10937v1",
        "pdf": "https://arxiv.org/pdf/2512.10937v1"
      },
      "arxiv_id": "2512.10937v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10936v1",
      "title": "Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks",
      "authors": [
        "Kristina Korotkova",
        "Aleksandr Katrutsa"
      ],
      "abstract": "The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10936v1",
        "pdf": "https://arxiv.org/pdf/2512.10936v1"
      },
      "arxiv_id": "2512.10936v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10935v1",
      "title": "Any4D: Unified Feed-Forward Metric 4D Reconstruction",
      "authors": [
        "Jay Karhade",
        "Nikhil Keetha",
        "Yuchen Zhang",
        "Tanisha Gupta",
        "Akash Sharma",
        "Sebastian Scherer",
        "Deva Ramanan"
      ],
      "abstract": "We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10935v1",
        "pdf": "https://arxiv.org/pdf/2512.10935v1"
      },
      "arxiv_id": "2512.10935v1",
      "comment": "Project Website: https://any-4d.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10934v1",
      "title": "Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit",
      "authors": [
        "Zamirddine Mari",
        "Jérôme Pasquet",
        "Julien Seinturier"
      ],
      "abstract": "Autonomous drone navigation in confined tubular environments remains a major challenge due to the constraining geometry of the conduits, the proximity of the walls, and the perceptual limitations inherent to such scenarios. We propose a reinforcement learning approach enabling a drone to navigate unknown three-dimensional tubes without any prior knowledge of their geometry, relying solely on local observations from LiDAR and a conditional visual detection of the tube center. In contrast, the Pure Pursuit algorithm, used as a deterministic baseline, benefits from explicit access to the centerline, creating an information asymmetry designed to assess the ability of RL to compensate for the absence of a geometric model. The agent is trained through a progressive Curriculum Learning strategy that gradually exposes it to increasingly curved geometries, where the tube center frequently disappears from the visual field. A turning-negotiation mechanism, based on the combination of direct visibility, directional memory, and LiDAR symmetry cues, proves essential for ensuring stable navigation under such partial observability conditions. Experiments show that the PPO policy acquires robust and generalizable behavior, consistently outperforming the deterministic controller despite its limited access to geometric information. Validation in a high-fidelity 3D environment further confirms the transferability of the learned behavior to a continuous physical dynamics.\n  The proposed approach thus provides a complete framework for autonomous navigation in unknown tubular environments and opens perspectives for industrial, underground, or medical applications where progressing through narrow and weakly perceptive conduits represents a central challenge.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10934v1",
        "pdf": "https://arxiv.org/pdf/2512.10934v1"
      },
      "arxiv_id": "2512.10934v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10932v1",
      "title": "BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models",
      "authors": [
        "Shengao Wang",
        "Wenqi Wang",
        "Zecheng Wang",
        "Max Whitton",
        "Michael Wakeham",
        "Arjun Chandra",
        "Joey Huang",
        "Pengyue Zhu",
        "Helen Chen",
        "David Li",
        "Jeffrey Li",
        "Shawn Li",
        "Andrew Zagula",
        "Amy Zhao",
        "Andrew Zhu",
        "Sayaka Nakamura",
        "Yuki Yamamoto",
        "Jerry Jun Yokono",
        "Aaron Mueller",
        "Bryan A. Plummer",
        "Kate Saenko",
        "Venkatesh Saligrama",
        "Boqing Gong"
      ],
      "abstract": "Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10932v1",
        "pdf": "https://arxiv.org/pdf/2512.10932v1"
      },
      "arxiv_id": "2512.10932v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10931v1",
      "title": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs",
      "authors": [
        "George Yakushev",
        "Nataliia Babina",
        "Masoud Vahid Dastgerdi",
        "Vyacheslav Zhdanovskiy",
        "Alina Shutova",
        "Denis Kuznedelev"
      ],
      "abstract": "Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10931v1",
        "pdf": "https://arxiv.org/pdf/2512.10931v1"
      },
      "arxiv_id": "2512.10931v1",
      "comment": "Preprint, work in progress",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10929v1",
      "title": "Noisy Quantum Learning Theory",
      "authors": [
        "Jordan Cotler",
        "Weiyuan Gong",
        "Ishaan Kannan"
      ],
      "abstract": "We develop a framework for learning from noisy quantum experiments, focusing on fault-tolerant devices accessing uncharacterized systems through noisy couplings. Our starting point is the complexity class $\\textsf{NBQP}$ (\"noisy BQP\"), modeling noisy fault-tolerant quantum computers that cannot, in general, error-correct the oracle systems they query. Using this class, we show that for natural oracle problems, noise can eliminate exponential quantum learning advantages of ideal noiseless learners while preserving a superpolynomial gap between NISQ and fault-tolerant devices. Beyond oracle separations, we study concrete noisy learning tasks. For purity testing, the exponential two-copy advantage collapses under a single application of local depolarizing noise. Nevertheless, we identify a setting motivated by AdS/CFT in which noise-resilient structure restores a quantum learning advantage in a noisy regime. We then analyze noisy Pauli shadow tomography, deriving lower bounds that characterize how instance size, quantum memory, and noise control sample complexity, and design algorithms with parametrically similar scalings. Together, our results show that the Bell-basis and SWAP-test primitives underlying most exponential quantum learning advantages are fundamentally fragile to noise unless the experimental system has latent noise-robust structure. Thus, realizing meaningful quantum advantages in future experiments will require understanding how noise-robust physical properties interface with available algorithmic techniques.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "quant-ph",
        "cs.CC",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10929v1",
        "pdf": "https://arxiv.org/pdf/2512.10929v1"
      },
      "arxiv_id": "2512.10929v1",
      "comment": "11+53 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10927v1",
      "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
      "authors": [
        "Yulu Gan",
        "Ligeng Zhu",
        "Dandan Shan",
        "Baifeng Shi",
        "Hongxu Yin",
        "Boris Ivanovic",
        "Song Han",
        "Trevor Darrell",
        "Jitendra Malik",
        "Marco Pavone",
        "Boyi Li"
      ],
      "abstract": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10927v1",
        "pdf": "https://arxiv.org/pdf/2512.10927v1"
      },
      "arxiv_id": "2512.10927v1",
      "comment": "Code is available at https://github.com/Wolfv0/FoundationMotion/tree/main",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10926v1",
      "title": "Decoupled Q-Chunking",
      "authors": [
        "Qiyang Li",
        "Seohong Park",
        "Sergey Levine"
      ],
      "abstract": "Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences (\"chunks\") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10926v1",
        "pdf": "https://arxiv.org/pdf/2512.10926v1"
      },
      "arxiv_id": "2512.10926v1",
      "comment": "76 pages, 14 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10925v1",
      "title": "Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation",
      "authors": [
        "Zamirddine Mari",
        "Mohamad Motasem Nawaf",
        "Pierre Drap"
      ],
      "abstract": "Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10925v1",
        "pdf": "https://arxiv.org/pdf/2512.10925v1"
      },
      "arxiv_id": "2512.10925v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10922v1",
      "title": "SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale",
      "authors": [
        "Max Zimmer",
        "Christophe Roux",
        "Moritz Wagner",
        "Deborah Hendrych",
        "Sebastian Pokutta"
      ],
      "abstract": "The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10922v1",
        "pdf": "https://arxiv.org/pdf/2512.10922v1"
      },
      "arxiv_id": "2512.10922v1",
      "comment": "15 pages, 2 figures, 4 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10907v1",
      "title": "Hermitian Yang--Mills connections on general vector bundles: geometry and physical Yukawa couplings",
      "authors": [
        "Challenger Mishra",
        "Justin Tan"
      ],
      "abstract": "We compute solutions to the Hermitian Yang-Mills equations on holomorphic vector bundles $V$ via an alternating optimisation procedure founded on geometric machine learning. The proposed method is fully general with respect to the rank and structure group of $V$, requiring only the ability to enumerate a basis of global sections for a given bundle. This enables us to compute the physically normalised Yukawa couplings in a broad class of heterotic string compactifications. Using this method, we carry out this computation in full for a heterotic compactification incorporating a gauge bundle with non-Abelian structure group.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "hep-th",
        "cs.LG"
      ],
      "primary_category": "hep-th",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10907v1",
        "pdf": "https://arxiv.org/pdf/2512.10907v1"
      },
      "arxiv_id": "2512.10907v1",
      "comment": "51 pages, Associated code open--sourced at https://github.com/Justin-Tan/cymyc",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10906v1",
      "title": "Distributionally Robust Regret Optimal Control Under Moment-Based Ambiguity Sets",
      "authors": [
        "Feras Al Taha",
        "Eilyan Bitar"
      ],
      "abstract": "In this paper, we consider a class of finite-horizon, linear-quadratic stochastic control problems, where the probability distribution governing the noise process is unknown but assumed to belong to an ambiguity set consisting of all distributions whose mean and covariance lie within norm balls centered at given nominal values. To address the distributional ambiguity, we explore the design of causal affine control policies to minimize the worst-case expected regret over all distributions in the given ambiguity set. The resulting minimax optimal control problem is shown to admit an equivalent reformulation as a tractable convex program that corresponds to a regularized version of the nominal linear-quadratic stochastic control problem. While this convex program can be recast as a semidefinite program, semidefinite programs are typically solved using primal-dual interior point methods that scale poorly with the problem size in practice. To address this limitation, we propose a scalable dual projected subgradient method to compute optimal controllers to an arbitrary accuracy. Numerical experiments are presented to benchmark the proposed method against state-of-the-art data-driven and distributionally robust control design approaches.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "math.OC",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "math.OC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10906v1",
        "pdf": "https://arxiv.org/pdf/2512.10906v1"
      },
      "arxiv_id": "2512.10906v1",
      "comment": "21 pages, 2 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10903v1",
      "title": "Multi-Granular Node Pruning for Circuit Discovery",
      "authors": [
        "Muhammad Umair Haider",
        "Hammad Rizwan",
        "Hassan Sajjad",
        "A. B. Siddique"
      ],
      "abstract": "Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10903v1",
        "pdf": "https://arxiv.org/pdf/2512.10903v1"
      },
      "arxiv_id": "2512.10903v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10895v1",
      "title": "LLMs Can Assist with Proposal Selection at Large User Facilities",
      "authors": [
        "Lijie Ding",
        "Janell Thomson",
        "Jon Taylor",
        "Changwoo Do"
      ],
      "abstract": "We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\\simeq 0.2-0.8$, improving to $\\geq 0.5$ after 10\\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10895v1",
        "pdf": "https://arxiv.org/pdf/2512.10895v1"
      },
      "arxiv_id": "2512.10895v1",
      "comment": "9 pages, 8figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10894v1",
      "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
      "authors": [
        "Peiying Zhang",
        "Nanxuan Zhao",
        "Matthew Fisher",
        "Yiran Xu",
        "Jing Liao",
        "Difan Liu"
      ],
      "abstract": "Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10894v1",
        "pdf": "https://arxiv.org/pdf/2512.10894v1"
      },
      "arxiv_id": "2512.10894v1",
      "comment": "Project page: https://intchous.github.io/DuetSVG-site",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10891v1",
      "title": "Iterative Compositional Data Generation for Robot Control",
      "authors": [
        "Anh-Quan Pham",
        "Marcel Hussing",
        "Shubhankar P. Patankar",
        "Dani S. Bassett",
        "Jorge Mendez-Mendez",
        "Eric Eaton"
      ],
      "abstract": "Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10891v1",
        "pdf": "https://arxiv.org/pdf/2512.10891v1"
      },
      "arxiv_id": "2512.10891v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10888v1",
      "title": "PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction",
      "authors": [
        "Brandon Smock",
        "Valerie Faucon-Morin",
        "Max Sokolov",
        "Libin Liang",
        "Tayyibah Khanam",
        "Maury Courtland"
      ],
      "abstract": "Table extraction (TE) is a key challenge in visual document understanding. Traditional approaches detect tables first, then recognize their structure. Recently, interest has surged in developing methods, such as vision-language models (VLMs), that can extract tables directly in their full page or document context. However, progress has been difficult to demonstrate due to a lack of annotated data. To address this, we create a new large-scale dataset, PubTables-v2. PubTables-v2 supports a number of current challenging table extraction tasks. Notably, it is the first large-scale benchmark for multi-page table structure recognition. We demonstrate its usefulness by evaluating domain-specialized VLMs on these tasks and highlighting current progress. Finally, we use PubTables-v2 to create the Page-Object Table Transformer (POTATR), an image-to-graph extension of the Table Transformer to comprehensive page-level TE. Data, code, and trained models will be released.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10888v1",
        "pdf": "https://arxiv.org/pdf/2512.10888v1"
      },
      "arxiv_id": "2512.10888v1",
      "comment": "15 pages, 7 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10886v1",
      "title": "Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields",
      "authors": [
        "Stefan Matthes",
        "Markus Schramm"
      ],
      "abstract": "Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.\n  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.\n  The model accurately reconstructs loop temperatures (RMSE $<2^\\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10886v1",
        "pdf": "https://arxiv.org/pdf/2512.10886v1"
      },
      "arxiv_id": "2512.10886v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10881v1",
      "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
      "authors": [
        "Kehong Gong",
        "Zhengyu Wen",
        "Weixia He",
        "Mingxi Xu",
        "Qi Wang",
        "Ning Zhang",
        "Zhengyu Li",
        "Dongze Lian",
        "Wei Zhao",
        "Xiaoyu He",
        "Mingyuan Zhang"
      ],
      "abstract": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10881v1",
        "pdf": "https://arxiv.org/pdf/2512.10881v1"
      },
      "arxiv_id": "2512.10881v1",
      "comment": "Project page: https://animotionlab.github.io/MoCapAnything/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10878v1",
      "title": "Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes",
      "authors": [
        "Xuan Zhao",
        "Zhuo Cao",
        "Arya Bangun",
        "Hanno Scharr",
        "Ira Assent"
      ],
      "abstract": "Counterfactual explanations provide actionable insights by identifying minimal input changes required to achieve a desired model prediction. Beyond their interpretability benefits, counterfactuals can also be leveraged for model reconstruction, where a surrogate model is trained to replicate the behavior of a target model. In this work, we demonstrate that model reconstruction can be significantly improved by recognizing that counterfactuals, which typically lie close to the decision boundary, can serve as informative though less representative samples for both classes. This is particularly beneficial in settings with limited access to labeled data. We propose a method that integrates original data samples with counterfactuals to approximate class prototypes using the Wasserstein barycenter, thereby preserving the underlying distributional structure of each class. This approach enhances the quality of the surrogate model and mitigates the issue of decision boundary shift, which commonly arises when counterfactuals are naively treated as ordinary training instances. Empirical results across multiple datasets show that our method improves fidelity between the surrogate and target models, validating its effectiveness.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10878v1",
        "pdf": "https://arxiv.org/pdf/2512.10878v1"
      },
      "arxiv_id": "2512.10878v1",
      "comment": "Accepted by Actionable Interpretability Workshop at ICML 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10877v1",
      "title": "Guided Transfer Learning for Discrete Diffusion Models",
      "authors": [
        "Julian Kleutgens",
        "Claudio Battiloro",
        "Lingkai Kong",
        "Benjamin Grewe",
        "Francesca Dominici",
        "Mauricio Tec"
      ],
      "abstract": "Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10877v1",
        "pdf": "https://arxiv.org/pdf/2512.10877v1"
      },
      "arxiv_id": "2512.10877v1",
      "comment": "7 pages (main text) + appendix",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10874v1",
      "title": "A Differentiable Digital Twin of Distributed Link Scheduling for Contention-Aware Networking",
      "authors": [
        "Zhongyuan Zhao",
        "Yujun Ming",
        "Kevin Chan",
        "Ananthram Swami",
        "Santiago Segarra"
      ],
      "abstract": "Many routing and flow optimization problems in wired networks can be solved efficiently using minimum cost flow formulations. However, this approach does not extend to wireless multi-hop networks, where the assumptions of fixed link capacity and linear cost structure collapse due to contention for shared spectrum resources. The key challenge is that the long-term capacity of a wireless link becomes a non-linear function of its network context, including network topology, link quality, and the traffic assigned to neighboring links. In this work, we pursue a new direction of modeling wireless network under randomized medium access control by developing an analytical network digital twin (NDT) that predicts link duty cycles from network context. We generalize randomized contention as finding a Maximal Independent Set (MIS) on the conflict graph using weighted Luby's algorithm, derive an analytical model of link duty cycles, and introduce an iterative procedure that resolves the circular dependency among duty cycle, link capacity, and contention probability. Our numerical experiments show that the proposed NDT accurately predicts link duty cycles and congestion patterns with up to a 5000x speedup over packet-level simulation, and enables us to optimize link scheduling using gradient descent for reduced congestion and radio footprint.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.NI",
        "cs.LG",
        "eess.SP",
        "eess.SY"
      ],
      "primary_category": "cs.NI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10874v1",
        "pdf": "https://arxiv.org/pdf/2512.10874v1"
      },
      "arxiv_id": "2512.10874v1",
      "comment": "5 pages, 8 figures, presented in Asilomar Conference on Signals, Systems, and Computers 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10873v1",
      "title": "Physics-informed Polynomial Chaos Expansion with Enhanced Constrained Optimization Solver and D-optimal Sampling",
      "authors": [
        "Qitian Lu",
        "Himanshu Sharma",
        "Michael D. Shields",
        "Lukáš Novák"
      ],
      "abstract": "Physics-informed polynomial chaos expansions (PC$^2$) provide an efficient physically constrained surrogate modeling framework by embedding governing equations and other physical constraints into the standard data-driven polynomial chaos expansions (PCE) and solving via the Karush-Kuhn-Tucker (KKT) conditions. This approach improves the physical interpretability of surrogate models while achieving high computational efficiency and accuracy. However, the performance and efficiency of PC$^2$ can still be degraded with high-dimensional parameter spaces, limited data availability, or unrepresentative training data. To address this problem, this study explores two complementary enhancements to the PC$^2$ framework. First, a numerically efficient constrained optimization solver, straightforward updating of Lagrange multipliers (SULM), is adopted as an alternative to the conventional KKT solver. The SULM method significantly reduces computational cost when solving physically constrained problems with high-dimensionality and derivative boundary conditions that require a large number of virtual points. Second, a D-optimal sampling strategy is utilized to select informative virtual points to improve the stability and achieve the balance of accuracy and efficiency of the PC$^2$. The proposed methods are integrated into the PC$^2$ framework and evaluated through numerical examples of representative physical systems governed by ordinary or partial differential equations. The results demonstrate that the enhanced PC$^2$ has better comprehensive capability than standard PC$^2$, and is well-suited for high-dimensional uncertainty quantification tasks.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10873v1",
        "pdf": "https://arxiv.org/pdf/2512.10873v1"
      },
      "arxiv_id": "2512.10873v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10867v1",
      "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
      "authors": [
        "Zongzhao Li",
        "Xiangzhe Kong",
        "Jiahui Su",
        "Zongyang Ma",
        "Mingze Li",
        "Songyou Li",
        "Yuelin Zhang",
        "Yu Rong",
        "Tingyang Xu",
        "Deli Zhao",
        "Wenbing Huang"
      ],
      "abstract": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10867v1",
        "pdf": "https://arxiv.org/pdf/2512.10867v1"
      },
      "arxiv_id": "2512.10867v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10866v1",
      "title": "UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting",
      "authors": [
        "Ruslan Gokhman"
      ],
      "abstract": "We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10866v1",
        "pdf": "https://arxiv.org/pdf/2512.10866v1"
      },
      "arxiv_id": "2512.10866v1",
      "comment": "NeurIPS 2025 Workshop UrbanAI",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10863v1",
      "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
      "authors": [
        "Jingli Lin",
        "Runsen Xu",
        "Shaohao Zhu",
        "Sihan Yang",
        "Peizhou Cao",
        "Yunlong Ran",
        "Miao Hu",
        "Chenming Zhu",
        "Yiman Xie",
        "Yilin Long",
        "Wenbo Hu",
        "Dahua Lin",
        "Tai Wang",
        "Jiangmiao Pang"
      ],
      "abstract": "Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10863v1",
        "pdf": "https://arxiv.org/pdf/2512.10863v1"
      },
      "arxiv_id": "2512.10863v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10860v1",
      "title": "SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation",
      "authors": [
        "Kehong Gong",
        "Zhengyu Wen",
        "Mingxi Xu",
        "Weixia He",
        "Qi Wang",
        "Ning Zhang",
        "Zhengyu Li",
        "Chenbin Li",
        "Dongze Lian",
        "Wei Zhao",
        "Xiaoyu He",
        "Mingyuan Zhang"
      ],
      "abstract": "Despite significant progress in 4D content generation, the conversion of monocular videos into high-quality animated 3D assets with explicit 4D meshes remains considerably challenging. The scarcity of large-scale, naturally captured 4D mesh datasets further limits the ability to train generalizable video-to-4D models from scratch in a purely data-driven manner. Meanwhile, advances in image-to-3D generation, supported by extensive datasets, offer powerful prior models that can be leveraged. To better utilize these priors while minimizing reliance on 4D supervision, we introduce SWiT-4D, a Sliding-Window Transformer for lossless, parameter-free temporal 4D mesh generation. SWiT-4D integrates seamlessly with any Diffusion Transformer (DiT)-based image-to-3D generator, adding spatial-temporal modeling across video frames while preserving the original single-image forward process, enabling 4D mesh reconstruction from videos of arbitrary length. To recover global translation, we further introduce an optimization-based trajectory module tailored for static-camera monocular videos. SWiT-4D demonstrates strong data efficiency: with only a single short (<10s) video for fine-tuning, it achieves high-fidelity geometry and stable temporal consistency, indicating practical deployability under extremely limited 4D supervision. Comprehensive experiments on both in-domain zoo-test sets and challenging out-of-domain benchmarks (C4D, Objaverse, and in-the-wild videos) show that SWiT-4D consistently outperforms existing baselines in temporal smoothness. Project page: https://animotionlab.github.io/SWIT4D/",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10860v1",
        "pdf": "https://arxiv.org/pdf/2512.10860v1"
      },
      "arxiv_id": "2512.10860v1",
      "comment": "Project page: https://animotionlab.github.io/SWIT4D/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10858v1",
      "title": "Scaling Behavior of Discrete Diffusion Language Models",
      "authors": [
        "Dimitri von Rütte",
        "Janis Fluri",
        "Omead Pooladzandi",
        "Bernhard Schölkopf",
        "Thomas Hofmann",
        "Antonio Orvieto"
      ],
      "abstract": "Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.\n  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for $10^{22}$ FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10858v1",
        "pdf": "https://arxiv.org/pdf/2512.10858v1"
      },
      "arxiv_id": "2512.10858v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10857v1",
      "title": "Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic Interpolants",
      "authors": [
        "Chirag Modi",
        "Jiequn Han",
        "Eric Vanden-Eijnden",
        "Joan Bruna"
      ],
      "abstract": "Transport-based methods have emerged as a leading paradigm for building generative models from large, clean datasets. However, in many scientific and engineering domains, clean data are often unavailable: instead, we only observe measurements corrupted through a noisy, ill-conditioned channel. A generative model for the original data thus requires solving an inverse problem at the level of distributions. In this work, we introduce a novel approach to this task based on Stochastic Interpolants: we iteratively update a transport map between corrupted and clean data samples using only access to the corrupted dataset as well as black box access to the corruption channel. Under appropriate conditions, this iterative procedure converges towards a self-consistent transport map that effectively inverts the corruption channel, thus enabling a generative model for the clean data. We refer to the resulting method as the self-consistent stochastic interpolant (SCSI). It (i) is computationally efficient compared to variational alternatives, (ii) highly flexible, handling arbitrary nonlinear forward models with only black-box access, and (iii) enjoys theoretical guarantees. We demonstrate superior performance on inverse problems in natural image processing and scientific reconstruction, and establish convergence guarantees of the scheme under appropriate assumptions.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10857v1",
        "pdf": "https://arxiv.org/pdf/2512.10857v1"
      },
      "arxiv_id": "2512.10857v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10849v1",
      "title": "Bayesian Symbolic Regression via Posterior Sampling",
      "authors": [
        "Geoffrey F. Bomarito",
        "Patrick E. Leser"
      ],
      "abstract": "Symbolic regression is a powerful tool for discovering governing equations directly from data, but its sensitivity to noise hinders its broader application. This paper introduces a Sequential Monte Carlo (SMC) framework for Bayesian symbolic regression that approximates the posterior distribution over symbolic expressions, enhancing robustness and enabling uncertainty quantification for symbolic regression in the presence of noise. Differing from traditional genetic programming approaches, the SMC-based algorithm combines probabilistic selection, adaptive tempering, and the use of normalized marginal likelihood to efficiently explore the search space of symbolic expressions, yielding parsimonious expressions with improved generalization. When compared to standard genetic programming baselines, the proposed method better deals with challenging, noisy benchmark datasets. The reduced tendency to overfit and enhanced ability to discover accurate and interpretable equations paves the way for more robust symbolic regression in scientific discovery and engineering design applications.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10849v1",
        "pdf": "https://arxiv.org/pdf/2512.10849v1"
      },
      "arxiv_id": "2512.10849v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10840v1",
      "title": "PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning",
      "authors": [
        "Jianqi Chen",
        "Biao Zhang",
        "Xiangjun Tang",
        "Peter Wonka"
      ],
      "abstract": "6D object pose estimation, which predicts the transformation of an object relative to the camera, remains challenging for unseen objects. Existing approaches typically rely on explicitly constructing feature correspondences between the query image and either the object model or template images. In this work, we propose PoseGAM, a geometry-aware multi-view framework that directly predicts object pose from a query image and multiple template images, eliminating the need for explicit matching. Built upon recent multi-view-based foundation model architectures, the method integrates object geometry information through two complementary mechanisms: explicit point-based geometry and learned features from geometry representation networks. In addition, we construct a large-scale synthetic dataset containing more than 190k objects under diverse environmental conditions to enhance robustness and generalization. Extensive evaluations across multiple benchmarks demonstrate our state-of-the-art performance, yielding an average AR improvement of 5.1% over prior methods and achieving up to 17.6% gains on individual datasets, indicating strong generalization to unseen objects. Project page: https://windvchen.github.io/PoseGAM/ .",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10840v1",
        "pdf": "https://arxiv.org/pdf/2512.10840v1"
      },
      "arxiv_id": "2512.10840v1",
      "comment": "Project page: https://windvchen.github.io/PoseGAM/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10835v1",
      "title": "Learning Controllable and Diverse Player Behaviors in Multi-Agent Environments",
      "authors": [
        "Atahan Cilan",
        "Atay Özgövde"
      ],
      "abstract": "This paper introduces a reinforcement learning framework that enables controllable and diverse player behaviors without relying on human gameplay data. Existing approaches often require large-scale player trajectories, train separate models for different player types, or provide no direct mapping between interpretable behavioral parameters and the learned policy, limiting their scalability and controllability. We define player behavior in an N-dimensional continuous space and uniformly sample target behavior vectors from a region that encompasses the subset representing real human styles. During training, each agent receives both its current and target behavior vectors as input, and the reward is based on the normalized reduction in distance between them. This allows the policy to learn how actions influence behavioral statistics, enabling smooth control over attributes such as aggressiveness, mobility, and cooperativeness. A single PPO-based multi-agent policy can reproduce new or unseen play styles without retraining. Experiments conducted in a custom multi-player Unity game show that the proposed framework produces significantly greater behavioral diversity than a win-only baseline and reliably matches specified behavior vectors across diverse targets. The method offers a scalable solution for automated playtesting, game balancing, human-like behavior simulation, and replacing disconnected players in online games.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10835v1",
        "pdf": "https://arxiv.org/pdf/2512.10835v1"
      },
      "arxiv_id": "2512.10835v1",
      "comment": "Submitted to IEEE Transactions on Games",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10825v1",
      "title": "An Elementary Proof of the Near Optimality of LogSumExp Smoothing",
      "authors": [
        "Thabo Samakhoana",
        "Benjamin Grimmer"
      ],
      "abstract": "We consider the design of smoothings of the (coordinate-wise) max function in $\\mathbb{R}^d$ in the infinity norm. The LogSumExp function $f(x)=\\ln(\\sum^d_i\\exp(x_i))$ provides a classical smoothing, differing from the max function in value by at most $\\ln(d)$. We provide an elementary construction of a lower bound, establishing that every overestimating smoothing of the max function must differ by at least $\\sim 0.8145\\ln(d)$. Hence, LogSumExp is optimal up to constant factors. However, in small dimensions, we provide stronger, exactly optimal smoothings attaining our lower bound, showing that the entropy-based LogSumExp approach to smoothing is not exactly optimal.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "math.ST",
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "math.ST",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10825v1",
        "pdf": "https://arxiv.org/pdf/2512.10825v1"
      },
      "arxiv_id": "2512.10825v1",
      "comment": "10 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10822v1",
      "title": "V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions",
      "authors": [
        "Mumuksh Tayal",
        "Manan Tayal",
        "Aditya Singh",
        "Shishir Kolathaya",
        "Ravi Prakash"
      ],
      "abstract": "Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10822v1",
        "pdf": "https://arxiv.org/pdf/2512.10822v1"
      },
      "arxiv_id": "2512.10822v1",
      "comment": "23 pages, 8 figure, 7 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10821v1",
      "title": "Agile Deliberation: Concept Deliberation for Subjective Visual Classification",
      "authors": [
        "Leijie Wang",
        "Otilia Stretcu",
        "Wei Qiao",
        "Thomas Denby",
        "Krishnamurthy Viswanathan",
        "Enming Luo",
        "Chun-Ta Lu",
        "Tushar Dogra",
        "Ranjay Krishna",
        "Ariel Fuxman"
      ],
      "abstract": "From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through \"concept deliberation\", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called \"Agile Deliberation\" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10821v1",
        "pdf": "https://arxiv.org/pdf/2512.10821v1"
      },
      "arxiv_id": "2512.10821v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10819v1",
      "title": "Deep sets and event-level maximum-likelihood estimation for fast pile-up jet rejection in ATLAS",
      "authors": [
        "Mohammed Aboelela"
      ],
      "abstract": "Multiple proton-proton collisions (pile-up) occur at every bunch crossing at the LHC, with the mean number of interactions expected to reach 80 during Run 3 and up to 200 at the High-Luminosity LHC. As a direct consequence, events with multijet signatures will occur at increasingly high rates. To cope with the increased luminosity, being able to efficiently group jets according to their origin along the beamline is crucial, particularly at the trigger level. In this work, a novel uncertainty-aware jet regression model based on a Deep Sets architecture is introduced, DIPz, to regress on a jet origin position along the beamline. The inputs to the DIPz algorithm are the charged particle tracks associated to each jet. An event-level discriminant, the Maximum Log Product of Likelihoods (MLPL), is constructed by combining the DIPz per-jet predictions. MLPL is cut-optimized to select events compatible with targeted multi-jet signature selection. This combined approach provides a robust and computationally efficient method for pile-up rejection in multi-jet final states, applicable to real-time event selections at the ATLAS High Level Trigger.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "hep-ex",
        "cs.LG"
      ],
      "primary_category": "hep-ex",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10819v1",
        "pdf": "https://arxiv.org/pdf/2512.10819v1"
      },
      "arxiv_id": "2512.10819v1",
      "comment": "6 pages, 3 figures, European Physical Society Conference on High Energy Physics (EPS-HEP2025), On behalf of the ATLAS Collaboration",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10818v1",
      "title": "Self-Ensemble Post Learning for Noisy Domain Generalization",
      "authors": [
        "Wang Lu",
        "Jindong Wang"
      ],
      "abstract": "While computer vision and machine learning have made great progress, their robustness is still challenged by two key issues: data distribution shift and label noise. When domain generalization (DG) encounters noise, noisy labels further exacerbate the emergence of spurious features in deep layers, i.e. spurious feature enlargement, leading to a degradation in the performance of existing algorithms. This paper, starting from domain generalization, explores how to make existing methods rework when meeting noise. We find that the latent features inside the model have certain discriminative capabilities, and different latent features focus on different parts of the image. Based on these observations, we propose the Self-Ensemble Post Learning approach (SEPL) to diversify features which can be leveraged. Specifically, SEPL consists of two parts: feature probing training and prediction ensemble inference. It leverages intermediate feature representations within the model architecture, training multiple probing classifiers to fully exploit the capabilities of pre-trained models, while the final predictions are obtained through the integration of outputs from these diverse classification heads. Considering the presence of noisy labels, we employ semi-supervised algorithms to train probing classifiers. Given that different probing classifiers focus on different areas, we integrate their predictions using a crowdsourcing inference approach. Extensive experimental evaluations demonstrate that the proposed method not only enhances the robustness of existing methods but also exhibits significant potential for real-world applications with high flexibility.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10818v1",
        "pdf": "https://arxiv.org/pdf/2512.10818v1"
      },
      "arxiv_id": "2512.10818v1",
      "comment": "18 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10817v1",
      "title": "Extrapolation of Periodic Functions Using Binary Encoding of Continuous Numerical Values",
      "authors": [
        "Brian P. Powell",
        "Jordan A. Caraballo-Vega",
        "Mark L. Carroll",
        "Thomas Maxwell",
        "Andrew Ptak",
        "Greg Olmschenk",
        "Jorge Martinez-Palomera"
      ],
      "abstract": "We report the discovery that binary encoding allows neural networks to extrapolate periodic functions beyond their training bounds. We introduce Normalized Base-2 Encoding (NB2E) as a method for encoding continuous numerical values and demonstrate that, using this input encoding, vanilla multi-layer perceptrons (MLP) successfully extrapolate diverse periodic signals without prior knowledge of their functional form. Internal activation analysis reveals that NB2E induces bit-phase representations, enabling MLPs to learn and extrapolate signal structure independently of position.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10817v1",
        "pdf": "https://arxiv.org/pdf/2512.10817v1"
      },
      "arxiv_id": "2512.10817v1",
      "comment": "Submitted to JMLR, under review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10813v1",
      "title": "Quantum Approaches to Urban Logistics: From Core QAOA to Clustered Scalability",
      "authors": [
        "F. Picariello",
        "G. Turati",
        "R. Antonelli",
        "I. Bailo",
        "S. Bonura",
        "G. Ciarfaglia",
        "S. Cipolla",
        "P. Cremonesi",
        "M. Ferrari Dacrema",
        "M. Gabusi",
        "I. Gentile",
        "V. Morreale",
        "A. Noto"
      ],
      "abstract": "The Traveling Salesman Problem (TSP) is a fundamental challenge in combinatorial optimization, widely applied in logistics and transportation. As the size of TSP instances grows, traditional algorithms often struggle to produce high-quality solutions within reasonable timeframes. This study investigates the potential of the Quantum Approximate Optimization Algorithm (QAOA), a hybrid quantum-classical method, to solve TSP under realistic constraints. We adopt a QUBO-based formulation of TSP that integrates real-world logistical constraints reflecting operational conditions, such as vehicle capacity, road accessibility, and time windows, while ensuring compatibility with the limitations of current quantum hardware. Our experiments are conducted in a simulated environment using high-performance computing (HPC) resources to assess QAOA's performance across different problem sizes and quantum circuit depths. In order to improve scalability, we propose clustering QAOA (Cl-QAOA), a hybrid approach combining classical machine learning with QAOA. This method decomposes large TSP instances into smaller sub-problems, making quantum optimization feasible even on devices with a limited number of qubits. The results offer a comprehensive evaluation of QAOA's strengths and limitations in solving constrained TSP scenarios. This study advances quantum optimization and lays groundwork for future large-scale applications.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10813v1",
        "pdf": "https://arxiv.org/pdf/2512.10813v1"
      },
      "arxiv_id": "2512.10813v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10808v1",
      "title": "Graph Laplacian Transformer with Progressive Sampling for Prostate Cancer Grading",
      "authors": [
        "Masum Shah Junayed",
        "John Derek Van Vessem",
        "Qian Wan",
        "Gahie Nam",
        "Sheida Nabavi"
      ],
      "abstract": "Prostate cancer grading from whole-slide images (WSIs) remains a challenging task due to the large-scale nature of WSIs, the presence of heterogeneous tissue structures, and difficulty of selecting diagnostically relevant regions. Existing approaches often rely on random or static patch selection, leading to the inclusion of redundant or non-informative regions that degrade performance. To address this, we propose a Graph Laplacian Attention-Based Transformer (GLAT) integrated with an Iterative Refinement Module (IRM) to enhance both feature learning and spatial consistency. The IRM iteratively refines patch selection by leveraging a pretrained ResNet50 for local feature extraction and a foundation model in no-gradient mode for importance scoring, ensuring only the most relevant tissue regions are preserved. The GLAT models tissue-level connectivity by constructing a graph where patches serve as nodes, ensuring spatial consistency through graph Laplacian constraints and refining feature representations via a learnable filtering mechanism that enhances discriminative histological structures. Additionally, a convex aggregation mechanism dynamically adjusts patch importance to generate a robust WSI-level representation. Extensive experiments on five public and one private dataset demonstrate that our model outperforms state-of-the-art methods, achieving higher performance and spatial consistency while maintaining computational efficiency.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10808v1",
        "pdf": "https://arxiv.org/pdf/2512.10808v1"
      },
      "arxiv_id": "2512.10808v1",
      "comment": "International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10807v1",
      "title": "HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition",
      "authors": [
        "Wang Lu",
        "Yao Zhu",
        "Jindong Wang"
      ],
      "abstract": "Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10807v1",
        "pdf": "https://arxiv.org/pdf/2512.10807v1"
      },
      "arxiv_id": "2512.10807v1",
      "comment": "18 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10805v1",
      "title": "Interpretable and Steerable Concept Bottleneck Sparse Autoencoders",
      "authors": [
        "Akshay Kulkarni",
        "Tsui-Wei Weng",
        "Vivek Narayanaswamy",
        "Shusen Liu",
        "Wesam A. Sakla",
        "Kowshik Thopalli"
      ],
      "abstract": "Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10805v1",
        "pdf": "https://arxiv.org/pdf/2512.10805v1"
      },
      "arxiv_id": "2512.10805v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10794v1",
      "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
      "authors": [
        "Jaskirat Singh",
        "Xingjian Leng",
        "Zongze Wu",
        "Liang Zheng",
        "Richard Zhang",
        "Eli Shechtman",
        "Saining Xie"
      ],
      "abstract": "Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \\textit{global} \\revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \\emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10794v1",
        "pdf": "https://arxiv.org/pdf/2512.10794v1"
      },
      "arxiv_id": "2512.10794v1",
      "comment": "Project page: https://end2end-diffusion.github.io/irepa",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10793v1",
      "title": "LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification",
      "authors": [
        "Michael Schlee",
        "Christoph Weisser",
        "Timo Kivimäki",
        "Melchizedek Mashiku",
        "Benjamin Saefken"
      ],
      "abstract": "LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10793v1",
        "pdf": "https://arxiv.org/pdf/2512.10793v1"
      },
      "arxiv_id": "2512.10793v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10791v1",
      "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
      "authors": [
        "Aileen Cheng",
        "Alon Jacovi",
        "Amir Globerson",
        "Ben Golan",
        "Charles Kwong",
        "Chris Alberti",
        "Connie Tao",
        "Eyal Ben-David",
        "Gaurav Singh Tomar",
        "Lukas Haas",
        "Yonatan Bitton",
        "Adam Bloniarz",
        "Aijun Bai",
        "Andrew Wang",
        "Anfal Siddiqui",
        "Arturo Bajuelos Castillo",
        "Aviel Atias",
        "Chang Liu",
        "Corey Fry",
        "Daniel Balle",
        "Deepanway Ghosal",
        "Doron Kukliansky",
        "Dror Marcus",
        "Elena Gribovskaya",
        "Eran Ofek",
        "Honglei Zhuang",
        "Itay Laish",
        "Jan Ackermann",
        "Lily Wang",
        "Meg Risdal",
        "Megan Barnes",
        "Michael Fink",
        "Mohamed Amin",
        "Moran Ambar",
        "Natan Potikha",
        "Nikita Gupta",
        "Nitzan Katz",
        "Noam Velan",
        "Ofir Roval",
        "Ori Ram",
        "Polina Zablotskaia",
        "Prathamesh Bang",
        "Priyanka Agrawal",
        "Rakesh Ghiya",
        "Sanjay Ganapathy",
        "Simon Baumgartner",
        "Sofia Erell",
        "Sushant Prakash",
        "Thibault Sellam",
        "Vikram Rao",
        "Xuanhui Wang",
        "Yaroslav Akulov",
        "Yulong Yang",
        "Zhen Yang",
        "Zhixin Lai",
        "Zhongru Wu",
        "Anca Dragan",
        "Avinatan Hassidim",
        "Fernando Pereira",
        "Slav Petrov",
        "Srinivasan Venkatachary",
        "Tulsee Doshi",
        "Yossi Matias",
        "Sasha Goldshtein",
        "Dipanjan Das"
      ],
      "abstract": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10791v1",
        "pdf": "https://arxiv.org/pdf/2512.10791v1"
      },
      "arxiv_id": "2512.10791v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10789v1",
      "title": "Natural Language Interface for Firewall Configuration",
      "authors": [
        "F. Taghiyev",
        "A. Aslanbayli"
      ],
      "abstract": "This paper presents the design and prototype implementation of a natural language interface for configuring enterprise firewalls. The framework allows administrators to express access control policies in plain language, which are then translated into vendor specific configurations. A compact schema bound intermediate representation separates human intent from device syntax and in the current prototype compiles to Palo Alto PAN OS command line configuration while remaining extensible to other platforms. Large language models are used only as assistive parsers that generate typed intermediate representation objects, while compilation and enforcement remain deterministic. The prototype integrates three validation layers, namely a static linter that checks structural and vendor specific constraints, a safety gate that blocks overly permissive rules such as any to any allows, and a Batfish based simulator that validates configuration syntax and referential integrity against a synthetic device model. The paper describes the architecture, implementation, and test methodology on synthetic network context datasets and discusses how this approach can evolve into a scalable auditable and human centered workflow for firewall policy management.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10789v1",
        "pdf": "https://arxiv.org/pdf/2512.10789v1"
      },
      "arxiv_id": "2512.10789v1",
      "comment": "7 pages, 3 figures. Preliminary version of an ongoing research project on natural language interfaces for firewall configuration",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10787v1",
      "title": "Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly",
      "authors": [
        "Moshe Lahmy",
        "Roi Yozevitch"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \\textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \\textbf{context dilution}, where distractors crowd out relevant information. We propose \\textbf{SEAL-RAG}, a training-free controller that adopts a \\textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\\textbf{S}earch $\\rightarrow$ \\textbf{E}xtract $\\rightarrow$ \\textbf{A}ssess $\\rightarrow$ \\textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \\textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \\textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \\textbf{HotpotQA} and \\textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \\textbf{+3--13 pp} and evidence precision by \\textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \\textbf{+8.0 pp} in accuracy and maintains \\textbf{96\\%} evidence precision compared to 22\\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10787v1",
        "pdf": "https://arxiv.org/pdf/2512.10787v1"
      },
      "arxiv_id": "2512.10787v1",
      "comment": "24 pages, 2 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10785v1",
      "title": "Developing and Evaluating a Large Language Model-Based Automated Feedback System Grounded in Evidence-Centered Design for Supporting Physics Problem Solving",
      "authors": [
        "Holger Maus",
        "Paul Tschisgale",
        "Fabian Kieser",
        "Stefan Petersen",
        "Peter Wulff"
      ],
      "abstract": "Generative AI offers new opportunities for individualized and adaptive learning, particularly through large language model (LLM)-based feedback systems. While LLMs can produce effective feedback for relatively straightforward conceptual tasks, delivering high-quality feedback for tasks that require advanced domain expertise, such as physics problem solving, remains a substantial challenge. This study presents the design of an LLM-based feedback system for physics problem solving grounded in evidence-centered design (ECD) and evaluates its performance within the German Physics Olympiad. Participants assessed the usefulness and accuracy of the generated feedback, which was generally perceived as useful and highly accurate. However, an in-depth analysis revealed that the feedback contained factual errors in 20% of cases; errors that often went unnoticed by the students. We discuss the risks associated with uncritical reliance on LLM-based feedback systems and outline potential directions for generating more adaptive and reliable LLM-based feedback in the future.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "physics.ed-ph",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "physics.ed-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10785v1",
        "pdf": "https://arxiv.org/pdf/2512.10785v1"
      },
      "arxiv_id": "2512.10785v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10780v1",
      "title": "Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting",
      "authors": [
        "Manurag Khullar",
        "Utkarsh Desai",
        "Poorva Malviya",
        "Aman Dalmia",
        "Zheyuan Ryan Shi"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10780v1",
        "pdf": "https://arxiv.org/pdf/2512.10780v1"
      },
      "arxiv_id": "2512.10780v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10772v1",
      "title": "Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation",
      "authors": [
        "Kevin Glocker",
        "Kätriin Kukk",
        "Romina Oji",
        "Marcel Bollmann",
        "Marco Kuhlmann",
        "Jenny Kunz"
      ],
      "abstract": "Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10772v1",
        "pdf": "https://arxiv.org/pdf/2512.10772v1"
      },
      "arxiv_id": "2512.10772v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10770v1",
      "title": "Template-Free Retrosynthesis with Graph-Prior Augmented Transformers",
      "authors": [
        "Youjun Zhao"
      ],
      "abstract": "Retrosynthesis reaction prediction seeks to infer plausible reactant molecules for a given product and is a central problem in computer-aided organic synthesis. Despite recent progress, many existing models still fall short of the accuracy and robustness required for practical deployment. This work studies a template-free, Transformer-based framework that eliminates reliance on handcrafted reaction templates or additional chemical rule engines. The model injects molecular graph information into the attention mechanism to jointly exploit \\SMILES\\ sequences and structural cues, and further applies a paired data augmentation strategy to enhance training diversity and scale. On the USPTO-50K benchmark, our proposed approach achieves state-of-the-art performance among template-free methods and substantially outperforming a vanilla Transformer baseline.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10770v1",
        "pdf": "https://arxiv.org/pdf/2512.10770v1"
      },
      "arxiv_id": "2512.10770v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10765v1",
      "title": "Blood Pressure Prediction for Coronary Artery Disease Diagnosis using Coronary Computed Tomography Angiography",
      "authors": [
        "Rene Lisasi",
        "Michele Esposito",
        "Chen Zhao"
      ],
      "abstract": "Computational fluid dynamics (CFD) based simulation of coronary blood flow provides valuable hemodynamic markers, such as pressure gradients, for diagnosing coronary artery disease (CAD). However, CFD is computationally expensive, time-consuming, and difficult to integrate into large-scale clinical workflows. These limitations restrict the availability of labeled hemodynamic data for training AI models and hinder broad adoption of non-invasive, physiology based CAD assessment. To address these challenges, we develop an end to end pipeline that automates coronary geometry extraction from coronary computed tomography angiography (CCTA), streamlines simulation data generation, and enables efficient learning of coronary blood pressure distributions. The pipeline reduces the manual burden associated with traditional CFD workflows while producing consistent training data. We further introduce a diffusion-based regression model designed to predict coronary blood pressure directly from CCTA derived features, bypassing the need for slow CFD computation during inference. Evaluated on a dataset of simulated coronary hemodynamics, the proposed model achieves state of the art performance, with an R2 of 64.42%, a root mean squared error of 0.0974, and a normalized RMSE of 0.154, outperforming several baseline approaches. This work provides a scalable and accessible framework for rapid, non-invasive blood pressure prediction to support CAD diagnosis.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10765v1",
        "pdf": "https://arxiv.org/pdf/2512.10765v1"
      },
      "arxiv_id": "2512.10765v1",
      "comment": "19 pages, 9 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10758v1",
      "title": "Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework",
      "authors": [
        "Kaihua Ding"
      ],
      "abstract": "The rapid adoption of generative AI has undermined traditional modular assessments in computing education, creating a disconnect between academic evaluation and industry practice. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and multi-year empirical validation.\n  We make three contributions. First, we establish two theoretical results: (1) assessments composed of interconnected problems, where outputs feed into subsequent stages, are more AI-resilient than modular assessments because current language models struggle with sustained multi-step reasoning and context; and (2) semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution patterns. These results challenge common policy and institutional guidance that promotes open-ended assessments as the primary safeguard for academic integrity.\n  Second, we validate these results using data from four university data science courses (N = 138). While students achieve near-perfect scores on AI-assisted modular homework, performance drops by roughly 30 percentage points on proctored exams, indicating substantial AI score inflation. Interconnected projects remain strongly correlated with modular assessments, suggesting they measure the same underlying skills while resisting AI misuse. Proctored exams show weaker alignment, implying they may assess test-taking ability rather than intended learning outcomes.\n  Third, we translate these findings into a practical assessment design framework. The proposed approach enables educators to create assessments that promote integrative thinking, reflect real-world AI-augmented workflows, and naturally resist trivial delegation to generative AI, thereby helping restore academic integrity.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10758v1",
        "pdf": "https://arxiv.org/pdf/2512.10758v1"
      },
      "arxiv_id": "2512.10758v1",
      "comment": "8 pages, 3 figures and 3 tables, under submission to IEEE FIE",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10756v1",
      "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
      "authors": [
        "Zijian Wu",
        "Lingkai Kong",
        "Wenwei Zhang",
        "Songyang Gao",
        "Yuzhe Gu",
        "Zhongrui Cai",
        "Tianyou Ma",
        "Yuhong Liu",
        "Zhi Wang",
        "Runyuan Ma",
        "Guangyu Wang",
        "Wei Li",
        "Conghui He",
        "Dahua Lin",
        "Kai Chen"
      ],
      "abstract": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10756v1",
        "pdf": "https://arxiv.org/pdf/2512.10756v1"
      },
      "arxiv_id": "2512.10756v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10750v1",
      "title": "LDP: Parameter-Efficient Fine-Tuning of Multimodal LLM for Medical Report Generation",
      "authors": [
        "Tianyu Zhou",
        "Junyi Tang",
        "Zehui Li",
        "Dahong Qian",
        "Suncheng Xiang"
      ],
      "abstract": "Colonoscopic polyp diagnosis is pivotal for early colorectal cancer detection, yet traditional automated reporting suffers from inconsistencies and hallucinations due to the scarcity of high-quality multimodal medical data. To bridge this gap, we propose LDP, a novel framework leveraging multimodal large language models (MLLMs) for professional polyp diagnosis report generation. Specifically, we curate MMEndo, a multimodal endoscopic dataset comprising expert-annotated colonoscopy image-text pairs. We fine-tune the Qwen2-VL-7B backbone using Parameter-Efficient Fine-Tuning (LoRA) and align it with clinical standards via Direct Preference Optimization (DPO). Extensive experiments show that our LDP outperforms existing baselines on both automated metrics and rigorous clinical expert evaluations (achieving a Physician Score of 7.2/10), significantly reducing training computational costs by 833x compared to full fine-tuning. The proposed solution offers a scalable, clinically viable path for primary healthcare, with additional validation on the IU-XRay dataset confirming its robustness.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10750v1",
        "pdf": "https://arxiv.org/pdf/2512.10750v1"
      },
      "arxiv_id": "2512.10750v1",
      "comment": "Work in progress",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10745v1",
      "title": "PMB-NN: Physiology-Centred Hybrid AI for Personalized Hemodynamic Monitoring from Photoplethysmography",
      "authors": [
        "Yaowen Zhang",
        "Libera Fresiello",
        "Peter H. Veltink",
        "Dirk W. Donker",
        "Ying Wang"
      ],
      "abstract": "Continuous monitoring of blood pressure (BP) and hemodynamic parameters such as peripheral resistance (R) and arterial compliance (C) are critical for early vascular dysfunction detection. While photoplethysmography (PPG) wearables has gained popularity, existing data-driven methods for BP estimation lack interpretability. We advanced our previously proposed physiology-centered hybrid AI method-Physiological Model-Based Neural Network (PMB-NN)-in blood pressure estimation, that unifies deep learning with a 2-element Windkessel based model parameterized by R and C acting as physics constraints. The PMB-NN model was trained in a subject-specific manner using PPG-derived timing features, while demographic information was used to infer an intermediate variable: cardiac output. We validated the model on 10 healthy adults performing static and cycling activities across two days for model's day-to-day robustness, benchmarked against deep learning (DL) models (FCNN, CNN-LSTM, Transformer) and standalone Windkessel based physiological model (PM). Validation was conducted on three perspectives: accuracy, interpretability and plausibility. PMB-NN achieved systolic BP accuracy (MAE: 7.2 mmHg) comparable to DL benchmarks, diastolic performance (MAE: 3.9 mmHg) lower than DL models. However, PMB-NN exhibited higher physiological plausibility than both DL baselines and PM, suggesting that the hybrid architecture unifies and enhances the respective merits of physiological principles and data-driven techniques. Beyond BP, PMB-NN identified R (ME: 0.15 mmHg$\\cdot$s/ml) and C (ME: -0.35 ml/mmHg) during training with accuracy similar to PM, demonstrating that the embedded physiological constraints confer interpretability to the hybrid AI framework. These results position PMB-NN as a balanced, physiologically grounded alternative to purely data-driven approaches for daily hemodynamic monitoring.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "physics.med-ph",
        "cs.LG"
      ],
      "primary_category": "physics.med-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10745v1",
        "pdf": "https://arxiv.org/pdf/2512.10745v1"
      },
      "arxiv_id": "2512.10745v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10740v1",
      "title": "Fast and Robust LRSD-based SAR/ISAR Imaging and Decomposition",
      "authors": [
        "Hamid Reza Hashempour",
        "Majid Moradikia",
        "Hamed Bastami",
        "Ahmed Abdelhadi",
        "Mojtaba Soltanalian"
      ],
      "abstract": "The earlier works in the context of low-rank-sparse-decomposition (LRSD)-driven stationary synthetic aperture radar (SAR) imaging have shown significant improvement in the reconstruction-decomposition process. Neither of the proposed frameworks, however, can achieve satisfactory performance when facing a platform residual phase error (PRPE) arising from the instability of airborne platforms. More importantly, in spite of the significance of real-time processing requirements in remote sensing applications, these prior works have only focused on enhancing the quality of the formed image, not reducing the computational burden. To address these two concerns, this article presents a fast and unified joint SAR imaging framework where the dominant sparse objects and low-rank features of the image background are decomposed and enhanced through a robust LRSD. In particular, our unified algorithm circumvents the tedious task of computing the inverse of large matrices for image formation and takes advantage of the recent advances in constrained quadratic programming to handle the unimodular constraint imposed due to the PRPE. Furthermore, we extend our approach to ISAR autofocusing and imaging. Specifically, due to the intrinsic sparsity of ISAR images, the LRSD framework is essentially tasked with the recovery of a sparse image. Several experiments based on synthetic and real data are presented to validate the superiority of the proposed method in terms of imaging quality and computational cost compared to the state-of-the-art methods.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "eess.IV",
        "eess.SP"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10740v1",
        "pdf": "https://arxiv.org/pdf/2512.10740v1"
      },
      "arxiv_id": "2512.10740v1",
      "comment": "",
      "journal_ref": "IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 1-13, 2022, Art no. 5227413",
      "has_code": false
    },
    {
      "id": "2512.10739v1",
      "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
      "authors": [
        "Songyang Gao",
        "Yuzhe Gu",
        "Zijian Wu",
        "Lingkai Kong",
        "Wenwei Zhang",
        "Zhongrui Cai",
        "Fan Zheng",
        "Tianyou Ma",
        "Junhao Shen",
        "Haiteng Zhao",
        "Duanyang Zhang",
        "Huilun Zhang",
        "Kuikun Liu",
        "Chengqi Lyu",
        "Yanhui Duan",
        "Chiyu Chen",
        "Ningsheng Ma",
        "Jianfei Gao",
        "Han Lyu",
        "Dahua Lin",
        "Kai Chen"
      ],
      "abstract": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \\textbf{O}utcome-based \\textbf{P}rocess \\textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\textsc{\\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10739v1",
        "pdf": "https://arxiv.org/pdf/2512.10739v1"
      },
      "arxiv_id": "2512.10739v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10735v1",
      "title": "LGAN: An Efficient High-Order Graph Neural Network via the Line Graph Aggregation",
      "authors": [
        "Lin Du",
        "Lu Bai",
        "Jincheng Li",
        "Lixin Cui",
        "Hangyuan Du",
        "Lichi Zhang",
        "Yuting Chen",
        "Zhao Li"
      ],
      "abstract": "Graph Neural Networks (GNNs) have emerged as a dominant paradigm for graph classification. Specifically, most existing GNNs mainly rely on the message passing strategy between neighbor nodes, where the expressivity is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Although a number of k-WL-based GNNs have been proposed to overcome this limitation, their computational cost increases rapidly with k, significantly restricting the practical applicability. Moreover, since the k-WL models mainly operate on node tuples, these k-WL-based GNNs cannot retain fine-grained node- or edge-level semantics required by attribution methods (e.g., Integrated Gradients), leading to the less interpretable problem. To overcome the above shortcomings, in this paper, we propose a novel Line Graph Aggregation Network (LGAN), that constructs a line graph from the induced subgraph centered at each node to perform the higher-order aggregation. We theoretically prove that the LGAN not only possesses the greater expressive power than the 2-WL under injective aggregation assumptions, but also has lower time complexity. Empirical evaluations on benchmarks demonstrate that the LGAN outperforms state-of-the-art k-WL-based GNNs, while offering better interpretability.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10735v1",
        "pdf": "https://arxiv.org/pdf/2512.10735v1"
      },
      "arxiv_id": "2512.10735v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10734v1",
      "title": "Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation",
      "authors": [
        "Rebekka Görge",
        "Sujan Sai Gannamaneni",
        "Tabea Naeven",
        "Hammam Abdelwahab",
        "Héctor Allende-Cid",
        "Armin B. Cremers",
        "Lennard Helmer",
        "Michael Mock",
        "Anna Schmitz",
        "Songkai Xue",
        "Elif Yildirir",
        "Maximilian Poretschkin",
        "Stefan Wrobel"
      ],
      "abstract": "Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10734v1",
        "pdf": "https://arxiv.org/pdf/2512.10734v1"
      },
      "arxiv_id": "2512.10734v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10730v1",
      "title": "IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation",
      "authors": [
        "Yuan-Ming Li",
        "Qize Yang",
        "Nan Lei",
        "Shenghao Fu",
        "Ling-An Zeng",
        "Jian-Fang Hu",
        "Xihan Wei",
        "Wei-Shi Zheng"
      ],
      "abstract": "Recent advances in motion-aware large language models have shown remarkable promise for unifying motion understanding and generation tasks. However, these models typically treat understanding and generation separately, limiting the mutual benefits that could arise from interactive feedback between tasks. In this work, we reveal that motion assessment and refinement tasks act as crucial bridges to enable bidirectional knowledge flow between understanding and generation. Leveraging this insight, we propose Interleaved Reasoning for Motion Generation (IRMoGen), a novel paradigm that tightly couples motion generation with assessment and refinement through iterative text-motion dialogue. To realize this, we introduce IRG-MotionLLM, the first model that seamlessly interleaves motion generation, assessment, and refinement to improve generation performance. IRG-MotionLLM is developed progressively with a novel three-stage training scheme, initializing and subsequently enhancing native IRMoGen capabilities. To facilitate this development, we construct an automated data engine to synthesize interleaved reasoning annotations from existing text-motion datasets. Extensive experiments demonstrate that: (i) Assessment and refinement tasks significantly improve text-motion alignment; (ii) Interleaving motion generation, assessment, and refinement steps yields consistent performance gains across training stages; and (iii) IRG-MotionLLM clearly outperforms the baseline model and achieves advanced performance on standard text-to-motion generation benchmarks. Cross-evaluator testing further validates its effectiveness. Code & Data: https://github.com/HumanMLLM/IRG-MotionLLM/tree/main.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10730v1",
        "pdf": "https://arxiv.org/pdf/2512.10730v1"
      },
      "arxiv_id": "2512.10730v1",
      "comment": "25 pages, 16 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10725v1",
      "title": "Video Depth Propagation",
      "authors": [
        "Luigi Piccinelli",
        "Thiemo Wandel",
        "Christos Sakaridis",
        "Wim Abbeloos",
        "Luc Van Gool"
      ],
      "abstract": "Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10725v1",
        "pdf": "https://arxiv.org/pdf/2512.10725v1"
      },
      "arxiv_id": "2512.10725v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10723v1",
      "title": "Generalized Spherical Neural Operators: Green's Function Formulation",
      "authors": [
        "Hao Tang",
        "Hao Chen",
        "Chao Li"
      ],
      "abstract": "Neural operators offer powerful approaches for solving parametric partial differential equations, but extending them to spherical domains remains challenging due to the need to preserve intrinsic geometry while avoiding distortions that break rotational consistency. Existing spherical operators rely on rotational equivariance but often lack the flexibility for real-world complexity. We propose a general operator-design framework based on the designable spherical Green's function and its harmonic expansion, establishing a solid operator-theoretic foundation for spherical learning. Based on this, we propose an absolute and relative position-dependent Green's function that enables flexible balance of equivariance and invariance for real-world modeling. The resulting operator, Green's-function Spherical Neural Operator (GSNO) with a novel spectral learning method, can adapt to anisotropic, constraint-rich systems while retaining spectral efficiency. To exploit GSNO, we develop GSHNet, a hierarchical architecture that combines multi-scale spectral modeling with spherical up-down sampling, enhancing global feature representation. Evaluations on diffusion MRI, shallow water dynamics, and global weather forecasting, GSNO and GSHNet consistently outperform state-of-the-art methods. Our results position GSNO as a principled and general framework for spherical operator learning, bridging rigorous theory with real-world complexity.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10723v1",
        "pdf": "https://arxiv.org/pdf/2512.10723v1"
      },
      "arxiv_id": "2512.10723v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10720v1",
      "title": "Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality",
      "authors": [
        "Lingjing Kong",
        "Shaoan Xie",
        "Guangyi Chen",
        "Yuewen Sun",
        "Xiangchen Song",
        "Eric P. Xing",
        "Kun Zhang"
      ],
      "abstract": "Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10720v1",
        "pdf": "https://arxiv.org/pdf/2512.10720v1"
      },
      "arxiv_id": "2512.10720v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10719v1",
      "title": "SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving",
      "authors": [
        "Peizheng Li",
        "Zhenghao Zhang",
        "David Holtz",
        "Hang Yu",
        "Yutong Yang",
        "Yuzhi Lai",
        "Rui Song",
        "Andreas Geiger",
        "Andreas Zell"
      ],
      "abstract": "End-to-end autonomous driving methods built on vision language models (VLMs) have undergone rapid development driven by their universal visual understanding and strong reasoning capabilities obtained from the large-scale pretraining. However, we find that current VLMs struggle to understand fine-grained 3D spatial relationships which is a fundamental requirement for systems interacting with the physical world. To address this issue, we propose SpaceDrive, a spatial-aware VLM-based driving framework that treats spatial information as explicit positional encodings (PEs) instead of textual digit tokens, enabling joint reasoning over semantic and spatial representations. SpaceDrive employs a universal positional encoder to all 3D coordinates derived from multi-view depth estimation, historical ego-states, and text prompts. These 3D PEs are first superimposed to augment the corresponding 2D visual tokens. Meanwhile, they serve as a task-agnostic coordinate representation, replacing the digit-wise numerical tokens as both inputs and outputs for the VLM. This mechanism enables the model to better index specific visual semantics in spatial reasoning and directly regress trajectory coordinates rather than generating digit-by-digit, thereby enhancing planning accuracy. Extensive experiments validate that SpaceDrive achieves state-of-the-art open-loop performance on the nuScenes dataset and the second-best Driving Score of 78.02 on the Bench2Drive closed-loop benchmark over existing VLM-based methods.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10719v1",
        "pdf": "https://arxiv.org/pdf/2512.10719v1"
      },
      "arxiv_id": "2512.10719v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10715v1",
      "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
      "authors": [
        "Matias Cosarinsky",
        "Nicolas Gaggion",
        "Rodrigo Echeveste",
        "Enzo Ferrante"
      ],
      "abstract": "Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10715v1",
        "pdf": "https://arxiv.org/pdf/2512.10715v1"
      },
      "arxiv_id": "2512.10715v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10713v1",
      "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code",
      "authors": [
        "Itay Dreyfuss",
        "Antonio Abu Nassar",
        "Samuel Ackerman",
        "Axel Ben David",
        "Rami Katan",
        "Orna Raz",
        "Marcel Zalmanovici"
      ],
      "abstract": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10713v1",
        "pdf": "https://arxiv.org/pdf/2512.10713v1"
      },
      "arxiv_id": "2512.10713v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10702v1",
      "title": "COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared to ChatGPT-5 and Junior Operators",
      "authors": [
        "Wei Fang",
        "Chiyao Wang",
        "Wenshuai Ma",
        "Hui Liu",
        "Jianqiang Hu",
        "Xiaona Niu",
        "Yi Chu",
        "Mingming Zhang",
        "Jingxiao Yang",
        "Dongwei Zhang",
        "Zelin Li",
        "Pengyun Liu",
        "Jiawei Zheng",
        "Pengke Zhang",
        "Chaoshi Qin",
        "Wangang Guo",
        "Bin Wang",
        "Yugang Xue",
        "Wei Zhang",
        "Zikuan Wang",
        "Rui Zhu",
        "Yihui Cao",
        "Quanmao Lu",
        "Rui Meng",
        "Yan Li"
      ],
      "abstract": "Background: While intravascular imaging, particularly optical coherence tomography (OCT), improves percutaneous coronary intervention (PCI) outcomes, its interpretation is operator-dependent. General-purpose artificial intelligence (AI) shows promise but lacks domain-specific reliability. We evaluated the performance of CA-GPT, a novel large model deployed on an AI-OCT system, against that of the general-purpose ChatGPT-5 and junior physicians for OCT-guided PCI planning and assessment.\n  Methods: In this single-center analysis of 96 patients who underwent OCT-guided PCI, the procedural decisions generated by the CA-GPT, ChatGPT-5, and junior physicians were compared with an expert-derived procedural record. Agreement was assessed using ten pre-specified metrics across pre-PCI and post-PCI phases.\n  Results: For pre-PCI planning, CA-GPT demonstrated significantly higher median agreement scores (5[IQR 3.75-5]) compared to both ChatGPT-5 (3[2-4], P<0.001) and junior physicians (4[3-4], P<0.001). CA-GPT significantly outperformed ChatGPT-5 across all individual pre-PCI metrics and showed superior performance to junior physicians in stent diameter (90.3% vs. 72.2%, P<0.05) and length selection (80.6% vs. 52.8%, P<0.01). In post-PCI assessment, CA-GPT maintained excellent overall agreement (5[4.75-5]), significantly higher than both ChatGPT-5 (4[4-5], P<0.001) and junior physicians (5[4-5], P<0.05). Subgroup analysis confirmed CA-GPT's robust performance advantage in complex scenarios.\n  Conclusion: The CA-GPT-based AI-OCT system achieved superior decision-making agreement versus a general-purpose large language model and junior physicians across both PCI planning and assessment phases. This approach provides a standardized and reliable method for intravascular imaging interpretation, demonstrating significant potential to augment operator expertise and optimize OCT-guided PCI.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10702v1",
        "pdf": "https://arxiv.org/pdf/2512.10702v1"
      },
      "arxiv_id": "2512.10702v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10701v1",
      "title": "HybridVFL: Disentangled Feature Learning for Edge-Enabled Vertical Federated Multimodal Classification",
      "authors": [
        "Mostafa Anoosha",
        "Zeinab Dehghani",
        "Kuniko Paxton",
        "Koorosh Aslansefat",
        "Dhavalkumar Thakker"
      ],
      "abstract": "Vertical Federated Learning (VFL) offers a privacy-preserving paradigm for Edge AI scenarios like mobile health diagnostics, where sensitive multimodal data reside on distributed, resource-constrained devices. Yet, standard VFL systems often suffer performance limitations due to simplistic feature fusion. This paper introduces HybridVFL, a novel framework designed to overcome this bottleneck by employing client-side feature disentanglement paired with a server-side cross-modal transformer for context-aware fusion. Through systematic evaluation on the multimodal HAM10000 skin lesion dataset, we demonstrate that HybridVFL significantly outperforms standard federated baselines, validating the criticality of advanced fusion mechanisms in robust, privacy-preserving systems.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10701v1",
        "pdf": "https://arxiv.org/pdf/2512.10701v1"
      },
      "arxiv_id": "2512.10701v1",
      "comment": "6 pages, 2 figures, 1 table. Accepted at UCC '25 (IEEE/ACM 18th International Conference on Utility and Cloud Computing), December 1-4, 2025, Nantes, France. DOI to be activated upon final publication",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10698v1",
      "title": "How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning",
      "authors": [
        "Jianbo Wang",
        "Galina Sidorenko",
        "Johan Thunberg"
      ],
      "abstract": "Connected and automated vehicles (CAVs) have the potential to enhance driving safety, for example by enabling safe vehicle following and more efficient traffic scheduling. For such future deployments, safety requirements should be addressed, where the primary such are avoidance of vehicle collisions and substantial mitigating of harm when collisions are unavoidable. However, conservative worst-case-based control strategies come at the price of reduced flexibility and may compromise overall performance. In light of this, we investigate how Deep Reinforcement Learning (DRL) can be leveraged to improve safety in multi-vehicle-following scenarios involving emergency braking. Specifically, we investigate how DRL with vehicle-to-vehicle communication can be used to ethically select an emergency breaking profile in scenarios where overall, or collective, three-vehicle harm reduction or collision avoidance shall be obtained instead of single-vehicle such. As an algorithm, we provide a hybrid approach that combines DRL with a previously published method based on analytical expressions for selecting optimal constant deceleration. By combining DRL with the previous method, the proposed hybrid approach increases the reliability compared to standalone DRL, while achieving superior performance in terms of overall harm reduction and collision avoidance.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10698v1",
        "pdf": "https://arxiv.org/pdf/2512.10698v1"
      },
      "arxiv_id": "2512.10698v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10696v1",
      "title": "Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution",
      "authors": [
        "Zouying Cao",
        "Jiaji Deng",
        "Li Yu",
        "Weikang Zhou",
        "Zhaoyang Liu",
        "Bolin Ding",
        "Hai Zhao"
      ],
      "abstract": "Procedural memory enables large language model (LLM) agents to internalize \"how-to\" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a \"passive accumulation\" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\\textbf{ReMe}$ ($\\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\\texttt{reme.library}$ dataset to facilitate further research.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10696v1",
        "pdf": "https://arxiv.org/pdf/2512.10696v1"
      },
      "arxiv_id": "2512.10696v1",
      "comment": "16 pages, 9 figures, 9 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10691v1",
      "title": "Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning",
      "authors": [
        "Benjamin Gundersen",
        "Nicolas Deperrois",
        "Samuel Ruiperez-Campillo",
        "Thomas M. Sutter",
        "Julia E. Vogt",
        "Michael Moor",
        "Farhad Nooralahzadeh",
        "Michael Krauthammer"
      ],
      "abstract": "Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning (\"thinking\") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10691v1",
        "pdf": "https://arxiv.org/pdf/2512.10691v1"
      },
      "arxiv_id": "2512.10691v1",
      "comment": "10 pages main text (3 figures, 3 tables), 31 pages in total",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10688v1",
      "title": "Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition",
      "authors": [
        "Lingfeng Liu",
        "Yixin Song",
        "Dazhong Shen",
        "Bing Yin",
        "Hao Li",
        "Yanyong Zhang",
        "Chao Wang"
      ],
      "abstract": "Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant \"popularity direction\" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10688v1",
        "pdf": "https://arxiv.org/pdf/2512.10688v1"
      },
      "arxiv_id": "2512.10688v1",
      "comment": "Accepted by SIGKDD 2026(First Cycle)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10687v1",
      "title": "Challenges of Evaluating LLM Safety for User Welfare",
      "authors": [
        "Manon Kempermann",
        "Sai Suresh Macharla Vasu",
        "Mahalakshmi Raveenthiran",
        "Theo Farrell",
        "Ingmar Weber"
      ],
      "abstract": "Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10687v1",
        "pdf": "https://arxiv.org/pdf/2512.10687v1"
      },
      "arxiv_id": "2512.10687v1",
      "comment": "Paper accepted at IASEAI'26; please cite that peer-reviewed version instead",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10685v1",
      "title": "Sharp Monocular View Synthesis in Less Than a Second",
      "authors": [
        "Lars Mescheder",
        "Wei Dong",
        "Shiwei Li",
        "Xuyang Bai",
        "Marcel Santos",
        "Peiyun Hu",
        "Bruno Lecouat",
        "Mingmin Zhen",
        "Amaël Delaunoy",
        "Tian Fang",
        "Yanghai Tsin",
        "Stephan R. Richter",
        "Vladlen Koltun"
      ],
      "abstract": "We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10685v1",
        "pdf": "https://arxiv.org/pdf/2512.10685v1"
      },
      "arxiv_id": "2512.10685v1",
      "comment": "Code and weights available at https://github.com/apple/ml-sharp",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.10683v1",
      "title": "Optimal transport unlocks end-to-end learning for single-molecule localization",
      "authors": [
        "Romain Seailles",
        "Jean-Baptiste Masson",
        "Jean Ponce",
        "Julien Mairal"
      ],
      "abstract": "Single-molecule localization microscopy (SMLM) allows reconstructing biology-relevant structures beyond the diffraction limit by detecting and localizing individual fluorophores -- fluorescent molecules stained onto the observed specimen -- over time to reconstruct super-resolved images. Currently, efficient SMLM requires non-overlapping emitting fluorophores, leading to long acquisition times that hinders live-cell imaging. Recent deep-learning approaches can handle denser emissions, but they rely on variants of non-maximum suppression (NMS) layers, which are unfortunately non-differentiable and may discard true positives with their local fusion strategy. In this presentation, we reformulate the SMLM training objective as a set-matching problem, deriving an optimal-transport loss that eliminates the need for NMS during inference and enables end-to-end training. Additionally, we propose an iterative neural network that integrates knowledge of the microscope's optical system inside our model. Experiments on synthetic benchmarks and real biological data show that both our new loss function and architecture surpass the state of the art at moderate and high emitter densities. Code is available at https://github.com/RSLLES/SHOT.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10683v1",
        "pdf": "https://arxiv.org/pdf/2512.10683v1"
      },
      "arxiv_id": "2512.10683v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10675v1",
      "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
      "authors": [
        "Gemini Robotics Team",
        "Coline Devin",
        "Yilun Du",
        "Debidatta Dwibedi",
        "Ruiqi Gao",
        "Abhishek Jindal",
        "Thomas Kipf",
        "Sean Kirmani",
        "Fangchen Liu",
        "Anirudha Majumdar",
        "Andrew Marmon",
        "Carolina Parada",
        "Yulia Rubanova",
        "Dhruv Shah",
        "Vikas Sindhwani",
        "Jie Tan",
        "Fei Xia",
        "Ted Xiao",
        "Sherry Yang",
        "Wenhao Yu",
        "Allan Zhou"
      ],
      "abstract": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10675v1",
        "pdf": "https://arxiv.org/pdf/2512.10675v1"
      },
      "arxiv_id": "2512.10675v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10674v1",
      "title": "Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching",
      "authors": [
        "Javier Villena Toro",
        "Mehdi Tarkian"
      ],
      "abstract": "Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10674v1",
        "pdf": "https://arxiv.org/pdf/2512.10674v1"
      },
      "arxiv_id": "2512.10674v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.10671v1",
      "title": "AEBNAS: Strengthening Exit Branches in Early-Exit Networks through Hardware-Aware Neural Architecture Search",
      "authors": [
        "Oscar Robben",
        "Saeed Khalilian",
        "Nirvana Meratnia"
      ],
      "abstract": "Early-exit networks are effective solutions for reducing the overall energy consumption and latency of deep learning models by adjusting computation based on the complexity of input data. By incorporating intermediate exit branches into the architecture, they provide less computation for simpler samples, which is particularly beneficial for resource-constrained devices where energy consumption is crucial. However, designing early-exit networks is a challenging and time-consuming process due to the need to balance efficiency and performance. Recent works have utilized Neural Architecture Search (NAS) to design more efficient early-exit networks, aiming to reduce average latency while improving model accuracy by determining the best positions and number of exit branches in the architecture. Another important factor affecting the efficiency and accuracy of early-exit networks is the depth and types of layers in the exit branches. In this paper, we use hardware-aware NAS to strengthen exit branches, considering both accuracy and efficiency during optimization. Our performance evaluation on the CIFAR-10, CIFAR-100, and SVHN datasets demonstrates that our proposed framework, which considers varying depths and layers for exit branches along with adaptive threshold tuning, designs early-exit networks that achieve higher accuracy with the same or lower average number of MACs compared to the state-of-the-art approaches.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10671v1",
        "pdf": "https://arxiv.org/pdf/2512.10671v1"
      },
      "arxiv_id": "2512.10671v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    }
  ]
}