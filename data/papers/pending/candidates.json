{
  "fetched_at": "2026-01-22T00:28:59.001878",
  "total_papers": 100,
  "papers": [
    {
      "id": "2601.14256v1",
      "title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding",
      "authors": [
        "Matthew Gwilliam",
        "Xiao Wang",
        "Xuefeng Hu",
        "Zhenheng Yang"
      ],
      "abstract": "Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14256v1",
        "pdf": "https://arxiv.org/pdf/2601.14256v1"
      },
      "arxiv_id": "2601.14256v1",
      "comment": "18 pages, 16 tables, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14255v1",
      "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
      "authors": [
        "Sangbeom Lim",
        "Seoung Wug Oh",
        "Jiahui Huang",
        "Heeji Yoon",
        "Seungryong Kim",
        "Joon-Young Lee"
      ],
      "abstract": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14255v1",
        "pdf": "https://arxiv.org/pdf/2601.14255v1"
      },
      "arxiv_id": "2601.14255v1",
      "comment": "Project page: https://cvlab-kaist.github.io/VideoMaMa/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.14253v1",
      "title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis",
      "authors": [
        "Hongyuan Chen",
        "Xingyu Chen",
        "Youjia Zhang",
        "Zexiang Xu",
        "Anpei Chen"
      ],
      "abstract": "We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14253v1",
        "pdf": "https://arxiv.org/pdf/2601.14253v1"
      },
      "arxiv_id": "2601.14253v1",
      "comment": "Project page: https://motion3-to-4.github.io/. Code: https://github.com/Inception3D/Motion324",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.14251v1",
      "title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR",
      "authors": [
        "Said Taghadouini",
        "Adrien Cavaillès",
        "Baptiste Aubertin"
      ],
      "abstract": "We present \\textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \\textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14251v1",
        "pdf": "https://arxiv.org/pdf/2601.14251v1"
      },
      "arxiv_id": "2601.14251v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14250v1",
      "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer",
      "authors": [
        "Pengze Zhang",
        "Yanze Wu",
        "Mengtian Li",
        "Xu Bai",
        "Songtao Zhao",
        "Fulong Ye",
        "Chong Mou",
        "Xinghui Li",
        "Zhuowei Chen",
        "Qian He",
        "Mingyuan Gao"
      ],
      "abstract": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14250v1",
        "pdf": "https://arxiv.org/pdf/2601.14250v1"
      },
      "arxiv_id": "2601.14250v1",
      "comment": "Github Page: https://pangzecheung.github.io/OmniTransfer/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.14246v1",
      "title": "Soft Tail-dropping for Adaptive Visual Tokenization",
      "authors": [
        "Zeyuan Chen",
        "Kai Zhang",
        "Zhuowen Tu",
        "Yuanjun Xiong"
      ],
      "abstract": "We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14246v1",
        "pdf": "https://arxiv.org/pdf/2601.14246v1"
      },
      "arxiv_id": "2601.14246v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14243v1",
      "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
      "authors": [
        "Haocheng Xi",
        "Charlie Ruan",
        "Peiyuan Liao",
        "Yujun Lin",
        "Han Cai",
        "Yilong Zhao",
        "Shuo Yang",
        "Kurt Keutzer",
        "Song Han",
        "Ligeng Zhu"
      ],
      "abstract": "Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14243v1",
        "pdf": "https://arxiv.org/pdf/2601.14243v1"
      },
      "arxiv_id": "2601.14243v1",
      "comment": "11 pages, 6 figures, 4 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14242v1",
      "title": "APEX-Agents",
      "authors": [
        "Bertie Vidgen",
        "Austin Mann",
        "Abby Fennelly",
        "John Wright Stanly",
        "Lucas Rothman",
        "Marco Burstein",
        "Julien Benchek",
        "David Ostrofsky",
        "Anirudh Ravichandran",
        "Debnil Sur",
        "Neel Venugopal",
        "Alannah Hsia",
        "Isaac Robinson",
        "Calix Huang",
        "Olivia Varones",
        "Daniyal Khan",
        "Michael Haines",
        "Zach Richards",
        "Chirag Mahapatra",
        "Brendan Foody",
        "Osvald Nitski"
      ],
      "abstract": "We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14242v1",
        "pdf": "https://arxiv.org/pdf/2601.14242v1"
      },
      "arxiv_id": "2601.14242v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14240v1",
      "title": "LRC-DHVC: Towards Local Rate Control in Neural Video Compression",
      "authors": [
        "Marc Windsheimer",
        "Simon Deniffel",
        "André Kaup"
      ],
      "abstract": "Local rate control is a key enabler to generalize image and video compression for dedicated challenges, such as video coding for machines. While traditional hybrid video coding can easily adapt the local rate-distortion trade-off by changing the local quantization parameter, no such approach is currently available for learning-based video compression. In this paper, we propose LRC-DHVC, a hierarchical video compression network, which allows continuous local rate control on a pixel level to vary the spatial quality distribution within individual video frames. This is achieved by concatenating a quality map to the input frame and applying a weighted MSE loss which matches the pixelwise trade-off factors in the quality map. During training, the model sees a variety of quality maps due to a constrained-random generation. Our model is the first neural video compression network, which can continuously and spatially adapt to varying quality constraints. Due to the wide quality and bit rate range, a single set of network parameters is sufficient. Compared to single rate point networks, which scale linearly with the number of rate points, the memory requirements for our network parameters remain constant. The code and model are available at link-updated-upon-acceptance.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "eess.IV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14240v1",
        "pdf": "https://arxiv.org/pdf/2601.14240v1"
      },
      "arxiv_id": "2601.14240v1",
      "comment": "5 pages, 5 figures, 1 table",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14238v1",
      "title": "Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression",
      "authors": [
        "Shaurya Mathur",
        "Shreyas Bellary Manjunath",
        "Nitin Kulkarni",
        "Alina Vereshchaka"
      ],
      "abstract": "Wildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing fires only after they are detected. We introduce \\textit{FireCastRL}, a proactive artificial intelligence (AI) framework that combines wildfire forecasting with intelligent suppression strategies. Our framework first uses a deep spatiotemporal model to predict wildfire ignition. For high-risk predictions, we deploy a pre-trained reinforcement learning (RL) agent to execute real-time suppression tactics with helitack units inside a physics-informed 3D simulation. The framework generates a threat assessment report to help emergency responders optimize resource allocation and planning. In addition, we are publicly releasing a large-scale, spatiotemporal dataset containing $\\mathbf{9.5}$ million samples of environmental variables for wildfire prediction. Our work demonstrates how deep learning and RL can be combined to support both forecasting and tactical wildfire response. More details can be found at https://sites.google.com/view/firecastrl.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14238v1",
        "pdf": "https://arxiv.org/pdf/2601.14238v1"
      },
      "arxiv_id": "2601.14238v1",
      "comment": "6 pages, 5 figures (two of them in tables), Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14235v1",
      "title": "Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration",
      "authors": [
        "LSST Dark Energy Science Collaboration",
        "Eric Aubourg",
        "Camille Avestruz",
        "Matthew R. Becker",
        "Biswajit Biswas",
        "Rahul Biswas",
        "Boris Bolliet",
        "Adam S. Bolton",
        "Clecio R. Bom",
        "Raphaël Bonnet-Guerrini",
        "Alexandre Boucaud",
        "Jean-Eric Campagne",
        "Chihway Chang",
        "Aleksandra Ćiprijanović",
        "Johann Cohen-Tanugi",
        "Michael W. Coughlin",
        "John Franklin Crenshaw",
        "Juan C. Cuevas-Tello",
        "Juan de Vicente",
        "Seth W. Digel",
        "Steven Dillmann",
        "Mariano Javier de León Dominguez Romero",
        "Alex Drlica-Wagner",
        "Sydney Erickson",
        "Alexander T. Gagliano",
        "Christos Georgiou",
        "Aritra Ghosh",
        "Matthew Grayling",
        "Kirill A. Grishin",
        "Alan Heavens",
        "Lindsay R. House",
        "Mustapha Ishak",
        "Wassim Kabalan",
        "Arun Kannawadi",
        "François Lanusse",
        "C. Danielle Leonard",
        "Pierre-François Léget",
        "Michelle Lochner",
        "Yao-Yuan Mao",
        "Peter Melchior",
        "Grant Merz",
        "Martin Millon",
        "Anais Möller",
        "Gautham Narayan",
        "Yuuki Omori",
        "Hiranya Peiris",
        "Laurence Perreault-Levasseur",
        "Andrés A. Plazas Malagón",
        "Nesar Ramachandra",
        "Benjamin Remy",
        "Cécile Roucelle",
        "Jaime Ruiz-Zapatero",
        "Stefan Schuldt",
        "Ignacio Sevilla-Noarbe",
        "Ved G. Shah",
        "Tjitske Starkenburg",
        "Stephen Thorp",
        "Laura Toribio San Cipriano",
        "Tilman Tröster",
        "Roberto Trotta",
        "Padma Venkatraman",
        "Amanda Wasserman",
        "Tim White",
        "Justine Zeghal",
        "Tianqing Zhang",
        "Yuanyuan Zhang"
      ],
      "abstract": "The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification to weak lensing inference and cosmological simulations. Yet their utility for precision cosmology hinges on trustworthy uncertainty quantification, robustness to covariate shift and model misspecification, and reproducible integration within scientific pipelines. This white paper surveys the current landscape of AI/ML across DESC's primary cosmological probes and cross-cutting analyses, revealing that the same core methodologies and fundamental challenges recur across disparate science cases. Since progress on these cross-cutting challenges would benefit multiple probes simultaneously, we identify key methodological research priorities, including Bayesian inference at scale, physics-informed methods, validation frameworks, and active learning for discovery. With an eye on emerging techniques, we also explore the potential of the latest foundation model methodologies and LLM-driven agentic AI systems to reshape DESC workflows, provided their deployment is coupled with rigorous evaluation and governance. Finally, we discuss critical software, computing, data infrastructure, and human capital requirements for the successful deployment of these new methodologies, and consider associated risks and opportunities for broader coordination with external actors.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "astro-ph.IM",
        "astro-ph.CO",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "astro-ph.IM",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14235v1",
        "pdf": "https://arxiv.org/pdf/2601.14235v1"
      },
      "arxiv_id": "2601.14235v1",
      "comment": "84 pages. This is v1.0 of the DESC's white paper on AI/ML, a collaboration document that is being made public but which is not planned for submission to a journal",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14234v1",
      "title": "Q-learning with Adjoint Matching",
      "authors": [
        "Qiyang Li",
        "Sergey Levine"
      ],
      "abstract": "We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14234v1",
        "pdf": "https://arxiv.org/pdf/2601.14234v1"
      },
      "arxiv_id": "2601.14234v1",
      "comment": "32 pages, 8 figures, 7 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14232v1",
      "title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning",
      "authors": [
        "Egor Cherepanov",
        "Daniil Zelezetsky",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
      ],
      "abstract": "Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14232v1",
        "pdf": "https://arxiv.org/pdf/2601.14232v1"
      },
      "arxiv_id": "2601.14232v1",
      "comment": "38 pages, 44 figures, 3 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14230v1",
      "title": "MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems",
      "authors": [
        "Yiyang Wang",
        "Yiqiao Jin",
        "Alex Cabral",
        "Josiah Hester"
      ],
      "abstract": "Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14230v1",
        "pdf": "https://arxiv.org/pdf/2601.14230v1"
      },
      "arxiv_id": "2601.14230v1",
      "comment": "15 pages, 9 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14228v1",
      "title": "Attention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment",
      "authors": [
        "Punit Kumar",
        "Vaibhav Saran",
        "Divyesh Patel",
        "Nitin Kulkarni",
        "Alina Vereshchaka"
      ],
      "abstract": "Sepsis remains one of the leading causes of mortality in intensive care units, where timely and accurate treatment decisions can significantly impact patient outcomes. In this work, we propose an interpretable decision support framework. Our system integrates four core components: (1) a clustering-based stratification module that categorizes patients into low, intermediate, and high-risk groups upon ICU admission, using clustering with statistical validation; (2) a synthetic data augmentation pipeline leveraging variational autoencoders (VAE) and diffusion models to enrich underrepresented trajectories such as fluid or vasopressor administration; (3) an offline reinforcement learning (RL) agent trained using Advantage Weighted Regression (AWR) with a lightweight attention encoder and supported by an ensemble models for conservative, safety-aware treatment recommendations; and (4) a rationale generation module powered by a multi-modal large language model (LLM), which produces natural-language justifications grounded in clinical context and retrieved expert knowledge. Evaluated on the MIMIC-III and eICU datasets, our approach achieves high treatment accuracy while providing clinicians with interpretable and robust policy recommendations.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14228v1",
        "pdf": "https://arxiv.org/pdf/2601.14228v1"
      },
      "arxiv_id": "2601.14228v1",
      "comment": "8 pages, 6 figures, Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14226v1",
      "title": "Deep Learning Approaches to Quantum Error Mitigation",
      "authors": [
        "Leonardo Placidi",
        "Ifan Williams",
        "Enrico Rinaldi",
        "Daniel Mills",
        "Cristina Cîrstoiu",
        "Vanya Eccles",
        "Ross Duncan"
      ],
      "abstract": "We present a systematic investigation of deep learning methods applied to quantum error mitigation of noisy output probability distributions from measured quantum circuits. We compare different architectures, from fully connected neural networks to transformers, and we test different design/training modalities, identifying sequence-to-sequence, attention-based models as the most effective on our datasets. These models consistently produce mitigated distributions that are closer to the ideal outputs when tested on both simulated and real device data obtained from IBM superconducting quantum processing units (QPU) up to five qubits. Across several different circuit depths, our approach outperforms other baseline error mitigation techniques. We perform a series of ablation studies to examine: how different input features (circuit, device properties, noisy output statistics) affect performance; cross-dataset generalization across circuit families; and transfer learning to a different IBM QPU. We observe that generalization performance across similar devices with the same architecture works effectively, without needing to fully retrain models.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14226v1",
        "pdf": "https://arxiv.org/pdf/2601.14226v1"
      },
      "arxiv_id": "2601.14226v1",
      "comment": "48 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14209v1",
      "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning",
      "authors": [
        "Matthew Y. R. Yang",
        "Hao Bai",
        "Ian Wu",
        "Gene Yang",
        "Amrith Setlur",
        "Aviral Kumar"
      ],
      "abstract": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14209v1",
        "pdf": "https://arxiv.org/pdf/2601.14209v1"
      },
      "arxiv_id": "2601.14209v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14208v1",
      "title": "Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting",
      "authors": [
        "Nitin Kulkarni",
        "Akhil Devarashetti",
        "Charlie Cluss",
        "Livio Forte",
        "Dan Buckmaster",
        "Philip Schneider",
        "Chunming Qiao",
        "Alina Vereshchaka"
      ],
      "abstract": "Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14208v1",
        "pdf": "https://arxiv.org/pdf/2601.14208v1"
      },
      "arxiv_id": "2601.14208v1",
      "comment": "8 pages, 9 figures, Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14207v1",
      "title": "Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints",
      "authors": [
        "Rotem Gatenyo",
        "Ohad Fried"
      ],
      "abstract": "We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation -- an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model. Our framework augments language supervision with geometry-aware objectives: a variant of soft-Iterative Closest Point (ICP) term to encourage surface attachment and a penetration loss to discourage interpenetration. A phased schedule strengthens contact constraints over time, and camera control concentrates the optimization on the interaction region. To enable evaluation, we curate a benchmark containing diverse categories and relations, and compare against baselines. Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14207v1",
        "pdf": "https://arxiv.org/pdf/2601.14207v1"
      },
      "arxiv_id": "2601.14207v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14196v1",
      "title": "Differentiated Pickup Point Offering for Emission Reduction in Last-Mile Delivery",
      "authors": [
        "Albina Galiullina",
        "Wouter van Heeswijk",
        "Tom van Woensel"
      ],
      "abstract": "Pickup points are widely recognized as a sustainable alternative to home delivery, as consolidating orders at pickup locations can shorten delivery routes and improve first-attempt success rates. However, these benefits may be negated when customers drive to pick up their orders. This study proposes a Differentiated Pickup Point Offering (DPO) policy that aims to jointly reduce emissions from delivery truck routes and customer travel. Under DPO, each arriving customer is offered a single recommended pickup point, rather than an unrestricted choice among all locations, while retaining the option of home delivery. We study this problem in a dynamic and stochastic setting, where the pickup point offered to each customer depends on previously realized customer locations and delivery choices. To design effective DPO policies, we adopt a reinforcement learning-based approach that accounts for spatial relationships between customers and pickup points and their implications for future route consolidation. Computational experiments show that differentiated pickup point offerings can substantially reduce total carbon emissions. The proposed policies reduce total emissions by up to 9% relative to home-only delivery and by 2% on average compared with alternative policies, including unrestricted pickup point choice and nearest pickup point assignment. Differentiated offerings are particularly effective in dense urban settings with many pickup points and short inter-location distances. Moreover, explicitly accounting for the dynamic nature of customer arrivals and choices is especially important when customers are less inclined to choose pickup point delivery over home delivery.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14196v1",
        "pdf": "https://arxiv.org/pdf/2601.14196v1"
      },
      "arxiv_id": "2601.14196v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14192v1",
      "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
      "authors": [
        "Xiaofang Yang",
        "Lijun Li",
        "Heng Zhou",
        "Tong Zhu",
        "Xiaoye Qu",
        "Yuchen Fan",
        "Qianshan Wei",
        "Rui Ye",
        "Li Kang",
        "Yiran Qin",
        "Zhiqiang Kou",
        "Daizong Liu",
        "Qi Li",
        "Ning Ding",
        "Siheng Chen",
        "Jing Shao"
      ],
      "abstract": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14192v1",
        "pdf": "https://arxiv.org/pdf/2601.14192v1"
      },
      "arxiv_id": "2601.14192v1",
      "comment": "35 pages, 200 references",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14188v1",
      "title": "IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models",
      "authors": [
        "Liang Shi",
        "Wei Li",
        "Kevin M Beussman",
        "Lin Chen",
        "Yun Fu"
      ],
      "abstract": "Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM's efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14188v1",
        "pdf": "https://arxiv.org/pdf/2601.14188v1"
      },
      "arxiv_id": "2601.14188v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14180v1",
      "title": "Progressive self-supervised blind-spot denoising method for LDCT denoising",
      "authors": [
        "Yichao Liu",
        "Yueyang Teng",
        "Junwen Guo"
      ],
      "abstract": "Self-supervised learning is increasingly investigated for low-dose computed tomography (LDCT) image denoising, as it alleviates the dependence on paired normal-dose CT (NDCT) data, which are often difficult to acquire in clinical practice. In this paper, we propose a novel self-supervised training strategy that relies exclusively on LDCT images. We introduce a step-wise blind-spot denoising mechanism that enforces conditional independence in a progressive manner, enabling more fine-grained denoising learning. In addition, we add Gaussian noise to LDCT images, which acts as a regularization and mitigates overfitting. Extensive experiments on the Mayo LDCT dataset demonstrate that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to, or better than, several representative supervised denoising methods.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14180v1",
        "pdf": "https://arxiv.org/pdf/2601.14180v1"
      },
      "arxiv_id": "2601.14180v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14175v1",
      "title": "A model of errors in transformers",
      "authors": [
        "Suvrat Raju",
        "Praneeth Netrapalli"
      ],
      "abstract": "We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "hep-th"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14175v1",
        "pdf": "https://arxiv.org/pdf/2601.14175v1"
      },
      "arxiv_id": "2601.14175v1",
      "comment": "8+17pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14173v1",
      "title": "Penalizing Localized Dirichlet Energies in Low Rank Tensor Products",
      "authors": [
        "Paris A. Karakasis",
        "Nicholas D. Sidiropoulos"
      ],
      "abstract": "We study low-rank tensor-product B-spline (TPBS) models for regression tasks and investigate Dirichlet energy as a measure of smoothness. We show that TPBS models admit a closed-form expression for the Dirichlet energy, and reveal scenarios where perfect interpolation is possible with exponentially small Dirichlet energy. This renders global Dirichlet energy-based regularization ineffective. To address this limitation, we propose a novel regularization strategy based on local Dirichlet energies defined on small hypercubes centered at the training points. Leveraging pretrained TPBS models, we also introduce two estimators for inference from incomplete samples. Comparative experiments with neural networks demonstrate that TPBS models outperform neural networks in the overfitting regime for most datasets, and maintain competitive performance otherwise. Overall, TPBS models exhibit greater robustness to overfitting and consistently benefit from regularization, while neural networks are more sensitive to overfitting and less effective in leveraging regularization.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14173v1",
        "pdf": "https://arxiv.org/pdf/2601.14173v1"
      },
      "arxiv_id": "2601.14173v1",
      "comment": "19 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14172v1",
      "title": "Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum",
      "authors": [
        "Víctor Yeste",
        "Paolo Rosso"
      ],
      "abstract": "We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task (\"does any value appear?\") and show that it is learnable from single sentences (positive-class F1 $\\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14172v1",
        "pdf": "https://arxiv.org/pdf/2601.14172v1"
      },
      "arxiv_id": "2601.14172v1",
      "comment": "Code: https://github.com/VictorMYeste/human-value-detection, 37 pages, 4 figures,",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.14171v1",
      "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance",
      "authors": [
        "Qianli Ma",
        "Chang Guo",
        "Zhiheng Tian",
        "Siyu Wang",
        "Jipeng Xiao",
        "Yuanhao Yue",
        "Zhipeng Zhang"
      ],
      "abstract": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14171v1",
        "pdf": "https://arxiv.org/pdf/2601.14171v1"
      },
      "arxiv_id": "2601.14171v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14165v1",
      "title": "ASBA: A-line State Space Model and B-line Attention for Sparse Optical Doppler Tomography Reconstruction",
      "authors": [
        "Zhenghong Li",
        "Wensheng Cheng",
        "Congwu Du",
        "Yingtian Pan",
        "Zhaozheng Yin",
        "Haibin Ling"
      ],
      "abstract": "Optical Doppler Tomography (ODT) is an emerging blood flow analysis technique. A 2D ODT image (B-scan) is generated by sequentially acquiring 1D depth-resolved raw A-scans (A-line) along the lateral axis (B-line), followed by Doppler phase-subtraction analysis. To ensure high-fidelity B-scan images, current practices rely on dense sampling, which prolongs scanning time, increases storage demands, and limits the capture of rapid blood flow dynamics. Recent studies have explored sparse sampling of raw A-scans to alleviate these limitations, but their effectiveness is hindered by the conservative sampling rates and the uniform modeling of flow and background signals. In this study, we introduce a novel blood flow-aware network, named ASBA (A-line ROI State space model and B-line phase Attention), to reconstruct ODT images from highly sparsely sampled raw A-scans. Specifically, we propose an A-line ROI state space model to extract sparsely distributed flow features along the A-line, and a B-line phase attention to capture long-range flow signals along each B-line based on phase difference. Moreover, we introduce a flow-aware weighted loss function that encourages the network to prioritize the accurate reconstruction of flow signals. Extensive experiments on real animal data demonstrate that the proposed approach clearly outperforms existing state-of-the-art reconstruction methods.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14165v1",
        "pdf": "https://arxiv.org/pdf/2601.14165v1"
      },
      "arxiv_id": "2601.14165v1",
      "comment": "17 pages, 11 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14161v1",
      "title": "One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion",
      "authors": [
        "Yitong Dong",
        "Qi Zhang",
        "Minchao Jiang",
        "Zhiqiang Wu",
        "Qingnan Fan",
        "Ying Feng",
        "Huaqi Zhang",
        "Hujun Bao",
        "Guofeng Zhang"
      ],
      "abstract": "We present a novel framework for high-fidelity novel view synthesis (NVS) from sparse images, addressing key limitations in recent feed-forward 3D Gaussian Splatting (3DGS) methods built on Vision Transformer (ViT) backbones. While ViT-based pipelines offer strong geometric priors, they are often constrained by low-resolution inputs due to computational costs. Moreover, existing generative enhancement methods tend to be 3D-agnostic, resulting in inconsistent structures across views, especially in unseen regions. To overcome these challenges, we design a Dual-Domain Detail Perception Module, which enables handling high-resolution images without being limited by the ViT backbone, and endows Gaussians with additional features to store high-frequency details. We develop a feature-guided diffusion network, which can preserve high-frequency details during the restoration process. We introduce a unified training strategy that enables joint optimization of the ViT-based geometric backbone and the diffusion-based refinement module. Experiments demonstrate that our method can maintain superior generation quality across multiple datasets.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14161v1",
        "pdf": "https://arxiv.org/pdf/2601.14161v1"
      },
      "arxiv_id": "2601.14161v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14160v1",
      "title": "Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law",
      "authors": [
        "Ali Hamza Bashir",
        "Muhammad Rehan Khalid",
        "Kostadin Cvejoski",
        "Jana Birr",
        "Jule Berghaus",
        "Armin Berger",
        "Sandra Halscheidt",
        "Christian Temath",
        "Rafet Sifa",
        "David Berghaus"
      ],
      "abstract": "Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14160v1",
        "pdf": "https://arxiv.org/pdf/2601.14160v1"
      },
      "arxiv_id": "2601.14160v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14157v1",
      "title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models",
      "authors": [
        "Bruno Sienkiewicz",
        "Łukasz Neumann",
        "Mateusz Modrzejewski"
      ],
      "abstract": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14157v1",
        "pdf": "https://arxiv.org/pdf/2601.14157v1"
      },
      "arxiv_id": "2601.14157v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14154v1",
      "title": "LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery",
      "authors": [
        "Shubham Pandey",
        "Bhavin Jawade",
        "Srirangaraj Setlur",
        "Venu Govindaraju",
        "Kenneth Seastedt"
      ],
      "abstract": "Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14154v1",
        "pdf": "https://arxiv.org/pdf/2601.14154v1"
      },
      "arxiv_id": "2601.14154v1",
      "comment": "Accepted to P2P-CV @ WACV 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14152v1",
      "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
      "authors": [
        "Hyunjong Ok",
        "Jaeho Lee"
      ],
      "abstract": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14152v1",
        "pdf": "https://arxiv.org/pdf/2601.14152v1"
      },
      "arxiv_id": "2601.14152v1",
      "comment": "preprint",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14133v1",
      "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
      "authors": [
        "Bin Yu",
        "Shijie Lian",
        "Xiaopeng Lin",
        "Yuliang Wei",
        "Zhaolong Shen",
        "Changti Wu",
        "Yuzhuo Miao",
        "Xinming Wang",
        "Bailing Wang",
        "Cong Huang",
        "Kai Chen"
      ],
      "abstract": "Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14133v1",
        "pdf": "https://arxiv.org/pdf/2601.14133v1"
      },
      "arxiv_id": "2601.14133v1",
      "comment": "GitHub: https://github.com/ZGC-EmbodyAI/TwinBrainVLA",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.14130v1",
      "title": "GIC-DLC: Differentiable Logic Circuits for Hardware-Friendly Grayscale Image Compression",
      "authors": [
        "Till Aczel",
        "David F. Jenny",
        "Simon Bührer",
        "Andreas Plesner",
        "Antonio Di Maio",
        "Roger Wattenhofer"
      ],
      "abstract": "Neural image codecs achieve higher compression ratios than traditional hand-crafted methods such as PNG or JPEG-XL, but often incur substantial computational overhead, limiting their deployment on energy-constrained devices such as smartphones, cameras, and drones. We propose Grayscale Image Compression with Differentiable Logic Circuits (GIC-DLC), a hardware-aware codec where we train lookup tables to combine the flexibility of neural networks with the efficiency of Boolean operations. Experiments on grayscale benchmark datasets show that GIC-DLC outperforms traditional codecs in compression efficiency while allowing substantial reductions in energy consumption and latency. These results demonstrate that learned compression can be hardware-friendly, offering a promising direction for low-power image compression on edge devices.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14130v1",
        "pdf": "https://arxiv.org/pdf/2601.14130v1"
      },
      "arxiv_id": "2601.14130v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14127v1",
      "title": "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning",
      "authors": [
        "Renmiao Chen",
        "Yida Lu",
        "Shiyao Cui",
        "Xuan Ouyang",
        "Victor Shea-Jay Huang",
        "Shumin Zhang",
        "Chengwei Pan",
        "Han Qiu",
        "Minlie Huang"
      ],
      "abstract": "As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14127v1",
        "pdf": "https://arxiv.org/pdf/2601.14127v1"
      },
      "arxiv_id": "2601.14127v1",
      "comment": "*15 pages, 5 figures. Introduces MIR-SafetyBench (2,676 instances; 9 multi-image relations). Equal contribution; †Corresponding author. Code/data: https://github.com/thu-coai/MIR-SafetyBench",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.14124v1",
      "title": "Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic",
      "authors": [
        "Saad Mankarious",
        "Aya Zirikly"
      ],
      "abstract": "Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14124v1",
        "pdf": "https://arxiv.org/pdf/2601.14124v1"
      },
      "arxiv_id": "2601.14124v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14115v1",
      "title": "Riemannian Liquid Spatio-Temporal Graph Network",
      "authors": [
        "Liangsi Lu",
        "Jingchao Wang",
        "Zhaorong Dai",
        "Hanqian Liu",
        "Yang Shi"
      ],
      "abstract": "Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural network, excel at modeling irregularly-sampled dynamics but are fundamentally confined to Euclidean space. This limitation introduces significant geometric distortion when representing real-world graphs with inherent non-Euclidean structures (e.g., hierarchies and cycles), degrading representation quality. To overcome this limitation, we introduce the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that unifies continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) formulated directly on a curved manifold, enabling it to faithfully capture the intrinsic geometry of both structurally static and dynamic spatio-temporal graphs. Moreover, we provide rigorous theoretical guarantees for RLSTG, extending stability theorems of LTCs to the Riemannian domain and quantifying its expressive power via state trajectory analysis. Extensive experiments on real-world benchmarks demonstrate that, by combining advanced temporal dynamics with a Riemannian spatial representation, RLSTG achieves superior performance on graphs with complex structures. Project Page: https://rlstg.github.io",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14115v1",
        "pdf": "https://arxiv.org/pdf/2601.14115v1"
      },
      "arxiv_id": "2601.14115v1",
      "comment": "This paper has been accepted to The Web Conference 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14112v1",
      "title": "Learning to Explain: Supervised Token Attribution from Transformer Attention Patterns",
      "authors": [
        "George Mihaila"
      ],
      "abstract": "Explainable AI (XAI) has become critical as transformer-based models are deployed in high-stakes applications including healthcare, legal systems, and financial services, where opacity hinders trust and accountability. Transformers self-attention mechanisms have proven valuable for model interpretability, with attention weights successfully used to understand model focus and behavior (Xu et al., 2015); (Wiegreffe and Pinter, 2019). However, existing attention-based explanation methods rely on manually defined aggregation strategies and fixed attribution rules (Abnar and Zuidema, 2020a); (Chefer et al., 2021), while model-agnostic approaches (LIME, SHAP) treat the model as a black box and incur significant computational costs through input perturbation. We introduce Explanation Network (ExpNet), a lightweight neural network that learns an explicit mapping from transformer attention patterns to token-level importance scores. Unlike prior methods, ExpNet discovers optimal attention feature combinations automatically rather than relying on predetermined rules. We evaluate ExpNet in a challenging cross-task setting and benchmark it against a broad spectrum of model-agnostic methods and attention-based techniques spanning four methodological families.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14112v1",
        "pdf": "https://arxiv.org/pdf/2601.14112v1"
      },
      "arxiv_id": "2601.14112v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14111v1",
      "title": "PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning",
      "authors": [
        "Jiaying Wu",
        "Can Gao",
        "Jinglu Hu",
        "Hui Li",
        "Xiaofeng Cao",
        "Jingcai Guo"
      ],
      "abstract": "Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic few-shot framework that leverages Multi-granularity semantics with Caption-guided Enhancement. PMCE constructs a nonparametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. At meta-test time, the most relevant base classes are retrieved based on the similarities of class name embeddings for each novel category. These statistics are then aggregated into category-specific prior information and fused with the support set prototypes via a simple MAP update. Simultaneously, a frozen BLIP captioner provides label-free instance-level image descriptions, and a lightweight enhancer trained on base classes optimizes both support prototypes and query features under an inductive protocol with a consistency regularization to stabilize noisy captions. Experiments on four benchmarks show that PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting. Our code is available at https://anonymous.4open.science/r/PMCE-275D",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14111v1",
        "pdf": "https://arxiv.org/pdf/2601.14111v1"
      },
      "arxiv_id": "2601.14111v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14104v1",
      "title": "Diffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning",
      "authors": [
        "Tairan Huang",
        "Qingqing Ye",
        "Yulin Jin",
        "Jiawei Lian",
        "Yi Wang",
        "Haibo Hu"
      ],
      "abstract": "Backdoor attacks embed hidden malicious behaviors in reinforcement learning (RL) policies and activate them using triggers at test time. Most existing attacks are validated only in simulation, while their effectiveness in real-world robotic systems remains unclear. In physical deployment, safety-constrained control pipelines such as velocity limiting, action smoothing, and collision avoidance suppress abnormal actions, causing strong attenuation of conventional backdoor attacks. We study this previously overlooked problem and propose a diffusion-guided backdoor attack framework (DGBA) for real-world RL. We design small printable visual patch triggers placed on the floor and generate them using a conditional diffusion model that produces diverse patch appearances under real-world visual variations. We treat the robot control stack as a black-box system. We further introduce an advantage-based poisoning strategy that injects triggers only at decision-critical training states. We evaluate our method on a TurtleBot3 mobile robot and demonstrate reliable activation of targeted attacks while preserving normal task performance. Demo videos and code are available in the supplementary material.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14104v1",
        "pdf": "https://arxiv.org/pdf/2601.14104v1"
      },
      "arxiv_id": "2601.14104v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14103v1",
      "title": "Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing",
      "authors": [
        "Xiaolu Liu",
        "Yicong Li",
        "Qiyuan He",
        "Jiayin Zhu",
        "Wei Ji",
        "Angela Yao",
        "Jianke Zhu"
      ],
      "abstract": "Textured 3D morphing seeks to generate smooth and plausible transitions between two 3D assets, preserving both structural coherence and fine-grained appearance. This ability is crucial not only for advancing 3D generation research but also for practical applications in animation, editing, and digital content creation. Existing approaches either operate directly on geometry, limiting them to shape-only morphing while neglecting textures, or extend 2D interpolation strategies into 3D, which often causes semantic ambiguity, structural misalignment, and texture blurring. These challenges underscore the necessity to jointly preserve geometric consistency, texture alignment, and robustness throughout the transition process. To address this, we propose Interp3D, a novel training-free framework for textured 3D morphing. It harnesses generative priors and adopts a progressive alignment principle to ensure both geometric fidelity and texture coherence. Starting from semantically aligned interpolation in condition space, Interp3D enforces structural consistency via SLAT (Structured Latent)-guided structure interpolation, and finally transfers appearance details through fine-grained texture fusion. For comprehensive evaluations, we construct a dedicated dataset, Interp3DData, with graded difficulty levels and assess generation results from fidelity, transition smoothness, and plausibility. Both quantitative metrics and human studies demonstrate the significant advantages of our proposed approach over previous methods. Source code is available at https://github.com/xiaolul2/Interp3D.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14103v1",
        "pdf": "https://arxiv.org/pdf/2601.14103v1"
      },
      "arxiv_id": "2601.14103v1",
      "comment": "22 pages, 12 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14101v1",
      "title": "Curriculum-Based Strategies for Efficient Cross-Domain Action Recognition",
      "authors": [
        "Emily Kim",
        "Allen Wu",
        "Jessica Hodgins"
      ],
      "abstract": "Despite significant progress in human action recognition, generalizing to diverse viewpoints remains a challenge. Most existing datasets are captured from ground-level perspectives, and models trained on them often struggle to transfer to drastically different domains such as aerial views. This paper examines how curriculum-based training strategies can improve generalization to unseen real aerial-view data without using any real aerial data during training.\n  We explore curriculum learning for cross-view action recognition using two out-of-domain sources: synthetic aerial-view data and real ground-view data. Our results on the evaluation on order of training (fine-tuning on synthetic aerial data vs. real ground data) shows that fine-tuning on real ground data but differ in how they transition from synthetic to real. The first uses a two-stage curriculum with direct fine-tuning, while the second applies a progressive curriculum that expands the dataset in multiple stages before fine-tuning. We evaluate both methods on the REMAG dataset using SlowFast (CNN-based) and MViTv2 (Transformer-based) architectures.\n  Results show that combining the two out-of-domain datasets clearly outperforms training on a single domain, whether real ground-view or synthetic aerial-view. Both curriculum strategies match the top-1 accuracy of simple dataset combination while offering efficiency gains. With the two-step fine-tuning method, SlowFast achieves up to a 37% reduction in iterations and MViTv2 up to a 30% reduction compared to simple combination. The multi-step progressive approach further reduces iterations, by up to 9% for SlowFast and 30% for MViTv2, relative to the two-step method. These findings demonstrate that curriculum-based training can maintain comparable performance (top-1 accuracy within 3% range) while improving training efficiency in cross-view action recognition.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14101v1",
        "pdf": "https://arxiv.org/pdf/2601.14101v1"
      },
      "arxiv_id": "2601.14101v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14099v1",
      "title": "Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping",
      "authors": [
        "Shi-Shun Chen",
        "Xiao-Yang Li",
        "Enrico Zio"
      ],
      "abstract": "Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14099v1",
        "pdf": "https://arxiv.org/pdf/2601.14099v1"
      },
      "arxiv_id": "2601.14099v1",
      "comment": "",
      "journal_ref": "Advanced Engineering Informatics 2026, 71, 104337",
      "has_code": false
    },
    {
      "id": "2601.14096v1",
      "title": "Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems",
      "authors": [
        "Benedikt Hartl",
        "Léo Pio-Lopez",
        "Chris Fields",
        "Michael Levin"
      ],
      "abstract": "The emerging field of diverse intelligence seeks an integrated view of problem-solving in agents of very different provenance, composition, and substrates. From subcellular chemical networks to swarms of organisms, and across evolved, engineered, and chimeric systems, it is hypothesized that scale-invariant principles of decision-making can be discovered. We propose that cognition in both natural and synthetic systems can be characterized and understood by the interplay between two equally important invariants: (1) the remapping of embedding spaces, and (2) the navigation within these spaces. Biological collectives, from single cells to entire organisms (and beyond), remap transcriptional, morphological, physiological, or 3D spaces to maintain homeostasis and regenerate structure, while navigating these spaces through distributed error correction. Modern Artificial Intelligence (AI) systems, including transformers, diffusion models, and neural cellular automata enact analogous processes by remapping data into latent embeddings and refining them iteratively through contextualization. We argue that this dual principle - remapping and navigation of embedding spaces via iterative error minimization - constitutes a substrate-independent invariant of cognition. Recognizing this shared mechanism not only illuminates deep parallels between living systems and artificial models, but also provides a unifying framework for engineering adaptive intelligence across scales.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14096v1",
        "pdf": "https://arxiv.org/pdf/2601.14096v1"
      },
      "arxiv_id": "2601.14096v1",
      "comment": "41 pages, 5 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14092v1",
      "title": "Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based Multi-Objective Reinforcement Learning",
      "authors": [
        "Babacar Toure",
        "Dimitrios Tsilimantos",
        "Omid Esrafilian",
        "Marios Kountouris"
      ],
      "abstract": "Due to their adaptability and mobility, Unmanned Aerial Vehicles (UAVs) are becoming increasingly essential for wireless network services, particularly for data harvesting tasks. In this context, Artificial Intelligence (AI)-based approaches have gained significant attention for addressing UAV path planning tasks in large and complex environments, bridging the gap with real-world deployments. However, many existing algorithms suffer from limited training data, which hampers their performance in highly dynamic environments. Moreover, they often overlook the inherently multi-objective nature of the task, treating it in an overly simplistic manner. To address these limitations, we propose an attention-based Multi-Objective Reinforcement Learning (MORL) architecture that explicitly handles the trade-off between data collection and energy consumption in urban environments, even without prior knowledge of wireless channel conditions. Our method develops a single model capable of adapting to varying trade-off preferences and dynamic scenario parameters without the need for fine-tuning or retraining. Extensive simulations show that our approach achieves substantial improvements in performance, model compactness, sample efficiency, and most importantly, generalization to previously unseen scenarios, outperforming existing RL solutions.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14092v1",
        "pdf": "https://arxiv.org/pdf/2601.14092v1"
      },
      "arxiv_id": "2601.14092v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14091v1",
      "title": "Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems",
      "authors": [
        "Hossein Naderi",
        "Alireza Shojaei",
        "Lifu Huang",
        "Philip Agee",
        "Kereshmeh Afsari",
        "Abiola Akanmu"
      ],
      "abstract": "Robots are expected to play a major role in the future construction industry but face challenges due to high costs and difficulty adapting to dynamic tasks. This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots. Four models are proposed and implemented using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate the improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14091v1",
        "pdf": "https://arxiv.org/pdf/2601.14091v1"
      },
      "arxiv_id": "2601.14091v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14087v1",
      "title": "'1'-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators",
      "authors": [
        "Ruichi Han",
        "Yizhi Chen",
        "Tong Lei",
        "Jordi Altayo Gonzalez",
        "Ahmed Hemani"
      ],
      "abstract": "Interconnect power consumption remains a bottleneck in Deep Neural Network (DNN) accelerators. While ordering data based on '1'-bit counts can mitigate this via reduced switching activity, practical hardware sorting implementations remain underexplored. This work proposes the hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN). By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. Our approximate sorting unit achieves up to 35.4% area reduction while maintaining 19.50\\% BT reduction compared to 20.42% of precise implementation.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14087v1",
        "pdf": "https://arxiv.org/pdf/2601.14087v1"
      },
      "arxiv_id": "2601.14087v1",
      "comment": "Accepted for oral presentation at the 2026 VLSI Symposium on Technology, Systems and Applications (VLSI TSA) on April 13-17, 2026, at the Ambassador Hotel, Hsinchu, Taiwan",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14086v1",
      "title": "Two-Stream temporal transformer for video action classification",
      "authors": [
        "Nattapong Kurpukdee",
        "Adrian G. Bors"
      ],
      "abstract": "Motion representation plays an important role in video understanding and has many applications including action recognition, robot and autonomous guidance or others. Lately, transformer networks, through their self-attention mechanism capabilities, have proved their efficiency in many applications. In this study, we introduce a new two-stream transformer video classifier, which extracts spatio-temporal information from content and optical flow representing movement information. The proposed model identifies self-attention features across the joint optical flow and temporal frame domain and represents their relationships within the transformer encoder mechanism. The experimental results show that our proposed methodology provides excellent classification results on three well-known video datasets of human activities.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14086v1",
        "pdf": "https://arxiv.org/pdf/2601.14086v1"
      },
      "arxiv_id": "2601.14086v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14084v1",
      "title": "DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning",
      "authors": [
        "Abdurrahim Yilmaz",
        "Ozan Erdem",
        "Ece Gokyayla",
        "Ayda Acar",
        "Burc Bugra Dagtas",
        "Dilara Ilhan Erdil",
        "Gulsum Gencoglan",
        "Burak Temelkuran"
      ],
      "abstract": "Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14084v1",
        "pdf": "https://arxiv.org/pdf/2601.14084v1"
      },
      "arxiv_id": "2601.14084v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14079v1",
      "title": "VENI: Variational Encoder for Natural Illumination",
      "authors": [
        "Paul Walker",
        "James A. D. Gardner",
        "Andreea Ardelean",
        "William A. P. Smith",
        "Bernhard Egger"
      ],
      "abstract": "Inverse rendering is an ill-posed problem, but priors like illumination priors, can simplify it. Existing work either disregards the spherical and rotation-equivariant nature of illumination environments or does not provide a well-behaved latent space. We propose a rotation-equivariant variational autoencoder that models natural illumination on the sphere without relying on 2D projections. To preserve the SO(2)-equivariance of environment maps, we use a novel Vector Neuron Vision Transformer (VN-ViT) as encoder and a rotation-equivariant conditional neural field as decoder. In the encoder, we reduce the equivariance from SO(3) to SO(2) using a novel SO(2)-equivariant fully connected layer, an extension of Vector Neurons. We show that our SO(2)-equivariant fully connected layer outperforms standard Vector Neurons when used in our SO(2)-equivariant model. Compared to previous methods, our variational autoencoder enables smoother interpolation in latent space and offers a more well-behaved latent space.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14079v1",
        "pdf": "https://arxiv.org/pdf/2601.14079v1"
      },
      "arxiv_id": "2601.14079v1",
      "comment": "Project Repo - https://github.com/paul-pw/veni Project page - https://paul-pw.github.io/veni",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.14077v1",
      "title": "MooneyMaker: A Python package to create ambiguous two-tone images",
      "authors": [
        "Lars C. Reining",
        "Thabo Matthies",
        "Luisa Haussner",
        "Rabea Turon",
        "Thomas S. A. Wallis"
      ],
      "abstract": "Mooney images are high-contrast, two-tone visual stimuli, created by thresholding photographic images. They allow researchers to separate image content from image understanding, making them valuable for studying visual perception. An ideal Mooney image for this purpose achieves a specific balance: it initially appears unrecognizable but becomes fully interpretable to the observer after seeing the original template. Researchers traditionally created these stimuli manually using subjective criteria, which is labor-intensive and can introduce inconsistencies across studies. Automated generation techniques now offer an alternative to this manual approach. Here, we present MooneyMaker, an open-source Python package that automates the generation of ambiguous Mooney images using several complementary approaches. Users can choose between various generation techniques that range from approaches based on image statistics to deep learning models. These models strategically alter edge information to increase initial ambiguity. The package lets users create two-tone images with multiple methods and directly compare the results visually. In an experiment, we validate MooneyMaker by generating Mooney images using different techniques and assess their recognizability for human observers before and after disambiguating them by presenting the template images. Our results reveal that techniques with lower initial recognizability are associated with higher post-template recognition (i.e. a larger disambiguation effect). To help vision scientists build effective databases of Mooney stimuli, we provide practical guidelines for technique selection. By standardizing the generation process, MooneyMaker supports more consistent and reproducible visual perception research.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "q-bio.NC",
        "cs.CV"
      ],
      "primary_category": "q-bio.NC",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14077v1",
        "pdf": "https://arxiv.org/pdf/2601.14077v1"
      },
      "arxiv_id": "2601.14077v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14069v1",
      "title": "Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management",
      "authors": [
        "Nattapong Kurpukdee",
        "Adrian G. Bors"
      ],
      "abstract": "Unsupervised video class incremental learning (uVCIL) represents an important learning paradigm for learning video information without forgetting, and without considering any data labels. Prior approaches have focused on supervised class-incremental learning, relying on using the knowledge of labels and task boundaries, which is costly, requires human annotation, or is simply not a realistic option. In this paper, we propose a simple yet effective approach to address the uVCIL. We first consider a deep feature extractor network, providing a set of representative video features during each task without assuming any class or task information. We then progressively build a series of deep clusters from the extracted features. During the successive task learning, the model updated from the previous task is used as an initial state in order to transfer knowledge to the current learning task. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting. Our approach significantly outperforms other baselines on all datasets.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14069v1",
        "pdf": "https://arxiv.org/pdf/2601.14069v1"
      },
      "arxiv_id": "2601.14069v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14066v1",
      "title": "VERIDAH: Solving Enumeration Anomaly Aware Vertebra Labeling across Imaging Sequences",
      "authors": [
        "Hendrik Möller",
        "Hanna Schoen",
        "Robert Graf",
        "Matan Atad",
        "Nathan Molinier",
        "Anjany Sekuboyina",
        "Bettina K. Budai",
        "Fabian Bamberg",
        "Steffen Ringhof",
        "Christopher Schlett",
        "Tobias Pischon",
        "Thoralf Niendorf",
        "Josua A. Decker",
        "Marc-André Weber",
        "Bjoern Menze",
        "Daniel Rueckert",
        "Jan S. Kirschke"
      ],
      "abstract": "The human spine commonly consists of seven cervical, twelve thoracic, and five lumbar vertebrae. However, enumeration anomalies may result in individuals having eleven or thirteen thoracic vertebrae and four or six lumbar vertebrae. Although the identification of enumeration anomalies has potential clinical implications for chronic back pain and operation planning, the thoracolumbar junction is often poorly assessed and rarely described in clinical reports. Additionally, even though multiple deep-learning-based vertebra labeling algorithms exist, there is a lack of methods to automatically label enumeration anomalies. Our work closes that gap by introducing \"Vertebra Identification with Anomaly Handling\" (VERIDAH), a novel vertebra labeling algorithm based on multiple classification heads combined with a weighted vertebra sequence prediction algorithm. We show that our approach surpasses existing models on T2w TSE sagittal (98.30% vs. 94.24% of subjects with all vertebrae correctly labeled, p < 0.001) and CT imaging (99.18% vs. 77.26% of subjects with all vertebrae correctly labeled, p < 0.001) and works in arbitrary field-of-view images. VERIDAH correctly labeled the presence 2 Möller et al. of thoracic enumeration anomalies in 87.80% and 96.30% of T2w and CT images, respectively, and lumbar enumeration anomalies in 94.48% and 97.22% for T2w and CT, respectively. Our code and models are available at: https://github.com/Hendrik-code/spineps.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14066v1",
        "pdf": "https://arxiv.org/pdf/2601.14066v1"
      },
      "arxiv_id": "2601.14066v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14063v1",
      "title": "XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs",
      "authors": [
        "Mohsinul Kabir",
        "Tasnim Ahmed",
        "Md Mezbaur Rahman",
        "Shaoxiong Ji",
        "Hassan Alhuzali",
        "Sophia Ananiadou"
      ],
      "abstract": "Cross-cultural competence in large language models (LLMs) requires the ability to identify Culture-Specific Items (CSIs) and to adapt them appropriately across cultural contexts. Progress in evaluating this capability has been constrained by the scarcity of high-quality CSI-annotated corpora with parallel cross-cultural sentence pairs. To address this limitation, we introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning tasks with corresponding evaluation metrics. Our corpus integrates Newmark's CSI framework with Hall's Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements such as social norms, beliefs, and values. Our findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference. Additionally, we find evidence that LLMs encode regional and ethno-religious biases even within a single linguistic setting during cultural adaptation. We release our corpus and code to facilitate future research on cross-cultural NLP.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14063v1",
        "pdf": "https://arxiv.org/pdf/2601.14063v1"
      },
      "arxiv_id": "2601.14063v1",
      "comment": "30 Pages, 13 Figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14060v1",
      "title": "Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration",
      "authors": [
        "Yongcong Ye",
        "Kai Zhang",
        "Yanghai Zhang",
        "Enhong Chen",
        "Longfei Li",
        "Jun Zhou"
      ],
      "abstract": "Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14060v1",
        "pdf": "https://arxiv.org/pdf/2601.14060v1"
      },
      "arxiv_id": "2601.14060v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14056v1",
      "title": "POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion",
      "authors": [
        "Andrea Rigo",
        "Luca Stornaiuolo",
        "Weijie Wang",
        "Mauro Martino",
        "Bruno Lepri",
        "Nicu Sebe"
      ],
      "abstract": "We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14056v1",
        "pdf": "https://arxiv.org/pdf/2601.14056v1"
      },
      "arxiv_id": "2601.14056v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14055v1",
      "title": "Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI",
      "authors": [
        "Andrea Protani",
        "Marc Molina Van Den Bosch",
        "Lorenzo Giusti",
        "Heloisa Barbosa Da Silva",
        "Paolo Cacace",
        "Albert Sund Aillet",
        "Miguel Angel Gonzalez Ballester",
        "Friedhelm Hummel",
        "Luigi Serio"
      ],
      "abstract": "Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14055v1",
        "pdf": "https://arxiv.org/pdf/2601.14055v1"
      },
      "arxiv_id": "2601.14055v1",
      "comment": "10 pages, 3 figures,",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14054v1",
      "title": "SecureSplit: Mitigating Backdoor Attacks in Split Learning",
      "authors": [
        "Zhihao Dou",
        "Dongfei Cui",
        "Weida Wang",
        "Anjun Gao",
        "Yueyang Quan",
        "Mengyao Ma",
        "Viet Vo",
        "Guangdong Bai",
        "Zhuqing Liu",
        "Minghong Fang"
      ],
      "abstract": "Split Learning (SL) offers a framework for collaborative model training that respects data privacy by allowing participants to share the same dataset while maintaining distinct feature sets. However, SL is susceptible to backdoor attacks, in which malicious clients subtly alter their embeddings to insert hidden triggers that compromise the final trained model. To address this vulnerability, we introduce SecureSplit, a defense mechanism tailored to SL. SecureSplit applies a dimensionality transformation strategy to accentuate subtle differences between benign and poisoned embeddings, facilitating their separation. With this enhanced distinction, we develop an adaptive filtering approach that uses a majority-based voting scheme to remove contaminated embeddings while preserving clean ones. Rigorous experiments across four datasets (CIFAR-10, MNIST, CINIC-10, and ImageNette), five backdoor attack scenarios, and seven alternative defenses confirm the effectiveness of SecureSplit under various challenging conditions.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CR",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14054v1",
        "pdf": "https://arxiv.org/pdf/2601.14054v1"
      },
      "arxiv_id": "2601.14054v1",
      "comment": "To appear in The Web Conference 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14053v1",
      "title": "LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems",
      "authors": [
        "Badri N. Patro",
        "Vijay S. Agneeswaran"
      ],
      "abstract": "The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MA",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14053v1",
        "pdf": "https://arxiv.org/pdf/2601.14053v1"
      },
      "arxiv_id": "2601.14053v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14052v1",
      "title": "Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model",
      "authors": [
        "Haoran Xu",
        "Yanlin Liu",
        "Zizhao Tong",
        "Jiaze Li",
        "Kexue Fu",
        "Yuyang Zhang",
        "Longxiang Gao",
        "Shuaiguang Li",
        "Xingyu Li",
        "Yanran Xu",
        "Changwei Wang"
      ],
      "abstract": "Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14052v1",
        "pdf": "https://arxiv.org/pdf/2601.14052v1"
      },
      "arxiv_id": "2601.14052v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14051v1",
      "title": "Kakugo: Distillation of Low-Resource Languages into Small Language Models",
      "authors": [
        "Peter Devine",
        "Mardhiyah Sanni",
        "Farid Adilazuarda",
        "Julieta Gil Loizaga",
        "Barry Haddow"
      ],
      "abstract": "We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks, including translation, classification, and question answering, demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14051v1",
        "pdf": "https://arxiv.org/pdf/2601.14051v1"
      },
      "arxiv_id": "2601.14051v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14047v1",
      "title": "Collective intelligence in science: direct elicitation of diverse information from experts with unknown information structure",
      "authors": [
        "Alexey V. Osipov",
        "Nikolay N. Osipov"
      ],
      "abstract": "Suppose we need a deep collective analysis of an open scientific problem: there is a complex scientific hypothesis and a large online group of mutually unrelated experts with relevant private information of a diverse and unpredictable nature. This information may be results of experts' individual experiments, original reasoning of some of them, results of AI systems they use, etc. We propose a simple mechanism based on a self-resolving play-money prediction market entangled with a chat. We show that such a system can easily be brought to an equilibrium where participants directly share their private information on the hypothesis through the chat and trade as if the market were resolved in accordance with the truth of the hypothesis. This approach will lead to efficient aggregation of relevant information in a completely interpretable form even if the ground truth cannot be established and experts initially know nothing about each other and cannot perform complex Bayesian calculations. Finally, by rewarding the experts with some real assets proportionally to the play money they end up with, we can get an innovative way to fund large-scale collaborative studies of any type.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA",
        "cs.SI",
        "econ.TH"
      ],
      "primary_category": "cs.GT",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14047v1",
        "pdf": "https://arxiv.org/pdf/2601.14047v1"
      },
      "arxiv_id": "2601.14047v1",
      "comment": "21 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14044v1",
      "title": "Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology",
      "authors": [
        "Kaiyu Wu",
        "Pucheng Han",
        "Hualong Zhang",
        "Naigeng Wu",
        "Keze Wang"
      ],
      "abstract": "While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model's reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at https://github.com/Marcowky/Weather-R1.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14044v1",
        "pdf": "https://arxiv.org/pdf/2601.14044v1"
      },
      "arxiv_id": "2601.14044v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14042v1",
      "title": "Federated Balanced Learning",
      "authors": [
        "Jiaze Li",
        "Haoran Xu",
        "Wanyi Wu",
        "Changwei Wang",
        "Shuaiguang Li",
        "Jianzhong Ju",
        "Zhenbo Luo",
        "Jian Luan",
        "Youyang Qu",
        "Longxiang Gao",
        "Xudong Yang",
        "Lumin Xing"
      ],
      "abstract": "Federated learning is a paradigm of joint learning in which clients collaborate by sharing model parameters instead of data. However, in the non-iid setting, the global model experiences client drift, which can seriously affect the final performance of the model. Previous methods tend to correct the global model that has already deviated based on the loss function or gradient, overlooking the impact of the client samples. In this paper, we rethink the role of the client side and propose Federated Balanced Learning, i.e., FBL, to prevent this issue from the beginning through sample balance on the client side. Technically, FBL allows unbalanced data on the client side to achieve sample balance through knowledge filling and knowledge sampling using edge-side generation models, under the limitation of a fixed number of data samples on clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the gap between synthetic and real data, and a Knowledge Drop Strategy to regularize our method. Meanwhile, we scale our method to real and complex scenarios, allowing different clients to adopt various methods, and extend our framework to further improve performance. Numerous experiments show that our method outperforms state-of-the-art baselines. The code is released upon acceptance.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14042v1",
        "pdf": "https://arxiv.org/pdf/2601.14042v1"
      },
      "arxiv_id": "2601.14042v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14041v1",
      "title": "Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants",
      "authors": [
        "Yunhe Wang",
        "Kai Han",
        "Huiling Zhen",
        "Yuchuan Tian",
        "Hanting Chen",
        "Yongbing Huang",
        "Yufei Cui",
        "Yingte Shu",
        "Shan Gao",
        "Ismail Elezi",
        "Roy Vaughan Miles",
        "Songcen Xu",
        "Feng Wen",
        "Chao Xu",
        "Sinan Zeng",
        "Dacheng Tao"
      ],
      "abstract": "The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential ``brick-by-brick'' process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14041v1",
        "pdf": "https://arxiv.org/pdf/2601.14041v1"
      },
      "arxiv_id": "2601.14041v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14039v1",
      "title": "Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation",
      "authors": [
        "Wesam Moustafa",
        "Hossam Elsafty",
        "Helen Schneider",
        "Lorenz Sparrenberg",
        "Rafet Sifa"
      ],
      "abstract": "Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14039v1",
        "pdf": "https://arxiv.org/pdf/2601.14039v1"
      },
      "arxiv_id": "2601.14039v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14038v1",
      "title": "Correcting and Quantifying Systematic Errors in 3D Box Annotations for Autonomous Driving",
      "authors": [
        "Alexandre Justo Miro",
        "Ludvig af Klinteberg",
        "Bogdan Timus",
        "Aron Asefaw",
        "Ajinkya Khoche",
        "Thomas Gustafsson",
        "Sina Sharif Mansouri",
        "Masoud Daneshtalab"
      ],
      "abstract": "Accurate ground truth annotations are critical to supervised learning and evaluating the performance of autonomous vehicle systems. These vehicles are typically equipped with active sensors, such as LiDAR, which scan the environment in predefined patterns. 3D box annotation based on data from such sensors is challenging in dynamic scenarios, where objects are observed at different timestamps, hence different positions. Without proper handling of this phenomenon, systematic errors are prone to being introduced in the box annotations. Our work is the first to discover such annotation errors in widely used, publicly available datasets. Through our novel offline estimation method, we correct the annotations so that they follow physically feasible trajectories and achieve spatial and temporal consistency with the sensor data. For the first time, we define metrics for this problem; and we evaluate our method on the Argoverse 2, MAN TruckScenes, and our proprietary datasets. Our approach increases the quality of box annotations by more than 17% in these datasets. Furthermore, we quantify the annotation errors in them and find that the original annotations are misplaced by up to 2.5 m, with highly dynamic objects being the most affected. Finally, we test the impact of the errors in benchmarking and find that the impact is larger than the improvements that state-of-the-art methods typically achieve with respect to the previous state-of-the-art methods; showing that accurate annotations are essential for correct interpretation of performance. Our code is available at https://github.com/alexandre-justo-miro/annotation-correction-3D-boxes.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14038v1",
        "pdf": "https://arxiv.org/pdf/2601.14038v1"
      },
      "arxiv_id": "2601.14038v1",
      "comment": "Accepted to The IEEE/CVF Winter Conference on Applications of Computer Vision 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14033v1",
      "title": "PAC-Private Responses with Adversarial Composition",
      "authors": [
        "Xiaochen Zhu",
        "Mayuri Sridhar",
        "Srinivas Devadas"
      ],
      "abstract": "Modern machine learning models are increasingly deployed behind APIs. This renders standard weight-privatization methods (e.g. DP-SGD) unnecessarily noisy at the cost of utility. While model weights may vary significantly across training datasets, model responses to specific inputs are much lower dimensional and more stable. This motivates enforcing privacy guarantees directly on model outputs.\n  We approach this under PAC privacy, which provides instance-based privacy guarantees for arbitrary black-box functions by controlling mutual information (MI). Importantly, PAC privacy explicitly rewards output stability with reduced noise levels. However, a central challenge remains: response privacy requires composing a large number of adaptively chosen, potentially adversarial queries issued by untrusted users, where existing composition results on PAC privacy are inadequate. We introduce a new algorithm that achieves adversarial composition via adaptive noise calibration and prove that mutual information guarantees accumulate linearly under adaptive and adversarial querying.\n  Experiments across tabular, vision, and NLP tasks show that our method achieves high utility at extremely small per-query privacy budgets. On CIFAR-10, we achieve 87.79% accuracy with a per-step MI budget of $2^{-32}$. This enables serving one million queries while provably bounding membership inference attack (MIA) success rates to 51.08% -- the same guarantee of $(0.04, 10^{-5})$-DP. Furthermore, we show that private responses can be used to label public data to distill a publishable privacy-preserving model; using an ImageNet subset as a public dataset, our model distilled from 210,000 responses achieves 91.86% accuracy on CIFAR-10 with MIA success upper-bounded by 50.49%, which is comparable to $(0.02,10^{-5})$-DP.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14033v1",
        "pdf": "https://arxiv.org/pdf/2601.14033v1"
      },
      "arxiv_id": "2601.14033v1",
      "comment": "16 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14031v1",
      "title": "Intermittent time series forecasting: local vs global models",
      "authors": [
        "Stefano Damato",
        "Nicolò Rubattu",
        "Dario Azzimonti",
        "Giorgio Corani"
      ],
      "abstract": "Intermittent time series, characterised by the presence of a significant amount of zeros, constitute a large percentage of inventory items in supply chain. Probabilistic forecasts are needed to plan the inventory levels; the predictive distribution should cover non-negative values, have a mass in zero and a long upper tail. Intermittent time series are commonly forecast using local models, which are trained individually on each time series. In the last years global models, which are trained on a large collection of time series, have become popular for time series forecasting. Global models are often based on neural networks. However, they have not yet been exhaustively tested on intermittent time series. We carry out the first study comparing state-of-the-art local (iETS, TweedieGP) and global models (D-Linear, DeepAR, Transformers) on intermittent time series. For neural networks models we consider three different distribution heads suitable for intermittent time series: negative binomial, hurdle-shifted negative binomial and Tweedie. We use, for the first time, the last two distribution heads with neural networks. We perform experiments on five large datasets comprising more than 40'000 real-world time series. Among neural networks D-Linear provides best accuracy; it also consistently outperforms the local models. Moreover, it has also low computational requirements. Transformers-based architectures are instead much more computationally demanding and less accurate. Among the distribution heads, the Tweedie provides the best estimates of the highest quantiles, while the negative binomial offers overall the best performance.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14031v1",
        "pdf": "https://arxiv.org/pdf/2601.14031v1"
      },
      "arxiv_id": "2601.14031v1",
      "comment": "Submitted to Data Mining and Knowledge Discovery",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14030v1",
      "title": "Likelihood-Separable Diffusion Inference for Multi-Image MRI Super-Resolution",
      "authors": [
        "Samuel W. Remedios",
        "Zhangxing Bian",
        "Shuwen Wei",
        "Aaron Carass",
        "Jerry L. Prince",
        "Blake E. Dewey"
      ],
      "abstract": "Diffusion models are the current state-of-the-art for solving inverse problems in imaging. Their impressive generative capability allows them to approximate sampling from a prior distribution, which alongside a known likelihood function permits posterior sampling without retraining the model. While recent methods have made strides in advancing the accuracy of posterior sampling, the majority focuses on single-image inverse problems. However, for modalities such as magnetic resonance imaging (MRI), it is common to acquire multiple complementary measurements, each low-resolution along a different axis. In this work, we generalize common diffusion-based inverse single-image problem solvers for multi-image super-resolution (MISR) MRI. We show that the DPS likelihood correction allows an exactly-separable gradient decomposition across independently acquired measurements, enabling MISR without constructing a joint operator, modifying the diffusion model, or increasing network function evaluations. We derive MISR versions of DPS, DMAP, DPPS, and diffusion-based PnP/ADMM, and demonstrate substantial gains over SISR across $4\\times/8\\times/16\\times$ anisotropic degradations. Our results achieve state-of-the-art super-resolution of anisotropic MRI volumes and, critically, enable reconstruction of near-isotropic anatomy from routine 2D multi-slice acquisitions, which are otherwise highly degraded in orthogonal views.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14030v1",
        "pdf": "https://arxiv.org/pdf/2601.14030v1"
      },
      "arxiv_id": "2601.14030v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14027v1",
      "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics",
      "authors": [
        "Junqi Liu",
        "Zihao Zhou",
        "Zekai Zhu",
        "Marco Dos Santos",
        "Weikun He",
        "Jiawei Liu",
        "Ran Wang",
        "Yunzhou Xie",
        "Junqiao Zhao",
        "Qiufeng Wang",
        "Lihong Zhi",
        "Jia Li",
        "Wenda Li"
      ],
      "abstract": "Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14027v1",
        "pdf": "https://arxiv.org/pdf/2601.14027v1"
      },
      "arxiv_id": "2601.14027v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14026v1",
      "title": "Universal Approximation Theorem for Input-Connected Multilayer Perceptrons",
      "authors": [
        "Vugar Ismailov"
      ],
      "abstract": "We introduce the Input-Connected Multilayer Perceptron (IC-MLP), a feedforward neural network architecture in which each hidden neuron receives, in addition to the outputs of the preceding layer, a direct affine connection from the raw input. We first study this architecture in the univariate setting and give an explicit and systematic description of IC-MLPs with an arbitrary finite number of hidden layers, including iterated formulas for the network functions. In this setting, we prove a universal approximation theorem showing that deep IC-MLPs can approximate any continuous function on a closed interval of the real line if and only if the activation function is nonlinear. We then extend the analysis to vector-valued inputs and establish a corresponding universal approximation theorem for continuous functions on compact subsets of $\\mathbb{R}^n$.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.NE",
        "math.FA"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14026v1",
        "pdf": "https://arxiv.org/pdf/2601.14026v1"
      },
      "arxiv_id": "2601.14026v1",
      "comment": "18 pages, 2 figures, 31 references",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14022v1",
      "title": "Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment",
      "authors": [
        "Rodrigo Pereira David",
        "Luciano Araujo Dourado Filho",
        "Daniel Marques da Silva",
        "João Alfredo Cal-Braz"
      ],
      "abstract": "Decarbonizing road transport requires consistent and transparent methods for comparing CO2 emissions across vehicle technologies. This paper proposes a machine learning-based framework for like-for-like operational assessment of internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical, real-world driving conditions. The approach isolates technology-specific effects by holding the observed speed profile and environmental context fixed, enabling direct comparison of powertrain performance. Recurrent neural network models are trained independently for each domain to learn the mapping from contextual driving variables (speed, acceleration, temperature) to internal actuation variables (torque, throttle) and instantaneous CO2-equivalent emission rates. This structure allows the construction of counterfactual scenarios that answer: What emissions would an EV have generated if it had followed the same driving profile as an ICEV? By aligning both vehicle types on a unified instantaneous emissions metric, the framework enables fair and reproducible evaluation of powertrain technologies. It offers a scalable foundation for credible, data-driven assessments of vehicle carbon performance under real-world operating conditions.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14022v1",
        "pdf": "https://arxiv.org/pdf/2601.14022v1"
      },
      "arxiv_id": "2601.14022v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14012v1",
      "title": "MATE: Matryoshka Audio-Text Embeddings for Open-Vocabulary Keyword Spotting",
      "authors": [
        "Youngmoon Jung",
        "Myunghun Jung",
        "Joon-Young Yang",
        "Yong-Hyeok Lee",
        "Jaeyoung Roh",
        "Hoon-Young Cho"
      ],
      "abstract": "Open-vocabulary keyword spotting (KWS) with text-based enrollment has emerged as a flexible alternative to fixed-phrase triggers. Prior utterance-level matching methods, from an embedding-learning standpoint, learn embeddings at a single fixed dimensionality. We depart from this design and propose Matryoshka Audio-Text Embeddings (MATE), a dual-encoder framework that encodes multiple embedding granularities within a single vector via nested sub-embeddings (\"prefixes\"). Specifically, we introduce a PCA-guided prefix alignment: PCA-compressed versions of the full text embedding for each prefix size serve as teacher targets to align both audio and text prefixes. This alignment concentrates salient keyword cues in lower-dimensional prefixes, while higher dimensions add detail. MATE is trained with standard deep metric learning objectives for audio-text KWS, and is loss-agnostic. To our knowledge, this is the first application of matryoshka-style embeddings to KWS, achieving state-of-the-art results on WSJ and LibriPhrase without any inference overhead.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14012v1",
        "pdf": "https://arxiv.org/pdf/2601.14012v1"
      },
      "arxiv_id": "2601.14012v1",
      "comment": "5 pages, 1 figure, Accepted at ICASSP 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14001v1",
      "title": "Auditory Brain Passage Retrieval: Cross-Sensory EEG Training for Neural Information Retrieval",
      "authors": [
        "Niall McGuire",
        "Yashar Moshfeghi"
      ],
      "abstract": "Query formulation from internal information needs remains fundamentally challenging across all Information Retrieval paradigms due to cognitive complexity and physical impairments. Brain Passage Retrieval (BPR) addresses this by directly mapping EEG signals to passage representations without intermediate text translation. However, existing BPR research exclusively uses visual stimuli, leaving critical questions unanswered: Can auditory EEG enable effective retrieval for voice-based interfaces and visually impaired users? Can training on combined EEG datasets from different sensory modalities improve performance despite severe data scarcity? We present the first systematic investigation of auditory EEG for BPR and evaluate cross-sensory training benefits. Using dual encoder architectures with four pooling strategies (CLS, mean, max, multi-vector), we conduct controlled experiments comparing auditory-only, visual-only, and combined training on the Alice (auditory) and Nieuwland (visual) datasets. Results demonstrate that auditory EEG consistently outperforms visual EEG, and cross-sensory training with CLS pooling achieves substantial improvements over individual training: 31% in MRR (0.474), 43% in Hit@1 (0.314), and 28% in Hit@10 (0.858). Critically, combined auditory EEG models surpass BM25 text baselines (MRR: 0.474 vs 0.428), establishing neural queries as competitive with traditional retrieval whilst enabling accessible interfaces. These findings validate auditory neural interfaces for IR tasks and demonstrate that cross-sensory training addresses data scarcity whilst outperforming single-modality approaches Code: https://github.com/NiallMcguire/Audio_BPR",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14001v1",
        "pdf": "https://arxiv.org/pdf/2601.14001v1"
      },
      "arxiv_id": "2601.14001v1",
      "comment": "Accepted At ECIR 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.14000v1",
      "title": "Group-Invariant Unsupervised Skill Discovery: Symmetry-aware Skill Representations for Generalizable Behavior",
      "authors": [
        "Junwoo Chang",
        "Joseph Park",
        "Roberto Horowitz",
        "Jongmin Lee",
        "Jongeun Choi"
      ],
      "abstract": "Unsupervised skill discovery aims to acquire behavior primitives that improve exploration and accelerate downstream task learning. However, existing approaches often ignore the geometric symmetries of physical environments, leading to redundant behaviors and sample inefficiency. To address this, we introduce Group-Invariant Skill Discovery (GISD), a framework that explicitly embeds group structure into the skill discovery objective. Our approach is grounded in a theoretical guarantee: we prove that in group-symmetric environments, the standard Wasserstein dependency measure admits a globally optimal solution comprised of an equivariant policy and a group-invariant scoring function. Motivated by this, we formulate the Group-Invariant Wasserstein dependency measure, which restricts the optimization to this symmetry-aware subspace without loss of optimality. Practically, we parameterize the scoring function using a group Fourier representation and define the intrinsic reward via the alignment of equivariant latent features, ensuring that the discovered skills generalize systematically under group transformations. Experiments on state-based and pixel-based locomotion benchmarks demonstrate that GISD achieves broader state-space coverage and improved efficiency in downstream task learning compared to a strong baseline.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2601.14000v1",
        "pdf": "https://arxiv.org/pdf/2601.14000v1"
      },
      "arxiv_id": "2601.14000v1",
      "comment": "14 pages, 6 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13999v1",
      "title": "DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification",
      "authors": [
        "Youngmoon Jung",
        "Joon-Young Yang",
        "Ju-ho Kim",
        "Jaeyoung Roh",
        "Chang Woo Han",
        "Hoon-Young Cho"
      ],
      "abstract": "Short-utterance speaker verification remains challenging due to limited speaker-discriminative cues in short speech segments. While existing methods focus on enhancing speaker encoders, the embedding learning strategy still forces a single fixed-dimensional representation reused for utterances of any length, leaving capacity misaligned with the information available at different durations. We propose Duration-Aware Matryoshka Embedding (DAME), a model-agnostic framework that builds a nested hierarchy of sub-embeddings aligned to utterance durations: lower-dimensional representations capture compact speaker traits from short utterances, while higher dimensions encode richer details from longer speech. DAME supports both training from scratch and fine-tuning, and serves as a direct alternative to conventional large-margin fine-tuning, consistently improving performance across durations. On the VoxCeleb1-O/E/H and VOiCES evaluation sets, DAME consistently reduces the equal error rate on 1-s and other short-duration trials, while maintaining full-length performance with no additional inference cost. These gains generalize across various speaker encoder architectures under both general training and fine-tuning setups.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13999v1",
        "pdf": "https://arxiv.org/pdf/2601.13999v1"
      },
      "arxiv_id": "2601.13999v1",
      "comment": "5 pages, 2 figures, Accepted at ICASSP 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13994v1",
      "title": "torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch",
      "authors": [
        "Mingyuan Chi"
      ],
      "abstract": "Industrial scientific computing predominantly uses sparse matrices to represent unstructured data -- finite element meshes, graphs, point clouds. We present \\torchsla{}, an open-source PyTorch library that enables GPU-accelerated, scalable, and differentiable sparse linear algebra. The library addresses three fundamental challenges: (1) GPU acceleration for sparse linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation; (2) Multi-GPU scaling via domain decomposition with halo exchange, reaching \\textbf{400 million DOF linear solve on 3 GPUs}; and (3) Adjoint-based differentiation} achieving $\\mathcal{O}(1)$ computational graph nodes (for autograd) and $\\mathcal{O}(\\text{nnz})$ memory -- independent of solver iterations. \\torchsla{} supports multiple backends (SciPy, cuDSS, PyTorch-native) and seamlessly integrates with PyTorch autograd for end-to-end differentiable simulations. Code is available at https://github.com/walkerchi/torch-sla.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13994v1",
        "pdf": "https://arxiv.org/pdf/2601.13994v1"
      },
      "arxiv_id": "2601.13994v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13992v1",
      "title": "\"The Whole Is Greater Than the Sum of Its Parts\": A Compatibility-Aware Multi-Teacher CoT Distillation Framework",
      "authors": [
        "Jin Cui",
        "Jiaqi Guo",
        "Jiepeng Zhou",
        "Ruixuan Yang",
        "Jiayi Lu",
        "Jiajun Xu",
        "Jiangcheng Song",
        "Boran Zhao",
        "Pengju Ren"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect \"epiphany moments\" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13992v1",
        "pdf": "https://arxiv.org/pdf/2601.13992v1"
      },
      "arxiv_id": "2601.13992v1",
      "comment": "11pages, 9figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13989v1",
      "title": "A universal linearized subspace refinement framework for neural networks",
      "authors": [
        "Wenbo Cao",
        "Weiwei Zhang"
      ],
      "abstract": "Neural networks are predominantly trained using gradient-based methods, yet in many applications their final predictions remain far from the accuracy attainable within the model's expressive capacity. We introduce Linearized Subspace Refinement (LSR), a general and architecture-agnostic framework that exploits the Jacobian-induced linear residual model at a fixed trained network state. By solving a reduced direct least-squares problem within this subspace, LSR computes a subspace-optimal solution of the linearized residual model, yielding a refined linear predictor with substantially improved accuracy over standard gradient-trained solutions, without modifying network architectures, loss formulations, or training procedures. Across supervised function approximation, data-driven operator learning, and physics-informed operator fine-tuning, we show that gradient-based training often fails to access this attainable accuracy, even when local linearization yields a convex problem. This observation indicates that loss-induced numerical ill-conditioning, rather than nonconvexity or model expressivity, can constitute a dominant practical bottleneck. In contrast, one-shot LSR systematically exposes accuracy levels not fully exploited by gradient-based training, frequently achieving order-of-magnitude error reductions. For operator-constrained problems with composite loss structures, we further introduce Iterative LSR, which alternates one-shot LSR with supervised nonlinear alignment, transforming ill-conditioned residual minimization into numerically benign fitting steps and yielding accelerated convergence and improved accuracy. By bridging nonlinear neural representations with reduced-order linear solvers at fixed linearization points, LSR provides a numerically grounded and broadly applicable refinement framework for supervised learning, operator learning, and scientific computing.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13989v1",
        "pdf": "https://arxiv.org/pdf/2601.13989v1"
      },
      "arxiv_id": "2601.13989v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13987v1",
      "title": "SHARE: A Fully Unsupervised Framework for Single Hyperspectral Image Restoration",
      "authors": [
        "Jiangwei Xie",
        "Zhang Wen",
        "Mike Davies",
        "Dongdong Chen"
      ],
      "abstract": "Hyperspectral image (HSI) restoration is a fundamental challenge in computational imaging and computer vision. It involves ill-posed inverse problems, such as inpainting and super-resolution. Although deep learning methods have transformed the field through data-driven learning, their effectiveness hinges on access to meticulously curated ground-truth datasets. This fundamentally restricts their applicability in real-world scenarios where such data is unavailable. This paper presents SHARE (Single Hyperspectral Image Restoration with Equivariance), a fully unsupervised framework that unifies geometric equivariance principles with low-rank spectral modelling to eliminate the need for ground truth. SHARE's core concept is to exploit the intrinsic invariance of hyperspectral structures under differentiable geometric transformations (e.g. rotations and scaling) to derive self-supervision signals through equivariance consistency constraints. Our novel Dynamic Adaptive Spectral Attention (DASA) module further enhances this paradigm shift by explicitly encoding the global low-rank property of HSI and adaptively refining local spectral-spatial correlations through learnable attention mechanisms. Extensive experiments on HSI inpainting and super-resolution tasks demonstrate the effectiveness of SHARE. Our method outperforms many state-of-the-art unsupervised approaches and achieves performance comparable to that of supervised methods. We hope that our approach will shed new light on HSI restoration and broader scientific imaging scenarios. The code will be released at https://github.com/xuwayyy/SHARE.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13987v1",
        "pdf": "https://arxiv.org/pdf/2601.13987v1"
      },
      "arxiv_id": "2601.13987v1",
      "comment": "Technical report",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13986v1",
      "title": "Equivariant Learning for Unsupervised Image Dehazing",
      "authors": [
        "Zhang Wen",
        "Jiangwei Xie",
        "Dongdong Chen"
      ],
      "abstract": "Image Dehazing (ID) aims to produce a clear image from an observation contaminated by haze. Current ID methods typically rely on carefully crafted priors or extensive haze-free ground truth, both of which are expensive or impractical to acquire, particularly in the context of scientific imaging. We propose a new unsupervised learning framework called Equivariant Image Dehazing (EID) that exploits the symmetry of image signals to restore clarity to hazy observations. By enforcing haze consistency and systematic equivariance, EID can recover clear patterns directly from raw, hazy images. Additionally, we propose an adversarial learning strategy to model unknown haze physics and facilitate EID learning. Experiments on two scientific image dehazing benchmarks (including cell microscopy and medical endoscopy) and on natural image dehazing have demonstrated that EID significantly outperforms state-of-the-art approaches. By unifying equivariant learning with modelling haze physics, we hope that EID will enable more versatile and effective haze removal in scientific imaging. Code and datasets will be published.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13986v1",
        "pdf": "https://arxiv.org/pdf/2601.13986v1"
      },
      "arxiv_id": "2601.13986v1",
      "comment": "Technical report",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13976v1",
      "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
      "authors": [
        "Jing Zuo",
        "Lingzhou Mu",
        "Fan Jiang",
        "Chengcheng Ma",
        "Mu Xu",
        "Yonggang Qi"
      ],
      "abstract": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13976v1",
        "pdf": "https://arxiv.org/pdf/2601.13976v1"
      },
      "arxiv_id": "2601.13976v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13975v1",
      "title": "Harmonizing the Deep: A Unified Information Pipeline for Robust Marine Biodiversity Assessment Across Heterogeneous Domains",
      "authors": [
        "Marco Piccolo",
        "Qiwei Han",
        "Astrid van Toor",
        "Joachim Vanneste"
      ],
      "abstract": "Marine biodiversity monitoring requires scalability and reliability across complex underwater environments to support conservation and invasive-species management. Yet existing detection solutions often exhibit a pronounced deployment gap, with performance degrading sharply when transferred to new sites. This work establishes the foundational detection layer for a multi-year invasive species monitoring initiative targeting Arctic and Atlantic marine ecosystems. We address this challenge by developing a Unified Information Pipeline that standardises heterogeneous datasets into a comparable information flow and evaluates a fixed, deployment-relevant detector under controlled cross-domain protocols. Across multiple domains, we find that structural factors, such as scene composition, object density, and contextual redundancy, explain cross-domain performance loss more strongly than visual degradation such as turbidity, with sparse scenes inducing a characteristic \"Context Collapse\" failure mode. We further validate operational feasibility by benchmarking inference on low-cost edge hardware, showing that runtime optimisation enables practical sampling rates for remote monitoring. The results shift emphasis from image enhancement toward structure-aware reliability, providing a democratised tool for consistent marine ecosystem assessment.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13975v1",
        "pdf": "https://arxiv.org/pdf/2601.13975v1"
      },
      "arxiv_id": "2601.13975v1",
      "comment": "9 pages, 4 figures 8 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13974v1",
      "title": "STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames",
      "authors": [
        "Shih-Yao Lin"
      ],
      "abstract": "Frame sampling is a fundamental component in video understanding and video--language model pipelines, yet evaluating the quality of sampled frames remains challenging. Existing evaluation metrics primarily focus on perceptual quality or reconstruction fidelity, and are not designed to assess whether a set of sampled frames adequately captures informative and representative video content.\n  We propose Spatio-Temporal Entropy Coverage (STEC), a simple and non-reference metric for evaluating the effectiveness of video frame sampling. STEC builds upon Spatio-Temporal Frame Entropy (STFE), which measures per-frame spatial information via entropy-based structural complexity, and evaluates sampled frames based on their temporal coverage and redundancy. By jointly modeling spatial information strength, temporal dispersion, and non-redundancy, STEC provides a principled and lightweight measure of sampling quality.\n  Experiments on the MSR-VTT test-1k benchmark demonstrate that STEC clearly differentiates common sampling strategies, including random, uniform, and content-aware methods. We further show that STEC reveals robustness patterns across individual videos that are not captured by average performance alone, highlighting its practical value as a general-purpose evaluation tool for efficient video understanding.\n  We emphasize that STEC is not designed to predict downstream task accuracy, but to provide a task-agnostic diagnostic signal for analyzing frame sampling behavior under constrained budgets.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13974v1",
        "pdf": "https://arxiv.org/pdf/2601.13974v1"
      },
      "arxiv_id": "2601.13974v1",
      "comment": "This paper corresponds to the camera-ready version of a WACV 2026 Workshop paper",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13969v1",
      "title": "Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval",
      "authors": [
        "Joaquín Polonuer",
        "Lucas Vittor",
        "Iñaki Arango",
        "Ayush Noori",
        "David A. Clifton",
        "Luciano Del Corro",
        "Marinka Zitnik"
      ],
      "abstract": "Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13969v1",
        "pdf": "https://arxiv.org/pdf/2601.13969v1"
      },
      "arxiv_id": "2601.13969v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13964v1",
      "title": "RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning",
      "authors": [
        "Cheol-Hui Lee",
        "Hwa-Yeon Lee",
        "Dong-Joo Kim"
      ],
      "abstract": "The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10\\%) of labeled data to guide the agent's policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69\\% and 8.80\\% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task -- for example, Time Masking with a 62\\% probability for sleep stage classification and Crop \\& Resize with a 77\\% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at \\href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13964v1",
        "pdf": "https://arxiv.org/pdf/2601.13964v1"
      },
      "arxiv_id": "2601.13964v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13954v1",
      "title": "DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging",
      "authors": [
        "Adrien Meyer",
        "Didier Mutter",
        "Nicolas Padoy"
      ],
      "abstract": "Detecting anatomical landmarks in medical imaging is essential for diagnosis and intervention guidance. However, object detection models rely on costly bounding box annotations, limiting scalability. Weakly Semi-Supervised Object Detection (WSSOD) with point annotations proposes annotating each instance with a single point, minimizing annotation time while preserving localization signals. A Point-to-Box teacher model, trained on a small box-labeled subset, converts these point annotations into pseudo-box labels to train a student detector. Yet, medical imagery presents unique challenges, including overlapping anatomy, variable object sizes, and elusive structures, which hinder accurate bounding box inference. To overcome these challenges, we introduce DExTeR (DETR with Experts), a transformer-based Point-to-Box regressor tailored for medical imaging. Built upon Point-DETR, DExTeR encodes single-point annotations as object queries, refining feature extraction with the proposed class-guided deformable attention, which guides attention sampling using point coordinates and class labels to capture class-specific characteristics. To improve discrimination in complex structures, it introduces CLICK-MoE (CLass, Instance, and Common Knowledge Mixture of Experts), decoupling class and instance representations to reduce confusion among adjacent or overlapping instances. Finally, we implement a multi-point training strategy which promotes prediction consistency across different point placements, improving robustness to annotation variability. DExTeR achieves state-of-the-art performance across three datasets spanning different medical domains (endoscopy, chest X-rays, and endoscopic ultrasound) highlighting its potential to reduce annotation costs while maintaining high detection accuracy.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13954v1",
        "pdf": "https://arxiv.org/pdf/2601.13954v1"
      },
      "arxiv_id": "2601.13954v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13953v1",
      "title": "Differentiable Logic Synthesis: Spectral Coefficient Selection via Sinkhorn-Constrained Composition",
      "authors": [
        "Gorgi Pavlov"
      ],
      "abstract": "Learning precise Boolean logic via gradient descent remains challenging: neural networks typically converge to \"fuzzy\" approximations that degrade under quantization. We introduce Hierarchical Spectral Composition, a differentiable architecture that selects spectral coefficients from a frozen Boolean Fourier basis and composes them via Sinkhorn-constrained routing with column-sign modulation. Our approach draws on recent insights from Manifold-Constrained Hyper-Connections (mHC), which demonstrated that projecting routing matrices onto the Birkhoff polytope preserves identity mappings and stabilizes large-scale training. We adapt this framework to logic synthesis, adding column-sign modulation to enable Boolean negation -- a capability absent in standard doubly stochastic routing.\n  We validate our approach across four phases of increasing complexity: (1) For n=2 (16 Boolean operations over 4-dim basis), gradient descent achieves 100% accuracy with zero routing drift and zero-loss quantization to ternary masks. (2) For n=3 (10 three-variable operations), gradient descent achieves 76% accuracy, but exhaustive enumeration over 3^8 = 6561 configurations proves that optimal ternary masks exist for all operations (100% accuracy, 39% sparsity). (3) For n=4 (10 four-variable operations over 16-dim basis), spectral synthesis -- combining exact Walsh-Hadamard coefficients, ternary quantization, and MCMC refinement with parallel tempering -- achieves 100% accuracy on all operations. This progression establishes (a) that ternary polynomial threshold representations exist for all tested functions, and (b) that finding them requires methods beyond pure gradient descent as dimensionality grows. All operations enable single-cycle combinational logic inference at 10,959 MOps/s on GPU, demonstrating viability for hardware-efficient neuro-symbolic logic synthesis.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AR",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13953v1",
        "pdf": "https://arxiv.org/pdf/2601.13953v1"
      },
      "arxiv_id": "2601.13953v1",
      "comment": "35 pages, 22 figures. Code available at https://github.com/gogipav14/spectral-llm",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.13951v1",
      "title": "VTONGuard: Automatic Detection and Authentication of AI-Generated Virtual Try-On Content",
      "authors": [
        "Shengyi Wu",
        "Yan Hong",
        "Shengyao Chen",
        "Zheng Wang",
        "Xianbing Sun",
        "Jiahui Zhan",
        "Jun Lan",
        "Jianfu Zhang"
      ],
      "abstract": "With the rapid advancement of generative AI, virtual try-on (VTON) systems are becoming increasingly common in e-commerce and digital entertainment. However, the growing realism of AI-generated try-on content raises pressing concerns about authenticity and responsible use. To address this, we present VTONGuard, a large-scale benchmark dataset containing over 775,000 real and synthetic try-on images. The dataset covers diverse real-world conditions, including variations in pose, background, and garment styles, and provides both authentic and manipulated examples. Based on this benchmark, we conduct a systematic evaluation of multiple detection paradigms under unified training and testing protocols. Our results reveal each method's strengths and weaknesses and highlight the persistent challenge of cross-paradigm generalization. To further advance detection, we design a multi-task framework that integrates auxiliary segmentation to enhance boundary-aware feature learning, achieving the best overall performance on VTONGuard. We expect this benchmark to enable fair comparisons, facilitate the development of more robust detection models, and promote the safe and responsible deployment of VTON technologies in practice.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13951v1",
        "pdf": "https://arxiv.org/pdf/2601.13951v1"
      },
      "arxiv_id": "2601.13951v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13948v1",
      "title": "Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models",
      "authors": [
        "Nikita Kuzmin",
        "Songting Liu",
        "Kong Aik Lee",
        "Eng Siong Chng"
      ],
      "abstract": "Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13948v1",
        "pdf": "https://arxiv.org/pdf/2601.13948v1"
      },
      "arxiv_id": "2601.13948v1",
      "comment": "Accepted by ICASSP2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13945v1",
      "title": "Efficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework",
      "authors": [
        "Yixuan Deng",
        "Tongrun Wu",
        "Donghao Wu",
        "Zeyu Wei",
        "Jiayuan Wang",
        "Zhenglong Sun",
        "Yuqing Tang",
        "Xiaoqiang Ji"
      ],
      "abstract": "As Embodied AI systems move from research prototypes to real world deployments, they tend to evolve rapidly while remaining reliable under workload changes and partial failures. In practice, many deployments are only partially decoupled: middleware moves messages, but shared context and feedback semantics are implicit, causing interface drift, cross-module interference, and brittle recovery at scale. We present ANCHOR, a modular framework that makes decoupling and robustness explicit system-level primitives. ANCHOR separates (i) Canonical Records, an evolvable contract for the standardized shared state, from (ii) a communication bus for many-to-many dissemination and feedback-oriented coordination, forming an inspectable end-to-end loop. We validate closed-loop feasibility on a de-identified workflow instantiation, characterize latency distributions under varying payload sizes and publish rates, and demonstrate automatic stream resumption after hard crashes and restarts even with shared-memory loss. Overall, ANCHOR turns ad-hoc integration glue into explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13945v1",
        "pdf": "https://arxiv.org/pdf/2601.13945v1"
      },
      "arxiv_id": "2601.13945v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13942v1",
      "title": "Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning",
      "authors": [
        "Hongbo Bai",
        "Yujin Zhou",
        "Yile Wu",
        "Chi-Min Chan",
        "Pengcheng Wen",
        "Kunhao Pan",
        "Sirui Han",
        "Yike Guo"
      ],
      "abstract": "Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13942v1",
        "pdf": "https://arxiv.org/pdf/2601.13942v1"
      },
      "arxiv_id": "2601.13942v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13938v1",
      "title": "IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization",
      "authors": [
        "Heyang Zhou",
        "JiaJia Chen",
        "Xiaolu Chen",
        "Jie Bao",
        "Zhen Chen",
        "Yong Liao"
      ],
      "abstract": "As Generative Engines revolutionize information retrieval by synthesizing direct answers from retrieved sources, ensuring source visibility becomes a significant challenge. Improving it through targeted content revisions is a practical strategy termed Generative Engine Optimization (GEO). However, optimizing a document for diverse queries presents a constrained optimization challenge where heterogeneous queries often impose conflicting and competing revision requirements under a limited content budget. To address this challenge, we propose IF-GEO, a \"diverge-then-converge\" framework comprising two phases: (i) mining distinct optimization preferences from representative latent queries; (ii) synthesizing a Global Revision Blueprint for guided editing by coordinating preferences via conflict-aware instruction fusion. To explicitly quantify IF-GEO's objective of cross-query stability, we introduce risk-aware stability metrics. Experiments on multi-query benchmarks demonstrate that IF-GEO achieves substantial performance gains while maintaining robustness across diverse retrieval scenarios.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13938v1",
        "pdf": "https://arxiv.org/pdf/2601.13938v1"
      },
      "arxiv_id": "2601.13938v1",
      "comment": "9 pages, 3 figures. Submitted to ACL 2026. Corresponding author: Zhen Chen",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13935v1",
      "title": "TrackletGPT: A Language-like GPT Framework for White Matter Tract Segmentation",
      "authors": [
        "Anoushkrit Goel",
        "Simroop Singh",
        "Ankita Joshi",
        "Ranjeet Ranjan Jha",
        "Chirag Ahuja",
        "Aditya Nigam",
        "Arnav Bhavsar"
      ],
      "abstract": "White Matter Tract Segmentation is imperative for studying brain structural connectivity, neurological disorders and neurosurgery. This task remains complex, as tracts differ among themselves, across subjects and conditions, yet have similar 3D structure across hemispheres and subjects. To address these challenges, we propose TrackletGPT, a language-like GPT framework which reintroduces sequential information in tokens using tracklets. TrackletGPT generalises seamlessly across datasets, is fully automatic, and encodes granular sub-streamline segments, Tracklets, scaling and refining GPT models in Tractography Segmentation. Based on our experiments, TrackletGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on TractoInferno and HCP datasets, even on inter-dataset experiments.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13935v1",
        "pdf": "https://arxiv.org/pdf/2601.13935v1"
      },
      "arxiv_id": "2601.13935v1",
      "comment": "Accepted at 23rd IEEE International Symposium on Biomedical Imaging (ISBI), 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13931v1",
      "title": "Towards Effective Negation Modeling in Joint Audio-Text Models for Music",
      "authors": [
        "Yannis Vasilakis",
        "Rachel Bittner",
        "Johan Pauwels"
      ],
      "abstract": "Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., \"with vocals\" vs. \"without vocals\"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13931v1",
        "pdf": "https://arxiv.org/pdf/2601.13931v1"
      },
      "arxiv_id": "2601.13931v1",
      "comment": "Accepted at IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13927v1",
      "title": "Towards Modality-Agnostic Continual Domain-Incremental Brain Lesion Segmentation",
      "authors": [
        "Yousef Sadegheih",
        "Dorit Merhof",
        "Pratibha Kumari"
      ],
      "abstract": "Brain lesion segmentation from multi-modal MRI often assumes fixed modality sets or predefined pathologies, making existing models difficult to adapt across cohorts and imaging protocols. Continual learning (CL) offers a natural solution but current approaches either impose a maximum modality configuration or suffer from severe forgetting in buffer-free settings. We introduce CLMU-Net, a replay-based CL framework for 3D brain lesion segmentation that supports arbitrary and variable modality combinations without requiring prior knowledge of the maximum set. A conceptually simple yet effective channel-inflation strategy maps any modality subset into a unified multi-channel representation, enabling a single model to operate across diverse datasets. To enrich inherently local 3D patch features, we incorporate lightweight domain-conditioned textual embeddings that provide global modality-disease context for each training case. Forgetting is further reduced through principled replay using a compact buffer composed of both prototypical and challenging samples. Experiments on five heterogeneous MRI brain datasets demonstrate that CLMU-Net consistently outperforms popular CL baselines. Notably, our method yields an average Dice score improvement of $\\geq$ 18\\% while remaining robust under heterogeneous-modality conditions. These findings underscore the value of flexible modality handling, targeted replay, and global contextual cues for continual medical image segmentation. Our implementation is available at https://github.com/xmindflow/CLMU-Net.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "eess.IV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13927v1",
        "pdf": "https://arxiv.org/pdf/2601.13927v1"
      },
      "arxiv_id": "2601.13927v1",
      "comment": "Submitted to MIDL 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13926v1",
      "title": "SCG With Your Phone: Diagnosis of Rhythmic Spectrum Disorders in Field Conditions",
      "authors": [
        "Peter Golenderov",
        "Yaroslav Matushenko",
        "Anastasia Tushina",
        "Michal Barodkin"
      ],
      "abstract": "Aortic valve opening (AO) events are crucial for detecting frequency and rhythm disorders, especially in real-world settings where seismocardiography (SCG) signals collected via consumer smartphones are subject to noise, motion artifacts, and variability caused by device heterogeneity. In this work, we present a robust deep-learning framework for SCG segmentation and rhythm analysis using accelerometer recordings obtained with consumer smartphones. We develop an enhanced U-Net v3 architecture that integrates multi-scale convolutions, residual connections, and attention gates, enabling reliable segmentation of noisy SCG signals. A dedicated post-processing pipeline converts probability masks into precise AO timestamps, whereas a novel adaptive 3D-to-1D projection method ensures robustness to arbitrary smartphone orientation. Experimental results demonstrate that the proposed method achieves consistently high accuracy and robustness across various device types and unsupervised data-collection conditions. Our approach enables practical, low-cost, and automated cardiac-rhythm monitoring using everyday mobile devices, paving the way for scalable, field-deployable cardiovascular assessment and future multimodal diagnostic systems.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "q-bio.QM",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "q-bio.QM",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13926v1",
        "pdf": "https://arxiv.org/pdf/2601.13926v1"
      },
      "arxiv_id": "2601.13926v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.13920v1",
      "title": "Asymmetric regularization mechanism for GAN training with Variational Inequalities",
      "authors": [
        "Spyridon C. Giagtzoglou",
        "Mark H. M. Winands",
        "Barbara Franci"
      ],
      "abstract": "We formulate the training of generative adversarial networks (GANs) as a Nash equilibrium seeking problem. To stabilize the training process and find a Nash equilibrium, we propose an asymmetric regularization mechanism based on the classic Tikhonov step and on a novel zero-centered gradient penalty. Under smoothness and a local identifiability condition induced by a Gauss-Newton Gramian, we obtain explicit Lipschitz and (strong)-monotonicity constants for the regularized operator. These constants ensure last-iterate linear convergence of a single-call Extrapolation-from-the-Past (EFTP) method. Empirical simulations on an academic example show that, even when strong monotonicity cannot be achieved, the asymmetric regularization is enough to converge to an equilibrium and stabilize the trajectory.",
      "published": "2026-01-20",
      "updated": "2026-01-20",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.GT",
      "links": {
        "paper": "http://arxiv.org/abs/2601.13920v1",
        "pdf": "https://arxiv.org/pdf/2601.13920v1"
      },
      "arxiv_id": "2601.13920v1",
      "comment": "6 pages, 3 figures, conference",
      "journal_ref": "",
      "has_code": false
    }
  ]
}