{
  "fetched_at": "2025-11-19T00:26:01.496331",
  "total_papers": 100,
  "papers": [
    {
      "id": "2511.13720v1",
      "title": "Back to Basics: Let Denoising Generative Models Denoise",
      "authors": [
        "Tianhong Li",
        "Kaiming He"
      ],
      "abstract": "Today's denoising diffusion models do not \"denoise\" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than \"$\\textbf{Just image Transformers}$\", or $\\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13720v1",
        "pdf": "https://arxiv.org/pdf/2511.13720v1"
      },
      "arxiv_id": "2511.13720v1",
      "comment": "Tech report. Code at https://github.com/LTH14/JiT",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.13719v1",
      "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
      "authors": [
        "Zhongang Cai",
        "Ruisi Wang",
        "Chenyang Gu",
        "Fanyi Pu",
        "Junxiang Xu",
        "Yubo Wang",
        "Wanqi Yin",
        "Zhitao Yang",
        "Chen Wei",
        "Qingping Sun",
        "Tongxi Zhou",
        "Jiaqi Li",
        "Hui En Pang",
        "Oscar Qian",
        "Yukun Wei",
        "Zhiqian Lin",
        "Xuanke Shi",
        "Kewang Deng",
        "Xiaoyang Han",
        "Zukai Chen",
        "Xiangyu Fan",
        "Hanming Deng",
        "Lewei Lu",
        "Liang Pan",
        "Bo Li",
        "Ziwei Liu",
        "Quan Wang",
        "Dahua Lin",
        "Lei Yang"
      ],
      "abstract": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13719v1",
        "pdf": "https://arxiv.org/pdf/2511.13719v1"
      },
      "arxiv_id": "2511.13719v1",
      "comment": "Model: https://huggingface.co/collections/sensenova/sensenova-si; Code: https://github.com/OpenSenseNova/SenseNova-SI",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.13715v1",
      "title": "Segment Anything Across Shots: A Method and Benchmark",
      "authors": [
        "Hengrui Hu",
        "Kaining Ying",
        "Henghui Ding"
      ],
      "abstract": "This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13715v1",
        "pdf": "https://arxiv.org/pdf/2511.13715v1"
      },
      "arxiv_id": "2511.13715v1",
      "comment": "AAAI 2026, Project Page: https://henghuiding.com/SAAS/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13714v1",
      "title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity",
      "authors": [
        "Junwei Yu",
        "Trevor Darrell",
        "XuDong Wang"
      ],
      "abstract": "The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\\text{NoC}_{90}$ (5.69 $\\rightarrow$ 4.75), 1-IoU (58.0 $\\rightarrow$ 73.1), and $\\text{AR}_{1000}$ (49.6 $\\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13714v1",
        "pdf": "https://arxiv.org/pdf/2511.13714v1"
      },
      "arxiv_id": "2511.13714v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13713v1",
      "title": "Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine",
      "authors": [
        "Xincheng Shuai",
        "Zhenyuan Qin",
        "Henghui Ding",
        "Dacheng Tao"
      ],
      "abstract": "Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13713v1",
        "pdf": "https://arxiv.org/pdf/2511.13713v1"
      },
      "arxiv_id": "2511.13713v1",
      "comment": "AAAI 2026, Project Page: https://henghuiding.com/FFSE/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13712v1",
      "title": "From Black Box to Insight: Explainable AI for Extreme Event Preparedness",
      "authors": [
        "Kiana Vu",
        "İsmet Selçuk Özer",
        "Phung Lai",
        "Zheng Wu",
        "Thilanka Munasinghe",
        "Jennifer Wei"
      ],
      "abstract": "As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13712v1",
        "pdf": "https://arxiv.org/pdf/2511.13712v1"
      },
      "arxiv_id": "2511.13712v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13710v1",
      "title": "From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands",
      "authors": [
        "Jianglong Ye",
        "Lai Wei",
        "Guangqi Jiang",
        "Changwei Jing",
        "Xueyan Zou",
        "Xiaolong Wang"
      ],
      "abstract": "Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13710v1",
        "pdf": "https://arxiv.org/pdf/2511.13710v1"
      },
      "arxiv_id": "2511.13710v1",
      "comment": "Project page: https://jianglongye.com/power-to-precision",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13705v1",
      "title": "Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering",
      "authors": [
        "Alaa Mezghiche"
      ],
      "abstract": "Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI \"Gene Expression Cancer RNA-Seq\" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "q-bio.GN"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13705v1",
        "pdf": "https://arxiv.org/pdf/2511.13705v1"
      },
      "arxiv_id": "2511.13705v1",
      "comment": "16 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13704v1",
      "title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models",
      "authors": [
        "Harold Haodong Chen",
        "Disen Lan",
        "Wen-Jie Shu",
        "Qingyang Liu",
        "Zihan Wang",
        "Sirui Chen",
        "Wenkai Cheng",
        "Kanghao Chen",
        "Hongfei Zhang",
        "Zixin Zhang",
        "Rongjin Guo",
        "Yu Cheng",
        "Ying-Cong Chen"
      ],
      "abstract": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13704v1",
        "pdf": "https://arxiv.org/pdf/2511.13704v1"
      },
      "arxiv_id": "2511.13704v1",
      "comment": "Project: https://haroldchen19.github.io/TiViBench-Page/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.13703v1",
      "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations",
      "authors": [
        "Lavender Y. Jiang",
        "Angelica Chen",
        "Xu Han",
        "Xujin Chris Liu",
        "Radhika Dua",
        "Kevin Eaton",
        "Frederick Wolff",
        "Robert Steele",
        "Jeff Zhang",
        "Anton Alyakin",
        "Qingkai Pan",
        "Yanbing Chen",
        "Karl L. Sangwon",
        "Daniel A. Alber",
        "Jaden Stryker",
        "Jin Vivian Lee",
        "Yindalon Aphinyanaphongs",
        "Kyunghyun Cho",
        "Eric Karl Oermann"
      ],
      "abstract": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13703v1",
        "pdf": "https://arxiv.org/pdf/2511.13703v1"
      },
      "arxiv_id": "2511.13703v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13702v1",
      "title": "ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification",
      "authors": [
        "Luyao Niu",
        "Nuoxian Huang"
      ],
      "abstract": "Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13702v1",
        "pdf": "https://arxiv.org/pdf/2511.13702v1"
      },
      "arxiv_id": "2511.13702v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13701v1",
      "title": "Learning stochasticity: a nonparametric framework for intrinsic noise estimation",
      "authors": [
        "Gianluigi Pillonetto",
        "Alberto Giaretta",
        "Mauro Bisiacco"
      ],
      "abstract": "Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13701v1",
        "pdf": "https://arxiv.org/pdf/2511.13701v1"
      },
      "arxiv_id": "2511.13701v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13699v1",
      "title": "Efficient Calibration for Decision Making",
      "authors": [
        "Parikshit Gopalan",
        "Konstantinos Stavropoulos",
        "Kunal Talwar",
        "Pranay Tankala"
      ],
      "abstract": "A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.\n  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.DS",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13699v1",
        "pdf": "https://arxiv.org/pdf/2511.13699v1"
      },
      "arxiv_id": "2511.13699v1",
      "comment": "50 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13689v1",
      "title": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation",
      "authors": [
        "Sofia Jamil",
        "Kotla Sai Charan",
        "Sriparna Saha",
        "Koustava Goswami",
        "Joseph K J"
      ],
      "abstract": "Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13689v1",
        "pdf": "https://arxiv.org/pdf/2511.13689v1"
      },
      "arxiv_id": "2511.13689v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13685v1",
      "title": "Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers",
      "authors": [
        "Disha Varshney",
        "Samarth Garg",
        "Sarthak Tyagi",
        "Deeksha Varshney",
        "Nayan Deep",
        "Asif Ekbal"
      ],
      "abstract": "In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13685v1",
        "pdf": "https://arxiv.org/pdf/2511.13685v1"
      },
      "arxiv_id": "2511.13685v1",
      "comment": "40 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13684v1",
      "title": "Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting",
      "authors": [
        "Jiangnan Ye",
        "Jiedong Zhuang",
        "Lianrui Mu",
        "Wenjie Zheng",
        "Jiaqi Hu",
        "Xingze Zou",
        "Jing Wang",
        "Haoji Hu"
      ],
      "abstract": "We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13684v1",
        "pdf": "https://arxiv.org/pdf/2511.13684v1"
      },
      "arxiv_id": "2511.13684v1",
      "comment": "Submitting for Neurocomputing",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13680v1",
      "title": "Cross-Learning from Scarce Data via Multi-Task Constrained Optimization",
      "authors": [
        "Leopoldo Agorio",
        "Juan Cerviño",
        "Miguel Calvo-Fullana",
        "Alejandro Ribeiro",
        "Juan Andrés Bazerque"
      ],
      "abstract": "A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \\emph{cross-learning} framework to overcome data scarcity by jointly estimating \\emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13680v1",
        "pdf": "https://arxiv.org/pdf/2511.13680v1"
      },
      "arxiv_id": "2511.13680v1",
      "comment": "13 pages, 11 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13679v1",
      "title": "QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention",
      "authors": [
        "Hyunwoo Oh",
        "Hanning Chen",
        "Sanggeon Yun",
        "Yang Ni",
        "Wenjun Huang",
        "Tamoghno Das",
        "Suyeon Jang",
        "Mohsen Imani"
      ],
      "abstract": "Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.AR",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13679v1",
        "pdf": "https://arxiv.org/pdf/2511.13679v1"
      },
      "arxiv_id": "2511.13679v1",
      "comment": "Accepted to DATE 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13676v1",
      "title": "T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization",
      "authors": [
        "Hyunwoo Oh",
        "KyungIn Nam",
        "Rajat Bhattacharjya",
        "Hanning Chen",
        "Tamoghno Das",
        "Sanggeon Yun",
        "Suyeon Jang",
        "Andrew Ding",
        "Nikil Dutt",
        "Mohsen Imani"
      ],
      "abstract": "Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.AR",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13676v1",
        "pdf": "https://arxiv.org/pdf/2511.13676v1"
      },
      "arxiv_id": "2511.13676v1",
      "comment": "Accepted to DATE 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13675v1",
      "title": "Scientific Data Compression and Super-Resolution Sampling",
      "authors": [
        "Minh Vu",
        "Andrey Lokhov"
      ],
      "abstract": "Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "physics.data-an",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13675v1",
        "pdf": "https://arxiv.org/pdf/2511.13675v1"
      },
      "arxiv_id": "2511.13675v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13670v1",
      "title": "Person-AI Bidirectional Fit - A Proof-Of-Concept Case Study Of Augmented Human-Ai Symbiosis In Management Decision-Making Process",
      "authors": [
        "Agnieszka Bieńkowska",
        "Jacek Małecki",
        "Alexander Mathiesen-Ohman",
        "Katarzyna Tworek"
      ],
      "abstract": "This article develops the concept of Person-AI bidirectional fit, defined as the continuously evolving, context-sensitive alignment-primarily cognitive, but also emotional and behavioral-between a human decision-maker and an artificial intelligence system. Grounded in contingency theory and quality theory, the study examines the role of P-AI fit in managerial decision-making through a proof-of-concept case study involving a real hiring process for a Senior AI Lead. Three decision pathways are compared: (1) independent evaluations by a CEO, CTO, and CSO; (2) an evaluation produced by an augmented human-AI symbiotic intelligence system (H3LIX-LAIZA); and (3) an assessment generated by a general-purpose large language model. The results reveal substantial role-based divergence in human judgments, high alignment between H3LIX-LAIZA and the CEOs implicit decision model-including ethical disqualification of a high-risk candidate and a critical false-positive recommendation from the LLMr. The findings demonstrate that higher P-AI fit, exemplified by the CEO H3LIX-LAIZA relationship, functions as a mechanism linking augmented symbiotic intelligence to accurate, trustworthy, and context-sensitive decisions. The study provides an initial verification of the P-AI fit construct and a proof-of-concept for H3LIX-LAIZA as an augmented human-AI symbiotic intelligence system.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13670v1",
        "pdf": "https://arxiv.org/pdf/2511.13670v1"
      },
      "arxiv_id": "2511.13670v1",
      "comment": "30 pages, 2 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13663v1",
      "title": "Cost-Driven Synthesis of Sound Abstract Interpreters",
      "authors": [
        "Qiuhan Gu",
        "Avaljot Singh",
        "Gagandeep Singh"
      ],
      "abstract": "Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.PL",
        "cs.LG"
      ],
      "primary_category": "cs.PL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13663v1",
        "pdf": "https://arxiv.org/pdf/2511.13663v1"
      },
      "arxiv_id": "2511.13663v1",
      "comment": "37 pages, 20 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13658v1",
      "title": "Why is \"Chicago\" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues",
      "authors": [
        "Jiaming Qu",
        "Mengtian Guo",
        "Yue Wang"
      ],
      "abstract": "Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13658v1",
        "pdf": "https://arxiv.org/pdf/2511.13658v1"
      },
      "arxiv_id": "2511.13658v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13655v1",
      "title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation",
      "authors": [
        "Henry Herzog",
        "Favyen Bastani",
        "Yawen Zhang",
        "Gabriel Tseng",
        "Joseph Redmon",
        "Hadrien Sablon",
        "Ryan Park",
        "Jacob Morrison",
        "Alexandra Buraczynski",
        "Karen Farley",
        "Joshua Hansen",
        "Andrew Howe",
        "Patrick Alan Johnson",
        "Mark Otterlee",
        "Ted Schmitt",
        "Hunter Pitelka",
        "Stephen Daspit",
        "Rachel Ratner",
        "Christopher Wilhelm",
        "Sebastian Wood",
        "Mike Jacobi",
        "Hannah Kerner",
        "Evan Shelhamer",
        "Ali Farhadi",
        "Ranjay Krishna",
        "Patrick Beukema"
      ],
      "abstract": "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\\href{https://github.com/allenai/olmoearth_pretrain}{\\text{https://github.com/allenai/olmoearth_pretrain}}$.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13655v1",
        "pdf": "https://arxiv.org/pdf/2511.13655v1"
      },
      "arxiv_id": "2511.13655v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13654v1",
      "title": "Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning",
      "authors": [
        "Pascal Zimmer",
        "Ghassan Karame"
      ],
      "abstract": "In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13654v1",
        "pdf": "https://arxiv.org/pdf/2511.13654v1"
      },
      "arxiv_id": "2511.13654v1",
      "comment": "To appear in the Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13653v1",
      "title": "Weight-sparse transformers have interpretable circuits",
      "authors": [
        "Leo Gao",
        "Achyuta Rajaram",
        "Jacob Coxon",
        "Soham V. Govande",
        "Bowen Baker",
        "Dan Mossing"
      ],
      "abstract": "Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13653v1",
        "pdf": "https://arxiv.org/pdf/2511.13653v1"
      },
      "arxiv_id": "2511.13653v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13649v1",
      "title": "Distribution Matching Distillation Meets Reinforcement Learning",
      "authors": [
        "Dengyang Jiang",
        "Dongyang Liu",
        "Zanyi Wang",
        "Qilong Wu",
        "Xin Jin",
        "David Liu",
        "Zhen Li",
        "Mengmeng Wang",
        "Peng Gao",
        "Harry Yang"
      ],
      "abstract": "Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13649v1",
        "pdf": "https://arxiv.org/pdf/2511.13649v1"
      },
      "arxiv_id": "2511.13649v1",
      "comment": "The synergy of reinforcement learning and distribution matching distillation. See more: https://github.com/vvvvvjdy/dmdr",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.13648v1",
      "title": "PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image",
      "authors": [
        "Ziang Cao",
        "Fangzhou Hong",
        "Zhaoxi Chen",
        "Liang Pan",
        "Ziwei Liu"
      ],
      "abstract": "3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13648v1",
        "pdf": "https://arxiv.org/pdf/2511.13648v1"
      },
      "arxiv_id": "2511.13648v1",
      "comment": "Project page: https://physx-anything.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.13647v1",
      "title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model",
      "authors": [
        "Chunshi Wang",
        "Junliang Ye",
        "Yunhan Yang",
        "Yang Li",
        "Zizhuo Lin",
        "Jun Zhu",
        "Zhuo Chen",
        "Yawei Luo",
        "Chunchao Guo"
      ],
      "abstract": "We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13647v1",
        "pdf": "https://arxiv.org/pdf/2511.13647v1"
      },
      "arxiv_id": "2511.13647v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13646v1",
      "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?",
      "authors": [
        "Chunqiu Steven Xia",
        "Zhe Wang",
        "Yan Yang",
        "Yuxiang Wei",
        "Lingming Zhang"
      ],
      "abstract": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13646v1",
        "pdf": "https://arxiv.org/pdf/2511.13646v1"
      },
      "arxiv_id": "2511.13646v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13645v1",
      "title": "FuseSampleAgg: Fused Neighbor Sampling and Aggregation for Mini-batch GNNs",
      "authors": [
        "Aleksandar Stanković"
      ],
      "abstract": "We present FuseSampleAgg, a CUDA operator that fuses neighbor sampling and mean aggregation into a single pass for one and two hop GraphSAGE. By eliminating block materialization and extra kernel launches, FuseSampleAgg reduces memory traffic and overhead while preserving GraphSAGE mean semantics via saved index replay. Across the Reddit, ogbn-arxiv, and ogbn-products benchmarks (batch size 1024, automatic mixed precision enabled), we observe step time speedups up to 51x on ogbn-products, about 4x on Reddit with fanouts 10-10 and 15-10, and about 3.3x on ogbn-arxiv at larger fanouts, with peak GPU memory reductions up to 100x, 36x, and about 3.5x, respectively. The operator is deterministic, integrates with standard PyTorch optimizers, and ships with scripts that reproduce all tables and figures from CSV logs. Code and scripts are available at https://github.com/SV25-22/FuseSampleAgg.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13645v1",
        "pdf": "https://arxiv.org/pdf/2511.13645v1"
      },
      "arxiv_id": "2511.13645v1",
      "comment": "15 pages. Code and reproducibility scripts: https://github.com/SV25-22/FuseSampleAgg",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.13644v1",
      "title": "CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding",
      "authors": [
        "Shrenik Patel",
        "Daivik Patel"
      ],
      "abstract": "Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13644v1",
        "pdf": "https://arxiv.org/pdf/2511.13644v1"
      },
      "arxiv_id": "2511.13644v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13640v1",
      "title": "Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures",
      "authors": [
        "Haohui Wang",
        "Jingyuan Qi",
        "Jianpeng Chen",
        "Jun Wu",
        "Lifu Huang",
        "Lecheng Zheng",
        "Kevin Choi",
        "Balaji Veeramani",
        "Edward Bowen",
        "Alison Hu",
        "Tyler Cody",
        "Dawei Zhou"
      ],
      "abstract": "The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13640v1",
        "pdf": "https://arxiv.org/pdf/2511.13640v1"
      },
      "arxiv_id": "2511.13640v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13637v1",
      "title": "Towards Multimodal Representation Learning in Paediatric Kidney Disease",
      "authors": [
        "Ana Durica",
        "John Booth",
        "Ivana Drobnjak"
      ],
      "abstract": "Paediatric kidney disease varies widely in its presentation and progression, which calls for continuous monitoring of renal function. Using electronic health records collected between 2019 and 2025 at Great Ormond Street Hospital, a leading UK paediatric hospital, we explored a temporal modelling approach that integrates longitudinal laboratory sequences with demographic information. A recurrent neural model trained on these data was used to predict whether a child would record an abnormal serum creatinine value within the following thirty days. Framed as a pilot study, this work provides an initial demonstration that simple temporal representations can capture useful patterns in routine paediatric data and lays the groundwork for future multimodal extensions using additional clinical signals and more detailed renal outcomes.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13637v1",
        "pdf": "https://arxiv.org/pdf/2511.13637v1"
      },
      "arxiv_id": "2511.13637v1",
      "comment": "4 pages, 3 figures. EurIPS 2025 Multimodal Representation Learning for Healthcare (MMRL4H) workshop paper",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13630v1",
      "title": "Beyond Mimicry: Preference Coherence in LLMs",
      "authors": [
        "Luhan Mikaelson",
        "Derek Shiller",
        "Hayley Clatterbuck"
      ],
      "abstract": "We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13630v1",
        "pdf": "https://arxiv.org/pdf/2511.13630v1"
      },
      "arxiv_id": "2511.13630v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13628v1",
      "title": "Smooth Total variation Regularization for Interference Detection and Elimination (STRIDE) for MRI",
      "authors": [
        "Alexander Mertens",
        "Diego Martinez",
        "Amgad Louka",
        "Ying Yang",
        "Chad Harris",
        "Ian Connell"
      ],
      "abstract": "MRI is increasingly desired to function near electronic devices that emit potentially dynamic electromagnetic interference (EMI). To accommodate for this, we propose the STRIDE method, which improves on previous external-sensor-based EMI removal methods by exploiting inherent MR image smoothness in its total variation. STRIDE measures data from both EMI detectors and primary MR imaging coils, transforms this data into the image domain, and for each column of the resulting image array, combines and subtracts data from the EMI detectors in a way that optimizes for total-variation smoothness. Performance was tested on phantom and in-vivo datasets with a 0.5T scanner. STRIDE resulted in visually better EMI removal, higher temporal SNR, larger EMI removal percentage, and lower RMSE than standard implementations. STRIDE is a robust technique that leverages inherent MR image properties to provide improved EMI removal performance over standard algorithms, particularly for time-varying noise sources.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "eess.IV",
        "eess.SP",
        "physics.med-ph"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13628v1",
        "pdf": "https://arxiv.org/pdf/2511.13628v1"
      },
      "arxiv_id": "2511.13628v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13626v1",
      "title": "CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product",
      "authors": [
        "Kaiwen Xue",
        "Chenglong Li",
        "Zhonghong Ou",
        "Guoxin Zhang",
        "Kaoyan Lu",
        "Shuai Lyu",
        "Yifan Zhu",
        "Ping Zong Junpeng Ding",
        "Xinyu Liu",
        "Qunlin Chen",
        "Weiwei Qin",
        "Yiran Shen",
        "Jiayi Cen"
      ],
      "abstract": "Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13626v1",
        "pdf": "https://arxiv.org/pdf/2511.13626v1"
      },
      "arxiv_id": "2511.13626v1",
      "comment": "13 pages, 3 figures,The 40th Annual AAAI Conference on Artificial Intelligence(AAAI 2026),Paper has been accepted for a poster presentation",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13625v1",
      "title": "Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization",
      "authors": [
        "Kaichi Irie",
        "Shuhei Watanabe",
        "Masaki Onishi"
      ],
      "abstract": "Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13625v1",
        "pdf": "https://arxiv.org/pdf/2511.13625v1"
      },
      "arxiv_id": "2511.13625v1",
      "comment": "Accepted to 5th Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13621v1",
      "title": "Alpha Divergence Losses for Biometric Verification",
      "authors": [
        "Dimitrios Koutsianos",
        "Ladislav Mosner",
        "Yannis Panagakis",
        "Themos Stafylakis"
      ],
      "abstract": "Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $α$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $α>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $α$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13621v1",
        "pdf": "https://arxiv.org/pdf/2511.13621v1"
      },
      "arxiv_id": "2511.13621v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13618v1",
      "title": "A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio",
      "authors": [
        "Ashlesha G. Sawant",
        "Shreyash S. Kamble",
        "Raj S. Kanade",
        "Raunak N. Kanugo",
        "Tanishq A. Kapse",
        "Karan A. Bhapse"
      ],
      "abstract": "One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13618v1",
        "pdf": "https://arxiv.org/pdf/2511.13618v1"
      },
      "arxiv_id": "2511.13618v1",
      "comment": "6 pages, 8 referenced papers",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13615v1",
      "title": "Tissue Aware Nuclei Detection and Classification Model for Histopathology Images",
      "authors": [
        "Kesi Xu",
        "Eleni Chiou",
        "Ali Varamesh",
        "Laura Acqualagna",
        "Nasir Rajpoot"
      ],
      "abstract": "Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13615v1",
        "pdf": "https://arxiv.org/pdf/2511.13615v1"
      },
      "arxiv_id": "2511.13615v1",
      "comment": "5 pages, 3 figures. Under review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13612v1",
      "title": "P1: Mastering Physics Olympiads with Reinforcement Learning",
      "authors": [
        "Jiacheng Chen",
        "Qianjia Cheng",
        "Fangchen Yu",
        "Haiyuan Wan",
        "Yuchen Zhang",
        "Shenghe Zheng",
        "Junchi Yao",
        "Qingyang Zhang",
        "Haonan He",
        "Yun Luo",
        "Yufeng Zhao",
        "Futing Wang",
        "Li Sheng",
        "Chengxing Xie",
        "Yuxin Zuo",
        "Yizhuo Li",
        "Wenxauan Zeng",
        "Yulun Wu",
        "Rui Huang",
        "Dongzhan Zhou",
        "Kai Chen",
        "Yu Qiao",
        "Lei Bai",
        "Yu Cheng",
        "Ning Ding",
        "Bowen Zhou",
        "Peng Ye",
        "Ganqu Cui"
      ],
      "abstract": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13612v1",
        "pdf": "https://arxiv.org/pdf/2511.13612v1"
      },
      "arxiv_id": "2511.13612v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13609v1",
      "title": "AtlasMorph: Learning conditional deformable templates for brain MRI",
      "authors": [
        "Marianne Rakic",
        "Andrew Hoopes",
        "S. Mazdak Abulnaga",
        "Mert R. Sabuncu",
        "John V. Guttag",
        "Adrian V. Dalca"
      ],
      "abstract": "Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13609v1",
        "pdf": "https://arxiv.org/pdf/2511.13609v1"
      },
      "arxiv_id": "2511.13609v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13608v1",
      "title": "A Gentle Introduction to Conformal Time Series Forecasting",
      "authors": [
        "M. Stocker",
        "W. Małgorzewicz",
        "M. Fontana",
        "S. Ben Taieb"
      ],
      "abstract": "Conformal prediction is a powerful post-hoc framework for uncertainty quantification that provides distribution-free coverage guarantees. However, these guarantees crucially rely on the assumption of exchangeability. This assumption is fundamentally violated in time series data, where temporal dependence and distributional shifts are pervasive. As a result, classical split-conformal methods may yield prediction intervals that fail to maintain nominal validity. This review unifies recent advances in conformal forecasting methods specifically designed to address nonexchangeable data. We first present a theoretical foundation, deriving finite-sample guarantees for split-conformal prediction under mild weak-dependence conditions. We then survey and classify state-of-the-art approaches that mitigate serial dependence by reweighting calibration data, dynamically updating residual distributions, or adaptively tuning target coverage levels in real time. Finally, we present a comprehensive simulation study that compares these techniques in terms of empirical coverage, interval width, and computational cost, highlighting practical trade-offs and open research directions.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "stat.ME",
        "cs.LG",
        "econ.EM"
      ],
      "primary_category": "stat.ME",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13608v1",
        "pdf": "https://arxiv.org/pdf/2511.13608v1"
      },
      "arxiv_id": "2511.13608v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13607v1",
      "title": "ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement",
      "authors": [
        "Xin Xu",
        "Hao Liu",
        "Wei Liu",
        "Wei Wang",
        "Jiayi Wu",
        "Kui Jiang"
      ],
      "abstract": "Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13607v1",
        "pdf": "https://arxiv.org/pdf/2511.13607v1"
      },
      "arxiv_id": "2511.13607v1",
      "comment": "Accepted by AAAI-26",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13598v1",
      "title": "Robust Client-Server Watermarking for Split Federated Learning",
      "authors": [
        "Jiaxiong Tang",
        "Zhengchunmin Dai",
        "Liantao Wu",
        "Peng Sun",
        "Honglong Chen",
        "Zhenfu Cao"
      ],
      "abstract": "Split Federated Learning (SFL) is renowned for its privacy-preserving nature and low computational overhead among decentralized machine learning paradigms. In this framework, clients employ lightweight models to process private data locally and transmit intermediate outputs to a powerful server for further computation. However, SFL is a double-edged sword: while it enables edge computing and enhances privacy, it also introduces intellectual property ambiguity as both clients and the server jointly contribute to training. Existing watermarking techniques fail to protect both sides since no single participant possesses the complete model. To address this, we propose RISE, a Robust model Intellectual property protection scheme using client-Server watermark Embedding for SFL. Specifically, RISE adopts an asymmetric client-server watermarking design: the server embeds feature-based watermarks through a loss regularization term, while clients embed backdoor-based watermarks by injecting predefined trigger samples into private datasets. This co-embedding strategy enables both clients and the server to verify model ownership. Experimental results on standard datasets and multiple network architectures show that RISE achieves over $95\\%$ watermark detection rate ($p-value \\lt 0.03$) across most settings. It exhibits no mutual interference between client- and server-side watermarks and remains robust against common removal attacks.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13598v1",
        "pdf": "https://arxiv.org/pdf/2511.13598v1"
      },
      "arxiv_id": "2511.13598v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13595v1",
      "title": "Physics-Informed Neural Networks for Nonlinear Output Regulation",
      "authors": [
        "Sebastiano Mengozzi",
        "Giovanni B. Esposito",
        "Michelangelo Bin",
        "Andrea Acquaviva",
        "Andrea Bartolini",
        "Lorenzo Marconi"
      ],
      "abstract": "This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold π(w) and a feedforward input c(w) that render such manifold invariant. The pair (π(w), c(w)) is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates π(w) and c(w) by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13595v1",
        "pdf": "https://arxiv.org/pdf/2511.13595v1"
      },
      "arxiv_id": "2511.13595v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13592v1",
      "title": "Power Homotopy for Zeroth-Order Non-Convex Optimizations",
      "authors": [
        "Chen Xu"
      ],
      "abstract": "We introduce GS-PowerHP, a novel zeroth-order method for non-convex optimization problems of the form $\\max_{x \\in \\mathbb{R}^d} f(x)$. Our approach leverages two key components: a power-transformed Gaussian-smoothed surrogate $F_{N,σ}(μ) = \\mathbb{E}_{x\\sim\\mathcal{N}(μ,σ^2 I_d)}[e^{N f(x)}]$ whose stationary points cluster near the global maximizer $x^*$ of $f$ for sufficiently large $N$, and an incrementally decaying $σ$ for enhanced data efficiency. Under mild assumptions, we prove convergence in expectation to a small neighborhood of $x^*$ with the iteration complexity of $O(d^2 \\varepsilon^{-2})$. Empirical results show our approach consistently ranks among the top three across a suite of competing algorithms. Its robustness is underscored by the final experiment on a substantially high-dimensional problem ($d=150,528$), where it achieved first place on least-likely targeted black-box attacks against images from ImageNet, surpassing all competing methods.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13592v1",
        "pdf": "https://arxiv.org/pdf/2511.13592v1"
      },
      "arxiv_id": "2511.13592v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13590v1",
      "title": "Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation",
      "authors": [
        "Hao Wang",
        "Yuanfeng Song",
        "Xiaoming Yin",
        "Xing Chen"
      ],
      "abstract": "Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13590v1",
        "pdf": "https://arxiv.org/pdf/2511.13590v1"
      },
      "arxiv_id": "2511.13590v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13588v1",
      "title": "Data-driven Acceleration of MPC with Guarantees",
      "authors": [
        "Agustin Castellano",
        "Shijie Pan",
        "Enrique Mallada"
      ],
      "abstract": "Model Predictive Control (MPC) is a powerful framework for optimal control but can be too slow for low-latency applications. We present a data-driven framework to accelerate MPC by replacing online optimization with a nonparametric policy constructed from offline MPC solutions. Our policy is greedy with respect to a constructed upper bound on the optimal cost-to-go, and can be implemented as a nonparametric lookup rule that is orders of magnitude faster than solving MPC online. Our analysis shows that under sufficient coverage condition of the offline data, the policy is recursively feasible and admits provable, bounded optimality gap. These conditions establish an explicit trade-off between the amount of data collected and the tightness of the bounds. Our experiments show that this policy is between 100 and 1000 times faster than standard MPC, with only a modest hit to optimality, showing potential for real-time control tasks.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "eess.SY",
        "cs.AI",
        "math.DS"
      ],
      "primary_category": "eess.SY",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13588v1",
        "pdf": "https://arxiv.org/pdf/2511.13588v1"
      },
      "arxiv_id": "2511.13588v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13587v1",
      "title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping",
      "authors": [
        "Haotian Dong",
        "Ye Li",
        "Rongwei Lu",
        "Chen Tang",
        "Shu-Tao Xia",
        "Zhi Wang"
      ],
      "abstract": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13587v1",
        "pdf": "https://arxiv.org/pdf/2511.13587v1"
      },
      "arxiv_id": "2511.13587v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13586v1",
      "title": "Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images",
      "authors": [
        "Yinuo Xu",
        "Yan Cui",
        "Mingyao Li",
        "Zhi Huang"
      ],
      "abstract": "Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.\n  To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.\n  To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13586v1",
        "pdf": "https://arxiv.org/pdf/2511.13586v1"
      },
      "arxiv_id": "2511.13586v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13575v1",
      "title": "Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification",
      "authors": [
        "Linhan Zhou",
        "Shuang Li",
        "Neng Dong",
        "Yonghang Tai",
        "Yafei Zhang",
        "Huafeng Li"
      ],
      "abstract": "Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13575v1",
        "pdf": "https://arxiv.org/pdf/2511.13575v1"
      },
      "arxiv_id": "2511.13575v1",
      "comment": "9 pages, 4 figures, accepted by AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13571v1",
      "title": "Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation",
      "authors": [
        "Ziyang Huang",
        "Jiagang Chen",
        "Jin Liu",
        "Shunping Ji"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13571v1",
        "pdf": "https://arxiv.org/pdf/2511.13571v1"
      },
      "arxiv_id": "2511.13571v1",
      "comment": "Accepted at AAAI 2026 as a Conference Paper",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13565v1",
      "title": "Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction",
      "authors": [
        "Jingyi Zhao",
        "Daqian Shi",
        "Zhengda Wang",
        "Xiongfeng Tang",
        "Yanguo Qin"
      ],
      "abstract": "Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13565v1",
        "pdf": "https://arxiv.org/pdf/2511.13565v1"
      },
      "arxiv_id": "2511.13565v1",
      "comment": "5 pages, l figure, l table. Accepted at AI4RWC@WI-IAT 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13561v1",
      "title": "RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise",
      "authors": [
        "Shihao Dong",
        "Yue Liu",
        "Xiaotong Zhou",
        "Yuhui Zheng",
        "Huiying Xu",
        "Xinzhong Zhu"
      ],
      "abstract": "Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13561v1",
        "pdf": "https://arxiv.org/pdf/2511.13561v1"
      },
      "arxiv_id": "2511.13561v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13552v1",
      "title": "TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images",
      "authors": [
        "Sining Chen",
        "Xiao Xiang Zhu"
      ],
      "abstract": "Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13552v1",
        "pdf": "https://arxiv.org/pdf/2511.13552v1"
      },
      "arxiv_id": "2511.13552v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13548v1",
      "title": "ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models",
      "authors": [
        "Siyang Cheng",
        "Gaotian Liu",
        "Rui Mei",
        "Yilin Wang",
        "Kejia Zhang",
        "Kaishuo Wei",
        "Yuqi Yu",
        "Weiping Wen",
        "Xiaojie Wu",
        "Junhua Liu"
      ],
      "abstract": "The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \\textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13548v1",
        "pdf": "https://arxiv.org/pdf/2511.13548v1"
      },
      "arxiv_id": "2511.13548v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13545v1",
      "title": "Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks",
      "authors": [
        "Md. Iqbal Hossain",
        "Afia Sajeeda",
        "Neeresh Kumar Perla",
        "Ming Shao"
      ],
      "abstract": "The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13545v1",
        "pdf": "https://arxiv.org/pdf/2511.13545v1"
      },
      "arxiv_id": "2511.13545v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13542v1",
      "title": "Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop",
      "authors": [
        "Amirreza Mehrabi",
        "Jason Wade Morphew",
        "Breejha Quezada",
        "N. Sanjay Rebello"
      ],
      "abstract": "Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.CY",
        "stat.AP"
      ],
      "primary_category": "cs.CE",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13542v1",
        "pdf": "https://arxiv.org/pdf/2511.13542v1"
      },
      "arxiv_id": "2511.13542v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13541v1",
      "title": "Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries",
      "authors": [
        "Yue Hou",
        "Ruomei Liu",
        "Yingke Su",
        "Junran Wu",
        "Ke Xu"
      ],
      "abstract": "A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13541v1",
        "pdf": "https://arxiv.org/pdf/2511.13541v1"
      },
      "arxiv_id": "2511.13541v1",
      "comment": "Accepted by AAAI 2026 (The 40th Annual AAAI Conference on Artificial Intelligence)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13540v1",
      "title": "Fairness-Aware Graph Representation Learning with Limited Demographic Information",
      "authors": [
        "Zichong Wang",
        "Zhipeng Yin",
        "Liping Yang",
        "Jun Zhuang",
        "Rui Yu",
        "Qingzhao Kong",
        "Wenbin Zhang"
      ],
      "abstract": "Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13540v1",
        "pdf": "https://arxiv.org/pdf/2511.13540v1"
      },
      "arxiv_id": "2511.13540v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13539v1",
      "title": "BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse",
      "authors": [
        "Yuanchao Wang",
        "Tian Qin",
        "Eduardo Valle",
        "Bruno Abrahao"
      ],
      "abstract": "Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13539v1",
        "pdf": "https://arxiv.org/pdf/2511.13539v1"
      },
      "arxiv_id": "2511.13539v1",
      "comment": "8 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13535v1",
      "title": "Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew",
      "authors": [
        "Farhin Farhad Riya",
        "Shahinul Hoque",
        "Jinyuan Stella Sun",
        "Olivera Kotevska"
      ],
      "abstract": "As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13535v1",
        "pdf": "https://arxiv.org/pdf/2511.13535v1"
      },
      "arxiv_id": "2511.13535v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13533v1",
      "title": "Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems",
      "authors": [
        "Jeffrey Wen",
        "Rizwan Ahmad",
        "Philip Schniter"
      ],
      "abstract": "In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13533v1",
        "pdf": "https://arxiv.org/pdf/2511.13533v1"
      },
      "arxiv_id": "2511.13533v1",
      "comment": "",
      "journal_ref": "Transactions on Machine Learning Research, 11/2025. https://openreview.net/forum?id=53FEYwDQK0",
      "has_code": false
    },
    {
      "id": "2511.13530v1",
      "title": "Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety",
      "authors": [
        "Vesna Poprcova",
        "Iulia Lefter",
        "Matthias Wieser",
        "Martijn Warnier",
        "Frances Brazier"
      ],
      "abstract": "Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13530v1",
        "pdf": "https://arxiv.org/pdf/2511.13530v1"
      },
      "arxiv_id": "2511.13530v1",
      "comment": "Accepted at the Workshop on Benefits of pErsonalization and behAvioral adaptation in assistive Robots (BEAR 2025), held at the IEEE RO-MAN Conference 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13529v1",
      "title": "Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets",
      "authors": [
        "Máté Gedeon",
        "Piroska Zsófia Barta",
        "Péter Mihajlik",
        "Tekla Etelka Gráczi",
        "Anna Kohári",
        "Katalin Mády"
      ],
      "abstract": "The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\\% on spontaneous and 4.8\\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\\% and 18.26\\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13529v1",
        "pdf": "https://arxiv.org/pdf/2511.13529v1"
      },
      "arxiv_id": "2511.13529v1",
      "comment": "Submitted to LREC 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13527v1",
      "title": "Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images",
      "authors": [
        "Ihab Asaad",
        "Maha Shadaydeh",
        "Joachim Denzler"
      ],
      "abstract": "Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13527v1",
        "pdf": "https://arxiv.org/pdf/2511.13527v1"
      },
      "arxiv_id": "2511.13527v1",
      "comment": "Accepted at EurIPS 2025 Workshop: Unifying Perspectives on Learning Biases (UPLB)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13526v1",
      "title": "Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models",
      "authors": [
        "Zhengda Wang",
        "Daqian Shi",
        "Jingyi Zhao",
        "Xiaolei Diao",
        "Xiongfeng Tang",
        "Yanguo Qin"
      ],
      "abstract": "Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13526v1",
        "pdf": "https://arxiv.org/pdf/2511.13526v1"
      },
      "arxiv_id": "2511.13526v1",
      "comment": "5 pages, 1 figure, 1 table. Accepted at AI4RWC@WI-IAT 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13525v1",
      "title": "AI Fairness Beyond Complete Demographics: Current Achievements and Future Directions",
      "authors": [
        "Zichong Wang",
        "Zhipeng Yin",
        "Roland H. C. Yap",
        "Wenbin Zhang"
      ],
      "abstract": "Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13525v1",
        "pdf": "https://arxiv.org/pdf/2511.13525v1"
      },
      "arxiv_id": "2511.13525v1",
      "comment": "ECAI 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13524v1",
      "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI",
      "authors": [
        "Yuhang Peng",
        "Yizhou Pan",
        "Xinning He",
        "Jihaoyu Yang",
        "Xinyu Yin",
        "Han Wang",
        "Xiaoji Zheng",
        "Chao Gao",
        "Jiangtao Gong"
      ],
      "abstract": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13524v1",
        "pdf": "https://arxiv.org/pdf/2511.13524v1"
      },
      "arxiv_id": "2511.13524v1",
      "comment": "9 pages, 4 figures",
      "journal_ref": "AAAI 2026 Oral",
      "has_code": false
    },
    {
      "id": "2511.13514v1",
      "title": "A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data",
      "authors": [
        "Pragatheeswaran Vipulananthan",
        "Kamal Premaratne",
        "Dilip Sarkar",
        "Manohar N. Murthi"
      ],
      "abstract": "Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{ö}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box\" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13514v1",
        "pdf": "https://arxiv.org/pdf/2511.13514v1"
      },
      "arxiv_id": "2511.13514v1",
      "comment": "IEEE International Conference on Knowledge Graph (ICKG), 378-387, 2024",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13510v1",
      "title": "Naga: Vedic Encoding for Deep State Space Models",
      "authors": [
        "Melanie Schaller",
        "Nick Janssen",
        "Bodo Rosenhahn"
      ],
      "abstract": "This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13510v1",
        "pdf": "https://arxiv.org/pdf/2511.13510v1"
      },
      "arxiv_id": "2511.13510v1",
      "comment": "submitted to JMLR",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13507v1",
      "title": "Mapping the Vanishing and Transformation of Urban Villages in China",
      "authors": [
        "Wenyu Zhang",
        "Yao Tong",
        "Yiqiu Liu",
        "Rui Cao"
      ],
      "abstract": "Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the \"remained-demolished-redeveloped\" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13507v1",
        "pdf": "https://arxiv.org/pdf/2511.13507v1"
      },
      "arxiv_id": "2511.13507v1",
      "comment": "Appendix A. Supplementary data at https://ars.els-cdn.com/content/image/1-s2.0-S2210670725008418-mmc1.docx",
      "journal_ref": "Zhang, W., Tong, Y., Liu, Y., & Cao, R. (2025). Mapping the vanishing and transformation of urban villages in China. Sustainable Cities and Society, 135, 106970",
      "has_code": false
    },
    {
      "id": "2511.13503v1",
      "title": "The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business",
      "authors": [
        "Ioannis Diamantis"
      ],
      "abstract": "Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \\textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "stat.ML",
        "cs.LG",
        "econ.EM",
        "stat.AP"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13503v1",
        "pdf": "https://arxiv.org/pdf/2511.13503v1"
      },
      "arxiv_id": "2511.13503v1",
      "comment": "36 pages, 22 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13497v1",
      "title": "Quantum Machine Learning via Contrastive Training",
      "authors": [
        "Liudmila A. Zhukas",
        "Vivian Ni Zhang",
        "Qiang Miao",
        "Qingfeng Wang",
        "Marko Cetina",
        "Jungsang Kim",
        "Lawrence Carin",
        "Christopher Monroe"
      ],
      "abstract": "Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13497v1",
        "pdf": "https://arxiv.org/pdf/2511.13497v1"
      },
      "arxiv_id": "2511.13497v1",
      "comment": "7 figures, 20 pages total",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13494v1",
      "title": "Language-Guided Invariance Probing of Vision-Language Models",
      "authors": [
        "Jae Joong Lee"
      ],
      "abstract": "Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.\n  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13494v1",
        "pdf": "https://arxiv.org/pdf/2511.13494v1"
      },
      "arxiv_id": "2511.13494v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13488v1",
      "title": "InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE",
      "authors": [
        "Lipeng Wang",
        "Hongxing Fan",
        "Haohua Chen",
        "Zehuan Huang",
        "Lu Sheng"
      ],
      "abstract": "Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13488v1",
        "pdf": "https://arxiv.org/pdf/2511.13488v1"
      },
      "arxiv_id": "2511.13488v1",
      "comment": "Accepted to AAAI-26. Codes: https://github.com/Lighten001/InterMoE",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.13487v1",
      "title": "Systematic evaluation of time-frequency features for binaural sound source localization",
      "authors": [
        "Davoud Shariat Panah",
        "Alessandro Ragano",
        "Dan Barry",
        "Jan Skoglund",
        "Andrew Hines"
      ],
      "abstract": "This study presents a systematic evaluation of time-frequency feature design for binaural sound source localization (SSL), focusing on how feature selection influences model performance across diverse conditions. We investigate the performance of a convolutional neural network (CNN) model using various combinations of amplitude-based features (magnitude spectrogram, interaural level difference - ILD) and phase-based features (phase spectrogram, interaural phase difference - IPD). Evaluations on in-domain and out-of-domain data with mismatched head-related transfer functions (HRTFs) reveal that carefully chosen feature combinations often outperform increases in model complexity. While two-feature sets such as ILD + IPD are sufficient for in-domain SSL, generalization to diverse content requires richer inputs combining channel spectrograms with both ILD and IPD. Using the optimal feature sets, our low-complexity CNN model achieves competitive performance. Our findings underscore the importance of feature design in binaural SSL and provide practical guidance for both domain-specific and general-purpose localization.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13487v1",
        "pdf": "https://arxiv.org/pdf/2511.13487v1"
      },
      "arxiv_id": "2511.13487v1",
      "comment": "Submitted to ICASSP 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13480v1",
      "title": "A Lexical Analysis of online Reviews on Human-AI Interactions",
      "authors": [
        "Parisa Arbab",
        "Xiaowen Fang"
      ],
      "abstract": "This study focuses on understanding the complex dynamics between humans and AI systems by analyzing user reviews. While previous research has explored various aspects of human-AI interaction, such as user perceptions and ethical considerations, there remains a gap in understanding the specific concerns and challenges users face. By using a lexical approach to analyze 55,968 online reviews from G2.com, Producthunt.com, and Trustpilot.com, this preliminary research aims to analyze human-AI interaction. Initial results from factor analysis reveal key factors influencing these interactions. The study aims to provide deeper insights into these factors through content analysis, contributing to the development of more user-centric AI systems. The findings are expected to enhance our understanding of human-AI interaction and inform future AI technology and user experience improvements.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13480v1",
        "pdf": "https://arxiv.org/pdf/2511.13480v1"
      },
      "arxiv_id": "2511.13480v1",
      "comment": "10 pages, 1 table",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13478v1",
      "title": "Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling",
      "authors": [
        "Adam Hazimeh",
        "Ke Wang",
        "Mark Collier",
        "Gilles Baechler",
        "Efi Kokiopoulou",
        "Pascal Frossard"
      ],
      "abstract": "Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13478v1",
        "pdf": "https://arxiv.org/pdf/2511.13478v1"
      },
      "arxiv_id": "2511.13478v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13476v1",
      "title": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation",
      "authors": [
        "Zhipeng Ma",
        "Ali Rida Bahja",
        "Andreas Burgdorf",
        "André Pomp",
        "Tobias Meisen",
        "Bo Nørregaard Jørgensen",
        "Zheng Grace Ma"
      ],
      "abstract": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13476v1",
        "pdf": "https://arxiv.org/pdf/2511.13476v1"
      },
      "arxiv_id": "2511.13476v1",
      "comment": "",
      "journal_ref": "Applied Sciences, 2025, 15(21), 11619",
      "has_code": false
    },
    {
      "id": "2511.13469v1",
      "title": "GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction",
      "authors": [
        "Shiyuan Luo",
        "Chonghao Qiu",
        "Runlong Yu",
        "Yiqun Xie",
        "Xiaowei Jia"
      ],
      "abstract": "Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13469v1",
        "pdf": "https://arxiv.org/pdf/2511.13469v1"
      },
      "arxiv_id": "2511.13469v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13466v1",
      "title": "The Quick Red Fox gets the best Data Driven Classroom Interviews: A manual for an interview app and its associated methodology",
      "authors": [
        "Jaclyn Ocumpaugh",
        "Luc Paquette",
        "Ryan S. Baker",
        "Amanda Barany",
        "Jeff Ginger",
        "Nathan Casano",
        "Andres F. Zambrano",
        "Xiner Liu",
        "Zhanlan Wei",
        "Yiqui Zhou",
        "Qianhui Liu",
        "Stephen Hutt",
        "Alexandra M. A. Andres",
        "Nidhi Nasiar",
        "Camille Giordano",
        "Martin van Velsen",
        "Micheal Mogessi"
      ],
      "abstract": "Data Driven Classroom Interviews (DDCIs) are an interviewing technique that is facilitated by recent technological developments in the learning analytics community. DDCIs are short, targeted interviews that allow researchers to contextualize students' interactions with a digital learning environment (e.g., intelligent tutoring systems or educational games) while minimizing the amount of time that the researcher interrupts that learning experience, and focusing researcher time on the events they most want to focus on DDCIs are facilitated by a research tool called the Quick Red Fox (QRF)--an open-source server-client Android app that optimizes researcher time by directing interviewers to users that have just displayed an interesting behavior (previously defined by the research team). QRF integrates with existing student modeling technologies (e.g., behavior-sensing, affect-sensing, detection of self-regulated learning) to alert researchers to key moments in a learner's experience. This manual documents the tech while providing training on the processes involved in developing triggers and interview techniques; it also suggests methods of analyses.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13466v1",
        "pdf": "https://arxiv.org/pdf/2511.13466v1"
      },
      "arxiv_id": "2511.13466v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13465v1",
      "title": "AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate",
      "authors": [
        "Meng Zhu",
        "Quan Xiao",
        "Weidong Min"
      ],
      "abstract": "Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13465v1",
        "pdf": "https://arxiv.org/pdf/2511.13465v1"
      },
      "arxiv_id": "2511.13465v1",
      "comment": "25 pages, 6 figures, 12 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13463v1",
      "title": "Multi-task GINN-LP for Multi-target Symbolic Regression",
      "authors": [
        "Hussein Rajabu",
        "Lijun Qian",
        "Xishuang Dong"
      ],
      "abstract": "In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13463v1",
        "pdf": "https://arxiv.org/pdf/2511.13463v1"
      },
      "arxiv_id": "2511.13463v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13458v1",
      "title": "Trust in Vision-Language Models: Insights from a Participatory User Workshop",
      "authors": [
        "Agnese Chiatti",
        "Lara Piccolo",
        "Sara Bernardini",
        "Matteo Matteucci",
        "Viola Schiaffonati"
      ],
      "abstract": "With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13458v1",
        "pdf": "https://arxiv.org/pdf/2511.13458v1"
      },
      "arxiv_id": "2511.13458v1",
      "comment": "",
      "journal_ref": "Proceedings of the The European Workshop on Trustworthy AI (Trust-AI) at ECAI 2025",
      "has_code": false
    },
    {
      "id": "2511.13457v1",
      "title": "Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure",
      "authors": [
        "Bin Liu",
        "Qinghao Zhao",
        "Yuxi Zhou",
        "Zhejun Sun",
        "Kaijie Lei",
        "Deyun Zhang",
        "Shijia Geng",
        "Shenda Hong"
      ],
      "abstract": "Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13457v1",
        "pdf": "https://arxiv.org/pdf/2511.13457v1"
      },
      "arxiv_id": "2511.13457v1",
      "comment": "19 pages, 5 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13453v1",
      "title": "Hardware optimization on Android for inference of AI models",
      "authors": [
        "Iulius Gherasim",
        "Carlos García Sánchez"
      ],
      "abstract": "The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13453v1",
        "pdf": "https://arxiv.org/pdf/2511.13453v1"
      },
      "arxiv_id": "2511.13453v1",
      "comment": "8 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13444v1",
      "title": "Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes",
      "authors": [
        "Zhipeng Ma",
        "Bo Nørregaard Jørgensen",
        "Zheng Grace Ma"
      ],
      "abstract": "Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13444v1",
        "pdf": "https://arxiv.org/pdf/2511.13444v1"
      },
      "arxiv_id": "2511.13444v1",
      "comment": "",
      "journal_ref": "Information 2025, 16(9), 816",
      "has_code": false
    },
    {
      "id": "2511.13442v1",
      "title": "Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline",
      "authors": [
        "Rui Zuo",
        "Qinyue Tong",
        "Zhe-Ming Lu",
        "Ziqian Lu"
      ],
      "abstract": "With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13442v1",
        "pdf": "https://arxiv.org/pdf/2511.13442v1"
      },
      "arxiv_id": "2511.13442v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13431v1",
      "title": "FUSE: A Flow-based Mapping Between Shapes",
      "authors": [
        "Lorenzo Olearo",
        "Giulio Viganò",
        "Daniele Baieri",
        "Filippo Maggioli",
        "Simone Melzi"
      ],
      "abstract": "We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13431v1",
        "pdf": "https://arxiv.org/pdf/2511.13431v1"
      },
      "arxiv_id": "2511.13431v1",
      "comment": "11 pages, 9 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13421v1",
      "title": "Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression",
      "authors": [
        "Tingkai Yan",
        "Haodong Wen",
        "Binghui Li",
        "Kairong Luo",
        "Wenguang Chen",
        "Kaifeng Lyu"
      ],
      "abstract": "While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \\textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \\approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \\textit{i.e.}, $E(K, N) \\approx K$ for $K \\le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \\approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13421v1",
        "pdf": "https://arxiv.org/pdf/2511.13421v1"
      },
      "arxiv_id": "2511.13421v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13420v1",
      "title": "VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task",
      "authors": [
        "Xingming Long",
        "Jie Zhang",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "abstract": "Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13420v1",
        "pdf": "https://arxiv.org/pdf/2511.13420v1"
      },
      "arxiv_id": "2511.13420v1",
      "comment": "8 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13419v1",
      "title": "MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction",
      "authors": [
        "Shaheen Mohammed Saleh Ahmed",
        "Hakan Hakan Guneyli"
      ],
      "abstract": "Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13419v1",
        "pdf": "https://arxiv.org/pdf/2511.13419v1"
      },
      "arxiv_id": "2511.13419v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13418v1",
      "title": "Exploring Multi-Table Retrieval Through Iterative Search",
      "authors": [
        "Allaa Boutaleb",
        "Bernd Amann",
        "Rafael Angarita",
        "Hubert Naacke"
      ],
      "abstract": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13418v1",
        "pdf": "https://arxiv.org/pdf/2511.13418v1"
      },
      "arxiv_id": "2511.13418v1",
      "comment": "Accepted @ the AI for Tabular Data Workshop, EurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13417v1",
      "title": "Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source",
      "authors": [
        "Mykola Lavreniuk",
        "Nataliia Kussul",
        "Andrii Shelestov",
        "Yevhenii Salii",
        "Volodymyr Kuzin",
        "Sergii Skakun",
        "Zoltan Szantoi"
      ],
      "abstract": "Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13417v1",
        "pdf": "https://arxiv.org/pdf/2511.13417v1"
      },
      "arxiv_id": "2511.13417v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13415v1",
      "title": "Attention Grounded Enhancement for Visual Document Retrieval",
      "authors": [
        "Wanqing Cui",
        "Wei Huang",
        "Yazhi Guo",
        "Yibo Hu",
        "Meiguang Jin",
        "Junfeng Ma",
        "Keping Bi"
      ],
      "abstract": "Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \\textbf{A}ttention-\\textbf{G}rounded \\textbf{RE}triever \\textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.IR",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13415v1",
        "pdf": "https://arxiv.org/pdf/2511.13415v1"
      },
      "arxiv_id": "2511.13415v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13414v1",
      "title": "PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation",
      "authors": [
        "Hanwen Hu",
        "Zimo Wen",
        "Shiyou Qian",
        "Jian Co"
      ],
      "abstract": "Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13414v1",
        "pdf": "https://arxiv.org/pdf/2511.13414v1"
      },
      "arxiv_id": "2511.13414v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.13411v1",
      "title": "An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence",
      "authors": [
        "Przemyslaw Chojecki"
      ],
      "abstract": "We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing \"baby AGI\" becomes Superintelligence intuition.",
      "published": "2025-11-17",
      "updated": "2025-11-17",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.13411v1",
        "pdf": "https://arxiv.org/pdf/2511.13411v1"
      },
      "arxiv_id": "2511.13411v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    }
  ]
}