{
  "fetched_at": "2026-01-16T00:27:21.891579",
  "total_papers": 100,
  "papers": [
    {
      "id": "2601.09708v1",
      "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
      "authors": [
        "Chi-Pin Huang",
        "Yunze Man",
        "Zhiding Yu",
        "Min-Hung Chen",
        "Jan Kautz",
        "Yu-Chiang Frank Wang",
        "Fu-En Yang"
      ],
      "abstract": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09708v1",
        "pdf": "https://arxiv.org/pdf/2601.09708v1"
      },
      "arxiv_id": "2601.09708v1",
      "comment": "Project page: https://jasper0314-huang.github.io/fast-thinkact/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.09706v1",
      "title": "Value-Aware Numerical Representations for Transformer Language Models",
      "authors": [
        "Andreea Dutulescu",
        "Stefan Ruseti",
        "Mihai Dascalu"
      ],
      "abstract": "Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. This mechanism injects magnitude information directly into the model's input space while remaining compatible with existing tokenizers and decoder-only Transformer architectures. Evaluation on arithmetic tasks shows that the proposed approach outperforms baselines across numerical formats, tasks, and operand lengths. These results indicate that explicitly encoding numerical value is an effective and efficient way to improve fundamental numerical robustness in language models.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09706v1",
        "pdf": "https://arxiv.org/pdf/2601.09706v1"
      },
      "arxiv_id": "2601.09706v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09703v1",
      "title": "ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation",
      "authors": [
        "Sicong Liu",
        "Yanxian Huang",
        "Mingwei Liu",
        "Jiachi Chen",
        "Ensheng Shi",
        "Yuchi Ma",
        "Hongyu Zhang",
        "Yin Zhang",
        "Yanlin Wang"
      ],
      "abstract": "Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09703v1",
        "pdf": "https://arxiv.org/pdf/2601.09703v1"
      },
      "arxiv_id": "2601.09703v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09699v1",
      "title": "SAM3-DMS: Decoupled Memory Selection for Multi-target Video Segmentation of SAM3",
      "authors": [
        "Ruiqi Shen",
        "Chang Liu",
        "Henghui Ding"
      ],
      "abstract": "Segment Anything 3 (SAM3) has established a powerful foundation that robustly detects, segments, and tracks specified targets in videos. However, in its original implementation, its group-level collective memory selection is suboptimal for complex multi-object scenarios, as it employs a synchronized decision across all concurrent targets conditioned on their average performance, often overlooking individual reliability. To this end, we propose SAM3-DMS, a training-free decoupled strategy that utilizes fine-grained memory selection on individual objects. Experiments demonstrate that our approach achieves robust identity preservation and tracking stability. Notably, our advantage becomes more pronounced with increased target density, establishing a solid foundation for simultaneous multi-target video segmentation in the wild.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09699v1",
        "pdf": "https://arxiv.org/pdf/2601.09699v1"
      },
      "arxiv_id": "2601.09699v1",
      "comment": "Code: https://github.com/FudanCVL/SAM3-DMS",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.09698v1",
      "title": "COMPOSE: Hypergraph Cover Optimization for Multi-view 3D Human Pose Estimation",
      "authors": [
        "Tony Danjun Wang",
        "Tolga Birdal",
        "Nassir Navab",
        "Lennart Bastian"
      ],
      "abstract": "3D pose estimation from sparse multi-views is a critical task for numerous applications, including action recognition, sports analysis, and human-robot interaction. Optimization-based methods typically follow a two-stage pipeline, first detecting 2D keypoints in each view and then associating these detections across views to triangulate the 3D pose. Existing methods rely on mere pairwise associations to model this correspondence problem, treating global consistency between views (i.e., cycle consistency) as a soft constraint. Yet, reconciling these constraints for multiple views becomes brittle when spurious associations propagate errors. We thus propose COMPOSE, a novel framework that formulates multi-view pose correspondence matching as a hypergraph partitioning problem rather than through pairwise association. While the complexity of the resulting integer linear program grows exponentially in theory, we introduce an efficient geometric pruning strategy to substantially reduce the search space. COMPOSE achieves improvements of up to 23% in average precision over previous optimization-based methods and up to 11% over self-supervised end-to-end learned methods, offering a promising solution to a widely studied problem.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09698v1",
        "pdf": "https://arxiv.org/pdf/2601.09698v1"
      },
      "arxiv_id": "2601.09698v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09697v1",
      "title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering",
      "authors": [
        "Jieying Chen",
        "Jeffrey Hu",
        "Joan Lasenby",
        "Ayush Tewari"
      ],
      "abstract": "Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09697v1",
        "pdf": "https://arxiv.org/pdf/2601.09697v1"
      },
      "arxiv_id": "2601.09697v1",
      "comment": "Project page: https://ayushtewari.com/projects/srender/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09694v1",
      "title": "LLMs can Compress LLMs: Adaptive Pruning by Agents",
      "authors": [
        "Sai Varun Kodathala",
        "Rakesh Vunnam"
      ],
      "abstract": "As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19x better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09694v1",
        "pdf": "https://arxiv.org/pdf/2601.09694v1"
      },
      "arxiv_id": "2601.09694v1",
      "comment": "17 Pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09693v1",
      "title": "Contrastive Geometric Learning Unlocks Unified Structure- and Ligand-Based Drug Design",
      "authors": [
        "Lisa Schneckenreiter",
        "Sohvi Luukkonen",
        "Lukas Friedrich",
        "Daniel Kuhn",
        "Günter Klambauer"
      ],
      "abstract": "Structure-based and ligand-based computational drug design have traditionally relied on disjoint data sources and modeling assumptions, limiting their joint use at scale. In this work, we introduce Contrastive Geometric Learning for Unified Computational Drug Design (ConGLUDe), a single contrastive geometric model that unifies structure- and ligand-based training. ConGLUDe couples a geometric protein encoder that produces whole-protein representations and implicit embeddings of predicted binding sites with a fast ligand encoder, removing the need for pre-defined pockets. By aligning ligands with both global protein representations and multiple candidate binding sites through contrastive learning, ConGLUDe supports ligand-conditioned pocket prediction in addition to virtual screening and target fishing, while being trained jointly on protein-ligand complexes and large-scale bioactivity data. Across diverse benchmarks, ConGLUDe achieves state-of-the-art zero-shot virtual screening performance in settings where no binding pocket information is provided as input, substantially outperforms existing methods on a challenging target fishing task, and demonstrates competitive ligand-conditioned pocket selection. These results highlight the advantages of unified structure-ligand training and position ConGLUDe as a step toward general-purpose foundation models for drug discovery.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09693v1",
        "pdf": "https://arxiv.org/pdf/2601.09693v1"
      },
      "arxiv_id": "2601.09693v1",
      "comment": "ELLIS ML4Molecules Workshop 2025, ELLIS Unconference, Copenhagen 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09692v1",
      "title": "Routing with Generated Data: Annotation-Free LLM Skill Estimation and Expert Selection",
      "authors": [
        "Tianyi Niu",
        "Justin Chih-Yao Chen",
        "Genta Indra Winata",
        "Shi-Xiong Zhang",
        "Supriyo Chakraborty",
        "Sambit Sahu",
        "Yue Zhang",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice, especially when user request distributions are heterogeneous and unknown. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated queries and answers produced from high-level task descriptions by generator LLMs. We evaluate query-answer routers (using both queries and labels) and query-only routers across four diverse benchmarks and 12 models, finding that query-answer routers degrade faster than query-only routers as generator quality decreases. Our analysis reveals two crucial characteristics of effective generators: they must accurately respond to their own questions, and their questions must produce sufficient performance differentiation among the model pool. We then show how filtering for these characteristics can improve the quality of generated data. We further propose CASCAL, a novel query-only router that estimates model correctness through consensus voting and identifies model-specific skill niches via hierarchical clustering. CASCAL is substantially more robust to generator quality, outperforming the best query-answer router by 4.6% absolute accuracy when trained on weak generator data.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09692v1",
        "pdf": "https://arxiv.org/pdf/2601.09692v1"
      },
      "arxiv_id": "2601.09692v1",
      "comment": "Code: https://github.com/tianyiniu/RoutingGenData",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.09684v1",
      "title": "Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection",
      "authors": [
        "Ziyu Yang",
        "Guibin Chen",
        "Yuxin Yang",
        "Aoxiong Zeng",
        "Xiangquan Yang"
      ],
      "abstract": "Multi-Task Learning (MTL) combined with Low-Rank Adaptation (LoRA) has emerged as a promising direction for parameter-efficient deployment of Large Language Models (LLMs). By sharing a single adapter across multiple tasks, one can significantly reduce storage overhead. However, this approach suffers from negative transfer, where conflicting gradient updates from distinct tasks degrade the performance of individual tasks compared to single-task fine-tuning. This problem is exacerbated in LoRA due to the low-rank constraint, which limits the optimization landscape's capacity to accommodate diverse task requirements. In this paper, we propose Ortho-LoRA, a gradient projection method specifically tailored for the bipartite structure of LoRA. Ortho-LoRA dynamically projects conflicting task gradients onto the orthogonal complement of each other within the intrinsic LoRA subspace. Extensive experiments on the GLUE benchmark demonstrate that Ortho-LoRA effectively mitigates task interference, outperforming standard joint training and recovering 95\\% of the performance gap between multi-task and single-task baselines with negligible computational overhead.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09684v1",
        "pdf": "https://arxiv.org/pdf/2601.09684v1"
      },
      "arxiv_id": "2601.09684v1",
      "comment": "preprint",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09680v1",
      "title": "Automating Supply Chain Disruption Monitoring via an Agentic AI Approach",
      "authors": [
        "Sara AlMahri",
        "Liming Xu",
        "Alexandra Brintrup"
      ],
      "abstract": "Modern supply chains are increasingly exposed to disruptions from geopolitical events, demand shocks, trade restrictions, to natural disasters. While many of these disruptions originate deep in the supply network, most companies still lack visibility beyond Tier-1 suppliers, leaving upstream vulnerabilities undetected until the impact cascades downstream. To overcome this blind-spot and move from reactive recovery to proactive resilience, we introduce a minimally supervised agentic AI framework that autonomously monitors, analyses, and responds to disruptions across extended supply networks. The architecture comprises seven specialised agents powered by large language models and deterministic tools that jointly detect disruption signals from unstructured news, map them to multi-tier supplier networks, evaluate exposure based on network structure, and recommend mitigations such as alternative sourcing options. \\rev{We evaluate the framework across 30 synthesised scenarios covering three automotive manufacturers and five disruption classes. The system achieves high accuracy across core tasks, with F1 scores between 0.962 and 0.991, and performs full end-to-end analyses in a mean of 3.83 minutes at a cost of \\$0.0836 per disruption. Relative to industry benchmarks of multi-day, analyst-driven assessments, this represents a reduction of more than three orders of magnitude in response time. A real-world case study of the 2022 Russia-Ukraine conflict further demonstrates operational applicability. This work establishes a foundational step toward building resilient, proactive, and autonomous supply chains capable of managing disruptions across deep-tier networks.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09680v1",
        "pdf": "https://arxiv.org/pdf/2601.09680v1"
      },
      "arxiv_id": "2601.09680v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09668v1",
      "title": "STEP3-VL-10B Technical Report",
      "authors": [
        "Ailin Huang",
        "Chengyuan Yao",
        "Chunrui Han",
        "Fanqi Wan",
        "Hangyu Guo",
        "Haoran Lv",
        "Hongyu Zhou",
        "Jia Wang",
        "Jian Zhou",
        "Jianjian Sun",
        "Jingcheng Hu",
        "Kangheng Lin",
        "Liang Zhao",
        "Mitt Huang",
        "Song Yuan",
        "Wenwen Qu",
        "Xiangfeng Wang",
        "Yanlin Lai",
        "Yingxiu Zhao",
        "Yinmin Zhang",
        "Yukang Shi",
        "Yuyang Chen",
        "Zejia Weng",
        "Ziyang Meng",
        "Ang Li",
        "Aobo Kong",
        "Bo Dong",
        "Changyi Wan",
        "David Wang",
        "Di Qi",
        "Dingming Li",
        "En Yu",
        "Guopeng Li",
        "Haiquan Yin",
        "Han Zhou",
        "Hanshan Zhang",
        "Haolong Yan",
        "Hebin Zhou",
        "Hongbo Peng",
        "Jiaran Zhang",
        "Jiashu Lv",
        "Jiayi Fu",
        "Jie Cheng",
        "Jie Zhou",
        "Jisheng Yin",
        "Jingjing Xie",
        "Jingwei Wu",
        "Jun Zhang",
        "Junfeng Liu",
        "Kaijun Tan",
        "Kaiwen Yan",
        "Liangyu Chen",
        "Lina Chen",
        "Mingliang Li",
        "Qian Zhao",
        "Quan Sun",
        "Shaoliang Pang",
        "Shengjie Fan",
        "Shijie Shang",
        "Siyuan Zhang",
        "Tianhao You",
        "Wei Ji",
        "Wuxun Xie",
        "Xiaobo Yang",
        "Xiaojie Hou",
        "Xiaoran Jiao",
        "Xiaoxiao Ren",
        "Xiangwen Kong",
        "Xin Huang",
        "Xin Wu",
        "Xing Chen",
        "Xinran Wang",
        "Xuelin Zhang",
        "Yana Wei",
        "Yang Li",
        "Yanming Xu",
        "Yeqing Shen",
        "Yuang Peng",
        "Yue Peng",
        "Yu Zhou",
        "Yusheng Li",
        "Yuxiang Yang",
        "Yuyang Zhang",
        "Zhe Xie",
        "Zhewei Huang",
        "Zhenyi Lu",
        "Zhimin Fan",
        "Zihui Cheng",
        "Daxin Jiang",
        "Qi Han",
        "Xiangyu Zhang",
        "Yibo Zhu",
        "Zheng Ge"
      ],
      "abstract": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10$\\times$-20$\\times$ larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09668v1",
        "pdf": "https://arxiv.org/pdf/2601.09668v1"
      },
      "arxiv_id": "2601.09668v1",
      "comment": "50 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09667v1",
      "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
      "authors": [
        "Zhiyuan Hu",
        "Yunhai Hu",
        "Juncheng Liu",
        "Shuyue Stella Li",
        "Yucheng Wang",
        "Zhen Xu",
        "See-Kiong Ng",
        "Anh Tuan Luu",
        "Xinxing Xu",
        "Bryan Hooi",
        "Cynthia Breazeal",
        "Hae Won Park"
      ],
      "abstract": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09667v1",
        "pdf": "https://arxiv.org/pdf/2601.09667v1"
      },
      "arxiv_id": "2601.09667v1",
      "comment": "Work in Progress",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09665v1",
      "title": "SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings",
      "authors": [
        "Yuchen Wu",
        "Jiahe Li",
        "Xiaohan Yu",
        "Lina Yu",
        "Jin Zheng",
        "Xiao Bai"
      ],
      "abstract": "Monocular visual SLAM enables 3D reconstruction from internet video and autonomous navigation on resource-constrained platforms, yet suffers from scale drift, i.e., the gradual divergence of estimated scale over long sequences. Existing frame-to-frame methods achieve real-time performance through local optimization but accumulate scale drift due to the lack of global constraints among independent windows. To address this, we propose SCE-SLAM, an end-to-end SLAM system that maintains scale consistency through scene coordinate embeddings, which are learned patch-level representations encoding 3D geometric relationships under a canonical scale reference. The framework consists of two key modules: geometry-guided aggregation that leverages 3D spatial proximity to propagate scale information from historical observations through geometry-modulated attention, and scene coordinate bundle adjustment that anchors current estimates to the reference scale through explicit 3D coordinate constraints decoded from the scene coordinate embeddings. Experiments on KITTI, Waymo, and vKITTI demonstrate substantial improvements: our method reduces absolute trajectory error by 8.36m on KITTI compared to the best prior approach, while maintaining 36 FPS and achieving scale consistency across large-scale scenes.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09665v1",
        "pdf": "https://arxiv.org/pdf/2601.09665v1"
      },
      "arxiv_id": "2601.09665v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09663v1",
      "title": "Self-Supervised Animal Identification for Long Videos",
      "authors": [
        "Xuyang Fang",
        "Sion Hannuna",
        "Edwin Simpson",
        "Neill Campbell"
      ],
      "abstract": "Identifying individual animals in long-duration videos is essential for behavioral ecology, wildlife monitoring, and livestock management. Traditional methods require extensive manual annotation, while existing self-supervised approaches are computationally demanding and ill-suited for long sequences due to memory constraints and temporal error propagation. We introduce a highly efficient, self-supervised method that reframes animal identification as a global clustering task rather than a sequential tracking problem. Our approach assumes a known, fixed number of individuals within a single video -- a common scenario in practice -- and requires only bounding box detections and the total count. By sampling pairs of frames, using a frozen pre-trained backbone, and employing a self-bootstrapping mechanism with the Hungarian algorithm for in-batch pseudo-label assignment, our method learns discriminative features without identity labels. We adapt a Binary Cross Entropy loss from vision-language models, enabling state-of-the-art accuracy ($>$97\\%) while consuming less than 1 GB of GPU memory per batch -- an order of magnitude less than standard contrastive methods. Evaluated on challenging real-world datasets (3D-POP pigeons and 8-calves feeding videos), our framework matches or surpasses supervised baselines trained on over 1,000 labeled frames, effectively removing the manual annotation bottleneck. This work enables practical, high-accuracy animal identification on consumer-grade hardware, with broad applicability in resource-constrained research settings. All code written for this paper are \\href{https://huggingface.co/datasets/tonyFang04/8-calves}{here}.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09663v1",
        "pdf": "https://arxiv.org/pdf/2601.09663v1"
      },
      "arxiv_id": "2601.09663v1",
      "comment": "11 pages, 1 figure",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09661v1",
      "title": "LiteEmbed: Adapting CLIP to Rare Classes",
      "authors": [
        "Aishwarya Agarwal",
        "Srikrishna Karanam",
        "Vineet Gandhi"
      ],
      "abstract": "Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP's vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP's original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09661v1",
        "pdf": "https://arxiv.org/pdf/2601.09661v1"
      },
      "arxiv_id": "2601.09661v1",
      "comment": "14 pages, 12 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09658v1",
      "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image",
      "authors": [
        "Selim Emir Can",
        "Jan Ackermann",
        "Kiyohiro Nakayama",
        "Ruofan Liu",
        "Tong Wu",
        "Yang Zheng",
        "Hugo Bertiche",
        "Menglei Chai",
        "Thabo Beeler",
        "Gordon Wetzstein"
      ],
      "abstract": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09658v1",
        "pdf": "https://arxiv.org/pdf/2601.09658v1"
      },
      "arxiv_id": "2601.09658v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09654v1",
      "title": "Exploring Fine-Tuning for Tabular Foundation Models",
      "authors": [
        "Aditya Tanna",
        "Pratinav Seth",
        "Mohamed Bouadi",
        "Vinay Kumar Sankarapu"
      ],
      "abstract": "Tabular Foundation Models (TFMs) have recently shown strong in-context learning capabilities on structured data, achieving zero-shot performance comparable to traditional machine learning methods. We find that zero-shot TFMs already achieve strong performance, while the benefits of fine-tuning are highly model and data-dependent. Meta-learning and PEFT provide moderate gains under specific conditions, whereas full supervised fine-tuning (SFT) often reduces accuracy or calibration quality. This work presents the first comprehensive study of fine-tuning in TFMs across benchmarks including TALENT, OpenML-CC18, and TabZilla. We compare Zero-Shot, Meta-Learning, Supervised (SFT), and parameter-efficient (PEFT) approaches, analyzing how dataset factors such as imbalance, size, and dimensionality affect outcomes. Our findings cover performance, calibration, and fairness, offering practical guidelines on when fine-tuning is most beneficial and its limitations.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09654v1",
        "pdf": "https://arxiv.org/pdf/2601.09654v1"
      },
      "arxiv_id": "2601.09654v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09652v1",
      "title": "AquaFeat+: an Underwater Vision Learning-based Enhancement Method for Object Detection, Classification, and Tracking",
      "authors": [
        "Emanuel da Costa Silva",
        "Tatiana Taís Schein",
        "José David García Ramos",
        "Eduardo Lawson da Silva",
        "Stephanie Loi Brião",
        "Felipe Gomes de Oliveira",
        "Paulo Lilles Jorge Drews-Jr"
      ],
      "abstract": "Underwater video analysis is particularly challenging due to factors such as low lighting, color distortion, and turbidity, which compromise visual data quality and directly impact the performance of perception modules in robotic applications. This work proposes AquaFeat+, a plug-and-play pipeline designed to enhance features specifically for automated vision tasks, rather than for human perceptual quality. The architecture includes modules for color correction, hierarchical feature enhancement, and an adaptive residual output, which are trained end-to-end and guided directly by the loss function of the final application. Trained and evaluated in the FishTrack23 dataset, AquaFeat+ achieves significant improvements in object detection, classification, and tracking metrics, validating its effectiveness for enhancing perception tasks in underwater robotic applications.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09652v1",
        "pdf": "https://arxiv.org/pdf/2601.09652v1"
      },
      "arxiv_id": "2601.09652v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09647v1",
      "title": "Identifying Models Behind Text-to-Image Leaderboards",
      "authors": [
        "Ali Naseh",
        "Yuefeng Peng",
        "Anshuman Suri",
        "Harsh Chaudhari",
        "Alina Oprea",
        "Amir Houmansadr"
      ],
      "abstract": "Text-to-image (T2I) models are increasingly popular, producing a large share of AI-generated images online. To compare model quality, voting-based leaderboards have become the standard, relying on anonymized model outputs for fairness. In this work, we show that such anonymity can be easily broken. We find that generations from each T2I model form distinctive clusters in the image embedding space, enabling accurate deanonymization without prompt control or training data. Using 22 models and 280 prompts (150K images), our centroid-based method achieves high accuracy and reveals systematic model-specific signatures. We further introduce a prompt-level distinguishability metric and conduct large-scale analyses showing how certain prompts can lead to near-perfect distinguishability. Our findings expose fundamental security flaws in T2I leaderboards and motivate stronger anonymization defenses.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09647v1",
        "pdf": "https://arxiv.org/pdf/2601.09647v1"
      },
      "arxiv_id": "2601.09647v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09636v1",
      "title": "PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records",
      "authors": [
        "Yibo Lyu",
        "Gongwei Chen",
        "Rui Shao",
        "Weili Guan",
        "Liqiang Nie"
      ],
      "abstract": "While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09636v1",
        "pdf": "https://arxiv.org/pdf/2601.09636v1"
      },
      "arxiv_id": "2601.09636v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09635v1",
      "title": "LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach",
      "authors": [
        "Kuo Liang",
        "Yuhang Lu",
        "Jianming Mao",
        "Shuyi Sun",
        "Chunwei Yang",
        "Congcong Zeng",
        "Xiao Jin",
        "Hanzhang Qin",
        "Ruihao Zhu",
        "Chung-Piaw Teo"
      ],
      "abstract": "Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09635v1",
        "pdf": "https://arxiv.org/pdf/2601.09635v1"
      },
      "arxiv_id": "2601.09635v1",
      "comment": "Updated version of https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5329027",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09626v1",
      "title": "From Prompt to Protocol: Fast Charging Batteries with Large Language Models",
      "authors": [
        "Ge Lei",
        "Ferran Brosa Planella",
        "Sterling G. Baird",
        "Samuel J. Cooper"
      ],
      "abstract": "Efficiently optimizing battery charging protocols is challenging because each evaluation is slow, costly, and non-differentiable. Many existing approaches address this difficulty by heavily constraining the protocol search space, which limits the diversity of protocols that can be explored, preventing the discovery of higher-performing solutions. We introduce two gradient-free, LLM-driven closed-loop methods: Prompt-to-Optimizer (P2O), which uses an LLM to propose the code for small neural-network-based protocols, which are then trained by an inner loop, and Prompt-to-Protocol (P2P), which simply writes an explicit function for the current and its scalar parameters. Across our case studies, LLM-guided P2O outperforms neural networks designed by Bayesian optimization, evolutionary algorithms, and random search. In a realistic fast charging scenario, both P2O and P2P yield around a 4.2 percent improvement in state of health (capacity retention based health metric under fast charging cycling) over a state-of-the-art multi-step constant current (CC) baseline, with P2P achieving this under matched evaluation budgets (same number of protocol evaluations). These results demonstrate that LLMs can expand the space of protocol functional forms, incorporate language-based constraints, and enable efficient optimization in high cost experimental settings.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09626v1",
        "pdf": "https://arxiv.org/pdf/2601.09626v1"
      },
      "arxiv_id": "2601.09626v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09625v1",
      "title": "The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware",
      "authors": [
        "Ben Nassi",
        "Bruce Schneier",
        "Oleg Brodt"
      ],
      "abstract": "The rapid adoption of large language model (LLM)-based systems -- from chatbots to autonomous agents capable of executing code and financial transactions -- has created a new attack surface that existing security frameworks inadequately address. The dominant framing of these threats as \"prompt injection\" -- a catch-all phrase for security failures in LLM-based systems -- obscures a more complex reality: Attacks on LLM-based systems increasingly involve multi-step sequences that mirror traditional malware campaigns. In this paper, we propose that attacks targeting LLM-based applications constitute a distinct class of malware, which we term \\textit{promptware}, and introduce a five-step kill chain model for analyzing these threats. The framework comprises Initial Access (prompt injection), Privilege Escalation (jailbreaking), Persistence (memory and retrieval poisoning), Lateral Movement (cross-system and cross-user propagation), and Actions on Objective (ranging from data exfiltration to unauthorized transactions). By mapping recent attacks to this structure, we demonstrate that LLM-related attacks follow systematic sequences analogous to traditional malware campaigns. The promptware kill chain offers security practitioners a structured methodology for threat modeling and provides a common vocabulary for researchers across AI safety and cybersecurity to address a rapidly evolving threat landscape.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09625v1",
        "pdf": "https://arxiv.org/pdf/2601.09625v1"
      },
      "arxiv_id": "2601.09625v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09624v1",
      "title": "Toward Understanding Unlearning Difficulty: A Mechanistic Perspective and Circuit-Guided Difficulty Metric",
      "authors": [
        "Jiali Cheng",
        "Ziheng Chen",
        "Chirag Agarwal",
        "Hadi Amiri"
      ],
      "abstract": "Machine unlearning is becoming essential for building trustworthy and compliant language models. Yet unlearning success varies considerably across individual samples: some are reliably erased, while others persist despite the same procedure. We argue that this disparity is not only a data-side phenomenon, but also reflects model-internal mechanisms that encode and protect memorized information. We study this problem from a mechanistic perspective based on model circuits--structured interaction pathways that govern how predictions are formed. We propose Circuit-guided Unlearning Difficulty (CUD), a {\\em pre-unlearning} metric that assigns each sample a continuous difficulty score using circuit-level signals. Extensive experiments demonstrate that CUD reliably separates intrinsically easy and hard samples, and remains stable across unlearning methods. We identify key circuit-level patterns that reveal a mechanistic signature of difficulty: easy-to-unlearn samples are associated with shorter, shallower interactions concentrated in earlier-to-intermediate parts of the original model, whereas hard samples rely on longer and deeper pathways closer to late-stage computation. Compared to existing qualitative studies, CUD takes a first step toward a principled, fine-grained, and interpretable analysis of unlearning difficulty; and motivates the development of unlearning methods grounded in model mechanisms.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09624v1",
        "pdf": "https://arxiv.org/pdf/2601.09624v1"
      },
      "arxiv_id": "2601.09624v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09620v1",
      "title": "Full Disclosure, Less Trust? How the Level of Detail about AI Use in News Writing Affects Readers' Trust",
      "authors": [
        "Pooja Prajod",
        "Hannes Cools",
        "Thomas Röggla",
        "Karthikeya Puttur Venkatraj",
        "Amber Kusters",
        "Alia ElKattan",
        "Pablo Cesar",
        "Abdallah El Ali"
      ],
      "abstract": "As artificial intelligence (AI) is increasingly integrated into news production, calls for transparency about the use of AI have gained considerable traction. Recent studies suggest that AI disclosures can lead to a ``transparency dilemma'', where disclosure reduces readers' trust. However, little is known about how the \\textit{level of detail} in AI disclosures influences trust and contributes to this dilemma within the news context. In this 3$\\times$2$\\times$2 mixed factorial study with 40 participants, we investigate how three levels of AI disclosures (none, one-line, detailed) across two types of news (politics and lifestyle) and two levels of AI involvement (low and high) affect news readers' trust. We measured trust using the News Media Trust questionnaire, along with two decision behaviors: source-checking and subscription decisions. Questionnaire responses and subscription rates showed a decline in trust only for detailed AI disclosures, whereas source-checking behavior increased for both one-line and detailed disclosures, with the effect being more pronounced for detailed disclosures. Insights from semi-structured interviews suggest that source-checking behavior was primarily driven by interest in the topic, followed by trust, whereas trust was the main factor influencing subscription decisions. Around two-thirds of participants expressed a preference for detailed disclosures, while most participants who preferred one-line indicated a need for detail-on-demand disclosure formats. Our findings show that not all AI disclosures lead to a transparency dilemma, but instead reflect a trade-off between readers' desire for more transparency and their trust in AI-assisted news content.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09620v1",
        "pdf": "https://arxiv.org/pdf/2601.09620v1"
      },
      "arxiv_id": "2601.09620v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09613v1",
      "title": "CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems",
      "authors": [
        "Yonglin Tian",
        "Qiyao Zhang",
        "Wei Xu",
        "Yutong Wang",
        "Yihao Wu",
        "Xinyi Li",
        "Xingyuan Dai",
        "Hui Zhang",
        "Zhiyong Cui",
        "Baoqing Guo",
        "Zujun Yu",
        "Yisheng Lv"
      ],
      "abstract": "Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09613v1",
        "pdf": "https://arxiv.org/pdf/2601.09613v1"
      },
      "arxiv_id": "2601.09613v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09609v1",
      "title": "DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing",
      "authors": [
        "Qian Cao",
        "Yahui Liu",
        "Wei Bi",
        "Yi Zhao",
        "Ruihua Song",
        "Xiting Wang",
        "Ruiming Tang",
        "Guorui Zhou",
        "Han Li"
      ],
      "abstract": "Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09609v1",
        "pdf": "https://arxiv.org/pdf/2601.09609v1"
      },
      "arxiv_id": "2601.09609v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09606v1",
      "title": "GRCF: Two-Stage Groupwise Ranking and Calibration Framework for Multimodal Sentiment Analysis",
      "authors": [
        "Manning Gao",
        "Leheng Zhang",
        "Shiqin Han",
        "Haifeng Hu",
        "Yuncheng Jiang",
        "Sijie Mai"
      ],
      "abstract": "Most Multimodal Sentiment Analysis research has focused on point-wise regression. While straightforward, this approach is sensitive to label noise and neglects whether one sample is more positive than another, resulting in unstable predictions and poor correlation alignment. Pairwise ordinal learning frameworks emerged to address this gap, capturing relative order by learning from comparisons. Yet, they introduce two new trade-offs: First, they assign uniform importance to all comparisons, failing to adaptively focus on hard-to-rank samples. Second, they employ static ranking margins, which fail to reflect the varying semantic distances between sentiment groups. To address this, we propose a Two-Stage Group-wise Ranking and Calibration Framework (GRCF) that adapts the philosophy of Group Relative Policy Optimization (GRPO). Our framework resolves these trade-offs by simultaneously preserving relative ordinal structure, ensuring absolute score calibration, and adaptively focusing on difficult samples. Specifically, Stage 1 introduces a GRPO-inspired Advantage-Weighted Dynamic Margin Ranking Loss to build a fine-grained ordinal structure. Stage 2 then employs an MAE-driven objective to align prediction magnitudes. To validate its generalizability, we extend GRCF to classification tasks, including multimodal humor detection and sarcasm detection. GRCF achieves state-of-the-art performance on core regression benchmarks, while also showing strong generalizability in classification tasks.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09606v1",
        "pdf": "https://arxiv.org/pdf/2601.09606v1"
      },
      "arxiv_id": "2601.09606v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09605v1",
      "title": "Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets",
      "authors": [
        "Jeremiah Coholich",
        "Justin Wit",
        "Robert Azarcon",
        "Zsolt Kira"
      ],
      "abstract": "Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge. To bridge this gap, we propose MANGO -- an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss. We find that these elements are crucial for maintaining viewpoint consistency during sim2real translation. When training MANGO, we only require a small amount of fixed-camera data from the real world, but show that our method can generate diverse unseen viewpoints by translating simulated observations. In this domain, MANGO outperforms all other image translation methods we tested. Imitation-learning policies trained on data augmented by MANGO are able to achieve success rates as high as 60\\% on views that the non-augmented policy fails completely on.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09605v1",
        "pdf": "https://arxiv.org/pdf/2601.09605v1"
      },
      "arxiv_id": "2601.09605v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09603v1",
      "title": "Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer",
      "authors": [
        "Petros Vavaroutsos",
        "Theodoros Palamas",
        "Pantelis Vikatos"
      ],
      "abstract": "In recent years, foundation models have become very popular due to their exceptional performance, mainly in natural language (NLP) tasks where they were first introduced. These models usually consist of hundreds of millions, or even billions, of parameters, making them resource-intensive during training and in production systems, leading to increased costs. This paper focuses on the reduction of a foundation's model size when applied to music information retrieval (MIR) tasks. Our research combines the Branchformer architecture with SummaryMixing, which were first applied in speech recognition, along with a random quantization process. To facilitate reproducibility, we conduct pre-training on publicly available datasets, complemented by a proprietary dataset comparable in scale to other private datasets reported in the literature. We ensure robust evaluation by using a framework consisting of a variety of downstream MIR tasks. Our results show that our architecture achieves competitive performance when compared with other state-of-the-art models that use multi-head self-attention, while reducing the model size from 8.5% up to 12.3%.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09603v1",
        "pdf": "https://arxiv.org/pdf/2601.09603v1"
      },
      "arxiv_id": "2601.09603v1",
      "comment": "accepted by ACM/SIGAPP Symposium on Applied Computing (SAC 2026)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09601v1",
      "title": "Iterative Differential Entropy Minimization (IDEM) method for fine rigid pairwise 3D Point Cloud Registration: A Focus on the Metric",
      "authors": [
        "Emmanuele Barberi",
        "Felice Sfravara",
        "Filippo Cucinotta"
      ],
      "abstract": "Point cloud registration is a central theme in computer vision, with alignment algorithms continuously improving for greater robustness. Commonly used methods evaluate Euclidean distances between point clouds and minimize an objective function, such as Root Mean Square Error (RMSE). However, these approaches are most effective when the point clouds are well-prealigned and issues such as differences in density, noise, holes, and limited overlap can compromise the results. Traditional methods, such as Iterative Closest Point (ICP), require choosing one point cloud as fixed, since Euclidean distances lack commutativity. When only one point cloud has issues, adjustments can be made, but in real scenarios, both point clouds may be affected, often necessitating preprocessing. The authors introduce a novel differential entropy-based metric, designed to serve as the objective function within an optimization framework for fine rigid pairwise 3D point cloud registration, denoted as Iterative Differential Entropy Minimization (IDEM). This metric does not depend on the choice of a fixed point cloud and, during transformations, reveals a clear minimum corresponding to the best alignment. Multiple case studies are conducted, and the results are compared with those obtained using RMSE, Chamfer distance, and Hausdorff distance. The proposed metric proves effective even with density differences, noise, holes, and partial overlap, where RMSE does not always yield optimal alignment.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09601v1",
        "pdf": "https://arxiv.org/pdf/2601.09601v1"
      },
      "arxiv_id": "2601.09601v1",
      "comment": "",
      "journal_ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025, Available in IEEE Xplore",
      "has_code": false
    },
    {
      "id": "2601.09600v1",
      "title": "Information Access of the Oppressed: A Problem-Posing Framework for Envisioning Emancipatory Information Access Platforms",
      "authors": [
        "Bhaskar Mitra",
        "Nicola Neophytou",
        "Sireesh Gururaja"
      ],
      "abstract": "Online information access (IA) platforms are targets of authoritarian capture. These concerns are particularly serious and urgent today in light of the rising levels of democratic erosion worldwide, the emerging capabilities of generative AI technologies such as AI persuasion, and the increasing concentration of economic and political power in the hands of Big Tech. This raises the question of what alternative IA infrastructure we must reimagine and build to mitigate the risks of authoritarian capture of our information ecosystems. We explore this question through the lens of Paulo Freire's theories of emancipatory pedagogy. Freire's theories provide a radically different lens for exploring IA's sociotechnical concerns relative to the current dominating frames of fairness, accountability, confidentiality, transparency, and safety. We make explicit, with the intention to challenge, the dichotomy of how we relate to technology as either technologists (who envision and build technology) and its users. We posit that this mirrors the teacher-student relationship in Freire's analysis. By extending Freire's analysis to IA, we challenge the notion that it is the burden of the (altruistic) technologists to come up with interventions to mitigate the risks that emerging technologies pose to marginalized communities. Instead, we advocate that the first task for the technologists is to pose these as problems to the marginalized communities, to encourage them to make and unmake the technology as part of their material struggle against oppression. Their second task is to redesign our online technology stacks to structurally expose spaces for community members to co-opt and co-construct the technology in aid of their emancipatory struggles. We operationalize Freire's theories to develop a problem-posing framework for envisioning emancipatory IA platforms of the future.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.CY",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09600v1",
        "pdf": "https://arxiv.org/pdf/2601.09600v1"
      },
      "arxiv_id": "2601.09600v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09588v1",
      "title": "Energy-Entropy Regularization: The True Power of Minimal Looped Transformers",
      "authors": [
        "Wai-Lun Lam"
      ],
      "abstract": "Recent research suggests that looped Transformers have superior reasoning capabilities compared to standard deep architectures. Current approaches to training single-head looped architectures on benchmark tasks frequently fail or yield suboptimal performance due to a highly non-convex and irregular loss landscape. In these settings, optimization often stagnates in poor local minima and saddle points of the loss landscape, preventing the model from discovering the global minimum point. The internal mechanisms of these single-head looped transformer models remain poorly understood, and training them from scratch remains a significant challenge. In this paper, we propose a novel training framework that leverages Tsallis entropy and Hamiltonian dynamics to transform the geometry of the loss landscape. By treating the parameter updates as a physical flow, we successfully trained a single-head looped Transformer with model dimension $d = 8$ to solve induction head task with input sequence length of 1000 tokens. This success reveals the internal mechanism behind the superior reasoning capability.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09588v1",
        "pdf": "https://arxiv.org/pdf/2601.09588v1"
      },
      "arxiv_id": "2601.09588v1",
      "comment": "19 pages, 2 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09586v1",
      "title": "Show, don't tell -- Providing Visual Error Feedback for Handwritten Documents",
      "authors": [
        "Said Yasin",
        "Torsten Zesch"
      ],
      "abstract": "Handwriting remains an essential skill, particularly in education. Therefore, providing visual feedback on handwritten documents is an important but understudied area. We outline the many challenges when going from an image of handwritten input to correctly placed informative error feedback. We empirically compare modular and end-to-end systems and find that both approaches currently do not achieve acceptable overall quality. We identify the major challenges and outline an agenda for future research.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09586v1",
        "pdf": "https://arxiv.org/pdf/2601.09586v1"
      },
      "arxiv_id": "2601.09586v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09579v1",
      "title": "Constraint- and Score-Based Nonlinear Granger Causality Discovery with Kernels",
      "authors": [
        "Fiona Murphy",
        "Alessio Benavoli"
      ],
      "abstract": "Kernel-based methods are used in the context of Granger Causality to enable the identification of nonlinear causal relationships between time series variables. In this paper, we show that two state of the art kernel-based Granger Causality (GC) approaches can be theoretically unified under the framework of Kernel Principal Component Regression (KPCR), and introduce a method based on this unification, demonstrating that this approach can improve causal identification. Additionally, we introduce a Gaussian Process score-based model with Smooth Information Criterion penalisation on the marginal likelihood, and demonstrate improved performance over existing state of the art time-series nonlinear causal discovery methods. Furthermore, we propose a contemporaneous causal identification algorithm, fully based on GC, using the proposed score-based $GP_{SIC}$ method, and compare its performance to a state of the art contemporaneous time series causal discovery algorithm.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09579v1",
        "pdf": "https://arxiv.org/pdf/2601.09579v1"
      },
      "arxiv_id": "2601.09579v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09578v1",
      "title": "Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping",
      "authors": [
        "Jiajun Sun",
        "Yangyi Ou",
        "Haoyuan Zheng",
        "Chao yang",
        "Yue Ma"
      ],
      "abstract": "In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology. This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information. By first performing pixel-level fusion of visible and infrared images, the system projects real-time LiDAR point clouds onto this fused image stream. It then segments heat source features in the thermal channel to instantly identify high temperature targets and applies this temperature information as a semantic layer on the final 3D map. This approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment, making it highly valuable for specific applications like rapid disaster assessment and industrial preventive maintenance.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09578v1",
        "pdf": "https://arxiv.org/pdf/2601.09578v1"
      },
      "arxiv_id": "2601.09578v1",
      "comment": "5 pages,7 figures. Under review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09575v1",
      "title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding",
      "authors": [
        "Sheng-Yu Huang",
        "Jaesung Choe",
        "Yu-Chiang Frank Wang",
        "Cheng Sun"
      ],
      "abstract": "We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09575v1",
        "pdf": "https://arxiv.org/pdf/2601.09575v1"
      },
      "arxiv_id": "2601.09575v1",
      "comment": "project page: https://peterjohnsonhuang.github.io/openvoxel-pages/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.09572v1",
      "title": "Trustworthy Longitudinal Brain MRI Completion: A Deformation-Based Approach with KAN-Enhanced Diffusion Model",
      "authors": [
        "Tianli Tao",
        "Ziyang Wang",
        "Delong Yang",
        "Han Zhang",
        "Le Zhang"
      ],
      "abstract": "Longitudinal brain MRI is essential for lifespan study, yet high attrition rates often lead to missing data, complicating analysis. Deep generative models have been explored, but most rely solely on image intensity, leading to two key limitations: 1) the fidelity or trustworthiness of the generated brain images are limited, making downstream studies questionable; 2) the usage flexibility is restricted due to fixed guidance rooted in the model structure, restricting full ability to versatile application scenarios. To address these challenges, we introduce DF-DiffCom, a Kolmogorov-Arnold Networks (KAN)-enhanced diffusion model that smartly leverages deformation fields for trustworthy longitudinal brain image completion. Trained on OASIS-3, DF-DiffCom outperforms state-of-the-art methods, improving PSNR by 5.6% and SSIM by 0.12. More importantly, its modality-agnostic nature allows smooth extension to varied MRI modalities, even to attribute maps such as brain tissue segmentation results.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09572v1",
        "pdf": "https://arxiv.org/pdf/2601.09572v1"
      },
      "arxiv_id": "2601.09572v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09566v1",
      "title": "Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese Language Modeling",
      "authors": [
        "Shuyang Xiang",
        "Hao Guan"
      ],
      "abstract": "Large language models typically represent Chinese characters as discrete index-based tokens, largely ignoring their visual form. For logographic scripts, visual structure carries semantic and phonetic information, which may aid prediction. We investigate whether low-resolution visual inputs can serve as an alternative for character-level modeling. Instead of token IDs, our decoder receives grayscale images of individual characters, with resolutions as low as $8 \\times 8$ pixels. Remarkably, these inputs achieve 39.2\\% accuracy, comparable to the index-based baseline of 39.1\\%. Such low-resource settings also exhibit a pronounced \\emph{hot-start} effect: by 0.4\\% of total training, accuracy reaches above 12\\%, while index-based models lag at below 6\\%. Overall, our results demonstrate that minimal visual structure can provide a robust and efficient signal for Chinese language modeling, offering an alternative perspective on character representation that complements traditional index-based approaches.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09566v1",
        "pdf": "https://arxiv.org/pdf/2601.09566v1"
      },
      "arxiv_id": "2601.09566v1",
      "comment": "15 pages, 5 figures, submitted to ACL 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09555v1",
      "title": "Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats",
      "authors": [
        "Manyi Zhang",
        "Ji-Fu Li",
        "Zhongao Sun",
        "Haoli Bai",
        "Hui-Ling Zhen",
        "Zhenhua Dong",
        "Xianzhi Yu"
      ],
      "abstract": "Microscaling Floating-Point (MXFP) has emerged as a promising low-precision format for large language models (LLMs). Despite various post-training quantization (PTQ) algorithms being proposed, they mostly focus on integer quantization, while their applicability and behavior under MXFP formats remain largely unexplored. To address this gap, this work conducts a systematic investigation of PTQ under MXFP formats, encompassing over 7 PTQ algorithms, 15 evaluation benchmarks, and 3 LLM families. The key findings include: 1) MXFP8 consistently achieves near-lossless performance, while MXFP4 introduces substantial accuracy degradation and remains challenging; 2) PTQ effectiveness under MXFP depends strongly on format compatibility, with some algorithmic paradigms being consistently more effective than others; 3) PTQ performance exhibits highly consistent trends across model families and modalities, in particular, quantization sensitivity is dominated by the language model rather than the vision encoder in multimodal LLMs; 4) The scaling factor of quantization is a critical error source in MXFP4, and a simple pre-scale optimization strategy can significantly mitigate its impact. Together, these results provide practical guidance on adapting existing PTQ methods to MXFP quantization.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09555v1",
        "pdf": "https://arxiv.org/pdf/2601.09555v1"
      },
      "arxiv_id": "2601.09555v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09536v1",
      "title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning",
      "authors": [
        "Dongjie Cheng",
        "Yongqi Li",
        "Zhixin Ma",
        "Hongru Cai",
        "Yupeng Hu",
        "Wenjie Wang",
        "Liqiang Nie",
        "Wenjie Li"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09536v1",
        "pdf": "https://arxiv.org/pdf/2601.09536v1"
      },
      "arxiv_id": "2601.09536v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09533v1",
      "title": "Residual Power Flow for Neural Solvers",
      "authors": [
        "Jochen Stiasny",
        "Jochen Cremer"
      ],
      "abstract": "The energy transition challenges operational tasks based on simulations and optimisation. These computations need to be fast and flexible as the grid is ever-expanding, and renewables' uncertainty requires a flexible operational environment. Learned approximations, proxies or surrogates -- we refer to them as Neural Solvers -- excel in terms of evaluation speed, but are inflexible with respect to adjusting to changing tasks. Hence, neural solvers are usually applicable to highly specific tasks, which limits their usefulness in practice; a widely reusable, foundational neural solver is required. Therefore, this work proposes the Residual Power Flow (RPF) formulation. RPF formulates residual functions based on Kirchhoff's laws to quantify the infeasibility of an operating condition. The minimisation of the residuals determines the voltage solution; an additional slack variable is needed to achieve AC-feasibility. RPF forms a natural, foundational subtask of tasks subject to power flow constraints. We propose to learn RPF with neural solvers to exploit their speed. Furthermore, RPF improves learning performance compared to common power flow formulations. To solve operational tasks, we integrate the neural solver in a Predict-then-Optimise (PO) approach to combine speed and flexibility. The case study investigates the IEEE 9-bus system and three tasks (AC Optimal Power Flow (OPF), power-flow and quasi-steady state power flow) solved by PO. The results demonstrate the accuracy and flexibility of learning with RPF.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "eess.SY",
        "cs.LG"
      ],
      "primary_category": "eess.SY",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09533v1",
        "pdf": "https://arxiv.org/pdf/2601.09533v1"
      },
      "arxiv_id": "2601.09533v1",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09531v1",
      "title": "Bipartite Mode Matching for Vision Training Set Search from a Hierarchical Data Server",
      "authors": [
        "Yue Yao",
        "Ruining Yang",
        "Tom Gedeon"
      ],
      "abstract": "We explore a situation in which the target domain is accessible, but real-time data annotation is not feasible. Instead, we would like to construct an alternative training set from a large-scale data server so that a competitive model can be obtained. For this problem, because the target domain usually exhibits distinct modes (i.e., semantic clusters representing data distribution), if the training set does not contain these target modes, the model performance would be compromised. While prior existing works improve algorithms iteratively, our research explores the often-overlooked potential of optimizing the structure of the data server. Inspired by the hierarchical nature of web search engines, we introduce a hierarchical data server, together with a bipartite mode matching algorithm (BMM) to align source and target modes. For each target mode, we look in the server data tree for the best mode match, which might be large or small in size. Through bipartite matching, we aim for all target modes to be optimally matched with source modes in a one-on-one fashion. Compared with existing training set search algorithms, we show that the matched server modes constitute training sets that have consistently smaller domain gaps with the target domain across object re-identification (re-ID) and detection tasks. Consequently, models trained on our searched training sets have higher accuracy than those trained otherwise. BMM allows data-centric unsupervised domain adaptation (UDA) orthogonal to existing model-centric UDA methods. By combining the BMM with existing UDA methods like pseudo-labeling, further improvement is observed.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09531v1",
        "pdf": "https://arxiv.org/pdf/2601.09531v1"
      },
      "arxiv_id": "2601.09531v1",
      "comment": "Accepted to AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09528v1",
      "title": "GlovEgo-HOI: Bridging the Synthetic-to-Real Gap for Industrial Egocentric Human-Object Interaction Detection",
      "authors": [
        "Alfio Spoto",
        "Rosario Leonardi",
        "Francesco Ragusa",
        "Giovanni Maria Farinella"
      ],
      "abstract": "Egocentric Human-Object Interaction (EHOI) analysis is crucial for industrial safety, yet the development of robust models is hindered by the scarcity of annotated domain-specific data. We address this challenge by introducing a data generation framework that combines synthetic data with a diffusion-based process to augment real-world images with realistic Personal Protective Equipment (PPE). We present GlovEgo-HOI, a new benchmark dataset for industrial EHOI, and GlovEgo-Net, a model integrating Glove-Head and Keypoint- Head modules to leverage hand pose information for enhanced interaction detection. Extensive experiments demonstrate the effectiveness of the proposed data generation framework and GlovEgo-Net. To foster further research, we release the GlovEgo-HOI dataset, augmentation pipeline, and pre-trained models at: GitHub project.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09528v1",
        "pdf": "https://arxiv.org/pdf/2601.09528v1"
      },
      "arxiv_id": "2601.09528v1",
      "comment": "8 pages, accepted as a Short Paper at VISAPP 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09527v1",
      "title": "Private LLM Inference on Consumer Blackwell GPUs: A Practical Guide for Cost-Effective Local Deployment in SMEs",
      "authors": [
        "Jonathan Knoop",
        "Hendrik Holtmann"
      ],
      "abstract": "SMEs increasingly seek alternatives to cloud LLM APIs, which raise data privacy concerns. Dedicated cloud GPU instances offer improved privacy but with limited guarantees and ongoing costs, while professional on-premise hardware (A100, H100) remains prohibitively expensive. We present a systematic evaluation of NVIDIA's Blackwell consumer GPUs (RTX 5060 Ti, 5070 Ti, 5090) for production LLM inference, benchmarking four open-weight models (Qwen3-8B, Gemma3-12B, Gemma3-27B, GPT-OSS-20B) across 79 configurations spanning quantization formats (BF16, W4A16, NVFP4, MXFP4), context lengths (8k-64k), and three workloads: RAG, multi-LoRA agentic serving, and high-concurrency APIs. The RTX 5090 delivers 3.5-4.6x higher throughput than the 5060 Ti with 21x lower latency for RAG, but budget GPUs achieve the highest throughput-per-dollar for API workloads with sub-second latency. NVFP4 quantization provides 1.6x throughput over BF16 with 41% energy reduction and only 2-4% quality loss. Self-hosted inference costs $0.001-0.04 per million tokens (electricity only), which is 40-200x cheaper than budget-tier cloud APIs, with hardware breaking even in under four months at moderate volume (30M tokens/day). Our results show that consumer GPUs can reliably replace cloud inference for most SME workloads, except latency-critical long-context RAG, where high-end GPUs remain essential. We provide deployment guidance and release all benchmark data for reproducible SME-scale deployments.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09527v1",
        "pdf": "https://arxiv.org/pdf/2601.09527v1"
      },
      "arxiv_id": "2601.09527v1",
      "comment": "15 pages, 18 tables, 7 figures. Includes link to GitHub repository and Docker image for reproducibility",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.09524v1",
      "title": "Video Joint-Embedding Predictive Architectures for Facial Expression Recognition",
      "authors": [
        "Lennart Eing",
        "Cristina Luna-Jiménez",
        "Silvan Mertes",
        "Elisabeth André"
      ],
      "abstract": "This paper introduces a novel application of Video Joint-Embedding Predictive Architectures (V-JEPAs) for Facial Expression Recognition (FER). Departing from conventional pre-training methods for video understanding that rely on pixel-level reconstructions, V-JEPAs learn by predicting embeddings of masked regions from the embeddings of unmasked regions. This enables the trained encoder to not capture irrelevant information about a given video like the color of a region of pixels in the background. Using a pre-trained V-JEPA video encoder, we train shallow classifiers using the RAVDESS and CREMA-D datasets, achieving state-of-the-art performance on RAVDESS and outperforming all other vision-based methods on CREMA-D (+1.48 WAR). Furthermore, cross-dataset evaluations reveal strong generalization capabilities, demonstrating the potential of purely embedding-based pre-training approaches to advance FER. We release our code at https://github.com/lennarteingunia/vjepa-for-fer.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09524v1",
        "pdf": "https://arxiv.org/pdf/2601.09524v1"
      },
      "arxiv_id": "2601.09524v1",
      "comment": "To appear in 2025 Proceedings of the 13th International Conference on Affective Computing and Intelligent Interaction (ACII), submitted to IEEE. \\c{opyright} 2025 IEEE",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09522v1",
      "title": "Class Adaptive Conformal Training",
      "authors": [
        "Badr-Eddine Marani",
        "Julio Silva-Rodriguez",
        "Ismail Ben Ayed",
        "Maria Vakalopoulou",
        "Stergios Christodoulidis",
        "Jose Dolz"
      ],
      "abstract": "Deep neural networks have achieved remarkable success across a variety of tasks, yet they often suffer from unreliable probability estimates. As a result, they can be overconfident in their predictions. Conformal Prediction (CP) offers a principled framework for uncertainty quantification, yielding prediction sets with rigorous coverage guarantees. Existing conformal training methods optimize for overall set size, but shaping the prediction sets in a class-conditional manner is not straightforward and typically requires prior knowledge of the data distribution. In this work, we introduce Class Adaptive Conformal Training (CaCT), which formulates conformal training as an augmented Lagrangian optimization problem that adaptively learns to shape prediction sets class-conditionally without making any distributional assumptions. Experiments on multiple benchmark datasets, including standard and long-tailed image recognition as well as text classification, demonstrate that CaCT consistently outperforms prior conformal training methods, producing significantly smaller and more informative prediction sets while maintaining the desired coverage guarantees.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09522v1",
        "pdf": "https://arxiv.org/pdf/2601.09522v1"
      },
      "arxiv_id": "2601.09522v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09520v1",
      "title": "Towards Realistic Synthetic Data for Automatic Drum Transcription",
      "authors": [
        "Pierfrancesco Melucci",
        "Paolo Merialdo",
        "Taketo Akama"
      ],
      "abstract": "Deep learning models define the state-of-the-art in Automatic Drum Transcription (ADT), yet their performance is contingent upon large-scale, paired audio-MIDI datasets, which are scarce. Existing workarounds that use synthetic data often introduce a significant domain gap, as they typically rely on low-fidelity SoundFont libraries that lack acoustic diversity. While high-quality one-shot samples offer a better alternative, they are not available in a standardized, large-scale format suitable for training. This paper introduces a new paradigm for ADT that circumvents the need for paired audio-MIDI training data. Our primary contribution is a semi-supervised method to automatically curate a large and diverse corpus of one-shot drum samples from unlabeled audio sources. We then use this corpus to synthesize a high-quality dataset from MIDI files alone, which we use to train a sequence-to-sequence transcription model. We evaluate our model on the ENST and MDB test sets, where it achieves new state-of-the-art results, significantly outperforming both fully supervised methods and previous synthetic-data approaches. The code for reproducing our experiments is publicly available at https://github.com/pier-maker92/ADT_STR",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09520v1",
        "pdf": "https://arxiv.org/pdf/2601.09520v1"
      },
      "arxiv_id": "2601.09520v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09518v1",
      "title": "Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations",
      "authors": [
        "Wei-Jin Huang",
        "Yue-Yi Zhang",
        "Yi-Lin Wei",
        "Zhi-Wei Xia",
        "Juantao Tan",
        "Yuan-Ming Li",
        "Zhilin Zhao",
        "Wei-Shi Zheng"
      ],
      "abstract": "Enabling humanoid robots to physically interact with humans is a critical frontier, but progress is hindered by the scarcity of high-quality Human-Humanoid Interaction (HHoI) data. While leveraging abundant Human-Human Interaction (HHI) data presents a scalable alternative, we first demonstrate that standard retargeting fails by breaking the essential contacts. We address this with PAIR (Physics-Aware Interaction Retargeting), a contact-centric, two-stage pipeline that preserves contact semantics across morphology differences to generate physically consistent HHoI data. This high-quality data, however, exposes a second failure: conventional imitation learning policies merely mimic trajectories and lack interactive understanding. We therefore introduce D-STAR (Decoupled Spatio-Temporal Action Reasoner), a hierarchical policy that disentangles when to act from where to act. In D-STAR, Phase Attention (when) and a Multi-Scale Spatial module (where) are fused by the diffusion head to produce synchronized whole-body behaviors beyond mimicry. By decoupling these reasoning streams, our model learns robust temporal phases without being distracted by spatial noise, leading to responsive, synchronized collaboration. We validate our framework through extensive and rigorous simulations, demonstrating significant performance gains over baseline approaches and a complete, effective pipeline for learning complex whole-body interactions from HHI data.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09518v1",
        "pdf": "https://arxiv.org/pdf/2601.09518v1"
      },
      "arxiv_id": "2601.09518v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09512v1",
      "title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion",
      "authors": [
        "Ralf Römer",
        "Yi Zhang",
        "Angela P. Schoellig"
      ],
      "abstract": "To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09512v1",
        "pdf": "https://arxiv.org/pdf/2601.09512v1"
      },
      "arxiv_id": "2601.09512v1",
      "comment": "Project page: https://tum-lsy.github.io/clare. 9 pages, 5 figures",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2601.09503v1",
      "title": "What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding",
      "authors": [
        "Siyuan Liu",
        "Hongbang Yuan",
        "Xinze Li",
        "Ziyue Zhu",
        "Yixin Cao",
        "Yu-Gang Jiang"
      ],
      "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09503v1",
        "pdf": "https://arxiv.org/pdf/2601.09503v1"
      },
      "arxiv_id": "2601.09503v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09499v1",
      "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
      "authors": [
        "Edgar Sucar",
        "Eldar Insafutdinov",
        "Zihang Lai",
        "Andrea Vedaldi"
      ],
      "abstract": "Powerful 3D representations such as DUSt3R invariant point maps, which encode 3D shape and camera parameters, have significantly advanced feed forward 3D reconstruction. While point maps assume static scenes, Dynamic Point Maps (DPMs) extend this concept to dynamic 3D content by additionally representing scene motion. However, existing DPMs are limited to image pairs and, like DUSt3R, require post processing via optimization when more than two views are involved. We argue that DPMs are more useful when applied to videos and introduce V-DPM to demonstrate this. First, we show how to formulate DPMs for video input in a way that maximizes representational power, facilitates neural prediction, and enables reuse of pretrained models. Second, we implement these ideas on top of VGGT, a recent and powerful 3D reconstructor. Although VGGT was trained on static scenes, we show that a modest amount of synthetic data is sufficient to adapt it into an effective V-DPM predictor. Our approach achieves state of the art performance in 3D and 4D reconstruction for dynamic scenes. In particular, unlike recent dynamic extensions of VGGT such as P3, DPMs recover not only dynamic depth but also the full 3D motion of every point in the scene.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09499v1",
        "pdf": "https://arxiv.org/pdf/2601.09499v1"
      },
      "arxiv_id": "2601.09499v1",
      "comment": "Project page: https://www.robots.ox.ac.uk/~vgg/research/vdpm/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09497v1",
      "title": "Towards Robust Cross-Dataset Object Detection Generalization under Domain Specificity",
      "authors": [
        "Ritabrata Chakraborty",
        "Hrishit Mitra",
        "Shivakumara Palaiahnakote",
        "Umapada Pal"
      ],
      "abstract": "Object detectors often perform well in-distribution, yet degrade sharply on a different benchmark. We study cross-dataset object detection (CD-OD) through a lens of setting specificity. We group benchmarks into setting-agnostic datasets with diverse everyday scenes and setting-specific datasets tied to a narrow environment, and evaluate a standard detector family across all train--test pairs. This reveals a clear structure in CD-OD: transfer within the same setting type is relatively stable, while transfer across setting types drops substantially and is often asymmetric. The most severe breakdowns occur when transferring from specific sources to agnostic targets, and persist after open-label alignment, indicating that domain shift dominates in the hardest regimes. To disentangle domain shift from label mismatch, we compare closed-label transfer with an open-label protocol that maps predicted classes to the nearest target label using CLIP similarity. Open-label evaluation yields consistent but bounded gains, and many corrected cases correspond to semantic near-misses supported by the image evidence. Overall, we provide a principled characterization of CD-OD under setting specificity and practical guidance for evaluating detectors under distribution shift. Code will be released at \\href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09497v1",
        "pdf": "https://arxiv.org/pdf/2601.09497v1"
      },
      "arxiv_id": "2601.09497v1",
      "comment": "15 pages, 4 figures, 6 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09495v1",
      "title": "Parallelizable memory recurrent units",
      "authors": [
        "Florent De Geeter",
        "Gaspard Lambrechts",
        "Damien Ernst",
        "Guillaume Drion"
      ],
      "abstract": "With the emergence of massively parallel processing units, parallelization has become a desirable property for new sequence models. The ability to parallelize the processing of sequences with respect to the sequence length during training is one of the main factors behind the uprising of the Transformer architecture. However, Transformers lack efficiency at sequence generation, as they need to reprocess all past timesteps at every generation step. Recently, state-space models (SSMs) emerged as a more efficient alternative. These new kinds of recurrent neural networks (RNNs) keep the efficient update of the RNNs while gaining parallelization by getting rid of nonlinear dynamics (or recurrence). SSMs can reach state-of-the art performance through the efficient training of potentially very large networks, but still suffer from limited representation capabilities. In particular, SSMs cannot exhibit persistent memory, or the capacity of retaining information for an infinite duration, because of their monostability. In this paper, we introduce a new family of RNNs, the memory recurrent units (MRUs), that combine the persistent memory capabilities of nonlinear RNNs with the parallelizable computations of SSMs. These units leverage multistability as a source of persistent memory, while getting rid of transient dynamics for efficient computations. We then derive a specific implementation as proof-of-concept: the bistable memory recurrent unit (BMRU). This new RNN is compatible with the parallel scan algorithm. We show that BMRU achieves good results in tasks with long-term dependencies, and can be combined with state-space models to create hybrid networks that are parallelizable and have transient dynamics as well as persistent memory.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09495v1",
        "pdf": "https://arxiv.org/pdf/2601.09495v1"
      },
      "arxiv_id": "2601.09495v1",
      "comment": "19 pages, 12 figures. This work has been the subject of a patent application (Number: EP26151077). This work has been submitted to the IEEE for possible publication",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09491v1",
      "title": "Deep Operator Networks for Surrogate Modeling of Cyclic Adsorption Processes with Varying Initial Conditions",
      "authors": [
        "Beatrice Ceccanti",
        "Mattia Galanti",
        "Ivo Roghair",
        "Martin van Sint Annaland"
      ],
      "abstract": "Deep Operator Networks are emerging as fundamental tools among various neural network types to learn mappings between function spaces, and have recently gained attention due to their ability to approximate nonlinear operators. In particular, DeepONets offer a natural formulation for PDE solving, since the solution of a partial differential equation can be interpreted as an operator mapping an initial condition to its corresponding solution field. In this work, we applied DeepONets in the context of process modeling for adsorption technologies, to assess their feasibility as surrogates for cyclic adsorption process simulation and optimization. The goal is to accelerate convergence of cyclic processes such as Temperature-Vacuum Swing Adsorption (TVSA), which require repeated solution of transient PDEs, which are computationally expensive. Since each step of a cyclic adsorption process starts from the final state of the preceding step, effective surrogate modeling requires generalization across a wide range of initial conditions. The governing equations exhibit steep traveling fronts, providing a demanding benchmark for operator learning. To evaluate functional generalization under these conditions, we construct a mixed training dataset composed of heterogeneous initial conditions and train DeepONets to approximate the corresponding solution operators. The trained models are then tested on initial conditions outside the parameter ranges used during training, as well as on completely unseen functional forms. The results demonstrate accurate predictions both within and beyond the training distribution, highlighting DeepONets as potential efficient surrogates for accelerating cyclic adsorption simulations and optimization workflows.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09491v1",
        "pdf": "https://arxiv.org/pdf/2601.09491v1"
      },
      "arxiv_id": "2601.09491v1",
      "comment": "36 pages, 11 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09478v1",
      "title": "Bridging Semantic Understanding and Popularity Bias with LLMs",
      "authors": [
        "Renqiang Luo",
        "Dong Zhang",
        "Yupeng Gao",
        "Wen Shi",
        "Mingliang Hou",
        "Jiaying Liu",
        "Zhe Wang",
        "Shuo Yu"
      ],
      "abstract": "Semantic understanding of popularity bias is a crucial yet underexplored challenge in recommender systems, where popular items are often favored at the expense of niche content. Most existing debiasing methods treat the semantic understanding of popularity bias as a matter of diversity enhancement or long-tail coverage, neglecting the deeper semantic layer that embodies the causal origins of the bias itself. Consequently, such shallow interpretations limit both their debiasing effectiveness and recommendation accuracy. In this paper, we propose FairLRM, a novel framework that bridges the gap in the semantic understanding of popularity bias with Recommendation via Large Language Model (RecLLM). FairLRM decomposes popularity bias into item-side and user-side components, using structured instruction-based prompts to enhance the model's comprehension of both global item distributions and individual user preferences. Unlike traditional methods that rely on surface-level features such as \"diversity\" or \"debiasing\", FairLRM improves the model's ability to semantically interpret and address the underlying bias. Through empirical evaluation, we show that FairLRM significantly enhances both fairness and recommendation accuracy, providing a more semantically aware and trustworthy approach to enhance the semantic understanding of popularity bias. The implementation is available at https://github.com/LuoRenqiang/FairLRM.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09478v1",
        "pdf": "https://arxiv.org/pdf/2601.09478v1"
      },
      "arxiv_id": "2601.09478v1",
      "comment": "10 pages, 4 figs, WWW 2026 accepted",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09474v1",
      "title": "Terminally constrained flow-based generative models from an optimal control perspective",
      "authors": [
        "Weiguo Gao",
        "Ming Li",
        "Qianxiao Li"
      ],
      "abstract": "We address the problem of sampling from terminally constrained distributions with pre-trained flow-based generative models through an optimal control formulation. Theoretically, we characterize the value function by a Hamilton-Jacobi-Bellman equation and derive the optimal feedback control as the minimizer of the associated Hamiltonian. We show that as the control penalty increases, the controlled process recovers the reference distribution, while as the penalty vanishes, the terminal law converges to a generalized Wasserstein projection onto the constraint manifold. Algorithmically, we introduce Terminal Optimal Control with Flow-based models (TOCFlow), a geometry-aware sampling-time guidance method for pre-trained flows. Solving the control problem in a terminal co-moving frame that tracks reference trajectories yields a closed-form scalar damping factor along the Riemannian gradient, capturing second-order curvature effects without matrix inversions. TOCFlow therefore matches the geometric consistency of Gauss-Newton updates at the computational cost of standard gradient guidance. We evaluate TOCFlow on three high-dimensional scientific tasks spanning equality, inequality, and global statistical constraints, namely Darcy flow, constrained trajectory planning, and turbulence snapshot generation with Kolmogorov spectral scaling. Across all settings, TOCFlow improves constraint satisfaction over Euclidean guidance and projection baselines while preserving the reference model's generative quality.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09474v1",
        "pdf": "https://arxiv.org/pdf/2601.09474v1"
      },
      "arxiv_id": "2601.09474v1",
      "comment": "59 pages, 9 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09473v1",
      "title": "SimMerge: Learning to Select Merge Operators from Similarity Signals",
      "authors": [
        "Oliver Bolton",
        "Aakanksha",
        "Arash Ahmadian",
        "Sara Hooker",
        "Marzieh Fadaee",
        "Beyza Ermis"
      ],
      "abstract": "Model merging enables multiple large language models (LLMs) to be combined into a single model while preserving performance. This makes it a valuable tool in LLM development, offering a competitive alternative to multi-task training. However, merging can be difficult at scale, as successful merging requires choosing the right merge operator, selecting the right models, and merging them in the right order. This often leads researchers to run expensive merge-and-evaluate searches to select the best merge. In this work, we provide an alternative by introducing \\simmerge{}, \\emph{a predictive merge-selection method} that selects the best merge using inexpensive, task-agnostic similarity signals between models. From a small set of unlabeled probes, we compute functional and structural features and use them to predict the performance of a given 2-way merge. Using these predictions, \\simmerge{} selects the best merge operator, the subset of models to merge, and the merge order, eliminating the expensive merge-and-evaluate loop. We demonstrate that we surpass standard merge-operator performance on 2-way merges of 7B-parameter LLMs, and that \\simmerge{} generalizes to multi-way merges and 111B-parameter LLM merges without retraining. Additionally, we present a bandit variant that supports adding new tasks, models, and operators on the fly. Our results suggest that learning how to merge is a practical route to scalable model composition when checkpoint catalogs are large and evaluation budgets are tight.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09473v1",
        "pdf": "https://arxiv.org/pdf/2601.09473v1"
      },
      "arxiv_id": "2601.09473v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09470v1",
      "title": "Personalized Multimodal Feedback Using Multiple External Representations: Strategy Profiles and Learning in High School Physics",
      "authors": [
        "Natalia Revenga-Lozano",
        "Karina E. Avila",
        "Steffen Steinert",
        "Matthias Schweinberger",
        "Clara E. Gómez-Pérez",
        "Jochen Kuhn",
        "Stefan Küchemann"
      ],
      "abstract": "Multiple external representations (MERs) and personalized feedback support physics learning, yet evidence on how personalized feedback can effectively integrate MERs remains limited. This question is particularly timely given the emergence of multimodal large language models. We conducted a 16-24 week observational study in high school physics (N=661) using a computer-based platform that provided verification and optional elaborated feedback in verbal, graphical and mathematical forms. Linear mixed-effects models and strategy-cluster analyses (ANCOVA-adjusted comparisons) tested associations between feedback use and post-test performance and moderation by representational competence. Elaborated multirepresentational feedback showed a small but consistent positive association with post-test scores independent of prior knowledge and confidence. Learners adopted distinct representation-selection strategies; among students with lower representational competence, using a diverse set of representations related to higher learning, whereas this advantage diminished as competence increased. These findings motivate adaptive feedback designs and inform intelligent tutoring systems capable of tailoring feedback elaboration and representational format to learner profiles, advancing personalized instruction in physics education.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "physics.ed-ph",
        "cs.AI"
      ],
      "primary_category": "physics.ed-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09470v1",
        "pdf": "https://arxiv.org/pdf/2601.09470v1"
      },
      "arxiv_id": "2601.09470v1",
      "comment": "Keywords: Adaptive Feedback, Multimodal Learning, Multiple External Representations, Physics Education, Science Education, Representational Competences, Intelligent Tutoring Systems",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09469v1",
      "title": "FairGU: Fairness-aware Graph Unlearning in Social Network",
      "authors": [
        "Renqiang Luo",
        "Yongshuai Yang",
        "Huafei Huang",
        "Qing Qing",
        "Mingliang Hou",
        "Ziqi Xu",
        "Yi Yu",
        "Jingjing Zhou",
        "Feng Xia"
      ],
      "abstract": "Graph unlearning has emerged as a critical mechanism for supporting sustainable and privacy-preserving social networks, enabling models to remove the influence of deleted nodes and thereby better safeguard user information. However, we observe that existing graph unlearning techniques insufficiently protect sensitive attributes, often leading to degraded algorithmic fairness compared with traditional graph learning methods. To address this gap, we introduce FairGU, a fairness-aware graph unlearning framework designed to preserve both utility and fairness during the unlearning process. FairGU integrates a dedicated fairness-aware module with effective data protection strategies, ensuring that sensitive attributes are neither inadvertently amplified nor structurally exposed when nodes are removed. Through extensive experiments on multiple real-world datasets, we demonstrate that FairGU consistently outperforms state-of-the-art graph unlearning methods and fairness-enhanced graph learning baselines in terms of both accuracy and fairness metrics. Our findings highlight a previously overlooked risk in current unlearning practices and establish FairGU as a robust and equitable solution for the next generation of socially sustainable networked systems. The codes are available at https://github.com/LuoRenqiang/FairGU.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09469v1",
        "pdf": "https://arxiv.org/pdf/2601.09469v1"
      },
      "arxiv_id": "2601.09469v1",
      "comment": "9 pages, 2 figs, WWW 2026 accepted",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09468v1",
      "title": "High-fidelity lunar topographic reconstruction across diverse terrain and illumination environments using deep learning",
      "authors": [
        "Hao Chen",
        "Philipp Gläser",
        "Konrad Willner",
        "Jürgen Oberst"
      ],
      "abstract": "Topographic models are essential for characterizing planetary surfaces and for inferring underlying geological processes. Nevertheless, meter-scale topographic data remain limited, which constrains detailed planetary investigations, even for the Moon, where extensive high-resolution orbital images are available. Recent advances in deep learning (DL) exploit single-view imagery, constrained by low-resolution topography, for fast and flexible reconstruction of fine-scale topography. However, their robustness and general applicability across diverse lunar landforms and illumination conditions remain insufficiently explored. In this study, we build upon our previously proposed DL framework by incorporating a more robust scale recovery scheme and extending the model to polar regions under low solar illumination conditions. We demonstrate that, compared with single-view shape-from-shading methods, the proposed DL approach exhibits greater robustness to varying illumination and achieves more consistent and accurate topographic reconstructions. Furthermore, it reliably reconstructs topography across lunar features of diverse scales, morphologies, and geological ages. High-quality topographic models are also produced for the lunar south polar areas, including permanently shadowed regions, demonstrating the method's capability in reconstructing complex and low-illumination terrain. These findings suggest that DL-based approaches have the potential to leverage extensive lunar datasets to support advanced exploration missions and enable investigations of the Moon at unprecedented topographic resolution.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "astro-ph.EP",
        "cs.LG"
      ],
      "primary_category": "astro-ph.EP",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09468v1",
        "pdf": "https://arxiv.org/pdf/2601.09468v1"
      },
      "arxiv_id": "2601.09468v1",
      "comment": "25 pages, 1 table, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09467v1",
      "title": "Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting",
      "authors": [
        "Tianye Li",
        "Qi Liu",
        "Hao Li",
        "Lei Chen",
        "Wencong Cheng",
        "Fei Zheng",
        "Xiangao Xia",
        "Ya Wang",
        "Gang Huang",
        "Weiwei Wang",
        "Xuan Tong",
        "Ziqing Zu",
        "Yi Fang",
        "Shenming Fu",
        "Jiang Jiang",
        "Haochen Li",
        "Mingxing Li",
        "Jiangjiang Xia"
      ],
      "abstract": "Accurate global medium-range weather forecasting is fundamental to Earth system science. Most existing Transformer-based forecasting models adopt vision-centric architectures that neglect the Earth's spherical geometry and zonal periodicity. In addition, conventional autoregressive training is computationally expensive and limits forecast horizons due to error accumulation. To address these challenges, we propose the Shifted Earth Transformer (Searth Transformer), a physics-informed architecture that incorporates zonal periodicity and meridional boundaries into window-based self-attention for physically consistent global information exchange. We further introduce a Relay Autoregressive (RAR) fine-tuning strategy that enables learning long-range atmospheric evolution under constrained memory and computational budgets. Based on these methods, we develop YanTian, a global medium-range weather forecasting model. YanTian achieves higher accuracy than the high-resolution forecast of the European Centre for Medium-Range Weather Forecasts and performs competitively with state-of-the-art AI models at one-degree resolution, while requiring roughly 200 times lower computational cost than standard autoregressive fine-tuning. Furthermore, YanTian attains a longer skillful forecast lead time for Z500 (10.3 days) than HRES (9 days). Beyond weather forecasting, this work establishes a robust algorithmic foundation for predictive modeling of complex global-scale geophysical circulation systems, offering new pathways for Earth system science.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09467v1",
        "pdf": "https://arxiv.org/pdf/2601.09467v1"
      },
      "arxiv_id": "2601.09467v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09465v1",
      "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
      "authors": [
        "Shuo Zhang",
        "Chaofa Yuan",
        "Ryan Guo",
        "Xiaomin Yu",
        "Rui Xu",
        "Zhangquan Chen",
        "Zinuo Li",
        "Zhi Yang",
        "Shuhao Guan",
        "Zhenheng Tang",
        "Sen Hu",
        "Liwen Zhang",
        "Ronghao Chen",
        "Huacan Wang"
      ],
      "abstract": "While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09465v1",
        "pdf": "https://arxiv.org/pdf/2601.09465v1"
      },
      "arxiv_id": "2601.09465v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09460v1",
      "title": "SoK: Enhancing Cryptographic Collaborative Learning with Differential Privacy",
      "authors": [
        "Francesco Capano",
        "Jonas Böhler",
        "Benjamin Weggenmann"
      ],
      "abstract": "In collaborative learning (CL), multiple parties jointly train a machine learning model on their private datasets. However, data can not be shared directly due to privacy concerns. To ensure input confidentiality, cryptographic techniques, e.g., multi-party computation (MPC), enable training on encrypted data. Yet, even securely trained models are vulnerable to inference attacks aiming to extract memorized data from model outputs. To ensure output privacy and mitigate inference attacks, differential privacy (DP) injects calibrated noise during training. While cryptography and DP offer complementary guarantees, combining them efficiently for cryptographic and differentially private CL (CPCL) is challenging. Cryptography incurs performance overheads, while DP degrades accuracy, creating a privacy-accuracy-performance trade-off that needs careful design considerations. This work systematizes the CPCL landscape. We introduce a unified framework that generalizes common phases across CPCL paradigms, and identify secure noise sampling as the foundational phase to achieve CPCL. We analyze trade-offs of different secure noise sampling techniques, noise types, and DP mechanisms discussing their implementation challenges and evaluating their accuracy and cryptographic overhead across CPCL paradigms. Additionally, we implement identified secure noise sampling options in MPC and evaluate their computation and communication costs in WAN and LAN. Finally, we propose future research directions based on identified key observations, gaps and possible enhancements in the literature.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09460v1",
        "pdf": "https://arxiv.org/pdf/2601.09460v1"
      },
      "arxiv_id": "2601.09460v1",
      "comment": "This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML 2026)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09455v1",
      "title": "On the Hardness of Computing Counterfactual and Semifactual Explanations in XAI",
      "authors": [
        "André Artelt",
        "Martin Olsen",
        "Kevin Tierney"
      ],
      "abstract": "Providing clear explanations to the choices of machine learning models is essential for these models to be deployed in crucial applications. Counterfactual and semi-factual explanations have emerged as two mechanisms for providing users with insights into the outputs of their models. We provide an overview of the computational complexity results in the literature for generating these explanations, finding that in many cases, generating explanations is computationally hard. We strengthen the argument for this considerably by further contributing our own inapproximability results showing that not only are explanations often hard to generate, but under certain assumptions, they are also hard to approximate. We discuss the implications of these complexity results for the XAI community and for policymakers seeking to regulate explanations in AI.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09455v1",
        "pdf": "https://arxiv.org/pdf/2601.09455v1"
      },
      "arxiv_id": "2601.09455v1",
      "comment": "Accepted in Transactions on Machine Learning Research (TMLR), 2025 -- https://openreview.net/pdf?id=aELzBw0q1O",
      "journal_ref": "Transactions on Machine Learning Research (TMLR), 2025",
      "has_code": false
    },
    {
      "id": "2601.09452v1",
      "title": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
      "authors": [
        "Ahmad Rahimi",
        "Valentin Gerard",
        "Eloi Zablocki",
        "Matthieu Cord",
        "Alexandre Alahi"
      ],
      "abstract": "Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable world models for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving world models with minimal supervision. The key idea is to decouple motion learning from appearance synthesis. First, the model is adapted to predict structured motion in a simplified form: videos of skeletonized agents and scene elements, focusing learning on physical and social plausibility. Then, the same backbone is reused to synthesize realistic RGB videos conditioned on these motion sequences, effectively \"dressing\" the motion with texture and lighting. This two-stage process mirrors a reasoning-rendering paradigm: first infer dynamics, then render appearance. Our experiments show this decoupled approach is exceptionally efficient: adapting SVD, we match prior SOTA models with less than 6% of their compute. Scaling to LTX, our MAD-LTX model outperforms all open-source competitors, and supports a comprehensive suite of text, ego, and object controls. Project page: https://vita-epfl.github.io/MAD-World-Model/",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09452v1",
        "pdf": "https://arxiv.org/pdf/2601.09452v1"
      },
      "arxiv_id": "2601.09452v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09451v1",
      "title": "Late Breaking Results: Quamba-SE: Soft-edge Quantizer for Activations in State Space Models",
      "authors": [
        "Yizhi Chen",
        "Ahmed Hemani"
      ],
      "abstract": "We propose Quamba-SE, a soft-edge quantizer for State Space Model (SSM) activation quantization. Unlike existing methods, using standard INT8 operation, Quamba-SE employs three adaptive scales: high-precision for small values, standard scale for normal values, and low-precision for outliers. This preserves outlier information instead of hard clipping, while maintaining precision for other values. We evaluate on Mamba- 130M across 6 zero-shot benchmarks. Results show that Quamba- SE consistently outperforms Quamba, achieving up to +2.68% on individual benchmarks and up to +0.83% improvement in the average accuracy of 6 datasets.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09451v1",
        "pdf": "https://arxiv.org/pdf/2601.09451v1"
      },
      "arxiv_id": "2601.09451v1",
      "comment": "Accepted to DATE Late Breaking Results 2026, Verona, Italy",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09449v1",
      "title": "PrivLEX: Detecting legal concepts in images through Vision-Language Models",
      "authors": [
        "Darya Baranouskaya",
        "Andrea Cavallaro"
      ],
      "abstract": "We present PrivLEX, a novel image privacy classifier that grounds its decisions in legally defined personal data concepts. PrivLEX is the first interpretable privacy classifier aligned with legal concepts that leverages the recognition capabilities of Vision-Language Models (VLMs). PrivLEX relies on zero-shot VLM concept detection to provide interpretable classification through a label-free Concept Bottleneck Model, without requiring explicit concept labels during training. We demonstrate PrivLEX's ability to identify personal data concepts that are present in images. We further analyse the sensitivity of such concepts as perceived by human annotators of image privacy datasets.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09449v1",
        "pdf": "https://arxiv.org/pdf/2601.09449v1"
      },
      "arxiv_id": "2601.09449v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09448v1",
      "title": "Population-Aligned Audio Reproduction With LLM-Based Equalizers",
      "authors": [
        "Ioannis Stylianou",
        "Jon Francombe",
        "Pablo Martinez-Nuevo",
        "Sven Ewan Shepstone",
        "Zheng-Hua Tan"
      ],
      "abstract": "Conventional audio equalization is a static process that requires manual and cumbersome adjustments to adapt to changing listening contexts (e.g., mood, location, or social setting). In this paper, we introduce a Large Language Model (LLM)-based alternative that maps natural language text prompts to equalization settings. This enables a conversational approach to sound system control. By utilizing data collected from a controlled listening experiment, our models exploit in-context learning and parameter-efficient fine-tuning techniques to reliably align with population-preferred equalization settings. Our evaluation methods, which leverage distributional metrics that capture users' varied preferences, show statistically significant improvements in distributional alignment over random sampling and static preset baselines. These results indicate that LLMs could function as \"artificial equalizers,\" contributing to the development of more accessible, context-aware, and expert-level audio tuning methods.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09448v1",
        "pdf": "https://arxiv.org/pdf/2601.09448v1"
      },
      "arxiv_id": "2601.09448v1",
      "comment": "12 pages, 13 figures, 2 tables, IEEE JSTSP journal submission under first revision",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09446v1",
      "title": "Improving Symbolic Translation of Language Models for Logical Reasoning",
      "authors": [
        "Ramya Keerthy Thatikonda",
        "Jiuzhou Han",
        "Wray Buntine",
        "Ehsan Shareghi"
      ],
      "abstract": "The use of formal language for deductive logical reasoning aligns well with language models (LMs), where translating natural language (NL) into first-order logic (FOL) and employing an external solver results in a verifiable and therefore reliable reasoning system. However, smaller LMs often struggle with this translation task, frequently producing incorrect symbolic outputs due to formatting and translation errors. Existing approaches typically rely on self-iteration to correct these errors, but such methods depend heavily on the capabilities of the underlying model. To address this, we first categorize common errors and fine-tune smaller LMs using data synthesized by large language models. The evaluation is performed using the defined error categories. We introduce incremental inference, which divides inference into two stages, predicate generation and FOL translation, providing greater control over model behavior and enhancing generation quality as measured by predicate metrics. This decomposition framework also enables the use of a verification module that targets predicate-arity errors to further improve performance. Our study evaluates three families of models across four logical-reasoning datasets. The comprehensive fine-tuning, incremental inference, and verification modules reduce error rates, increase predicate coverage, and improve reasoning performance for smaller LMs, moving us closer to developing reliable and accessible symbolic-reasoning systems.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09446v1",
        "pdf": "https://arxiv.org/pdf/2601.09446v1"
      },
      "arxiv_id": "2601.09446v1",
      "comment": "The Third workshop of NeusymBridge @AAAI 2026 (Bridging Neurons and Symbols for NLP and Knowledge Graph Reasoning)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09445v1",
      "title": "Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models",
      "authors": [
        "Minh Vu Pham",
        "Hsuvas Borkakoty",
        "Yufang Hou"
      ],
      "abstract": "In language models (LMs), intra-memory knowledge conflict largely arises when inconsistent information about the same event is encoded within the model's parametric knowledge. While prior work has primarily focused on resolving conflicts between a model's internal knowledge and external resources through approaches such as fine-tuning or knowledge editing, the problem of localizing conflicts that originate during pre-training within the model's internal representations remain unexplored. In this work, we design a framework based on mechanistic interpretability methods to identify where and how conflicting knowledge from the pre-training data is encoded within LMs. Our findings contribute to a growing body of evidence that specific internal components of a language model are responsible for encoding conflicting knowledge from pre-training, and we demonstrate how mechanistic interpretability methods can be leveraged to causally intervene in and control conflicting knowledge at inference time.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09445v1",
        "pdf": "https://arxiv.org/pdf/2601.09445v1"
      },
      "arxiv_id": "2601.09445v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09439v1",
      "title": "DeepLight: A Sobolev-trained Image-to-Image Surrogate Model for Light Transport in Tissue",
      "authors": [
        "Philipp Haim",
        "Vasilis Ntziachristos",
        "Torsten Enßlin",
        "Dominik Jüstel"
      ],
      "abstract": "In optoacoustic imaging, recovering the absorption coefficients of tissue by inverting the light transport remains a challenging problem. Improvements in solving this problem can greatly benefit the clinical value of optoacoustic imaging. Existing variational inversion methods require an accurate and differentiable model of this light transport. As neural surrogate models allow fast and differentiable simulations of complex physical processes, they are considered promising candidates to be used in solving such inverse problems. However, there are in general no guarantees that the derivatives of these surrogate models accurately match those of the underlying physical operator. As accurate derivatives are central to solving inverse problems, errors in the model derivative can considerably hinder high fidelity reconstructions. To overcome this limitation, we present a surrogate model for light transport in tissue that uses Sobolev training to improve the accuracy of the model derivatives. Additionally, the form of Sobolev training we used is suitable for high-dimensional models in general. Our results demonstrate that Sobolev training for a light transport surrogate model not only improves derivative accuracy but also reduces generalization error for in-distribution and out-of-distribution samples. These improvements promise to considerably enhance the utility of the surrogate model in downstream tasks, especially in solving inverse problems.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "physics.med-ph"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09439v1",
        "pdf": "https://arxiv.org/pdf/2601.09439v1"
      },
      "arxiv_id": "2601.09439v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09433v1",
      "title": "Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?",
      "authors": [
        "David Reid",
        "Ognjen Arandjelovic"
      ],
      "abstract": "Automated analysis of ancient coins has the potential to help researchers extract more historical insights from large collections of coins and to help collectors understand what they are buying or selling. Recent research in this area has shown promise in focusing on identification of semantic elements as they are commonly depicted on ancient coins, by using convolutional neural networks (CNNs). This paper is the first to apply the recently proposed Vision Transformer (ViT) deep learning architecture to the task of identification of semantic elements on coins, using fully automatic learning from multi-modal data (images and unstructured text). This article summarises previous research in the area, discusses the training and implementation of ViT and CNN models for ancient coins analysis and provides an evaluation of their performance. The ViT models were found to outperform the newly trained CNN models in accuracy.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09433v1",
        "pdf": "https://arxiv.org/pdf/2601.09433v1"
      },
      "arxiv_id": "2601.09433v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09430v1",
      "title": "Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs",
      "authors": [
        "Rui Zhu",
        "Xin Shen",
        "Shuchen Wu",
        "Chenxi Miao",
        "Xin Yu",
        "Yang Li",
        "Weikang Li",
        "Deguo Xia",
        "Jizhou Huang"
      ],
      "abstract": "Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09430v1",
        "pdf": "https://arxiv.org/pdf/2601.09430v1"
      },
      "arxiv_id": "2601.09430v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09428v1",
      "title": "Draw it like Euclid: Teaching transformer models to generate CAD profiles using ruler and compass construction steps",
      "authors": [
        "Siyi Li",
        "Joseph G. Lambourne",
        "Longfei Zhang",
        "Pradeep Kumar Jayaraman",
        "Karl. D. D. Willis"
      ],
      "abstract": "We introduce a new method of generating Computer Aided Design (CAD) profiles via a sequence of simple geometric constructions including curve offsetting, rotations and intersections. These sequences start with geometry provided by a designer and build up the points and curves of the final profile step by step. We demonstrate that adding construction steps between the designer's input geometry and the final profile improves generation quality in a similar way to the introduction of a chain of thought in language models. Similar to the constraints in a parametric CAD model, the construction sequences reduce the degrees of freedom in the modeled shape to a small set of parameter values which can be adjusted by the designer, allowing parametric editing with the constructed geometry evaluated to floating point precision. In addition we show that applying reinforcement learning to the construction sequences gives further improvements over a wide range of metrics, including some which were not explicitly optimized.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.GR"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09428v1",
        "pdf": "https://arxiv.org/pdf/2601.09428v1"
      },
      "arxiv_id": "2601.09428v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09421v1",
      "title": "Bias Dynamics in BabyLMs: Towards a Compute-Efficient Sandbox for Democratising Pre-Training Debiasing",
      "authors": [
        "Filip Trhlik",
        "Andrew Caines",
        "Paula Buttery"
      ],
      "abstract": "Pre-trained language models (LMs) have, over the last few years, grown substantially in both societal adoption and training costs. This rapid growth in size has constrained progress in understanding and mitigating their biases. Since re-training LMs is prohibitively expensive, most debiasing work has focused on post-hoc or masking-based strategies, which often fail to address the underlying causes of bias. In this work, we seek to democratise pre-model debiasing research by using low-cost proxy models. Specifically, we investigate BabyLMs, compact BERT-like models trained on small and mutable corpora that can approximate bias acquisition and learning dynamics of larger models. We show that BabyLMs display closely aligned patterns of intrinsic bias formation and performance development compared to standard BERT models, despite their drastically reduced size. Furthermore, correlations between BabyLMs and BERT hold across multiple intra-model and post-model debiasing methods. Leveraging these similarities, we conduct pre-model debiasing experiments with BabyLMs, replicating prior findings and presenting new insights regarding the influence of gender imbalance and toxicity on bias formation. Our results demonstrate that BabyLMs can serve as an effective sandbox for large-scale LMs, reducing pre-training costs from over 500 GPU-hours to under 30 GPU-hours. This provides a way to democratise pre-model debiasing research and enables faster, more accessible exploration of methods for building fairer LMs.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09421v1",
        "pdf": "https://arxiv.org/pdf/2601.09421v1"
      },
      "arxiv_id": "2601.09421v1",
      "comment": "21 pages, 18 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09416v1",
      "title": "Radiomics-Integrated Deep Learning with Hierarchical Loss for Osteosarcoma Histology Classification",
      "authors": [
        "Yaxi Chen",
        "Zi Ye",
        "Shaheer U. Saeed",
        "Oliver Yu",
        "Simin Ni",
        "Jie Huang",
        "Yipeng Hu"
      ],
      "abstract": "Osteosarcoma (OS) is an aggressive primary bone malignancy. Accurate histopathological assessment of viable versus non-viable tumor regions after neoadjuvant chemotherapy is critical for prognosis and treatment planning, yet manual evaluation remains labor-intensive, subjective, and prone to inter-observer variability. Recent advances in digital pathology have enabled automated necrosis quantification. Evaluating on test data, independently sampled on patient-level, revealed that the deep learning model performance dropped significantly from the tile-level generalization ability reported in previous studies. First, this work proposes the use of radiomic features as additional input in model training. We show that, despite that they are derived from the images, such a multimodal input effectively improved the classification performance, in addition to its added benefits in interpretability. Second, this work proposes to optimize two binary classification tasks with hierarchical classes (i.e. tumor-vs-non-tumor and viable-vs-non-viable), as opposed to the alternative ``flat'' three-class classification task (i.e. non-tumor, non-viable tumor, viable tumor), thereby enabling a hierarchical loss. We show that such a hierarchical loss, with trainable weightings between the two tasks, the per-class performance can be improved significantly. Using the TCIA OS Tumor Assessment dataset, we experimentally demonstrate the benefits from each of the proposed new approaches and their combination, setting a what we consider new state-of-the-art performance on this open dataset for this application. Code and trained models: https://github.com/YaxiiC/RadiomicsOS.git.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09416v1",
        "pdf": "https://arxiv.org/pdf/2601.09416v1"
      },
      "arxiv_id": "2601.09416v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09413v1",
      "title": "Speech-Hands: A Self-Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception",
      "authors": [
        "Zhen Wan",
        "Chao-Han Huck Yang",
        "Jinchuan Tian",
        "Hanrong Ye",
        "Ankita Pasad",
        "Szu-wei Fu",
        "Arushi Goel",
        "Ryo Hachiuma",
        "Shizhe Diao",
        "Kunal Dhawan",
        "Sreyan Ghosh",
        "Yusuke Hirota",
        "Zhehuai Chen",
        "Rafael Valle",
        "Ehsan Hosseini Asl",
        "Chenhui Chu",
        "Shinji Watanabe",
        "Yu-Chiang Frank Wang",
        "Boris Ginsburg"
      ],
      "abstract": "We introduce a voice-agentic framework that learns one critical omni-understanding skill: knowing when to trust itself versus when to consult external audio perception. Our work is motivated by a crucial yet counterintuitive finding: naively fine-tuning an omni-model on both speech recognition and external sound understanding tasks often degrades performance, as the model can be easily misled by noisy hypotheses. To address this, our framework, Speech-Hands, recasts the problem as an explicit self-reflection decision. This learnable reflection primitive proves effective in preventing the model from being derailed by flawed external candidates. We show that this agentic action mechanism generalizes naturally from speech recognition to complex, multiple-choice audio reasoning. Across the OpenASR leaderboard, Speech-Hands consistently outperforms strong baselines by 12.1% WER on seven benchmarks. The model also achieves 77.37% accuracy and high F1 on audio QA decisions, showing robust generalization and reliability across diverse audio question answering datasets. By unifying perception and decision-making, our work offers a practical path toward more reliable and resilient audio intelligence.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MA",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09413v1",
        "pdf": "https://arxiv.org/pdf/2601.09413v1"
      },
      "arxiv_id": "2601.09413v1",
      "comment": "Preprint. The version was submitted in October 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09410v1",
      "title": "Detail Loss in Super-Resolution Models Based on the Laplacian Pyramid and Repeated Upscaling and Downscaling Process",
      "authors": [
        "Sangjun Han",
        "Youngmi Hur"
      ],
      "abstract": "With advances in artificial intelligence, image processing has gained significant interest. Image super-resolution is a vital technology closely related to real-world applications, as it enhances the quality of existing images. Since enhancing fine details is crucial for the super-resolution task, pixels that contribute to high-frequency information should be emphasized. This paper proposes two methods to enhance high-frequency details in super-resolution images: a Laplacian pyramid-based detail loss and a repeated upscaling and downscaling process. Total loss with our detail loss guides a model by separately generating and controlling super-resolution and detail images. This approach allows the model to focus more effectively on high-frequency components, resulting in improved super-resolution images. Additionally, repeated upscaling and downscaling amplify the effectiveness of the detail loss by extracting diverse information from multiple low-resolution features. We conduct two types of experiments. First, we design a CNN-based model incorporating our methods. This model achieves state-of-the-art results, surpassing all currently available CNN-based and even some attention-based models. Second, we apply our methods to existing attention-based models on a small scale. In all our experiments, attention-based models adding our detail loss show improvements compared to the originals. These results demonstrate our approaches effectively enhance super-resolution images across different model structures.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09410v1",
        "pdf": "https://arxiv.org/pdf/2601.09410v1"
      },
      "arxiv_id": "2601.09410v1",
      "comment": "Accepted for publication in IET Image Processing. This is the authors' final accepted manuscript",
      "journal_ref": "IET Image Processing, 2025; 19:e70238",
      "has_code": false
    },
    {
      "id": "2601.09400v1",
      "title": "Preliminary Tests of the Anticipatory Classifier System with Hindsight Experience Replay",
      "authors": [
        "Olgierd Unold",
        "Stanisław Franczyk"
      ],
      "abstract": "This paper introduces ACS2HER, a novel integration of the Anticipatory Classifier System (ACS2) with the Hindsight Experience Replay (HER) mechanism. While ACS2 is highly effective at building cognitive maps through latent learning, its performance often stagnates in environments characterized by sparse rewards. We propose a specific architectural variant that triggers hindsight learning when the agent fails to reach its primary goal, re-labeling visited states as virtual goals to densify the learning signal. The proposed model was evaluated on two benchmarks: the deterministic \\texttt{Maze 6} and the stochastic \\texttt{FrozenLake}. The results demonstrate that ACS2HER significantly accelerates knowledge acquisition and environmental mastery compared to the standard ACS2. However, this efficiency gain is accompanied by increased computational overhead and a substantial expansion in classifier numerosity. This work provides the first analysis of combining anticipatory mechanisms with retrospective goal-relabeling in Learning Classifier Systems.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09400v1",
        "pdf": "https://arxiv.org/pdf/2601.09400v1"
      },
      "arxiv_id": "2601.09400v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09398v1",
      "title": "Ability Transfer and Recovery via Modularized Parameters Localization",
      "authors": [
        "Songyao Jin",
        "Kun Zhou",
        "Wenqi Li",
        "Peng Wang",
        "Biwei Huang"
      ],
      "abstract": "Large language models can be continually pre-trained or fine-tuned to improve performance in specific domains, languages, or skills, but this specialization often degrades other capabilities and may cause catastrophic forgetting. We investigate how abilities are distributed within LLM parameters by analyzing module activations under domain- and language-specific inputs for closely related models. Across layers and modules, we find that ability-related activations are highly concentrated in a small set of channels (typically <5\\%), and these channels are largely disentangled with good sufficiency and stability. Building on these observations, we propose ACT (Activation-Guided Channel-wise Ability Transfer), which localizes ability-relevant channels via activation differences and selectively transfers only the corresponding parameters, followed by lightweight fine-tuning for compatibility. Experiments on multilingual mathematical and scientific reasoning show that ACT can recover forgotten abilities while preserving retained skills. It can also merge multiple specialized models to integrate several abilities into a single model with minimal interference. Our code and data will be publicly released.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09398v1",
        "pdf": "https://arxiv.org/pdf/2601.09398v1"
      },
      "arxiv_id": "2601.09398v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09394v1",
      "title": "FairGE: Fairness-Aware Graph Encoding in Incomplete Social Networks",
      "authors": [
        "Renqiang Luo",
        "Huafei Huang",
        "Tao Tang",
        "Jing Ren",
        "Ziqi Xu",
        "Mingliang Hou",
        "Enyan Dai",
        "Feng Xia"
      ],
      "abstract": "Graph Transformers (GTs) are increasingly applied to social network analysis, yet their deployment is often constrained by fairness concerns. This issue is particularly critical in incomplete social networks, where sensitive attributes are frequently missing due to privacy and ethical restrictions. Existing solutions commonly generate these incomplete attributes, which may introduce additional biases and further compromise user privacy. To address this challenge, FairGE (Fair Graph Encoding) is introduced as a fairness-aware framework for GTs in incomplete social networks. Instead of generating sensitive attributes, FairGE encodes fairness directly through spectral graph theory. By leveraging the principal eigenvector to represent structural information and padding incomplete sensitive attributes with zeros to maintain independence, FairGE ensures fairness without data reconstruction. Theoretical analysis demonstrates that the method suppresses the influence of non-principal spectral components, thereby enhancing fairness. Extensive experiments on seven real-world social network datasets confirm that FairGE achieves at least a 16% improvement in both statistical parity and equality of opportunity compared with state-of-the-art baselines. The source code is shown in https://github.com/LuoRenqiang/FairGE.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09394v1",
        "pdf": "https://arxiv.org/pdf/2601.09394v1"
      },
      "arxiv_id": "2601.09394v1",
      "comment": "12 pages, WWW 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09382v1",
      "title": "Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments",
      "authors": [
        "Qinglong Shi",
        "Donghai Wang",
        "Hantao Zhou",
        "Jiguo Li",
        "Jun Xu",
        "Jiuchong Gao",
        "Jinghua Hao",
        "Renqing He"
      ],
      "abstract": "Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09382v1",
        "pdf": "https://arxiv.org/pdf/2601.09382v1"
      },
      "arxiv_id": "2601.09382v1",
      "comment": "8 pages, 2 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09381v1",
      "title": "Query Languages for Machine-Learning Models",
      "authors": [
        "Martin Grohe"
      ],
      "abstract": "In this paper, I discuss two logics for weighted finite structures: first-order logic with summation (FO(SUM)) and its recursive extension IFP(SUM). These logics originate from foundational work by Grädel, Gurevich, and Meer in the 1990s. In recent joint work with Standke, Steegmans, and Van den Bussche, we have investigated these logics as query languages for machine learning models, specifically neural networks, which are naturally represented as weighted graphs. I present illustrative examples of queries to neural networks that can be expressed in these logics and discuss fundamental results on their expressiveness and computational complexity.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LO",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09381v1",
        "pdf": "https://arxiv.org/pdf/2601.09381v1"
      },
      "arxiv_id": "2601.09381v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09365v1",
      "title": "Frame of Reference: Addressing the Challenges of Common Ground Representation in Situational Dialogs",
      "authors": [
        "Biswesh Mohapatra",
        "Théo Charlot",
        "Giovanni Duca",
        "Mayank Palan",
        "Laurent Romary",
        "Justine Cassell"
      ],
      "abstract": "Common ground plays a critical role in situated spoken dialogues, where interlocutors must establish and maintain shared references to entities, events, and relations to sustain coherent interaction. For dialog systems, the ability to correctly ground conversational content in order to refer back to it later is particularly important. Prior studies have demonstrated that LLMs are capable of performing grounding acts such as requesting clarification or producing acknowledgments, yet relatively little work has investigated how common ground can be explicitly represented and stored for later use. Without such mechanisms, it remains unclear whether acknowledgment or clarification behaviors truly reflect a grounded understanding. In this work, we evaluate a model's ability to establish and exploit common ground through relational references to entities within the shared context in a situational dialogue. We test multiple methods for representing common ground in situated dialogues and further propose approaches to improve both the establishment of common ground and its subsequent use in the conversation.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09365v1",
        "pdf": "https://arxiv.org/pdf/2601.09365v1"
      },
      "arxiv_id": "2601.09365v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09361v1",
      "title": "GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR",
      "authors": [
        "Jiaying Zhang",
        "Lei Shi",
        "Jiguo Li",
        "Jun Xu",
        "Jiuchong Gao",
        "Jinghua Hao",
        "Renqing He"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09361v1",
        "pdf": "https://arxiv.org/pdf/2601.09361v1"
      },
      "arxiv_id": "2601.09361v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09353v1",
      "title": "Monte-Carlo Tree Search with Neural Network Guidance for Lane-Free Autonomous Driving",
      "authors": [
        "Ioannis Peridis",
        "Dimitrios Troullinos",
        "Georgios Chalkiadakis",
        "Pantelis Giankoulidis",
        "Ioannis Papamichail",
        "Markos Papageorgiou"
      ],
      "abstract": "Lane-free traffic environments allow vehicles to better harness the lateral capacity of the road without being restricted to lane-keeping, thereby increasing the traffic flow rates. As such, we have a distinct and more challenging setting for autonomous driving. In this work, we consider a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic, where the associated Markov Decision Process we formulate is influenced from existing approaches tied to reinforcement learning frameworks. In addition, MCTS is equipped with a pre-trained neural network (NN) that guides the selection phase. This procedure incorporates the predictive capabilities of NNs for a more informed tree search process under computational constraints. In our experimental evaluation, we consider metrics that address both safety (through collision rates) and efficacy (through measured speed). Then, we examine: (a) the influence of isotropic state information for vehicles in a lane-free environment, resulting in nudging behaviour--vehicles' policy reacts due to the presence of faster tailing ones, (b) the acceleration of performance for the NN-guided variant of MCTS, and (c) the trade-off between computational resources and solution quality.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09353v1",
        "pdf": "https://arxiv.org/pdf/2601.09353v1"
      },
      "arxiv_id": "2601.09353v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09352v1",
      "title": "Spectral Complex Autoencoder Pruning: A Fidelity-Guided Criterion for Extreme Structured Channel Compression",
      "authors": [
        "Wei Liu",
        "Xing Deng",
        "Haijian Shao",
        "Yingtao Jiang"
      ],
      "abstract": "We propose Spectral Complex Autoencoder Pruning (SCAP), a reconstruction-based criterion that measures functional redundancy at the level of individual output channels. For each convolutional layer, we construct a complex interaction field by pairing the full multi-channel input activation as the real part with a single output-channel activation (spatially aligned and broadcast across input channels) as the imaginary part. We transform this complex field to the frequency domain and train a low-capacity autoencoder to reconstruct normalized spectra. Channels whose spectra are reconstructed with high fidelity are interpreted as lying close to a low-dimensional manifold captured by the autoencoder and are therefore more compressible; conversely, channels with low fidelity are retained as they encode information that cannot be compactly represented by the learned manifold. This yields an importance score (optionally fused with the filter L1 norm) that supports simple threshold-based pruning and produces a structurally consistent pruned network. On VGG16 trained on CIFAR-10, at a fixed threshold of 0.6, we obtain 90.11% FLOP reduction and 96.30% parameter reduction with an absolute Top-1 accuracy drop of 1.67% from a 93.44% baseline after fine-tuning, demonstrating that spectral reconstruction fidelity of complex interaction fields is an effective proxy for channel-level redundancy under aggressive compression.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09352v1",
        "pdf": "https://arxiv.org/pdf/2601.09352v1"
      },
      "arxiv_id": "2601.09352v1",
      "comment": "17 pages, 9 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09351v1",
      "title": "Navigating Ethical AI Challenges in the Industrial Sector: Balancing Innovation and Responsibility",
      "authors": [
        "Ruomu Tan",
        "Martin W Hoffmann"
      ],
      "abstract": "The integration of artificial intelligence (AI) into the industrial sector has not only driven innovation but also expanded the ethical landscape, necessitating a reevaluation of principles governing technology and its applications and awareness in research and development of industrial AI solutions. This chapter explores how AI-empowered industrial innovation inherently intersects with ethics, as advancements in AI introduce new challenges related to transparency, accountability, and fairness. In the chapter, we then examine the ethical aspects of several examples of AI manifestation in industrial use cases and associated factors such as ethical practices in the research and development process and data sharing. With the progress of ethical industrial AI solutions, we emphasize the importance of embedding ethical principles into industrial AI systems and its potential to inspire technological breakthroughs and foster trust among stakeholders. This chapter also offers actionable insights to guide industrial research and development toward a future where AI serves as an enabler for ethical and responsible industrial progress as well as a more inclusive industrial ecosystem.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09351v1",
        "pdf": "https://arxiv.org/pdf/2601.09351v1"
      },
      "arxiv_id": "2601.09351v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09350v1",
      "title": "See More, Store Less: Memory-Efficient Resolution for Video Moment Retrieval",
      "authors": [
        "Mingyu Jeon",
        "Sungjin Han",
        "Jinkwon Hwang",
        "Minchol Kwon",
        "Jonghee Kim",
        "Junyeong Kim"
      ],
      "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have improved image recognition and reasoning, but video-related tasks remain challenging due to memory constraints from dense frame processing. Existing Video Moment Retrieval (VMR) methodologies rely on sparse frame sampling, risking potential information loss, especially in lengthy videos. We propose SMORE (See MORE, store less), a framework that enhances memory efficiency while maintaining high information resolution. SMORE (1) uses query-guided captions to encode semantics aligned with user intent, (2) applies query-aware importance modulation to highlight relevant segments, and (3) adaptively compresses frames to preserve key content while reducing redundancy. This enables efficient video understanding without exceeding memory budgets. Experimental validation reveals that SMORE achieves state-of-the-art performance on QVHighlights, Charades-STA, and ActivityNet-Captions benchmarks.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09350v1",
        "pdf": "https://arxiv.org/pdf/2601.09350v1"
      },
      "arxiv_id": "2601.09350v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09342v1",
      "title": "Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework",
      "authors": [
        "Ewelina Gajewska",
        "Katarzyna Budzynska",
        "Jarosław A Chudziak"
      ],
      "abstract": "This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents representing specific demographic groups. Our approach explicitly integrates socio-cultural context from publicly available knowledge sources, enabling identity-aware moderation that surpasses state-of-the-art prompting methods (zero-shot prompting, few-shot prompting, chain-of-thought prompting) and alternative approaches on a challenging ToxiGen dataset. We enhance the technical rigour of performance evaluation by incorporating balanced accuracy as a central metric of classification fairness that accounts for the trade-off between true positive and true negative rates. We demonstrate that our community-driven consultative framework significantly improves both classification accuracy and fairness across all target groups.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09342v1",
        "pdf": "https://arxiv.org/pdf/2601.09342v1"
      },
      "arxiv_id": "2601.09342v1",
      "comment": "This paper has been accepted for the upcoming 18th International Conference on Agents and Artificial Intelligence (ICAART-2026), Marbella, Spain. The final published version will appear in the official conference proceedings",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09334v1",
      "title": "High-Performance Serverless Computing: A Systematic Literature Review on Serverless for HPC, AI, and Big Data",
      "authors": [
        "Valerio Besozzi",
        "Matteo Della Bartola",
        "Patrizio Dazzi",
        "Marco Danelutto"
      ],
      "abstract": "The widespread deployment of large-scale, compute-intensive applications such as high-performance computing, artificial intelligence, and big data is leading to convergence between cloud and high-performance computing infrastructures. Cloud providers are increasingly integrating high-performance computing capabilities in their infrastructures, such as hardware accelerators and high-speed interconnects, while researchers in the high-performance computing community are starting to explore cloud-native paradigms to improve scalability, elasticity, and resource utilization. In this context, serverless computing emerges as a promising execution model to efficiently handle highly dynamic, parallel, and distributed workloads. This paper presents a comprehensive systematic literature review of 122 research articles published between 2018 and early 2025, exploring the use of the serverless paradigm to develop, deploy, and orchestrate compute-intensive applications across cloud, high-performance computing, and hybrid environments. From these, a taxonomy comprising eight primary research directions and nine targeted use case domains is proposed, alongside an analysis of recent publication trends and collaboration networks among authors, highlighting the growing interest and interconnections within this emerging research field. Overall, this work aims to offer a valuable foundation for both new researchers and experienced practitioners, guiding the development of next-generation serverless solutions for parallel compute-intensive applications.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09334v1",
        "pdf": "https://arxiv.org/pdf/2601.09334v1"
      },
      "arxiv_id": "2601.09334v1",
      "comment": "",
      "journal_ref": "IEEE Access, vol. 13, pp. 195611-195656, 2025",
      "has_code": false
    },
    {
      "id": "2601.09322v1",
      "title": "Beyond the final layer: Attentive multilayer fusion for vision transformers",
      "authors": [
        "Laure Ciernik",
        "Marco Morik",
        "Lukas Thede",
        "Luca Eyring",
        "Shinichi Nakajima",
        "Zeynep Akata",
        "Lukas Muttenthaler"
      ],
      "abstract": "With the rise of large-scale foundation models, efficiently adapting them to downstream tasks remains a central challenge. Linear probing, which freezes the backbone and trains a lightweight head, is computationally efficient but often restricted to last-layer representations. We show that task-relevant information is distributed across the network hierarchy rather than solely encoded in any of the last layers. To leverage this distribution of information, we apply an attentive probing mechanism that dynamically fuses representations from all layers of a Vision Transformer. This mechanism learns to identify the most relevant layers for a target task and combines low-level structural cues with high-level semantic abstractions. Across 20 diverse datasets and multiple pretrained foundation models, our method achieves consistent, substantial gains over standard linear probes. Attention heatmaps further reveal that tasks different from the pre-training domain benefit most from intermediate representations. Overall, our findings underscore the value of intermediate layer information and demonstrate a principled, task aware approach for unlocking their potential in probing-based adaptation.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09322v1",
        "pdf": "https://arxiv.org/pdf/2601.09322v1"
      },
      "arxiv_id": "2601.09322v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09316v1",
      "title": "Frequency Error-Guided Under-sampling Optimization for Multi-Contrast MRI Reconstruction",
      "authors": [
        "Xinming Fang",
        "Chaoyan Huang",
        "Juncheng Li",
        "Jun Wang",
        "Jun Shi",
        "Guixu Zhang"
      ],
      "abstract": "Magnetic resonance imaging (MRI) plays a vital role in clinical diagnostics, yet it remains hindered by long acquisition times and motion artifacts. Multi-contrast MRI reconstruction has emerged as a promising direction by leveraging complementary information from fully-sampled reference scans. However, existing approaches suffer from three major limitations: (1) superficial reference fusion strategies, such as simple concatenation, (2) insufficient utilization of the complementary information provided by the reference contrast, and (3) fixed under-sampling patterns. We propose an efficient and interpretable frequency error-guided reconstruction framework to tackle these issues. We first employ a conditional diffusion model to learn a Frequency Error Prior (FEP), which is then incorporated into a unified framework for jointly optimizing both the under-sampling pattern and the reconstruction network. The proposed reconstruction model employs a model-driven deep unfolding framework that jointly exploits frequency- and image-domain information. In addition, a spatial alignment module and a reference feature decomposition strategy are incorporated to improve reconstruction quality and bridge model-based optimization with data-driven learning for improved physical interpretability. Comprehensive validation across multiple imaging modalities, acceleration rates (4-30x), and sampling schemes demonstrates consistent superiority over state-of-the-art methods in both quantitative metrics and visual quality. All codes are available at https://github.com/fangxinming/JUF-MRI.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09316v1",
        "pdf": "https://arxiv.org/pdf/2601.09316v1"
      },
      "arxiv_id": "2601.09316v1",
      "comment": "44 pages, 12 figures, 7 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09313v1",
      "title": "Understanding or Memorizing? A Case Study of German Definite Articles in Language Models",
      "authors": [
        "Jonathan Drechsel",
        "Erisa Bytyqi",
        "Steffen Herbold"
      ],
      "abstract": "Language models perform well on grammatical agreement, but it is unclear whether this reflects rule-based generalization or memorization. We study this question for German definite singular articles, whose forms depend on gender and case. Using GRADIEND, a gradient-based interpretability method, we learn parameter update directions for gender-case specific article transitions. We find that updates learned for a specific gender-case article transition frequently affect unrelated gender-case settings, with substantial overlap among the most affected neurons across settings. These results argue against a strictly rule-based encoding of German definite articles, indicating that models at least partly rely on memorized associations rather than abstract grammatical rules.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09313v1",
        "pdf": "https://arxiv.org/pdf/2601.09313v1"
      },
      "arxiv_id": "2601.09313v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09306v1",
      "title": "On-Device Large Language Models for Sequential Recommendation",
      "authors": [
        "Xin Xia",
        "Hongzhi Yin",
        "Shane Culpepper"
      ],
      "abstract": "On-device recommendation is critical for a number of real-world applications, especially in scenarios that have agreements on execution latency, user privacy, and robust functionality when internet connectivity is unstable or even impossible. While large language models (LLMs) can now provide exceptional capabilities that model user behavior for sequential recommendation tasks, their substantial memory footprint and computational overhead make the deployment on resource-constrained devices a high risk proposition. In this paper, we propose OD-LLM, the first task-adaptive compression framework explicitly designed to provide efficient and accurate on-device deployment of LLMs for sequential recommendation tasks. OD-LLM uniquely integrates two complementary compression strategies: a low-rank structural compression algorithm which uses Singular Value Decomposition (SVD) to significantly reduce parameter redundancy in the model, and a novel tokenization normalization technique that better complements the low-rank decomposition process being used. Additionally, to minimize any potential performance degradation when using higher compression ratios, a novel progressive alignment algorithm is used to iteratively refine the parameters required layerwise in the target model. Empirical evaluations conducted on sequential recommendation benchmarks show that OD-LLM exhibits no loss in effectiveness when compared to the original recommendation model, when the deployed model size is halved. These promising results demonstrate the efficacy and scalability of OD-LLM, making this novel solution a practical alternative for real-time, on-device solutions wishing to replace expensive, remotely executed LLMs.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09306v1",
        "pdf": "https://arxiv.org/pdf/2601.09306v1"
      },
      "arxiv_id": "2601.09306v1",
      "comment": "WSDM'26",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09304v1",
      "title": "Single-Round Clustered Federated Learning via Data Collaboration Analysis for Non-IID Data",
      "authors": [
        "Sota Sugawara",
        "Yuji Kawamata",
        "Akihiro Toyoda",
        "Tomoru Nakayama",
        "Yukihiko Okada"
      ],
      "abstract": "Federated Learning (FL) enables distributed learning across multiple clients without sharing raw data. When statistical heterogeneity across clients is severe, Clustered Federated Learning (CFL) can improve performance by grouping similar clients and training cluster-wise models. However, most CFL approaches rely on multiple communication rounds for cluster estimation and model updates, which limits their practicality under tight constraints on communication rounds. We propose Data Collaboration-based Clustered Federated Learning (DC-CFL), a single-round framework that completes both client clustering and cluster-wise learning, using only the information shared in DC analysis. DC-CFL quantifies inter-client similarity via total variation distance between label distributions, estimates clusters using hierarchical clustering, and performs cluster-wise learning via DC analysis. Experiments on multiple open datasets under representative non-IID conditions show that DC-CFL achieves accuracy comparable to multi-round baselines while requiring only one communication round. These results indicate that DC-CFL is a practical alternative for collaborative AI model development when multiple communication rounds are impractical.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09304v1",
        "pdf": "https://arxiv.org/pdf/2601.09304v1"
      },
      "arxiv_id": "2601.09304v1",
      "comment": "9 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2601.09298v1",
      "title": "Multi-Modal LLM based Image Captioning in ICT: Bridging the Gap Between General and Industry Domain",
      "authors": [
        "Lianying Chao",
        "Haoran Cai",
        "Xubin Li",
        "Kai Zhang",
        "Sijie Wu",
        "Rui Xu"
      ],
      "abstract": "In the information and communications technology (ICT) industry, training a domain-specific large language model (LLM) or constructing a retrieval-augmented generation system requires a substantial amount of high-value domain knowledge. However, the knowledge is not only hidden in the textual modality but also in the image modality. Traditional methods can parse text from domain documents but dont have image captioning ability. Multi-modal LLM (MLLM) can understand images, but they do not have sufficient domain knowledge. To address the above issues, this paper proposes a multi-stage progressive training strategy to train a Domain-specific Image Captioning Model (DICModel) in ICT, and constructs a standard evaluation system to validate the performance of DICModel. Specifically, this work first synthesizes about 7K image-text pairs by combining the Mermaid tool and LLMs, which are used for the first-stage supervised-fine-tuning (SFT) of DICModel. Then, ICT-domain experts manually annotate about 2K image-text pairs for the second-stage SFT of DICModel. Finally, experts and LLMs jointly synthesize about 1.5K visual question answering data for the instruction-based SFT. Experimental results indicate that our DICModel with only 7B parameters performs better than other state-of-the-art models with 32B parameters. Compared to the SOTA models with 7B and 32B parameters, our DICModel increases the BLEU metric by approximately 56.8% and 20.8%, respectively. On the objective questions constructed by ICT domain experts, our DICModel outperforms Qwen2.5-VL 32B by 1% in terms of accuracy rate. In summary, this work can efficiently and accurately extract the logical text from images, which is expected to promote the development of multimodal models in the ICT domain.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09298v1",
        "pdf": "https://arxiv.org/pdf/2601.09298v1"
      },
      "arxiv_id": "2601.09298v1",
      "comment": "",
      "journal_ref": "2025 CCF BigData",
      "has_code": false
    },
    {
      "id": "2601.09293v1",
      "title": "Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop Scheduling under Uncertainty: Handling Random Arrivals and Machine Failures",
      "authors": [
        "Sofiene Lassoued",
        "Stefan Lier",
        "Andreas Schwung"
      ],
      "abstract": "We present a novel framework for solving Dynamic Job Shop Scheduling Problems under uncertainty, addressing the challenges introduced by stochastic job arrivals and unexpected machine breakdowns. Our approach follows a model-based paradigm, using Coloured Timed Petri Nets to represent the scheduling environment, and Maskable Proximal Policy Optimization to enable dynamic decision-making while restricting the agent to feasible actions at each decision point. To simulate realistic industrial conditions, dynamic job arrivals are modeled using a Gamma distribution, which captures complex temporal patterns such as bursts, clustering, and fluctuating workloads. Machine failures are modeled using a Weibull distribution to represent age-dependent degradation and wear-out dynamics. These stochastic models enable the framework to reflect real-world manufacturing scenarios better. In addition, we study two action-masking strategies: a non-gradient approach that overrides the probabilities of invalid actions, and a gradient-based approach that assigns negative gradients to invalid actions within the policy network. We conduct extensive experiments on dynamic JSSP benchmarks, demonstrating that our method consistently outperforms traditional heuristic and rule-based approaches in terms of makespan minimization. The results highlight the strength of combining interpretable Petri-net-based models with adaptive reinforcement learning policies, yielding a resilient, scalable, and explainable framework for real-time scheduling in dynamic and uncertain manufacturing environments.",
      "published": "2026-01-14",
      "updated": "2026-01-14",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.09293v1",
        "pdf": "https://arxiv.org/pdf/2601.09293v1"
      },
      "arxiv_id": "2601.09293v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    }
  ]
}