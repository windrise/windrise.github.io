{
  "fetched_at": "2025-12-20T00:25:49.708699",
  "total_papers": 100,
  "papers": [
    {
      "id": "2512.16923v1",
      "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
      "authors": [
        "Chun-Wei Tuan Mu",
        "Jia-Bin Huang",
        "Yu-Lun Liu"
      ],
      "abstract": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16923v1",
        "pdf": "https://arxiv.org/pdf/2512.16923v1"
      },
      "arxiv_id": "2512.16923v1",
      "comment": "Project website: https://generative-refocusing.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16924v1",
      "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
      "authors": [
        "Hanlin Wang",
        "Hao Ouyang",
        "Qiuyu Wang",
        "Yue Yu",
        "Yihao Meng",
        "Wen Wang",
        "Ka Leong Cheng",
        "Shuailei Ma",
        "Qingyan Bai",
        "Yixuan Li",
        "Cheng Chen",
        "Yanhong Zeng",
        "Xing Zhu",
        "Yujun Shen",
        "Qifeng Chen"
      ],
      "abstract": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16924v1",
        "pdf": "https://arxiv.org/pdf/2512.16924v1"
      },
      "arxiv_id": "2512.16924v1",
      "comment": "Project page and code: https://worldcanvas.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16922v1",
      "title": "Next-Embedding Prediction Makes Strong Vision Learners",
      "authors": [
        "Sihan Xu",
        "Ziqiao Ma",
        "Wenhao Chai",
        "Xuweiyi Chen",
        "Weiyang Jin",
        "Joyce Chai",
        "Saining Xie",
        "Stella X. Yu"
      ],
      "abstract": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16922v1",
        "pdf": "https://arxiv.org/pdf/2512.16922v1"
      },
      "arxiv_id": "2512.16922v1",
      "comment": "Project Page: https://sihanxu.me/nepa",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16920v1",
      "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
      "authors": [
        "Jinjie Mai",
        "Chaoyang Wang",
        "Guocheng Gordon Qian",
        "Willi Menapace",
        "Sergey Tulyakov",
        "Bernard Ghanem",
        "Peter Wonka",
        "Ashkan Mirzaei"
      ],
      "abstract": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16920v1",
        "pdf": "https://arxiv.org/pdf/2512.16920v1"
      },
      "arxiv_id": "2512.16920v1",
      "comment": "Project page: https://snap-research.github.io/easyv2v/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16919v1",
      "title": "DVGT: Driving Visual Geometry Transformer",
      "authors": [
        "Sicheng Zuo",
        "Zixun Xie",
        "Wenzhao Zheng",
        "Shaoqing Xu",
        "Fang Li",
        "Shengyin Jiang",
        "Long Chen",
        "Zhi-Xin Yang",
        "Jiwen Lu"
      ],
      "abstract": "Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16919v1",
        "pdf": "https://arxiv.org/pdf/2512.16919v1"
      },
      "arxiv_id": "2512.16919v1",
      "comment": "Code is available at https://github.com/wzzheng/DVGT",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16921v1",
      "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
      "authors": [
        "Qihao Liu",
        "Chengzhi Mao",
        "Yaojie Liu",
        "Alan Yuille",
        "Wen-Sheng Chu"
      ],
      "abstract": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16921v1",
        "pdf": "https://arxiv.org/pdf/2512.16921v1"
      },
      "arxiv_id": "2512.16921v1",
      "comment": "project page: https://auditdm.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16918v1",
      "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
      "authors": [
        "Chaoyang Wang",
        "Kaituo Feng",
        "Dongyang Chen",
        "Zhongyu Wang",
        "Zhixun Li",
        "Sicheng Gao",
        "Meng Meng",
        "Xu Zhou",
        "Manyuan Zhang",
        "Yuzhang Shang",
        "Xiangyu Yue"
      ],
      "abstract": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16918v1",
        "pdf": "https://arxiv.org/pdf/2512.16918v1"
      },
      "arxiv_id": "2512.16918v1",
      "comment": "Project page: https://github.com/CYWang735/AdaTooler-V",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16917v1",
      "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
      "authors": [
        "Qihao Liu",
        "Luoxin Ye",
        "Wufei Ma",
        "Yu-Cheng Chou",
        "Alan Yuille"
      ],
      "abstract": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16917v1",
        "pdf": "https://arxiv.org/pdf/2512.16917v1"
      },
      "arxiv_id": "2512.16917v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16915v1",
      "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
      "authors": [
        "Guibao Shen",
        "Yihua Du",
        "Wenhang Ge",
        "Jing He",
        "Chirui Chang",
        "Donghao Zhou",
        "Zhen Yang",
        "Luozhou Wang",
        "Xin Tao",
        "Ying-Cong Chen"
      ],
      "abstract": "The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16915v1",
        "pdf": "https://arxiv.org/pdf/2512.16915v1"
      },
      "arxiv_id": "2512.16915v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16913v1",
      "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
      "authors": [
        "Xin Lin",
        "Meixi Song",
        "Dizhe Zhang",
        "Wenxuan Lu",
        "Haodong Li",
        "Bo Du",
        "Ming-Hsuan Yang",
        "Truong Nguyen",
        "Lu Qi"
      ],
      "abstract": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \\href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\\_website/}",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16913v1",
        "pdf": "https://arxiv.org/pdf/2512.16913v1"
      },
      "arxiv_id": "2512.16913v1",
      "comment": "Project Page: https://insta360-research-team.github.io/DAP_website/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16912v1",
      "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
      "authors": [
        "Peter Chen",
        "Xiaopeng Li",
        "Ziniu Li",
        "Wotao Yin",
        "Xi Chen",
        "Tianyi Lin"
      ],
      "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16912v1",
        "pdf": "https://arxiv.org/pdf/2512.16912v1"
      },
      "arxiv_id": "2512.16912v1",
      "comment": "35 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16911v1",
      "title": "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning",
      "authors": [
        "Andrew Wagenmaker",
        "Perry Dong",
        "Raymond Tsao",
        "Chelsea Finn",
        "Sergey Levine"
      ],
      "abstract": "Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16911v1",
        "pdf": "https://arxiv.org/pdf/2512.16911v1"
      },
      "arxiv_id": "2512.16911v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16910v1",
      "title": "SFTok: Bridging the Performance Gap in Discrete Tokenizers",
      "authors": [
        "Qihang Rao",
        "Borui Zhang",
        "Wenzhao Zheng",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \\textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \\textbf{self-forcing guided visual reconstruction} and \\textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16910v1",
        "pdf": "https://arxiv.org/pdf/2512.16910v1"
      },
      "arxiv_id": "2512.16910v1",
      "comment": "Under review. Code is available at https://github.com/Neur-IO/SFTok",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16909v1",
      "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
      "authors": [
        "Yuanchen Ju",
        "Yongyuan Liang",
        "Yen-Jen Wang",
        "Nandiraju Gireesh",
        "Yuanliang Ju",
        "Seungjae Lee",
        "Qiao Gu",
        "Elvis Hsieh",
        "Furong Huang",
        "Koushil Sreenath"
      ],
      "abstract": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16909v1",
        "pdf": "https://arxiv.org/pdf/2512.16909v1"
      },
      "arxiv_id": "2512.16909v1",
      "comment": "25 pages, 10 figures. Project page:https://hybridrobotics.github.io/MomaGraph/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16908v1",
      "title": "SceneDiff: A Benchmark and Method for Multiview Object Change Detection",
      "authors": [
        "Yuqun Wu",
        "Chih-hao Lin",
        "Henry Che",
        "Aditi Tiwari",
        "Chuhang Zou",
        "Shenlong Wang",
        "Derek Hoiem"
      ],
      "abstract": "We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16908v1",
        "pdf": "https://arxiv.org/pdf/2512.16908v1"
      },
      "arxiv_id": "2512.16908v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16907v1",
      "title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos",
      "authors": [
        "Mingfei Chen",
        "Yifan Wang",
        "Zhengqin Li",
        "Homanga Bharadhwaj",
        "Yujin Chen",
        "Chuan Qin",
        "Ziyi Kou",
        "Yuan Tian",
        "Eric Whitmire",
        "Rajinder Sodhi",
        "Hrvoje Benko",
        "Eli Shlizerman",
        "Yue Liu"
      ],
      "abstract": "Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16907v1",
        "pdf": "https://arxiv.org/pdf/2512.16907v1"
      },
      "arxiv_id": "2512.16907v1",
      "comment": "Project website: https://egoman-project.github.io",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16906v1",
      "title": "VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization",
      "authors": [
        "Xiaoyan Cong",
        "Haotian Yang",
        "Angtian Wang",
        "Yizhi Wang",
        "Yiding Yang",
        "Canyu Zhang",
        "Chongyang Ma"
      ],
      "abstract": "Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16906v1",
        "pdf": "https://arxiv.org/pdf/2512.16906v1"
      },
      "arxiv_id": "2512.16906v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16905v1",
      "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
      "authors": [
        "Kaixin Ding",
        "Yang Zhou",
        "Xi Chen",
        "Miao Yang",
        "Jiarong Ou",
        "Rui Chen",
        "Xin Tao",
        "Hengshuang Zhao"
      ],
      "abstract": "Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16905v1",
        "pdf": "https://arxiv.org/pdf/2512.16905v1"
      },
      "arxiv_id": "2512.16905v1",
      "comment": "project page: https://kxding.github.io/project/Alchemist/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16902v1",
      "title": "In-Context Algebra",
      "authors": [
        "Eric Todd",
        "Jannik Brinkmann",
        "Rohit Gandikota",
        "David Bau"
      ],
      "abstract": "We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16902v1",
        "pdf": "https://arxiv.org/pdf/2512.16902v1"
      },
      "arxiv_id": "2512.16902v1",
      "comment": "28 pages, 18 figures. Code and data at https://algebra.baulab.info",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16901v1",
      "title": "Impacts of Racial Bias in Historical Training Data for News AI",
      "authors": [
        "Rahul Bhargava",
        "Malene Hornstrup Jespersen",
        "Emily Boardman Ndulue",
        "Vivica Dsouza"
      ],
      "abstract": "AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning \"blacks\" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the \"blacks\" label operates partially as a general \"racism detector\" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16901v1",
        "pdf": "https://arxiv.org/pdf/2512.16901v1"
      },
      "arxiv_id": "2512.16901v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16900v1",
      "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
      "authors": [
        "Shuyuan Tu",
        "Yueming Pan",
        "Yinming Huang",
        "Xintong Han",
        "Zhen Xing",
        "Qi Dai",
        "Kai Qiu",
        "Chong Luo",
        "Zuxuan Wu"
      ],
      "abstract": "Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16900v1",
        "pdf": "https://arxiv.org/pdf/2512.16900v1"
      },
      "arxiv_id": "2512.16900v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16899v1",
      "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
      "authors": [
        "Yushi Hu",
        "Reyhane Askari-Hemmat",
        "Melissa Hall",
        "Emily Dinan",
        "Luke Zettlemoyer",
        "Marjan Ghazvininejad"
      ],
      "abstract": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16899v1",
        "pdf": "https://arxiv.org/pdf/2512.16899v1"
      },
      "arxiv_id": "2512.16899v1",
      "comment": "Code and data available at https://github.com/facebookresearch/MMRB2",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16896v1",
      "title": "Sceniris: A Fast Procedural Scene Generation Framework",
      "authors": [
        "Jinghuan Shang",
        "Harsh Patel",
        "Ran Gong",
        "Karl Schmeckpeper"
      ],
      "abstract": "Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16896v1",
        "pdf": "https://arxiv.org/pdf/2512.16896v1"
      },
      "arxiv_id": "2512.16896v1",
      "comment": "Code is available at https://github.com/rai-inst/sceniris",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16893v1",
      "title": "Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation",
      "authors": [
        "Kaiwen Jiang",
        "Xueting Li",
        "Seonwook Park",
        "Ravi Ramamoorthi",
        "Shalini De Mello",
        "Koki Nagano"
      ],
      "abstract": "Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16893v1",
        "pdf": "https://arxiv.org/pdf/2512.16893v1"
      },
      "arxiv_id": "2512.16893v1",
      "comment": "Project website is https://research.nvidia.com/labs/amri/projects/instant4d",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16891v1",
      "title": "LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation",
      "authors": [
        "Haichao Zhang",
        "Yao Lu",
        "Lichen Wang",
        "Yunzhe Li",
        "Daiwei Chen",
        "Yunpeng Xu",
        "Yun Fu"
      ],
      "abstract": "Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16891v1",
        "pdf": "https://arxiv.org/pdf/2512.16891v1"
      },
      "arxiv_id": "2512.16891v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16885v1",
      "title": "M-PhyGs: Multi-Material Object Dynamics from Video",
      "authors": [
        "Norika Wada",
        "Kohei Yamashita",
        "Ryo Kawahara",
        "Ko Nishino"
      ],
      "abstract": "Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16885v1",
        "pdf": "https://arxiv.org/pdf/2512.16885v1"
      },
      "arxiv_id": "2512.16885v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16882v1",
      "title": "Cartesian-nj: Extending e3nn to Irreducible Cartesian Tensor Product and Contracion",
      "authors": [
        "Zemin Xu",
        "Chenyu Wu",
        "Wenbo Xie",
        "Daiqian Xie",
        "P. Hu"
      ],
      "abstract": "Equivariant atomistic machine learning models have brought substantial gains in both extrapolation capability and predictive accuracy. Depending on the basis of the space, two distinct types of irreducible representations are utilized. From architectures built upon spherical tensors (STs) to more recent formulations employing irreducible Cartesian tensors (ICTs), STs have remained dominant owing to their compactness, elegance, and theoretical completeness. Nevertheless, questions have persisted regarding whether ST constructions are the only viable design principle, motivating continued development of Cartesian networks. In this work, we introduce the Cartesian-3j and Cartesian-nj symbol, which serve as direct analogues of the Wigner-3j and Wigner-nj symbol defined for tensor coupling. These coefficients enable the combination of any two ICTs into a new ICT. Building on this foundation, we extend e3nn to support irreducible Cartesian tensor product, and we release the resulting Python package as cartnn. Within this framework, we implement Cartesian counterparts of MACE, NequIP, and Allegro, allowing the first systematic comparison of Cartesian and spherical models to assess whether Cartesian formulations may offer advantages under specific conditions. Using TACE as a representative example, we further examine whether architectures constructed from irreducible Cartesian tensor product and contraction(ICTP and ICTC) are conceptually well-founded in Cartesian space and whether opportunities remain for improving their design.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "physics.chem-ph",
        "cond-mat.mtrl-sci",
        "cs.LG"
      ],
      "primary_category": "physics.chem-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16882v1",
        "pdf": "https://arxiv.org/pdf/2512.16882v1"
      },
      "arxiv_id": "2512.16882v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16881v1",
      "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies",
      "authors": [
        "Arhan Jain",
        "Mingtong Zhang",
        "Kanav Arora",
        "William Chen",
        "Marcel Torne",
        "Muhammad Zubair Irshad",
        "Sergey Zakharov",
        "Yue Wang",
        "Sergey Levine",
        "Chelsea Finn",
        "Wei-Chiu Ma",
        "Dhruv Shah",
        "Abhishek Gupta",
        "Karl Pertsch"
      ],
      "abstract": "A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16881v1",
        "pdf": "https://arxiv.org/pdf/2512.16881v1"
      },
      "arxiv_id": "2512.16881v1",
      "comment": "Website: https://polaris-evals.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16880v1",
      "title": "Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation",
      "authors": [
        "Valay Bundele",
        "Mehran Hosseinzadeh",
        "Hendrik P. A. Lensch"
      ],
      "abstract": "Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16880v1",
        "pdf": "https://arxiv.org/pdf/2512.16880v1"
      },
      "arxiv_id": "2512.16880v1",
      "comment": "Under Review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16876v1",
      "title": "Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies",
      "authors": [
        "Astrid Brull",
        "Sara Aguti",
        "Vronique Bolduc",
        "Ying Hu",
        "Daniel M. Jimenez-Gutierrez",
        "Enrique Zuazua",
        "Joaquin Del-Rio",
        "Oleksii Sliusarenko",
        "Haiyan Zhou",
        "Francesco Muntoni",
        "Carsten G. Bnnemann",
        "Xabi Uribe-Etxebarria"
      ],
      "abstract": "The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16876v1",
        "pdf": "https://arxiv.org/pdf/2512.16876v1"
      },
      "arxiv_id": "2512.16876v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16875v1",
      "title": "Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery",
      "authors": [
        "Chao Gao",
        "Liren Shan",
        "Vaidehi Srinivas",
        "Aravindan Vijayaraghavan"
      ],
      "abstract": "We study the problem of finding confidence ellipsoids for an arbitrary distribution in high dimensions. Given samples from a distribution $D$ and a confidence parameter $$, the goal is to find the smallest volume ellipsoid $E$ which has probability mass $\\Pr_{D}[E] \\ge 1-$. Ellipsoids are a highly expressive class of confidence sets as they can capture correlations in the distribution, and can approximate any convex set. This problem has been studied in many different communities. In statistics, this is the classic minimum volume estimator introduced by Rousseeuw as a robust non-parametric estimator of location and scatter. However in high dimensions, it becomes NP-hard to obtain any non-trivial approximation factor in volume when the condition number $$ of the ellipsoid (ratio of the largest to the smallest axis length) goes to $\\infty$. This motivates the focus of our paper: can we efficiently find confidence ellipsoids with volume approximation guarantees when compared to ellipsoids of bounded condition number $$?\n  Our main result is a polynomial time algorithm that finds an ellipsoid $E$ whose volume is within a $O(^{d})$ multiplicative factor of the volume of best $$-conditioned ellipsoid while covering at least $1-O(/)$ probability mass for any $< $. We complement this with a computational hardness result that shows that such a dependence seems necessary up to constants in the exponent. The algorithm and analysis uses the rich primal-dual structure of the minimum volume enclosing ellipsoid and the geometric Brascamp-Lieb inequality. As a consequence, we obtain the first polynomial time algorithm with approximation guarantees on worst-case instances of the robust subspace recovery problem.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.DS",
        "cs.LG",
        "math.ST",
        "stat.ML"
      ],
      "primary_category": "cs.DS",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16875v1",
        "pdf": "https://arxiv.org/pdf/2512.16875v1"
      },
      "arxiv_id": "2512.16875v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16874v1",
      "title": "Pixel Seal: Adversarial-only training for invisible image and video watermarking",
      "authors": [
        "Tom Souek",
        "Pierre Fernandez",
        "Hady Elsahar",
        "Sylvestre-Alvise Rebuffi",
        "Valeriu Lacatusu",
        "Tuan Tran",
        "Tom Sander",
        "Alexandre Mourachko"
      ],
      "abstract": "Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16874v1",
        "pdf": "https://arxiv.org/pdf/2512.16874v1"
      },
      "arxiv_id": "2512.16874v1",
      "comment": "Code and model available at https://github.com/facebookresearch/videoseal",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16873v1",
      "title": "The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI",
      "authors": [
        "Otman A. Basir"
      ],
      "abstract": "Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16873v1",
        "pdf": "https://arxiv.org/pdf/2512.16873v1"
      },
      "arxiv_id": "2512.16873v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16872v1",
      "title": "On the Universal Representation Property of Spiking Neural Networks",
      "authors": [
        "Shayan Hundrieser",
        "Philipp Tuchel",
        "Insung Kong",
        "Johannes Schmidt-Hieber"
      ],
      "abstract": "Inspired by biology, spiking neural networks (SNNs) process information via discrete spikes over time, offering an energy-efficient alternative to the classical computing paradigm and classical artificial neural networks (ANNs). In this work, we analyze the representational power of SNNs by viewing them as sequence-to-sequence processors of spikes, i.e., systems that transform a stream of input spikes into a stream of output spikes. We establish the universal representation property for a natural class of spike train functions. Our results are fully quantitative, constructive, and near-optimal in the number of required weights and neurons. The analysis reveals that SNNs are particularly well-suited to represent functions with few inputs, low temporal complexity, or compositions of such functions. The latter is of particular interest, as it indicates that deep SNNs can efficiently capture composite functions via a modular design. As an application of our results, we discuss spike train classification. Overall, these results contribute to a rigorous foundation for understanding the capabilities and limitations of spike-based neuromorphic systems.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.NE",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.NE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16872v1",
        "pdf": "https://arxiv.org/pdf/2512.16872v1"
      },
      "arxiv_id": "2512.16872v1",
      "comment": "54 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16871v1",
      "title": "Sequencing to Mitigate Catastrophic Forgetting in Continual Learning",
      "authors": [
        "Hesham G. Moussa",
        "Aroosa Hameed",
        "Arashmid Akhavain"
      ],
      "abstract": "To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16871v1",
        "pdf": "https://arxiv.org/pdf/2512.16871v1"
      },
      "arxiv_id": "2512.16871v1",
      "comment": "The Manuscript is submitted for review under IEEE Transactions on Artificial intelligence",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16866v1",
      "title": "Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models",
      "authors": [
        "Jiabin Xue"
      ],
      "abstract": "Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: \"How to determine labels for truly future, unseen data points\". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16866v1",
        "pdf": "https://arxiv.org/pdf/2512.16866v1"
      },
      "arxiv_id": "2512.16866v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16864v1",
      "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
      "authors": [
        "Tianyuan Qu",
        "Lei Ke",
        "Xiaohang Zhan",
        "Longxiang Tang",
        "Yuqi Liu",
        "Bohao Peng",
        "Bei Yu",
        "Dong Yu",
        "Jiaya Jia"
      ],
      "abstract": "Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16864v1",
        "pdf": "https://arxiv.org/pdf/2512.16864v1"
      },
      "arxiv_id": "2512.16864v1",
      "comment": "Precise region control and planning for instruction-based image editing. Our project page: https://replan-iv-edit.github.io",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16861v1",
      "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning",
      "authors": [
        "Zihan Zhou",
        "Animesh Garg",
        "Ajay Mandlekar",
        "Caelan Garrett"
      ],
      "abstract": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16861v1",
        "pdf": "https://arxiv.org/pdf/2512.16861v1"
      },
      "arxiv_id": "2512.16861v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16856v1",
      "title": "Distributional AGI Safety",
      "authors": [
        "Nenad Tomaev",
        "Matija Franklin",
        "Julian Jacobs",
        "Sbastien Krier",
        "Simon Osindero"
      ],
      "abstract": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16856v1",
        "pdf": "https://arxiv.org/pdf/2512.16856v1"
      },
      "arxiv_id": "2512.16856v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16855v1",
      "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge",
      "authors": [
        "Khurram Khalil",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16855v1",
        "pdf": "https://arxiv.org/pdf/2512.16855v1"
      },
      "arxiv_id": "2512.16855v1",
      "comment": "Published in the IEEE ICCAD 2025 conference",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16853v1",
      "title": "GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation",
      "authors": [
        "Amita Kamath",
        "Kai-Wei Chang",
        "Ranjay Krishna",
        "Luke Zettlemoyer",
        "Yushi Hu",
        "Marjan Ghazvininejad"
      ],
      "abstract": "Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16853v1",
        "pdf": "https://arxiv.org/pdf/2512.16853v1"
      },
      "arxiv_id": "2512.16853v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16851v1",
      "title": "PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy",
      "authors": [
        "Ripan Kumar Kundu",
        "Istiak Ahmed",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16851v1",
        "pdf": "https://arxiv.org/pdf/2512.16851v1"
      },
      "arxiv_id": "2512.16851v1",
      "comment": "Published in the IEEE ISMAR 2025 conference",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16848v1",
      "title": "Meta-RL Induces Exploration in Language Agents",
      "authors": [
        "Yulun Jiang",
        "Liangze Jiang",
        "Damien Teney",
        "Michael Moor",
        "Maria Brbic"
      ],
      "abstract": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16848v1",
        "pdf": "https://arxiv.org/pdf/2512.16848v1"
      },
      "arxiv_id": "2512.16848v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16843v1",
      "title": "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference",
      "authors": [
        "Harsh Vardhan Bansal"
      ],
      "abstract": "Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16843v1",
        "pdf": "https://arxiv.org/pdf/2512.16843v1"
      },
      "arxiv_id": "2512.16843v1",
      "comment": "Accepted and presented at 13th IEEE International Conference on Intelligent Systems and Embedded Design (ISED-2025)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16842v1",
      "title": "OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction",
      "authors": [
        "Yuxin Ray Song",
        "Jinzhou Li",
        "Rao Fu",
        "Devin Murphy",
        "Kaichen Zhou",
        "Rishi Shiv",
        "Yaqi Li",
        "Haoyu Xiong",
        "Crystal Elaine Owens",
        "Yilun Du",
        "Yiyue Luo",
        "Xianyi Cheng",
        "Antonio Torralba",
        "Wojciech Matusik",
        "Paul Pu Liang"
      ],
      "abstract": "The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16842v1",
        "pdf": "https://arxiv.org/pdf/2512.16842v1"
      },
      "arxiv_id": "2512.16842v1",
      "comment": "https://opentouch-tactile.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16841v1",
      "title": "Radiology Report Generation with Layer-Wise Anatomical Attention",
      "authors": [
        "Emmanuel D. Muiz-De-Len",
        "Jorge A. Rosales-de-Golferichs",
        "Ana S. Muoz-Rodrguez",
        "Alejandro I. Trejo-Castro",
        "Eduardo de Avila-Armenta",
        "Antonio Martnez-Torteya"
      ],
      "abstract": "Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16841v1",
        "pdf": "https://arxiv.org/pdf/2512.16841v1"
      },
      "arxiv_id": "2512.16841v1",
      "comment": "11 pages, 6 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16826v1",
      "title": "Next-Generation License Plate Detection and Recognition System using YOLOv8",
      "authors": [
        "Arslan Amin",
        "Rafia Mumtaz",
        "Muhammad Jawad Bashir",
        "Syed Mohammad Hassan Zaidi"
      ],
      "abstract": "In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16826v1",
        "pdf": "https://arxiv.org/pdf/2512.16826v1"
      },
      "arxiv_id": "2512.16826v1",
      "comment": "6 pages, 5 figures. Accepted and published in the 2023 IEEE 20th International Conference on Smart Communities: Improving Quality of Life using AI, Robotics and IoT (HONET)",
      "journal_ref": "2023 IEEE 20th International Conference on Smart Communities: Improving Quality of Life using AI, Robotics and IoT (HONET)",
      "has_code": false
    },
    {
      "id": "2512.16824v1",
      "title": "Tiny Recursive Control: Iterative Reasoning for Efficient Optimal Control",
      "authors": [
        "Amit Jain",
        "Richard Linares"
      ],
      "abstract": "Neural network controllers increasingly demand millions of parameters, and language model approaches push into the billions. For embedded aerospace systems with strict power and latency constraints, this scaling is prohibitive. We present Tiny Recursive Control (TRC), a neural architecture based on a counterintuitive principle: capacity can emerge from iteration depth rather than parameter count. TRC applies compact networks (approximately 1.5M parameters) repeatedly through a two-level hierarchical latent structure, refining control sequences by simulating trajectories and correcting based on tracking error. Because the same weights process every refinement step, adding iterations increases computation without increasing memory. We evaluate TRC on nonlinear control problems including oscillator stabilization and powered descent with fuel constraints. Across these domains, TRC achieves near-optimal control costs while requiring only millisecond-scale inference on GPU and under 10~MB memory, two orders of magnitude smaller than language model baselines. These results demonstrate that recursive reasoning, previously confined to discrete tasks, transfers effectively to continuous control synthesis.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "math.DS"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16824v1",
        "pdf": "https://arxiv.org/pdf/2512.16824v1"
      },
      "arxiv_id": "2512.16824v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16822v1",
      "title": "MEPIC: Memory Efficient Position Independent Caching for LLM Serving",
      "authors": [
        "Qian Wang",
        "Zahra Yousefijamarani",
        "Morgan Lindsay Heisler",
        "Rongzhi Gu",
        "Bai Xiaolong",
        "Shan Yizhou",
        "Wei Zhang",
        "Wang Lan",
        "Ying Xiong",
        "Yong Zhang",
        "Zhenan Fan"
      ],
      "abstract": "Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.\n  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16822v1",
        "pdf": "https://arxiv.org/pdf/2512.16822v1"
      },
      "arxiv_id": "2512.16822v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16818v1",
      "title": "DenseBEV: Transforming BEV Grid Cells into 3D Objects",
      "authors": [
        "Marius Dhling",
        "Sebastian Krebs",
        "J. Marius Zllner"
      ],
      "abstract": "In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16818v1",
        "pdf": "https://arxiv.org/pdf/2512.16818v1"
      },
      "arxiv_id": "2512.16818v1",
      "comment": "15 pages, 8 figures, accepted by WACV 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16814v1",
      "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs",
      "authors": [
        "William English",
        "Dominic Simon",
        "Sumit Kumar Jha",
        "Rickard Ewetz"
      ],
      "abstract": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16814v1",
        "pdf": "https://arxiv.org/pdf/2512.16814v1"
      },
      "arxiv_id": "2512.16814v1",
      "comment": "",
      "journal_ref": "Proceedings of the 42nd International Conference on Machine Learning, PMLR 267:15370-15383, 2025",
      "has_code": false
    },
    {
      "id": "2512.16813v1",
      "title": "Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning",
      "authors": [
        "Bahman Abolhassani",
        "Tugba Erpek",
        "Kemal Davaslioglu",
        "Yalin E. Sagduyu",
        "Sastry Kompella"
      ],
      "abstract": "Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.DC",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.NI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16813v1",
        "pdf": "https://arxiv.org/pdf/2512.16813v1"
      },
      "arxiv_id": "2512.16813v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16811v1",
      "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation",
      "authors": [
        "Jingjing Qian",
        "Boyao Han",
        "Chen Shi",
        "Lei Xiao",
        "Long Yang",
        "Shaoshuai Shi",
        "Li Jiang"
      ],
      "abstract": "Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16811v1",
        "pdf": "https://arxiv.org/pdf/2512.16811v1"
      },
      "arxiv_id": "2512.16811v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16795v1",
      "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs",
      "authors": [
        "Shubham Mishra",
        "Samyek Jain",
        "Gorang Mehrishi",
        "Shiv Tiwari",
        "Harsh Sharma",
        "Pratik Narang",
        "Dhruv Kumar"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16795v1",
        "pdf": "https://arxiv.org/pdf/2512.16795v1"
      },
      "arxiv_id": "2512.16795v1",
      "comment": "Under Review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16792v1",
      "title": "Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint",
      "authors": [
        "Endar Suprih Wihidayat",
        "Sieteng Soh",
        "Kwan-Wu Chin",
        "Duc-son Pham"
      ],
      "abstract": "In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16792v1",
        "pdf": "https://arxiv.org/pdf/2512.16792v1"
      },
      "arxiv_id": "2512.16792v1",
      "comment": "17 pages, 9 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16791v1",
      "title": "KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals",
      "authors": [
        "Shuting Zhao",
        "Zeyu Xiao",
        "Xinrong Chen"
      ],
      "abstract": "Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16791v1",
        "pdf": "https://arxiv.org/pdf/2512.16791v1"
      },
      "arxiv_id": "2512.16791v1",
      "comment": "Accepted by AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16786v1",
      "title": "Few-Shot Specific Emitter Identification via Integrated Complex Variational Mode Decomposition and Spatial Attention Transfer",
      "authors": [
        "Chenyu Zhu",
        "Zeyang Li",
        "Ziyi Xie",
        "Jie Zhang"
      ],
      "abstract": "Specific emitter identification (SEI) utilizes passive hardware characteristics to authenticate transmitters, providing a robust physical-layer security solution. However, most deep-learning-based methods rely on extensive data or require prior information, which poses challenges in real-world scenarios with limited labeled data. We propose an integrated complex variational mode decomposition algorithm that decomposes and reconstructs complex-valued signals to approximate the original transmitted signals, thereby enabling more accurate feature extraction. We further utilize a temporal convolutional network to effectively model the sequential signal characteristics, and introduce a spatial attention mechanism to adaptively weight informative signal segments, significantly enhancing identification performance. Additionally, the branch network allows leveraging pre-trained weights from other data while reducing the need for auxiliary datasets. Ablation experiments on the simulated data demonstrate the effectiveness of each component of the model. An accuracy comparison on a public dataset reveals that our method achieves 96% accuracy using only 10 symbols without requiring any prior knowledge.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16786v1",
        "pdf": "https://arxiv.org/pdf/2512.16786v1"
      },
      "arxiv_id": "2512.16786v1",
      "comment": "14 pages, 12 Figures, 5 Table",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16784v1",
      "title": "R3ST: A Synthetic 3D Dataset With Realistic Trajectories",
      "authors": [
        "Simone Teglia",
        "Claudia Melis Tonti",
        "Francesco Pro",
        "Leonardo Russo",
        "Andrea Alfarano",
        "Leonardo Pentassuglia",
        "Irene Amerini"
      ],
      "abstract": "Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16784v1",
        "pdf": "https://arxiv.org/pdf/2512.16784v1"
      },
      "arxiv_id": "2512.16784v1",
      "comment": "",
      "journal_ref": "Computer Analysis of Images and Patterns. CAIP 2025",
      "has_code": false
    },
    {
      "id": "2512.16780v1",
      "title": "Towards Mass Spectrum Analysis with ASP",
      "authors": [
        "Nils Kchenmeister",
        "Alex Ivliev",
        "Markus Krtzsch"
      ],
      "abstract": "We present a new use of Answer Set Programming (ASP) to discover the molecular structure of chemical samples based on the relative abundance of elements and structural fragments, as measured in mass spectrometry. To constrain the exponential search space for this combinatorial problem, we develop canonical representations of molecular structures and an ASP implementation that uses these definitions. We evaluate the correctness of our implementation over a large set of known molecular structures, and we compare its quality and performance to other ASP symmetry-breaking methods and to a commercial tool from analytical chemistry. Under consideration in Theory and Practice of Logic Programming (TPLP).",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16780v1",
        "pdf": "https://arxiv.org/pdf/2512.16780v1"
      },
      "arxiv_id": "2512.16780v1",
      "comment": "22 pages, 11 figures. Extended version of a paper accepted at 17th International Conference on Logic Programming and Non-monotonic Reasoning (LPNMR 2024). Under consideration in Theory and Practice of Logic Programming (TPLP)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16778v1",
      "title": "Non-Linear Strong Data-Processing for Quantum Hockey-Stick Divergences",
      "authors": [
        "Theshani Nuradha",
        "Ian George",
        "Christoph Hirche"
      ],
      "abstract": "Data-processing is a desired property of classical and quantum divergences and information measures. In information theory, the contraction coefficient measures how much the distinguishability of quantum states decreases when they are transmitted through a quantum channel, establishing linear strong data-processing inequalities (SDPI). However, these linear SDPI are not always tight and can be improved in most of the cases. In this work, we establish non-linear SDPI for quantum hockey-stick divergence for noisy channels that satisfy a certain noise criterion. We also note that our results improve upon existing linear SDPI for quantum hockey-stick divergences and also non-linear SDPI for classical hockey-stick divergence. We define $F_$ curves generalizing Dobrushin curves for the quantum setting while characterizing SDPI for the sequential composition of heterogeneous channels. In addition, we derive reverse-Pinsker type inequalities for $f$-divergences with additional constraints on hockey-stick divergences. We show that these non-linear SDPI can establish tighter finite mixing times that cannot be achieved through linear SDPI. Furthermore, we find applications of these in establishing stronger privacy guarantees for the composition of sequential private quantum channels when privacy is quantified by quantum local differential privacy.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "quant-ph",
        "cs.CR",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16778v1",
        "pdf": "https://arxiv.org/pdf/2512.16778v1"
      },
      "arxiv_id": "2512.16778v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16776v1",
      "title": "Kling-Omni Technical Report",
      "authors": [
        "Kling Team",
        "Jialu Chen",
        "Yuanzheng Ci",
        "Xiangyu Du",
        "Zipeng Feng",
        "Kun Gai",
        "Sainan Guo",
        "Feng Han",
        "Jingbin He",
        "Kang He",
        "Xiao Hu",
        "Xiaohua Hu",
        "Boyuan Jiang",
        "Fangyuan Kong",
        "Hang Li",
        "Jie Li",
        "Qingyu Li",
        "Shen Li",
        "Xiaohan Li",
        "Yan Li",
        "Jiajun Liang",
        "Borui Liao",
        "Yiqiao Liao",
        "Weihong Lin",
        "Quande Liu",
        "Xiaokun Liu",
        "Yilun Liu",
        "Yuliang Liu",
        "Shun Lu",
        "Hangyu Mao",
        "Yunyao Mao",
        "Haodong Ouyang",
        "Wenyu Qin",
        "Wanqi Shi",
        "Xiaoyu Shi",
        "Lianghao Su",
        "Haozhi Sun",
        "Peiqin Sun",
        "Pengfei Wan",
        "Chao Wang",
        "Chenyu Wang",
        "Meng Wang",
        "Qiulin Wang",
        "Runqi Wang",
        "Xintao Wang",
        "Xuebo Wang",
        "Zekun Wang",
        "Min Wei",
        "Tiancheng Wen",
        "Guohao Wu",
        "Xiaoshi Wu",
        "Zhenhua Wu",
        "Da Xie",
        "Yingtong Xiong",
        "Yulong Xu",
        "Sile Yang",
        "Zikang Yang",
        "Weicai Ye",
        "Ziyang Yuan",
        "Shenglong Zhang",
        "Shuaiyu Zhang",
        "Yuanxing Zhang",
        "Yufan Zhang",
        "Wenzheng Zhao",
        "Ruiliang Zhou",
        "Yan Zhou",
        "Guosheng Zhu",
        "Yongjie Zhu"
      ],
      "abstract": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16776v1",
        "pdf": "https://arxiv.org/pdf/2512.16776v1"
      },
      "arxiv_id": "2512.16776v1",
      "comment": "Kling-Omni Technical Report",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16771v1",
      "title": "FlowDet: Unifying Object Detection and Generative Transport Flows",
      "authors": [
        "Enis Baty",
        "C. P. Bridges",
        "Simon Hadfield"
      ],
      "abstract": "We present FlowDet, the first formulation of object detection using modern Conditional Flow Matching techniques. This work follows from DiffusionDet, which originally framed detection as a generative denoising problem in the bounding box space via diffusion. We revisit and generalise this formulation to a broader class of generative transport problems, while maintaining the ability to vary the number of boxes and inference steps without re-training. In contrast to the curved stochastic transport paths induced by diffusion, FlowDet learns simpler and straighter paths resulting in faster scaling of detection performance as the number of inference steps grows. We find that this reformulation enables us to outperform diffusion based detection systems (as well as non-generative baselines) across a wide range of experiments, including various precision/recall operating points using multiple feature backbones and datasets. In particular, when evaluating under recall-constrained settings, we can highlight the effects of the generative transport without over-compensating with large numbers of proposals. This provides gains of up to +3.6% AP and +4.2% AP$_{rare}$ over DiffusionDet on the COCO and LVIS datasets, respectively.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16771v1",
        "pdf": "https://arxiv.org/pdf/2512.16771v1"
      },
      "arxiv_id": "2512.16771v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16770v1",
      "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation",
      "authors": [
        "William English",
        "Chase Walker",
        "Dominic Simon",
        "Rickard Ewetz"
      ],
      "abstract": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically -- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16770v1",
        "pdf": "https://arxiv.org/pdf/2512.16770v1"
      },
      "arxiv_id": "2512.16770v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16768v1",
      "title": "On The Hidden Biases of Flow Matching Samplers",
      "authors": [
        "Soon Hoe Lim"
      ],
      "abstract": "We study the implicit bias of flow matching (FM) samplers via the lens of empirical flow matching. Although population FM may produce gradient-field velocities resembling optimal transport (OT), we show that the empirical FM minimizer is almost never a gradient field, even when each conditional flow is. Consequently, empirical FM is intrinsically energetically suboptimal. In view of this, we analyze the kinetic energy of generated samples. With Gaussian sources, both instantaneous and integrated kinetic energies exhibit exponential concentration, while heavy-tailed sources lead to polynomial tails. These behaviors are governed primarily by the choice of source distribution rather than the data. Overall, these notes provide a concise mathematical account of the structural and energetic biases arising in empirical FM.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.PR"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16768v1",
        "pdf": "https://arxiv.org/pdf/2512.16768v1"
      },
      "arxiv_id": "2512.16768v1",
      "comment": "20 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16767v1",
      "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
      "authors": [
        "Zhiyang Guo",
        "Ori Zhang",
        "Jax Xiang",
        "Alan Zhao",
        "Wengang Zhou",
        "Houqiang Li"
      ],
      "abstract": "Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16767v1",
        "pdf": "https://arxiv.org/pdf/2512.16767v1"
      },
      "arxiv_id": "2512.16767v1",
      "comment": "Project page: https://jasongzy.github.io/Make-It-Poseable/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16763v1",
      "title": "Pattern recognition in complex systems via vector-field representations of spatio-temporal data",
      "authors": [
        "Ingrid Amaranta Membrillo Solis",
        "Maria van Rossem",
        "Tristan Madeleine",
        "Tetiana Orlova",
        "Nina Podoliak",
        "Giampaolo D'Alessandro",
        "Jacek Brodzki",
        "Malgosia Kaczmarek"
      ],
      "abstract": "A complex system comprises multiple interacting entities whose interdependencies form a unified whole, exhibiting emergent behaviours not present in individual components. Examples include the human brain, living cells, soft matter, Earth's climate, ecosystems, and the economy. These systems exhibit high-dimensional, non-linear dynamics, making their modelling, classification, and prediction particularly challenging. Advances in information technology have enabled data-driven approaches to studying such systems. However, the sheer volume and complexity of spatio-temporal data often hinder traditional methods like dimensionality reduction, phase-space reconstruction, and attractor characterisation. This paper introduces a geometric framework for analysing spatio-temporal data from complex systems, grounded in the theory of vector fields over discrete measure spaces. We propose a two-parameter family of metrics suitable for data analysis and machine learning applications. The framework supports time-dependent images, image gradients, and real- or vector-valued functions defined on graphs and simplicial complexes. We validate our approach using data from numerical simulations of biological and physical systems on flat and curved domains. Our results show that the proposed metrics, combined with multidimensional scaling, effectively address key analytical challenges. They enable dimensionality reduction, mode decomposition, phase-space reconstruction, and attractor characterisation. Our findings offer a robust pathway for understanding complex dynamical systems, especially in contexts where traditional modelling is impractical but abundant experimental data are available.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "cond-mat.soft",
        "nlin.CD",
        "nlin.PS"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16763v1",
        "pdf": "https://arxiv.org/pdf/2512.16763v1"
      },
      "arxiv_id": "2512.16763v1",
      "comment": "24 pages, 10 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16762v1",
      "title": "NRGPT: An Energy-based Alternative for GPT",
      "authors": [
        "Nima Dehmamy",
        "Benjamin Hoover",
        "Bishwajit Saha",
        "Leo Kozachkov",
        "Jean-Jacques Slotine",
        "Dmitry Krotov"
      ],
      "abstract": "Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16762v1",
        "pdf": "https://arxiv.org/pdf/2512.16762v1"
      },
      "arxiv_id": "2512.16762v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16755v1",
      "title": "CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?",
      "authors": [
        "Siqi Wang",
        "Chao Liang",
        "Yunfan Gao",
        "Erxin Yu",
        "Sen Li",
        "Yushi Li",
        "Jing Li",
        "Haofen Wang"
      ],
      "abstract": "Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., \"I am thirsty\") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling \"last-mile\" navigation challenges.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16755v1",
        "pdf": "https://arxiv.org/pdf/2512.16755v1"
      },
      "arxiv_id": "2512.16755v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16750v1",
      "title": "Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error",
      "authors": [
        "Claudia Vale Oliveira",
        "Nelson Zagalo",
        "Filipe Silva",
        "Anabela Brandao",
        "Syeda Faryal Hussain Khurrum",
        "Joaquim Santos"
      ],
      "abstract": "Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16750v1",
        "pdf": "https://arxiv.org/pdf/2512.16750v1"
      },
      "arxiv_id": "2512.16750v1",
      "comment": "19 pages, 2 tables, 77 references, 6 appendices",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16743v1",
      "title": "TreeNet: A Light Weight Model for Low Bitrate Image Compression",
      "authors": [
        "Mahadev Prasad Panda",
        "Purnachandra Rao Makkena",
        "Srivatsa Prativadibhayankaram",
        "Siegfried Fel",
        "Andr Kaup"
      ],
      "abstract": "Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16743v1",
        "pdf": "https://arxiv.org/pdf/2512.16743v1"
      },
      "arxiv_id": "2512.16743v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16742v1",
      "title": "Machine Learning Algorithms: Detection Official Hajj and Umrah Travel Agency Based on Text and Metadata Analysis",
      "authors": [
        "Wisnu Uriawan",
        "Muhamad Veva Ramadhan",
        "Firman Adi Nugraha",
        "Hasbi Nur Wahid",
        "M Dantha Arianvasya",
        "Muhammad Zaki Alghifari"
      ],
      "abstract": "The rapid digitalization of Hajj and Umrah services in Indonesia has significantly facilitated pilgrims but has concurrently opened avenues for digital fraud through counterfeit mobile applications. These fraudulent applications not only inflict financial losses but also pose severe privacy risks by harvesting sensitive personal data. This research aims to address this critical issue by implementing and evaluating machine learning algorithms to verify application authenticity automatically. Using a comprehensive dataset comprising both official applications registered with the Ministry of Religious Affairs and unofficial applications circulating on app stores, we compare the performance of three robust classifiers: Support Vector Machine (SVM), Random Forest (RF), and Na\"ive Bayes (NB). The study utilizes a hybrid feature extraction methodology that combines Textual Analysis (TF-IDF) of application descriptions with Metadata Analysis of sensitive access permissions. The experimental results indicate that the SVM algorithm achieves the highest performance with an accuracy of 92.3%, a precision of 91.5%, and an F1-score of 92.0%. Detailed feature analysis reveals that specific keywords related to legality and high-risk permissions (e.g., READ PHONE STATE) are the most significant discriminators. This system is proposed as a proactive, scalable solution to enhance digital trust in the religious tourism sector, potentially serving as a prototype for a national verification system.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16742v1",
        "pdf": "https://arxiv.org/pdf/2512.16742v1"
      },
      "arxiv_id": "2512.16742v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16740v1",
      "title": "Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation",
      "authors": [
        "Yunkai Yang",
        "Yudong Zhang",
        "Kunquan Zhang",
        "Jinxiao Zhang",
        "Xinying Chen",
        "Haohuan Fu",
        "Runmin Dong"
      ],
      "abstract": "With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16740v1",
        "pdf": "https://arxiv.org/pdf/2512.16740v1"
      },
      "arxiv_id": "2512.16740v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16739v1",
      "title": "AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach",
      "authors": [
        "Yipeng Zhuang",
        "Yifeng Guo",
        "Yuewen Li",
        "Yuheng Wu",
        "Philip Leung-Ho Yu",
        "Tingting Song",
        "Zhiyong Wang",
        "Kunzhong Zhou",
        "Weifang Wang",
        "Li Zhuang"
      ],
      "abstract": "Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16739v1",
        "pdf": "https://arxiv.org/pdf/2512.16739v1"
      },
      "arxiv_id": "2512.16739v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16733v1",
      "title": "Discovering and Learning Probabilistic Models of Black-Box AI Capabilities",
      "authors": [
        "Daniel Bramblett",
        "Rushang Karia",
        "Adrian Ciotinga",
        "Ruthvick Suresh",
        "Pulkit Verma",
        "YooJung Choi",
        "Siddharth Srivastava"
      ],
      "abstract": "Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16733v1",
        "pdf": "https://arxiv.org/pdf/2512.16733v1"
      },
      "arxiv_id": "2512.16733v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16727v1",
      "title": "OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition",
      "authors": [
        "Haochen Chang",
        "Pengfei Ren",
        "Buyuan Zhang",
        "Da Li",
        "Tianhao Han",
        "Haoyang Zhang",
        "Liang Xie",
        "Hongbo Chen",
        "Erwei Yin"
      ],
      "abstract": "Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16727v1",
        "pdf": "https://arxiv.org/pdf/2512.16727v1"
      },
      "arxiv_id": "2512.16727v1",
      "comment": "Project page: https://omg-bench.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16724v1",
      "title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation",
      "authors": [
        "Yixiang Chen",
        "Yan Huang",
        "Keji He",
        "Peiyan Li",
        "Liang Wang"
      ],
      "abstract": "When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16724v1",
        "pdf": "https://arxiv.org/pdf/2512.16724v1"
      },
      "arxiv_id": "2512.16724v1",
      "comment": "Accepted at RA-L 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16723v1",
      "title": "KOSS: Kalman-Optimal Selective State Spaces for Long-Term Sequence Modeling",
      "authors": [
        "Lei Wang",
        "Xin Tan",
        "Mingwei Wang",
        "Ying Zhang"
      ],
      "abstract": "Recent selective state space models (SSMs), such as Mamba and Mamba-2, have demonstrated strong performance in sequence modeling owing to input-dependent selection mechanisms. However, these mechanisms lack theoretical grounding and cannot support context-aware selection from latent state dynamics. To address these limitations, we propose KOSS, a Kalman-optimal Selective State Space model that formulates selection as latent state uncertainty minimization. Derived from estimation theory, KOSS adopts a continuous-time latent update driven by a Kalman gain that dynamically modulates information propagation based on content and context, enabling a closed-loop, context-aware selectivity mechanism. To ensure stable computation and near-linear scalability, KOSS employs global spectral differentiation for frequency-domain derivative estimation, along with a segment-wise scan for hardware-efficient processing. On a selective copying task with distractors, KOSS achieves over 79\\% accuracy while baselines drop below 20\\%, demonstrating robust context-aware selection. Furthermore, across nine long-term forecasting benchmarks, KOSS reduces MSE by 2.92--36.23\\% and consistently outperforms state-of-the-art models in both accuracy and stability. To assess real-world applicability, a case study on secondary surveillance radar (SSR) tracking confirms KOSS's robustness under irregular intervals and noisy conditions and demonstrates its effectiveness in real-world applications. Finally, supplementary experiments verify Kalman gain convergence and the frequency response of spectral differentiation, providing theoretical support for the proposed closed-loop design.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16723v1",
        "pdf": "https://arxiv.org/pdf/2512.16723v1"
      },
      "arxiv_id": "2512.16723v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16718v1",
      "title": "Polyharmonic Spline Packages: Composition, Efficient Procedures for Computation and Differentiation",
      "authors": [
        "Yuriy N. Bakhvalov"
      ],
      "abstract": "In a previous paper it was shown that a machine learning regression problem can be solved within the framework of random function theory, with the optimal kernel analytically derived from symmetry and indifference principles and coinciding with a polyharmonic spline. However, a direct application of that solution is limited by O(N^3) computational cost and by a breakdown of the original theoretical assumptions when the input space has excessive dimensionality. This paper proposes a cascade architecture built from packages of polyharmonic splines that simultaneously addresses scalability and is theoretically justified for problems with unknown intrinsic low dimensionality. Efficient matrix procedures are presented for forward computation and end-to-end differentiation through the cascade.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16718v1",
        "pdf": "https://arxiv.org/pdf/2512.16718v1"
      },
      "arxiv_id": "2512.16718v1",
      "comment": "Part 2 of 4 in the \"Polyharmonic Cascade\" cycle. Continues the theory from arXiv.2512.12731. Source code is available at: https://github.com/xolod7/polyharmonic-cascade",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.16717v1",
      "title": "Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering",
      "authors": [
        "Rudra Dubey",
        "Arpit Mani Tripathi",
        "Archit Srivastava",
        "Sarvpal Singh"
      ],
      "abstract": "In actuality, phishing attacks remain one of the most prevalent cybersecurity risks in existence today, with malevolent actors constantly changing their strategies to successfully trick users. This paper presents an AI model for a phishing detection system that uses an ensemble approach to combine character-level Convolutional Neural Networks (CNN) and LightGBM with engineered features. Our system uses a character-level CNN to extract sequential features after extracting 36 lexical, structural, and domain-based features from the URLs. On a test dataset of 19,873 URLs, the ensemble model achieves an accuracy of 99.819 percent, precision of 100 percent, recall of 99.635 percent, and ROC-AUC of 99.947 percent. Through a FastAPI-based service with an intuitive user interface, the suggested system has been utilised to offer real-time detection. In contrast, the results demonstrate that the suggested solution performs better than individual models; LightGBM contributes 40 percent and character-CNN contributes 60 percent to the final prediction. The suggested method maintains extremely low false positive rates while doing a good job of identifying contemporary phishing techniques. Index Terms - Phishing detection, machine learning, deep learning, CNN, ensemble methods, cybersecurity, URL analysis",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16717v1",
        "pdf": "https://arxiv.org/pdf/2512.16717v1"
      },
      "arxiv_id": "2512.16717v1",
      "comment": "7 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16715v1",
      "title": "Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library",
      "authors": [
        "Oliver Stritzel",
        "Nick Hhnerbein",
        "Simon Rauch",
        "Itzel Zarate",
        "Lukas Fleischmann",
        "Moike Buck",
        "Attila Lischka",
        "Christian Frey"
      ],
      "abstract": "In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16715v1",
        "pdf": "https://arxiv.org/pdf/2512.16715v1"
      },
      "arxiv_id": "2512.16715v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16710v1",
      "title": "A multi-centre, multi-device benchmark dataset for landmark-based comprehensive fetal biometry",
      "authors": [
        "Chiara Di Vece",
        "Zhehua Mao",
        "Netanell Avisdris",
        "Brian Dromey",
        "Raffaele Napolitano",
        "Dafna Ben Bashat",
        "Francisco Vasconcelos",
        "Danail Stoyanov",
        "Leo Joskowicz",
        "Sophia Bano"
      ],
      "abstract": "Accurate fetal growth assessment from ultrasound (US) relies on precise biometry measured by manually identifying anatomical landmarks in standard planes. Manual landmarking is time-consuming, operator-dependent, and sensitive to variability across scanners and sites, limiting the reproducibility of automated approaches. There is a need for multi-source annotated datasets to develop artificial intelligence-assisted fetal growth assessment methods. To address this bottleneck, we present an open, multi-centre, multi-device benchmark dataset of fetal US images with expert anatomical landmark annotations for clinically used fetal biometric measurements. These measurements include head bi-parietal and occipito-frontal diameters, abdominal transverse and antero-posterior diameters, and femoral length. The dataset contains 4,513 de-identified US images from 1,904 subjects acquired at three clinical sites using seven different US devices. We provide standardised, subject-disjoint train/test splits, evaluation code, and baseline results to enable fair and reproducible comparison of methods. Using an automatic biometry model, we quantify domain shift and demonstrate that training and evaluation confined to a single centre substantially overestimate performance relative to multi-centre testing. To the best of our knowledge, this is the first publicly available multi-centre, multi-device, landmark-annotated dataset that covers all primary fetal biometry measures, providing a robust benchmark for domain adaptation and multi-centre generalisation in fetal biometry and enabling more reliable AI-assisted fetal growth assessment across centres. All data, annotations, training code, and evaluation pipelines are made publicly available.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16710v1",
        "pdf": "https://arxiv.org/pdf/2512.16710v1"
      },
      "arxiv_id": "2512.16710v1",
      "comment": "11 pages, 5 figures, 3 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16707v1",
      "title": "Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems",
      "authors": [
        "Abhisek Ganguly"
      ],
      "abstract": "We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16707v1",
        "pdf": "https://arxiv.org/pdf/2512.16707v1"
      },
      "arxiv_id": "2512.16707v1",
      "comment": "6 Pages, 0 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16706v1",
      "title": "SDFoam: Signed-Distance Foam for explicit surface reconstruction",
      "authors": [
        "Antonella Rech",
        "Nicola Conci",
        "Nicola Garau"
      ],
      "abstract": "Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16706v1",
        "pdf": "https://arxiv.org/pdf/2512.16706v1"
      },
      "arxiv_id": "2512.16706v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16705v1",
      "title": "Olaf: Bringing an Animated Character to Life in the Physical World",
      "authors": [
        "David Mller",
        "Espen Knoop",
        "Dario Mylonopoulos",
        "Agon Serifi",
        "Michael A. Hopkins",
        "Ruben Grandia",
        "Moritz Bcher"
      ],
      "abstract": "Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16705v1",
        "pdf": "https://arxiv.org/pdf/2512.16705v1"
      },
      "arxiv_id": "2512.16705v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16702v1",
      "title": "How accurate are foundational machine learning interatomic potentials for heterogeneous catalysis?",
      "authors": [
        "Luuk H. E. Kempen",
        "Raffaele Cheula",
        "Mie Andersen"
      ],
      "abstract": "Foundational machine learning interatomic potentials (MLIPs) are being developed at a rapid pace, promising closer and closer approximation to ab initio accuracy. This unlocks the possibility to simulate much larger length and time scales. However, benchmarks for these MLIPs are usually limited to ordered, crystalline and bulk materials. Hence, reported performance does not necessarily accurately reflect MLIP performance in real applications such as heterogeneous catalysis. Here, we systematically analyze zero-shot performance of 80 different MLIPs, evaluating tasks typical for heterogeneous catalysis across a range of different data sets, including adsorption and reaction on surfaces of alloyed metals, oxides, and metal-oxide interfacial systems. We demonstrate that current-generation foundational MLIPs can already perform at high accuracy for applications such as predicting vacancy formation energies of perovskite oxides or zero-point energies of supported nanoclusters. However, limitations also exist. We find that many MLIPs catastrophically fail when applied to magnetic materials, and structure relaxation in the MLIP generally increases the energy prediction error compared to single-point evaluation of a previously optimized structure. Comparing low-cost task-specific models to foundational MLIPs, we highlight some core differences between these model approaches and show that -- if considering only accuracy -- these models can compete with the current generation of best-performing MLIPs. Furthermore, we show that no single MLIP universally performs best, requiring users to investigate MLIP suitability for their desired application.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.LG",
        "physics.chem-ph"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16702v1",
        "pdf": "https://arxiv.org/pdf/2512.16702v1"
      },
      "arxiv_id": "2512.16702v1",
      "comment": "16 pages, 5 figures, 1 table + supplementary information (37 pages, 16 figures, 15 tables)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16701v1",
      "title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences",
      "authors": [
        "Giovanni Adorni"
      ],
      "abstract": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.\n  We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16701v1",
        "pdf": "https://arxiv.org/pdf/2512.16701v1"
      },
      "arxiv_id": "2512.16701v1",
      "comment": "15 pages, 16 references, Key Note preented at the \"WAILS 2025 - The 2nd. Workshop on Artificial Intelligence with and for Learning Sciences\", Cagliary, Italy, 10-12 December 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16700v1",
      "title": "CLARiTy: A Vision Transformer for Multi-Label Classification and Weakly-Supervised Localization of Chest X-ray Pathologies",
      "authors": [
        "John M. Statheros",
        "Hairong Wang",
        "Richard Klein"
      ],
      "abstract": "The interpretation of chest X-rays (CXRs) poses significant challenges, particularly in achieving accurate multi-label pathology classification and spatial localization. These tasks demand different levels of annotation granularity but are frequently constrained by the scarcity of region-level (dense) annotations. We introduce CLARiTy (Class Localizing and Attention Refining Image Transformer), a vision transformer-based model for joint multi-label classification and weakly-supervised localization of thoracic pathologies. CLARiTy employs multiple class-specific tokens to generate discriminative attention maps, and a SegmentCAM module for foreground segmentation and background suppression using explicit anatomical priors. Trained on image-level labels from the NIH ChestX-ray14 dataset, it leverages distillation from a ConvNeXtV2 teacher for efficiency. Evaluated on the official NIH split, the CLARiTy-S-16-512 (a configuration of CLARiTy), achieves competitive classification performance across 14 pathologies, and state-of-the-art weakly-supervised localization performance on 8 pathologies, outperforming prior methods by 50.7%. In particular, pronounced gains occur for small pathologies like nodules and masses. The lower-resolution variant of CLARiTy, CLARiTy-S-16-224, offers high efficiency while decisively surpassing baselines, thereby having the potential for use in low-resource settings. An ablation study confirms contributions of SegmentCAM, DINO pretraining, orthogonal class token loss, and attention pooling. CLARiTy advances beyond CNN-ViT hybrids by harnessing ViT self-attention for global context and class-specific localization, refined through convolutional background suppression for precise, noise-reduced heatmaps.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16700v1",
        "pdf": "https://arxiv.org/pdf/2512.16700v1"
      },
      "arxiv_id": "2512.16700v1",
      "comment": "23 pages, 11 figures, submitted to Medical Image Analysis",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16698v1",
      "title": "Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning",
      "authors": [
        "Mahbub E Sobhani",
        "Md. Faiyaz Abdullah Sayeedi",
        "Mohammad Nehad Alam",
        "Proma Hossain Progga",
        "Swakkhar Shatabda"
      ],
      "abstract": "Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI",
        "cs.CG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16698v1",
        "pdf": "https://arxiv.org/pdf/2512.16698v1"
      },
      "arxiv_id": "2512.16698v1",
      "comment": "Accepted to the ARR October 2025 cycle",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16694v1",
      "title": "Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm",
      "authors": [
        "Wisnu Uriawan",
        "Achmad Ajie Priyajie",
        "Angga Gustian",
        "Fikri Nur Hidayat",
        "Sendi Ahmad Rafiudin",
        "Muhamad Fikri Zaelani"
      ],
      "abstract": "This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16694v1",
        "pdf": "https://arxiv.org/pdf/2512.16694v1"
      },
      "arxiv_id": "2512.16694v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16688v1",
      "title": "Detecting Localized Deepfakes: How Well Do Synthetic Image Detectors Handle Inpainting?",
      "authors": [
        "Serafino Pandolfini",
        "Lorenzo Pellegrini",
        "Matteo Ferrara",
        "Davide Maltoni"
      ],
      "abstract": "The rapid progress of generative AI has enabled highly realistic image manipulations, including inpainting and region-level editing. These approaches preserve most of the original visual context and are increasingly exploited in cybersecurity-relevant threat scenarios. While numerous detectors have been proposed for identifying fully synthetic images, their ability to generalize to localized manipulations remains insufficiently characterized. This work presents a systematic evaluation of state-of-the-art detectors, originally trained for the deepfake detection on fully synthetic images, when applied to a distinct challenge: localized inpainting detection. The study leverages multiple datasets spanning diverse generators, mask sizes, and inpainting techniques. Our experiments show that models trained on a large set of generators exhibit partial transferability to inpainting-based edits and can reliably detect medium- and large-area manipulations or regeneration-style inpainting, outperforming many existing ad hoc detection approaches.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16688v1",
        "pdf": "https://arxiv.org/pdf/2512.16688v1"
      },
      "arxiv_id": "2512.16688v1",
      "comment": "17 pages, 5 figures, 9 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16687v1",
      "title": "Blog Data Showdown: Machine Learning vs Neuro-Symbolic Models for Gender Classification",
      "authors": [
        "Natnael Tilahun Sinshaw",
        "Mengmei He",
        "Tadesse K. Bahiru",
        "Sudhir Kumar Mohapatra"
      ],
      "abstract": "Text classification problems, such as gender classification from a blog, have been a well-matured research area that has been well studied using machine learning algorithms. It has several application domains in market analysis, customer recommendation, and recommendation systems. This study presents a comparative analysis of the widely used machine learning algorithms, namely Support Vector Machines (SVM), Naive Bayes (NB), Logistic Regression (LR), AdaBoost, XGBoost, and an SVM variant (SVM_R) with neuro-symbolic AI (NeSy). The paper also explores the effect of text representations such as TF-IDF, the Universal Sentence Encoder (USE), and RoBERTa. Additionally, various feature extraction techniques, including Chi-Square, Mutual Information, and Principal Component Analysis, are explored. Building on these, we introduce a comparative analysis of the machine learning and deep learning approaches in comparison to the NeSy. The experimental results show that the use of the NeSy approach matched strong MLP results despite a limited dataset. Future work on this research will expand the knowledge base, the scope of embedding types, and the hyperparameter configuration to further study the effectiveness of the NeSy approach.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16687v1",
        "pdf": "https://arxiv.org/pdf/2512.16687v1"
      },
      "arxiv_id": "2512.16687v1",
      "comment": "6 pages",
      "journal_ref": "2025 International Conference on Information and Communication Technology for Development for Africa (ICT4DA)",
      "has_code": false
    },
    {
      "id": "2512.16685v1",
      "title": "Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray",
      "authors": [
        "Gonalo Gaspar Alves",
        "Shekoufeh Gorgi Zadeh",
        "Andreas Husch",
        "Ben Bausch"
      ],
      "abstract": "Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16685v1",
        "pdf": "https://arxiv.org/pdf/2512.16685v1"
      },
      "arxiv_id": "2512.16685v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16676v1",
      "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
      "authors": [
        "Hao Liang",
        "Xiaochen Ma",
        "Zhou Liu",
        "Zhen Hao Wong",
        "Zhengyang Zhao",
        "Zimo Meng",
        "Runming He",
        "Chengyu Shen",
        "Qifeng Cai",
        "Zhaoyang Han",
        "Meiyi Qiang",
        "Yalin Feng",
        "Tianyi Bai",
        "Zewei Pan",
        "Ziyi Guo",
        "Yizhen Jiang",
        "Jingwen Deng",
        "Qijie You",
        "Peichao Lai",
        "Tianyu Guo",
        "Chi Hsu Tsai",
        "Hengyi Feng",
        "Rui Hu",
        "Wenkai Yu",
        "Junbo Niu",
        "Bohan Zeng",
        "Ruichuan An",
        "Lu Ma",
        "Jihao Huang",
        "Yaowei Zheng",
        "Conghui He",
        "Linpeng Tang",
        "Bin Cui",
        "Weinan E",
        "Wentao Zhang"
      ],
      "abstract": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16676v1",
        "pdf": "https://arxiv.org/pdf/2512.16676v1"
      },
      "arxiv_id": "2512.16676v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16670v1",
      "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
      "authors": [
        "Ole Beisswenger",
        "Jan-Niklas Dihlmann",
        "Hendrik P. A. Lensch"
      ],
      "abstract": "Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16670v1",
        "pdf": "https://arxiv.org/pdf/2512.16670v1"
      },
      "arxiv_id": "2512.16670v1",
      "comment": "Project Page: https://framediffuser.jdihlmann.com/",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16661v1",
      "title": "Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance",
      "authors": [
        "Jacob Reiss",
        "Shikshya Shiwakoti",
        "Samuel Goldsmith",
        "Ujjwal Pandit"
      ],
      "abstract": "In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16661v1",
        "pdf": "https://arxiv.org/pdf/2512.16661v1"
      },
      "arxiv_id": "2512.16661v1",
      "comment": "5 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16658v1",
      "title": "Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking",
      "authors": [
        "Sangeeth B",
        "Serena Nicolazzo",
        "Deepa K.",
        "Vinod P"
      ],
      "abstract": "The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16658v1",
        "pdf": "https://arxiv.org/pdf/2512.16658v1"
      },
      "arxiv_id": "2512.16658v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16656v1",
      "title": "Comprehensive AI Literacy: The Case for Centering Human Agency",
      "authors": [
        "Sri Yash Tadimalla",
        "Justin Cary",
        "Gordon Hull",
        "Jordan Register",
        "Daniel Maxwell",
        "David Pugalee",
        "Tina Heafner"
      ],
      "abstract": "The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16656v1",
        "pdf": "https://arxiv.org/pdf/2512.16656v1"
      },
      "arxiv_id": "2512.16656v1",
      "comment": "2 figures, 2 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16650v1",
      "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
      "authors": [
        "Jirui Yang",
        "Hengqi Guo",
        "Zhihui Lu",
        "Yi Zhao",
        "Yuansen Zhang",
        "Shijing Hu",
        "Qiang Duan",
        "Yinggui Wang",
        "Tao Wei"
      ],
      "abstract": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16650v1",
        "pdf": "https://arxiv.org/pdf/2512.16650v1"
      },
      "arxiv_id": "2512.16650v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16648v1",
      "title": "Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario",
      "authors": [
        "Liu Yang",
        "Qiang Li",
        "Luxiong Wen",
        "Jian Yang"
      ],
      "abstract": "With the rapid proliferation of edge computing, Radio Frequency Fingerprint Identification (RFFI) has become increasingly important for secure device authentication. However, practical deployment of deep learning-based RFFI models is hindered by a critical challenge: their performance often degrades significantly when applied across receivers with different hardware characteristics due to distribution shifts introduced by receiver variation. To address this, we investigate the source-data-free cross-receiver RFFI (SCRFFI) problem, where a model pretrained on labeled signals from a source receiver must adapt to unlabeled signals from a target receiver, without access to any source-domain data during adaptation. We first formulate a novel constrained pseudo-labeling-based SCRFFI adaptation framework, and provide a theoretical analysis of its generalization performance. Our analysis highlights a key insight: the target-domain performance is highly sensitive to the quality of the pseudo-labels generated during adaptation. Motivated by this, we propose Momentum Soft pseudo-label Source Hypothesis Transfer (MS-SHOT), a new method for SCRFFI that incorporates momentum-center-guided soft pseudo-labeling and enforces global structural constraints to encourage confident and diverse predictions. Notably, MS-SHOT effectively addresses scenarios involving label shift or unknown, non-uniform class distributions in the target domain -- a significant limitation of prior methods. Extensive experiments on real-world datasets demonstrate that MS-SHOT consistently outperforms existing approaches in both accuracy and robustness, offering a practical and scalable solution for source-data-free cross-receiver adaptation in RFFI.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16648v1",
        "pdf": "https://arxiv.org/pdf/2512.16648v1"
      },
      "arxiv_id": "2512.16648v1",
      "comment": "IEEE Transactions on Mobile Computing",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.16644v1",
      "title": "Implementing a Sharia Chatbot as a Consultation Medium for Questions About Islam",
      "authors": [
        "Wisnu Uriawan",
        "Aria Octavian Hamza",
        "Ade Ripaldi Nuralim",
        "Adi Purnama",
        "Ahmad Juaeni Yunus",
        "Anissya Auliani Supriadi Putri"
      ],
      "abstract": "This research presents the implementation of a Sharia-compliant chatbot as an interactive medium for consulting Islamic questions, leveraging Reinforcement Learning (Q-Learning) integrated with Sentence-Transformers for semantic embedding to ensure contextual and accurate responses. Utilizing the CRISP-DM methodology, the system processes a curated Islam QA dataset of 25,000 question-answer pairs from authentic sources like the Qur'an, Hadith, and scholarly fatwas, formatted in JSON for flexibility and scalability. The chatbot prototype, developed with a Flask API backend and Flutter-based mobile frontend, achieves 87% semantic accuracy in functional testing across diverse topics including fiqh, aqidah, ibadah, and muamalah, demonstrating its potential to enhance religious literacy, digital da'wah, and access to verified Islamic knowledge in the Industry 4.0 era. While effective for closed-domain queries, limitations such as static learning and dataset dependency highlight opportunities for future enhancements like continuous adaptation and multi-turn conversation support, positioning this innovation as a bridge between traditional Islamic scholarship and modern AI-driven consultation.",
      "published": "2025-12-18",
      "updated": "2025-12-18",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.16644v1",
        "pdf": "https://arxiv.org/pdf/2512.16644v1"
      },
      "arxiv_id": "2512.16644v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    }
  ]
}