{
  "fetched_at": "2025-11-16T00:26:56.161610",
  "total_papers": 100,
  "papers": [
    {
      "id": "2511.10648v1",
      "title": "Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling",
      "authors": [
        "Jiahao Wang",
        "Weiye Xu",
        "Aijun Yang",
        "Wengang Zhou",
        "Lewei Lu",
        "Houqiang Li",
        "Xiaohua Wang",
        "Jinguo Zhu"
      ],
      "abstract": "Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10648v1",
        "pdf": "https://arxiv.org/pdf/2511.10648v1"
      },
      "arxiv_id": "2511.10648v1",
      "comment": "Accepted to NeurIPS 2025 (The Thirty-Ninth Annual Conference on Neural Information Processing Systems)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10647v1",
      "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
      "authors": [
        "Haotong Lin",
        "Sili Chen",
        "Junhao Liew",
        "Donny Y. Chen",
        "Zhenyu Li",
        "Guang Shi",
        "Jiashi Feng",
        "Bingyi Kang"
      ],
      "abstract": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10647v1",
        "pdf": "https://arxiv.org/pdf/2511.10647v1"
      },
      "arxiv_id": "2511.10647v1",
      "comment": "https://depth-anything-3.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.10643v1",
      "title": "Black-Box On-Policy Distillation of Large Language Models",
      "authors": [
        "Tianzhu Ye",
        "Li Dong",
        "Zewen Chi",
        "Xun Wu",
        "Shaohan Huang",
        "Furu Wei"
      ],
      "abstract": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10643v1",
        "pdf": "https://arxiv.org/pdf/2511.10643v1"
      },
      "arxiv_id": "2511.10643v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10635v1",
      "title": "Robot Crash Course: Learning Soft and Stylized Falling",
      "authors": [
        "Pascal Strauch",
        "David Müller",
        "Sammy Christen",
        "Agon Serifi",
        "Ruben Grandia",
        "Espen Knoop",
        "Moritz Bächer"
      ],
      "abstract": "Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward function that balances the achievement of a desired end pose with impact minimization and the protection of critical robot parts during reinforcement learning. To make the policy robust to a broad range of initial falling conditions and to enable the specification of an arbitrary and unseen end pose at inference time, we introduce a simulation-based sampling strategy of initial and end poses. Through simulated and real-world experiments, our work demonstrates that even bipedal robots can perform controlled, soft falls.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10635v1",
        "pdf": "https://arxiv.org/pdf/2511.10635v1"
      },
      "arxiv_id": "2511.10635v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10629v1",
      "title": "One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models",
      "authors": [
        "Aleksandr Razin",
        "Danil Kazantsev",
        "Ilya Makarov"
      ],
      "abstract": "Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10629v1",
        "pdf": "https://arxiv.org/pdf/2511.10629v1"
      },
      "arxiv_id": "2511.10629v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10628v1",
      "title": "Instella: Fully Open Language Models with Stellar Performance",
      "authors": [
        "Jiang Liu",
        "Jialian Wu",
        "Xiaodong Yu",
        "Yusheng Su",
        "Prakamya Mishra",
        "Gowtham Ramesh",
        "Sudhanshu Ranjan",
        "Chaitanya Manem",
        "Ximeng Sun",
        "Ze Wang",
        "Pratik Prabhanjan Brahma",
        "Zicheng Liu",
        "Emad Barsoum"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10628v1",
        "pdf": "https://arxiv.org/pdf/2511.10628v1"
      },
      "arxiv_id": "2511.10628v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10627v1",
      "title": "Querying Labeled Time Series Data with Scenario Programs",
      "authors": [
        "Edward Kim",
        "Devan Shanker",
        "Varun Bharadwaj",
        "Hongbeen Park",
        "Jinkyu Kim",
        "Hazem Torfah",
        "Daniel J Fremont",
        "Sanjit A Seshia"
      ],
      "abstract": "Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.FL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10627v1",
        "pdf": "https://arxiv.org/pdf/2511.10627v1"
      },
      "arxiv_id": "2511.10627v1",
      "comment": "",
      "journal_ref": "NASA Formal Methods Conference 2025",
      "has_code": false
    },
    {
      "id": "2511.10626v1",
      "title": "Global Solutions to Non-Convex Functional Constrained Problems with Hidden Convexity",
      "authors": [
        "Ilyas Fatkhullin",
        "Niao He",
        "Guanghui Lan",
        "Florian Wolf"
      ],
      "abstract": "Constrained non-convex optimization is fundamentally challenging, as global solutions are generally intractable and constraint qualifications may not hold. However, in many applications, including safe policy optimization in control and reinforcement learning, such problems possess hidden convexity, meaning they can be reformulated as convex programs via a nonlinear invertible transformation. Typically such transformations are implicit or unknown, making the direct link with the convex program impossible. On the other hand, (sub-)gradients with respect to the original variables are often accessible or can be easily estimated, which motivates algorithms that operate directly in the original (non-convex) problem space using standard (sub-)gradient oracles. In this work, we develop the first algorithms to provably solve such non-convex problems to global minima. First, using a modified inexact proximal point method, we establish global last-iterate convergence guarantees with $\\widetilde{\\mathcal{O}}(\\varepsilon^{-3})$ oracle complexity in non-smooth setting. For smooth problems, we propose a new bundle-level type method based on linearly constrained quadratic subproblems, improving the oracle complexity to $\\widetilde{\\mathcal{O}}(\\varepsilon^{-1})$. Surprisingly, despite non-convexity, our methodology does not require any constraint qualifications, can handle hidden convex equality constraints, and achieves complexities matching those for solving unconstrained hidden convex optimization.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10626v1",
        "pdf": "https://arxiv.org/pdf/2511.10626v1"
      },
      "arxiv_id": "2511.10626v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10621v1",
      "title": "SSR: Socratic Self-Refine for Large Language Model Reasoning",
      "authors": [
        "Haizhou Shi",
        "Ye Liu",
        "Bo Pang",
        "Zeyu Leo Liu",
        "Hao Wang",
        "Silvio Savarese",
        "Caiming Xiong",
        "Yingbo Zhou",
        "Semih Yavuz"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10621v1",
        "pdf": "https://arxiv.org/pdf/2511.10621v1"
      },
      "arxiv_id": "2511.10621v1",
      "comment": "Preprint; work in progress",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10619v1",
      "title": "Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem",
      "authors": [
        "Avrim Blum",
        "Marten Garicano",
        "Kavya Ravichandran",
        "Dravyansh Sharma"
      ],
      "abstract": "The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Ω(k)$ and $Ω(\\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10619v1",
        "pdf": "https://arxiv.org/pdf/2511.10619v1"
      },
      "arxiv_id": "2511.10619v1",
      "comment": "25 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10618v1",
      "title": "Know Your Limits: Entropy Estimation Modeling for Compression and Generalization",
      "authors": [
        "Benjamin L. Badger",
        "Matthew Neligeorge"
      ],
      "abstract": "Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10618v1",
        "pdf": "https://arxiv.org/pdf/2511.10618v1"
      },
      "arxiv_id": "2511.10618v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10615v1",
      "title": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals",
      "authors": [
        "Shruti Singh Baghel",
        "Yash Pratap Singh Rathore",
        "Sushovan Jena",
        "Anurag Pradhan",
        "Amit Shukla",
        "Arnav Bhavsar",
        "Pawan Goyal"
      ],
      "abstract": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10615v1",
        "pdf": "https://arxiv.org/pdf/2511.10615v1"
      },
      "arxiv_id": "2511.10615v1",
      "comment": "8 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10611v1",
      "title": "Towards an Agentic Workflow for Internet Measurement Research",
      "authors": [
        "Alagappan Ramanathan",
        "Eunju Kang",
        "Dongsu Han",
        "Sangeetha Abdu Jyothi"
      ],
      "abstract": "Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools that demands specialized domain expertise. When network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling. However, developing these workflows requires specialized knowledge and significant manual effort.\n  We present ArachNet, the first system demonstrating that LLM agents can independently generate measurement workflows that mimics expert reasoning. Our core insight is that measurement expertise follows predictable compositional patterns that can be systematically automated. ArachNet operates through four specialized agents that mirror expert workflow, from problem decomposition to solution implementation. We validate ArachNet with progressively challenging Internet resilience scenarios. The system independently generates workflows that match expert-level reasoning and produce analytical outputs similar to specialist solutions. Generated workflows handle complex multi-framework integration that traditionally requires days of manual coordination. ArachNet lowers barriers to measurement workflow composition by automating the systematic reasoning process that experts use, enabling broader access to sophisticated measurement capabilities while maintaining the technical rigor required for research-quality analysis.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10611v1",
        "pdf": "https://arxiv.org/pdf/2511.10611v1"
      },
      "arxiv_id": "2511.10611v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10604v1",
      "title": "Multitask GLocal OBIA-Mamba for Sentinel-2 Landcover Mapping",
      "authors": [
        "Zack Dewis",
        "Yimin Zhu",
        "Zhengsen Xu",
        "Mabel Heffring",
        "Saeid Taleghanidoozdoozan",
        "Kaylee Xiao",
        "Motasem Alkayid",
        "Lincoln Linlin Xu"
      ],
      "abstract": "Although Sentinel-2 based land use and land cover (LULC) classification is critical for various environmental monitoring applications, it is a very difficult task due to some key data challenges (e.g., spatial heterogeneity, context information, signature ambiguity). This paper presents a novel Multitask Glocal OBIA-Mamba (MSOM) for enhanced Sentinel-2 classification with the following contributions. First, an object-based image analysis (OBIA) Mamba model (OBIA-Mamba) is designed to reduce redundant computation without compromising fine-grained details by using superpixels as Mamba tokens. Second, a global-local (GLocal) dual-branch convolutional neural network (CNN)-mamba architecture is designed to jointly model local spatial detail and global contextual information. Third, a multitask optimization framework is designed to employ dual loss functions to balance local precision with global consistency. The proposed approach is tested on Sentinel-2 imagery in Alberta, Canada, in comparison with several advanced classification approaches, and the results demonstrate that the proposed approach achieves higher classification accuracy and finer details that the other state-of-the-art methods.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10604v1",
        "pdf": "https://arxiv.org/pdf/2511.10604v1"
      },
      "arxiv_id": "2511.10604v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10597v1",
      "title": "From 2D to 3D Without Extra Baggage: Data-Efficient Cancer Detection in Digital Breast Tomosynthesis",
      "authors": [
        "Yen Nhi Truong Vu",
        "Dan Guo",
        "Sripad Joshi",
        "Harshit Kumar",
        "Jason Su",
        "Thomas Paul Matthews"
      ],
      "abstract": "Digital Breast Tomosynthesis (DBT) enhances finding visibility for breast cancer detection by providing volumetric information that reduces the impact of overlapping tissues; however, limited annotated data has constrained the development of deep learning models for DBT. To address data scarcity, existing methods attempt to reuse 2D full-field digital mammography (FFDM) models by either flattening DBT volumes or processing slices individually, thus discarding volumetric information. Alternatively, 3D reasoning approaches introduce complex architectures that require more DBT training data. Tackling these drawbacks, we propose M&M-3D, an architecture that enables learnable 3D reasoning while remaining parameter-free relative to its FFDM counterpart, M&M. M&M-3D constructs malignancy-guided 3D features, and 3D reasoning is learned through repeatedly mixing these 3D features with slice-level information. This is achieved by modifying operations in M&M without adding parameters, thus enabling direct weight transfer from FFDM. Extensive experiments show that M&M-3D surpasses 2D projection and 3D slice-based methods by 11-54% for localization and 3-10% for classification. Additionally, M&M-3D outperforms complex 3D reasoning variants by 20-47% for localization and 2-10% for classification in the low-data regime, while matching their performance in high-data regime. On the popular BCS-DBT benchmark, M&M-3D outperforms previous top baseline by 4% for classification and 10% for localization.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10597v1",
        "pdf": "https://arxiv.org/pdf/2511.10597v1"
      },
      "arxiv_id": "2511.10597v1",
      "comment": "",
      "journal_ref": "In Machine Learning for Health (ML4H). PMLR 297, 2025",
      "has_code": false
    },
    {
      "id": "2511.10593v1",
      "title": "Regular Games -- an Automata-Based General Game Playing Language",
      "authors": [
        "Radosław Miernik",
        "Marek Szykuła",
        "Jakub Kowalski",
        "Jakub Cieśluk",
        "Łukasz Galas",
        "Wojciech Pawlik"
      ],
      "abstract": "We propose a new General Game Playing (GGP) system called Regular Games (RG). The main goal of RG is to be both computationally efficient and convenient for game design. The system consists of several languages. The core component is a low-level language that defines the rules by a finite automaton. It is minimal with only a few mechanisms, which makes it easy for automatic processing (by agents, analysis, optimization, etc.). The language is universal for the class of all finite turn-based games with imperfect information. Higher-level languages are introduced for game design (by humans or Procedural Content Generation), which are eventually translated to a low-level language. RG generates faster forward models than the current state of the art, beating other GGP systems (Regular Boardgames, Ludii) in terms of efficiency. Additionally, RG's ecosystem includes an editor with LSP, automaton visualization, benchmarking tools, and a debugger of game description transformations.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10593v1",
        "pdf": "https://arxiv.org/pdf/2511.10593v1"
      },
      "arxiv_id": "2511.10593v1",
      "comment": "Full version of AAAI 2026 paper",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10591v1",
      "title": "Mined Prompting and Metadata-Guided Generation for Wound Care Visual Question Answering",
      "authors": [
        "Bavana Durgapraveen",
        "Sornaraj Sivasankaran",
        "Abhinand Balachandran",
        "Sriram Rajkumar"
      ],
      "abstract": "The rapid expansion of asynchronous remote care has intensified provider workload, creating demand for AI systems that can assist clinicians in managing patient queries more efficiently. The MEDIQA-WV 2025 shared task addresses this challenge by focusing on generating free-text responses to wound care queries paired with images. In this work, we present two complementary approaches developed for the English track. The first leverages a mined prompting strategy, where training data is embedded and the top-k most similar examples are retrieved to serve as few-shot demonstrations during generation. The second approach builds on a metadata ablation study, which identified four metadata attributes that consistently enhance response quality. We train classifiers to predict these attributes for test cases and incorporate them into the generation pipeline, dynamically adjusting outputs based on prediction confidence. Experimental results demonstrate that mined prompting improves response relevance, while metadata-guided generation further refines clinical precision. Together, these methods highlight promising directions for developing AI-driven tools that can provide reliable and efficient wound care support.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10591v1",
        "pdf": "https://arxiv.org/pdf/2511.10591v1"
      },
      "arxiv_id": "2511.10591v1",
      "comment": "2 figures, 11 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10590v1",
      "title": "Pretrained Joint Predictions for Scalable Batch Bayesian Optimization of Molecular Designs",
      "authors": [
        "Miles Wang-Henderson",
        "Ben Kaufman",
        "Edward Williams",
        "Ryan Pederson",
        "Matteo Rossi",
        "Owen Howell",
        "Carl Underkoffler",
        "Narbe Mardirossian",
        "John Parkhill"
      ],
      "abstract": "Batched synthesis and testing of molecular designs is the key bottleneck of drug development. There has been great interest in leveraging biomolecular foundation models as surrogates to accelerate this process. In this work, we show how to obtain scalable probabilistic surrogates of binding affinity for use in Batch Bayesian Optimization (Batch BO). This demands parallel acquisition functions that hedge between designs and the ability to rapidly sample from a joint predictive density to approximate them. Through the framework of Epistemic Neural Networks (ENNs), we obtain scalable joint predictive distributions of binding affinity on top of representations taken from large structure-informed models. Key to this work is an investigation into the importance of prior networks in ENNs and how to pretrain them on synthetic data to improve downstream performance in Batch BO. Their utility is demonstrated by rediscovering known potent EGFR inhibitors on a semi-synthetic benchmark in up to 5x fewer iterations, as well as potent inhibitors from a real-world small-molecule library in up to 10x fewer iterations, offering a promising solution for large-scale drug discovery applications.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10590v1",
        "pdf": "https://arxiv.org/pdf/2511.10590v1"
      },
      "arxiv_id": "2511.10590v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10585v1",
      "title": "Textual understanding boost in the WikiRace",
      "authors": [
        "Raman Ebrahimi",
        "Sean Fuhrman",
        "Kendrick Nguyen",
        "Harini Gurusankar",
        "Massimo Franceschetti"
      ],
      "abstract": "The WikiRace game, where players navigate between Wikipedia articles using only hyperlinks, serves as a compelling benchmark for goal-directed search in complex information networks. This paper presents a systematic evaluation of navigation strategies for this task, comparing agents guided by graph-theoretic structure (betweenness centrality), semantic meaning (language model embeddings), and hybrid approaches. Through rigorous benchmarking on a large Wikipedia subgraph, we demonstrate that a purely greedy agent guided by the semantic similarity of article titles is overwhelmingly effective. This strategy, when combined with a simple loop-avoidance mechanism, achieved a perfect success rate and navigated the network with an efficiency an order of magnitude better than structural or hybrid methods. Our findings highlight the critical limitations of purely structural heuristics for goal-directed search and underscore the transformative potential of large language models to act as powerful, zero-shot semantic navigators in complex information spaces.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10585v1",
        "pdf": "https://arxiv.org/pdf/2511.10585v1"
      },
      "arxiv_id": "2511.10585v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10583v1",
      "title": "Evaluating Prompting Strategies with MedGemma for Medical Order Extraction",
      "authors": [
        "Abhinand Balachandran",
        "Bavana Durgapraveen",
        "Gowsikkan Sikkan Sudhagar",
        "Vidhya Varshany J S",
        "Sriram Rajkumar"
      ],
      "abstract": "The accurate extraction of medical orders from doctor-patient conversations is a critical task for reducing clinical documentation burdens and ensuring patient safety. This paper details our team submission to the MEDIQA-OE-2025 Shared Task. We investigate the performance of MedGemma, a new domain-specific open-source language model, for structured order extraction. We systematically evaluate three distinct prompting paradigms: a straightforward one-Shot approach, a reasoning-focused ReAct framework, and a multi-step agentic workflow. Our experiments reveal that while more complex frameworks like ReAct and agentic flows are powerful, the simpler one-shot prompting method achieved the highest performance on the official validation set. We posit that on manually annotated transcripts, complex reasoning chains can lead to \"overthinking\" and introduce noise, making a direct approach more robust and efficient. Our work provides valuable insights into selecting appropriate prompting strategies for clinical information extraction in varied data conditions.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10583v1",
        "pdf": "https://arxiv.org/pdf/2511.10583v1"
      },
      "arxiv_id": "2511.10583v1",
      "comment": "2 figures 7 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10576v1",
      "title": "Tight Robustness Certification through the Convex Hull of $\\ell_0$ Attacks",
      "authors": [
        "Yuval Shapira",
        "Dana Drachsler-Cohen"
      ],
      "abstract": "Few-pixel attacks mislead a classifier by modifying a few pixels of an image. Their perturbation space is an $\\ell_0$-ball, which is not convex, unlike $\\ell_p$-balls for $p\\geq1$. However, existing local robustness verifiers typically scale by relying on linear bound propagation, which captures convex perturbation spaces. We show that the convex hull of an $\\ell_0$-ball is the intersection of its bounding box and an asymmetrically scaled $\\ell_1$-like polytope. The volumes of the convex hull and this polytope are nearly equal as the input dimension increases. We then show a linear bound propagation that precisely computes bounds over the convex hull and is significantly tighter than bound propagations over the bounding box or our $\\ell_1$-like polytope. This bound propagation scales the state-of-the-art $\\ell_0$ verifier on its most challenging robustness benchmarks by 1.24x-7.07x, with a geometric mean of 3.16.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10576v1",
        "pdf": "https://arxiv.org/pdf/2511.10576v1"
      },
      "arxiv_id": "2511.10576v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10575v1",
      "title": "Semi-Unified Sparse Dictionary Learning with Learnable Top-K LISTA and FISTA Encoders",
      "authors": [
        "Fengsheng Lin",
        "Shengyi Yan",
        "Trac Duy Tran"
      ],
      "abstract": "We present a semi-unified sparse dictionary learning framework that bridges the gap between classical sparse models and modern deep architectures. Specifically, the method integrates strict Top-$K$ LISTA and its convex FISTA-based variant (LISTAConv) into the discriminative LC-KSVD2 model, enabling co-evolution between the sparse encoder and the dictionary under supervised or unsupervised regimes. This unified design retains the interpretability of traditional sparse coding while benefiting from efficient, differentiable training.\n  We further establish a PALM-style convergence analysis for the convex variant, ensuring theoretical stability under block alternation. Experimentally, our method achieves 95.6\\% on CIFAR-10, 86.3\\% on CIFAR-100, and 88.5\\% on TinyImageNet with faster convergence and lower memory cost ($<$4GB GPU). The results confirm that the proposed LC-KSVD2 + LISTA/LISTAConv pipeline offers an interpretable and computationally efficient alternative for modern deep architectures.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10575v1",
        "pdf": "https://arxiv.org/pdf/2511.10575v1"
      },
      "arxiv_id": "2511.10575v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10573v1",
      "title": "Towards Emotionally Intelligent and Responsible Reinforcement Learning",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Personalized decision systems in healthcare and behavioral support often rely on static rule-based or engagement-maximizing heuristics that overlook users' emotional context and ethical constraints. Such approaches risk recommending insensitive or unsafe interventions, especially in domains involving serious mental illness, substance use disorders, or depression. To address this limitation, we propose a Responsible Reinforcement Learning (RRL) framework that integrates emotional and contextual understanding with ethical considerations into the sequential decision-making process. RRL formulates personalization as a Constrained Markov Decision Process (CMDP), where the agent optimizes engagement and adherence while ensuring emotional alignment and ethical safety. We introduce a multi-objective reward function that explicitly balances short-term behavioral engagement with long-term user well-being, and define an emotion-informed state representation that captures fluctuations in emotional readiness, affect, and risk. The proposed architecture can be instantiated with any RL algorithm (e.g., DQN, PPO) augmented with safety constraints or Lagrangian regularization. Conceptually, this framework operationalizes empathy and responsibility within machine learning policy optimization, bridging safe RL, affective computing and responsible AI. We discuss the implications of this approach for human-centric domains such as behavioral health, education, and digital therapeutics, and outline simulation-based validation paths for future empirical work. This paper aims to initiate a methodological conversation about ethically aligned reinforcement learning for emotionally aware and trustworthy personalization systems.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10573v1",
        "pdf": "https://arxiv.org/pdf/2511.10573v1"
      },
      "arxiv_id": "2511.10573v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10572v1",
      "title": "Bi-Level Contextual Bandits for Individualized Resource Allocation under Delayed Feedback",
      "authors": [
        "Mohammadsina Almasi",
        "Hadis Anahideh"
      ],
      "abstract": "Equitably allocating limited resources in high-stakes domains-such as education, employment, and healthcare-requires balancing short-term utility with long-term impact, while accounting for delayed outcomes, hidden heterogeneity, and ethical constraints. However, most learning-based allocation frameworks either assume immediate feedback or ignore the complex interplay between individual characteristics and intervention dynamics. We propose a novel bi-level contextual bandit framework for individualized resource allocation under delayed feedback, designed to operate in real-world settings with dynamic populations, capacity constraints, and time-sensitive impact. At the meta level, the model optimizes subgroup-level budget allocations to satisfy fairness and operational constraints. At the base level, it identifies the most responsive individuals within each group using a neural network trained on observational data, while respecting cooldown windows and delayed treatment effects modeled via resource-specific delay kernels. By explicitly modeling temporal dynamics and feedback delays, the algorithm continually refines its policy as new data arrive, enabling more responsive and adaptive decision-making. We validate our approach on two real-world datasets from education and workforce development, showing that it achieves higher cumulative outcomes, better adapts to delay structures, and ensures equitable distribution across subgroups. Our results highlight the potential of delay-aware, data-driven decision-making systems to improve institutional policy and social welfare.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10572v1",
        "pdf": "https://arxiv.org/pdf/2511.10572v1"
      },
      "arxiv_id": "2511.10572v1",
      "comment": "Accepted at AAAI-26 (AISI Track). Final version to appear in the Proceedings of the AAAI Conference on Artificial Intelligence (AAAI-26), 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10571v1",
      "title": "Belief Net: A Filter-Based Framework for Learning Hidden Markov Models from Observations",
      "authors": [
        "Reginald Zhiyan Chen",
        "Heng-Sheng Chang",
        "Prashant G. Mehta"
      ],
      "abstract": "Hidden Markov Models (HMMs) are fundamental for modeling sequential data, yet learning their parameters from observations remains challenging. Classical methods like the Baum-Welch (EM) algorithm are computationally intensive and prone to local optima, while modern spectral algorithms offer provable guarantees but may produce probability outputs outside valid ranges. This work introduces Belief Net, a novel framework that learns HMM parameters through gradient-based optimization by formulating the HMM's forward filter as a structured neural network. Unlike black-box Transformer models, Belief Net's learnable weights are explicitly the logits of the initial distribution, transition matrix, and emission matrix, ensuring full interpretability. The model processes observation sequences using a decoder-only architecture and is trained end-to-end with standard autoregressive next-observation prediction loss. On synthetic HMM data, Belief Net achieves superior convergence speed compared to Baum-Welch, successfully recovering parameters in both undercomplete and overcomplete settings where spectral methods fail. Comparisons with Transformer-based models are also presented on real-world language data.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG",
        "eess.SY",
        "math.PR"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10571v1",
        "pdf": "https://arxiv.org/pdf/2511.10571v1"
      },
      "arxiv_id": "2511.10571v1",
      "comment": "19 pages, 7 pages, submitted to conference: L4DC 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10566v1",
      "title": "Impact of Layer Norm on Memorization and Generalization in Transformers",
      "authors": [
        "Rishi Singhal",
        "Jung-Eun Kim"
      ],
      "abstract": "Layer Normalization (LayerNorm) is one of the fundamental components in transformers that stabilizes training and improves optimization. In recent times, Pre-LayerNorm transformers have become the preferred choice over Post-LayerNorm transformers due to their stable gradient flow. However, the impact of LayerNorm on learning and memorization across these architectures remains unclear. In this work, we investigate how LayerNorm influences memorization and learning for Pre- and Post-LayerNorm transformers. We identify that LayerNorm serves as a key factor for stable learning in Pre-LayerNorm transformers, while in Post-LayerNorm transformers, it impacts memorization. Our analysis reveals that eliminating LayerNorm parameters in Pre-LayerNorm models exacerbates memorization and destabilizes learning, while in Post-LayerNorm models, it effectively mitigates memorization by restoring genuine labels. We further precisely identify that early layers LayerNorm are the most critical over middle/later layers and their influence varies across Pre and Post LayerNorm models. We have validated it through 13 models across 6 Vision and Language datasets. These insights shed new light on the role of LayerNorm in shaping memorization and learning in transformers.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10566v1",
        "pdf": "https://arxiv.org/pdf/2511.10566v1"
      },
      "arxiv_id": "2511.10566v1",
      "comment": "NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10562v1",
      "title": "Oya: Deep Learning for Accurate Global Precipitation Estimation",
      "authors": [
        "Emmanuel Asiedu Brempong",
        "Mohammed Alewi Hassen",
        "MohamedElfatih MohamedKhair",
        "Vusumuzi Dube",
        "Santiago Hincapie Potes",
        "Olivia Graham",
        "Amanie Brik",
        "Amy McGovern",
        "George Huffman",
        "Jason Hickey"
      ],
      "abstract": "Accurate precipitation estimation is critical for hydrological applications, especially in the Global South where ground-based observation networks are sparse and forecasting skill is limited. Existing satellite-based precipitation products often rely on the longwave infrared channel alone or are calibrated with data that can introduce significant errors, particularly at sub-daily timescales. This study introduces Oya, a novel real-time precipitation retrieval algorithm utilizing the full spectrum of visible and infrared (VIS-IR) observations from geostationary (GEO) satellites. Oya employs a two-stage deep learning approach, combining two U-Net models: one for precipitation detection and another for quantitative precipitation estimation (QPE), to address the inherent data imbalance between rain and no-rain events. The models are trained using high-resolution GPM Combined Radar-Radiometer Algorithm (CORRA) v07 data as ground truth and pre-trained on IMERG-Final retrievals to enhance robustness and mitigate overfitting due to the limited temporal sampling of CORRA. By leveraging multiple GEO satellites, Oya achieves quasi-global coverage and demonstrates superior performance compared to existing competitive regional and global precipitation baselines, offering a promising pathway to improved precipitation monitoring and forecasting.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10562v1",
        "pdf": "https://arxiv.org/pdf/2511.10562v1"
      },
      "arxiv_id": "2511.10562v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10561v1",
      "title": "Maximizing Efficiency of Dataset Compression for Machine Learning Potentials With Information Theory",
      "authors": [
        "Benjamin Yu",
        "Vincenzo Lordi",
        "Daniel Schwalbe-Koda"
      ],
      "abstract": "Machine learning interatomic potentials (MLIPs) balance high accuracy and lower costs compared to density functional theory calculations, but their performance often depends on the size and diversity of training datasets. Large datasets improve model accuracy and generalization but are computationally expensive to produce and train on, while smaller datasets risk discarding rare but important atomic environments and compromising MLIP accuracy/reliability. Here, we develop an information-theoretical framework to quantify the efficiency of dataset compression methods and propose an algorithm that maximizes this efficiency. By framing atomistic dataset compression as an instance of the minimum set cover (MSC) problem over atom-centered environments, our method identifies the smallest subset of structures that contains as much information as possible from the original dataset while pruning redundant information. The approach is extensively demonstrated on the GAP-20 and TM23 datasets, and validated on 64 varied datasets from the ColabFit repository. Across all cases, MSC consistently retains outliers, preserves dataset diversity, and reproduces the long-tail distributions of forces even at high compression rates, outperforming other subsampling methods. Furthermore, MLIPs trained on MSC-compressed datasets exhibit reduced error for out-of-distribution data even in low-data regimes. We explain these results using an outlier analysis and show that such quantitative conclusions could not be achieved with conventional dimensionality reduction methods. The algorithm is implemented in the open-source QUESTS package and can be used for several tasks in atomistic modeling, from data subsampling, outlier detection, and training improved MLIPs at a lower cost.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10561v1",
        "pdf": "https://arxiv.org/pdf/2511.10561v1"
      },
      "arxiv_id": "2511.10561v1",
      "comment": "main text + SI; code at https://github.com/dskoda/quests",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.10560v1",
      "title": "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded",
      "authors": [
        "Haosong Peng",
        "Hao Li",
        "Yalun Dai",
        "Yushi Lan",
        "Yihang Luo",
        "Tianyu Qi",
        "Zhengshen Zhang",
        "Yufeng Zhan",
        "Junfei Zhang",
        "Wenchao Xu",
        "Ziwei Liu"
      ],
      "abstract": "General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10560v1",
        "pdf": "https://arxiv.org/pdf/2511.10560v1"
      },
      "arxiv_id": "2511.10560v1",
      "comment": "Project Page: https://livioni.github.io/OmniVGGT-offcial/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.10555v1",
      "title": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space",
      "authors": [
        "Huijie Liu",
        "Shuhao Cui",
        "Haoxiang Cao",
        "Shuai Ma",
        "Kai Wu",
        "Guoliang Kang"
      ],
      "abstract": "Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10555v1",
        "pdf": "https://arxiv.org/pdf/2511.10555v1"
      },
      "arxiv_id": "2511.10555v1",
      "comment": "16 pages, 13 figures, 5 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10547v1",
      "title": "Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation",
      "authors": [
        "Isabela Albuquerque",
        "Ira Ktena",
        "Olivia Wiles",
        "Ivana Kajić",
        "Amal Rannen-Triki",
        "Cristina Vasconcelos",
        "Aida Nematzadeh"
      ],
      "abstract": "Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.\n  Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10547v1",
        "pdf": "https://arxiv.org/pdf/2511.10547v1"
      },
      "arxiv_id": "2511.10547v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10542v1",
      "title": "Two Americas of Well-Being: Divergent Rural-Urban Patterns of Life Satisfaction and Happiness from 2.6 B Social Media Posts",
      "authors": [
        "Stefano Maria Iacus",
        "Giuseppe Porro"
      ],
      "abstract": "Using 2.6 billion geolocated social-media posts (2014-2022) and a fine-tuned generative language model, we construct county-level indicators of life satisfaction and happiness for the United States. We document an apparent rural-urban paradox: rural counties express higher life satisfaction while urban counties exhibit greater happiness. We reconcile this by treating the two as distinct layers of subjective well-being, evaluative vs. hedonic, showing that each maps differently onto place, politics, and time. Republican-leaning areas appear more satisfied in evaluative terms, but partisan gaps in happiness largely flatten outside major metros, indicating context-dependent political effects. Temporal shocks dominate the hedonic layer: happiness falls sharply during 2020-2022, whereas life satisfaction moves more modestly. These patterns are robust across logistic and OLS specifications and align with well-being theory. Interpreted as associations for the population of social-media posts, the results show that large-scale, language-based indicators can resolve conflicting findings about the rural-urban divide by distinguishing the type of well-being expressed, offering a transparent, reproducible complement to traditional surveys.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.SI",
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "cs.SI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10542v1",
        "pdf": "https://arxiv.org/pdf/2511.10542v1"
      },
      "arxiv_id": "2511.10542v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10540v1",
      "title": "Edge Machine Learning for Cluster Counting in Next-Generation Drift Chambers",
      "authors": [
        "Deniz Yilmaz",
        "Liangyu Wu",
        "Julia Gonski"
      ],
      "abstract": "Drift chambers have long been central to collider tracking, but future machines like a Higgs factory motivate higher granularity and cluster counting for particle ID, posing new data processing challenges. Machine learning (ML) at the \"edge\", or in cell-level readout, can dramatically reduce the off-detector data rate for high-granularity drift chambers by performing cluster counting at-source. We present machine learning algorithms for cluster counting in real-time readout of future drift chambers. These algorithms outperform traditional derivative-based techniques based on achievable pion-kaon separation. When synthesized to FPGA resources, they can achieve latencies consistent with real-time operation in a future Higgs factory scenario, thus advancing both R&D for future collider detectors as well as hardware-based ML for edge applications in high energy physics.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "physics.ins-det",
        "cs.LG"
      ],
      "primary_category": "physics.ins-det",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10540v1",
        "pdf": "https://arxiv.org/pdf/2511.10540v1"
      },
      "arxiv_id": "2511.10540v1",
      "comment": "6 pages, 3 figures, 1 table. Machine Learning and the Physical Sciences Workshop, NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10539v1",
      "title": "Dynamic Avatar-Scene Rendering from Human-centric Context",
      "authors": [
        "Wenqing Wang",
        "Haosen Yang",
        "Josef Kittler",
        "Xiatian Zhu"
      ],
      "abstract": "Reconstructing dynamic humans interacting with real-world environments from monocular videos is an important and challenging task. Despite considerable progress in 4D neural rendering, existing approaches either model dynamic scenes holistically or model scenes and backgrounds separately aim to introduce parametric human priors. However, these approaches either neglect distinct motion characteristics of various components in scene especially human, leading to incomplete reconstructions, or ignore the information exchange between the separately modeled components, resulting in spatial inconsistencies and visual artifacts at human-scene boundaries. To address this, we propose {\\bf Separate-then-Map} (StM) strategy that introduces a dedicated information mapping mechanism to bridge separately defined and optimized models. Our method employs a shared transformation function for each Gaussian attribute to unify separately modeled components, enhancing computational efficiency by avoiding exhaustive pairwise interactions while ensuring spatial and visual coherence between humans and their surroundings. Extensive experiments on monocular video datasets demonstrate that StM significantly outperforms existing state-of-the-art methods in both visual quality and rendering accuracy, particularly at challenging human-scene interaction boundaries.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10539v1",
        "pdf": "https://arxiv.org/pdf/2511.10539v1"
      },
      "arxiv_id": "2511.10539v1",
      "comment": "13 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10532v1",
      "title": "Preview, Accept or Discard? A Predictive Low-Motion Interaction Paradigm",
      "authors": [
        "Jose Berengueres"
      ],
      "abstract": "Repetitive strain injury (RSI) affects roughly one in five computer users and remains largely unresolved despite decades of ergonomic mouse redesign. All such devices share a fundamental limitation: they still require fine-motor motion to operate. This work investigates whether predictive, AI-assisted input can reduce that motion by replacing physical pointing with ranked on-screen suggestions. To preserve user agency, we introduce Preview Accept Discard (PAD), a zero-click interaction paradigm that lets users preview predicted GUI targets, cycle through a small set of ranked alternatives, and accept or discard them via key-release timing. We evaluate PAD in two settings: a browser-based email client and a ISO 9241-9 keyboard-prediction task under varying top-3 accuracies. Across both studies, PAD substantially reduces hand motion relative to trackpad use while maintaining comparable task times with the trackpad only when accuracies are similar to those of the best spell-checkers.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10532v1",
        "pdf": "https://arxiv.org/pdf/2511.10532v1"
      },
      "arxiv_id": "2511.10532v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10524v1",
      "title": "Rethinking Science in the Age of Artificial Intelligence",
      "authors": [
        "Maksim E. Eren",
        "Dorianis M. Perez"
      ],
      "abstract": "Artificial intelligence (AI) is reshaping how research is conceived, conducted, and communicated across fields from chemistry to biomedicine. This commentary examines how AI is transforming the research workflow. AI systems now help researchers manage the information deluge, filtering the literature, surfacing cross-disciplinary links for ideas and collaborations, generating hypotheses, and designing and executing experiments. These developments mark a shift from AI as a mere computational tool to AI as an active collaborator in science. Yet this transformation demands thoughtful integration and governance. We argue that at this time AI must augment but not replace human judgment in academic workflows such as peer review, ethical evaluation, and validation of results. This paper calls for the deliberate adoption of AI within the scientific practice through policies that promote transparency, reproducibility, and accountability.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10524v1",
        "pdf": "https://arxiv.org/pdf/2511.10524v1"
      },
      "arxiv_id": "2511.10524v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10519v1",
      "title": "Say It Differently: Linguistic Styles as Jailbreak Vectors",
      "authors": [
        "Srikant Panda",
        "Avinash Rai"
      ],
      "abstract": "Large Language Models (LLMs) are commonly evaluated for robustness against paraphrased or semantically equivalent jailbreak prompts, yet little attention has been paid to linguistic variation as an attack surface. In this work, we systematically study how linguistic styles such as fear or curiosity can reframe harmful intent and elicit unsafe responses from aligned models. We construct style-augmented jailbreak benchmark by transforming prompts from 3 standard datasets into 11 distinct linguistic styles using handcrafted templates and LLM-based rewrites, while preserving semantic intent. Evaluating 16 open- and close-source instruction-tuned models, we find that stylistic reframing increases jailbreak success rates by up to +57 percentage points. Styles such as fearful, curious and compassionate are most effective and contextualized rewrites outperform templated variants.\n  To mitigate this, we introduce a style neutralization preprocessing step using a secondary LLM to strip manipulative stylistic cues from user inputs, significantly reducing jailbreak success rates. Our findings reveal a systemic and scaling-resistant vulnerability overlooked in current safety pipelines.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10519v1",
        "pdf": "https://arxiv.org/pdf/2511.10519v1"
      },
      "arxiv_id": "2511.10519v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10518v1",
      "title": "SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation",
      "authors": [
        "Wei Li",
        "Renshan Zhang",
        "Rui Shao",
        "Zhijian Fang",
        "Kaiwen Zhou",
        "Zhuotao Tian",
        "Liqiang Nie"
      ],
      "abstract": "Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10518v1",
        "pdf": "https://arxiv.org/pdf/2511.10518v1"
      },
      "arxiv_id": "2511.10518v1",
      "comment": "Accepted to AAAI 2026 (Oral), Project Page: https://github.com/JiuTian-VL/SemanticVLA",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.10515v1",
      "title": "LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025",
      "authors": [
        "Dong-Shan Jian",
        "Xiang Li",
        "Chen-Xu Yan",
        "Hui-Wen Zheng",
        "Zhi-Zhang Bian",
        "You-Le Fang",
        "Sheng-Qi Zhang",
        "Bing-Rui Gong",
        "Ren-Xi He",
        "Jing-Tian Zhang",
        "Ce Meng",
        "Yan-Qing Ma"
      ],
      "abstract": "Olympiad-level physics problem-solving presents a significant challenge for both humans and artificial intelligence (AI), as it requires a sophisticated integration of precise calculation, abstract reasoning, and a fundamental grasp of physical principles. The Chinese Physics Olympiad (CPhO), renowned for its complexity and depth, serves as an ideal and rigorous testbed for these advanced capabilities. In this paper, we introduce LOCA-R (LOgical Chain Augmentation for Reasoning), an improved version of the LOCA framework adapted for complex reasoning, and apply it to the CPhO 2025 theory examination. LOCA-R achieves a near-perfect score of 313 out of 320 points, solidly surpassing the highest-scoring human competitor and significantly outperforming all baseline methods.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CL",
        "cs.AI",
        "physics.ed-ph"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10515v1",
        "pdf": "https://arxiv.org/pdf/2511.10515v1"
      },
      "arxiv_id": "2511.10515v1",
      "comment": "19 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10504v1",
      "title": "Holonorm",
      "authors": [
        "Daryl Noupa Yongueng",
        "Hamidou Tembine"
      ],
      "abstract": "Normalization is a key point in transformer training . In Dynamic Tanh (DyT), the author demonstrated that Tanh can be used as an alternative layer normalization (LN) and confirmed the effectiveness of the idea. But Tanh itself faces orthogonality, linearity and distortion problems. Due to that, his proposition cannot be reliable. So we propose a Holonorm (hn) which has residual connections and nonlinearity. Holonorm is suitable for replacing Tanh in the context of normalization. Although the HoloNorm expression could be similar to the softsign function in dimension one, softsign is a componentwise function which is not good for tensors and vectors of great dimension. Holonorm preserves the orthogonality, the direction, the invertibility of the signal. Holonorm is also a suitable metric, maps all vectors into the open unit ball. This prevents exploding activations and improves stability in deep Transformer models. In this work, we have meticulously examined the normalization in transformers and say that Holonorm, a generalized form of softsign function suited as a normalization function first.Second, defined between 0 and 1 hn serves as a percentage, and $1 - \\text{Holonorm}$ is its complement, making it better understandable in evaluating a model.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10504v1",
        "pdf": "https://arxiv.org/pdf/2511.10504v1"
      },
      "arxiv_id": "2511.10504v1",
      "comment": "17 pages, 11 figures, 10 tables, 2 datasets. A stable geometric alternative to LayerNorm and Tanh normalization in deep neural networks",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10502v1",
      "title": "On the Detectability of Active Gradient Inversion Attacks in Federated Learning",
      "authors": [
        "Vincenzo Carletti",
        "Pasquale Foggia",
        "Carlo Mazzocca",
        "Giuseppe Parrella",
        "Mario Vento"
      ],
      "abstract": "One of the key advantages of Federated Learning (FL) is its ability to collaboratively train a Machine Learning (ML) model while keeping clients' data on-site. However, this can create a false sense of security. Despite not sharing private data increases the overall privacy, prior studies have shown that gradients exchanged during the FL training remain vulnerable to Gradient Inversion Attacks (GIAs). These attacks allow reconstructing the clients' local data, breaking the privacy promise of FL. GIAs can be launched by either a passive or an active server. In the latter case, a malicious server manipulates the global model to facilitate data reconstruction. While effective, earlier attacks falling under this category have been demonstrated to be detectable by clients, limiting their real-world applicability. Recently, novel active GIAs have emerged, claiming to be far stealthier than previous approaches. This work provides the first comprehensive analysis of these claims, investigating four state-of-the-art GIAs. We propose novel lightweight client-side detection techniques, based on statistically improbable weight structures and anomalous loss and gradient dynamics. Extensive evaluation across several configurations demonstrates that our methods enable clients to effectively detect active GIAs without any modifications to the FL training protocol.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10502v1",
        "pdf": "https://arxiv.org/pdf/2511.10502v1"
      },
      "arxiv_id": "2511.10502v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10501v1",
      "title": "Strategic Opponent Modeling with Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling",
      "authors": [
        "Georgios Chalkiadakis",
        "Charilaos Akasiadis",
        "Gerasimos Koresis",
        "Stergios Plataniots",
        "Leonidas Bakopoulos"
      ],
      "abstract": "This paper provides a comprehensive review of mainly Graph Neural Networks, Deep Reinforcement Learning, and Probabilistic Topic Modeling methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) Machine Learning methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of Graph Neural Networks (GNN). Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of Reinforcement Learning (RL), and in particular that of Multiagent Deep Reinforcement Learning (MADRL). Following, we describe existing relevant game theoretic solution concepts and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes PTM in domains other than that of document analysis and classification. The capability of PTM to estimate unknown underlying distributions can help with tackling heterogeneity and unknown agent beliefs. Finally, we identify certain open challenges specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10501v1",
        "pdf": "https://arxiv.org/pdf/2511.10501v1"
      },
      "arxiv_id": "2511.10501v1",
      "comment": "26 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10500v1",
      "title": "Learnable Total Variation with Lambda Mapping for Low-Dose CT Denoising",
      "authors": [
        "Yusuf Talha Basak",
        "Mehmet Ozan Unal",
        "Metin Ertas",
        "Isa Yildirim"
      ],
      "abstract": "Although Total Variation (TV) performs well in noise reduction and edge preservation on images, its dependence on the lambda parameter limits its efficiency and makes it difficult to use effectively. In this study, we present a Learnable Total Variation (LTV) framework that couples an unrolled TV solver with a data-driven Lambda Mapping Network (LambdaNet) predicting a per-pixel regularization map. The pipeline is trained end-to-end so that reconstruction and regularization are optimized jointly, yielding spatially adaptive smoothing: strong in homogeneous regions, relaxed near anatomical boundaries. Experiments on the DeepLesion dataset, using a realistic noise model adapted from the LoDoPaB-CT methodology, show consistent gains over classical TV and FBP+U-Net: +2.9 dB PSNR and +6% SSIM on average. LTV provides an interpretable alternative to black-box CNNs and a basis for 3D and data-consistency-driven reconstruction. Our codes are available at: https://github.com/itu-biai/deep_tv_for_ldct",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10500v1",
        "pdf": "https://arxiv.org/pdf/2511.10500v1"
      },
      "arxiv_id": "2511.10500v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10494v1",
      "title": "Weak Relation Enforcement for Kinematic-Informed Long-Term Stock Prediction with Artificial Neural Networks",
      "authors": [
        "Stanislav Selitskiy"
      ],
      "abstract": "We propose loss function week enforcement of the velocity relations between time-series points in the Kinematic-Informed artificial Neural Networks (KINN) for long-term stock prediction. Problems of the series volatility, Out-of-Distribution (OOD) test data, and outliers in training data are addressed by (Artificial Neural Networks) ANN's learning not only future points prediction but also by learning velocity relations between the points, such a way as avoiding unrealistic spurious predictions. The presented loss function penalizes not only errors between predictions and supervised label data, but also errors between the next point prediction and the previous point plus velocity prediction. The loss function is tested on the multiple popular and exotic AR ANN architectures, and around fifteen years of Dow Jones function demonstrated statistically meaningful improvement across the normalization-sensitive activation functions prone to spurious behaviour in the OOD data conditions. Results show that such architecture addresses the issue of the normalization in the auto-regressive models that break the data topology by weakly enforcing the data neighbourhood proximity (relation) preservation during the ANN transformation.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10494v1",
        "pdf": "https://arxiv.org/pdf/2511.10494v1"
      },
      "arxiv_id": "2511.10494v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10492v1",
      "title": "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding",
      "authors": [
        "Yunkai Zhang",
        "Qiang Zhang",
        "Feng",
        "Lin",
        "Ruizhong Qiu",
        "Hanchao Yu",
        "Jason Liu",
        "Yinglong Xia",
        "Zhuoran Yu",
        "Zeyu Zheng",
        "Diji Yang"
      ],
      "abstract": "Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.\n  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10492v1",
        "pdf": "https://arxiv.org/pdf/2511.10492v1"
      },
      "arxiv_id": "2511.10492v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10488v1",
      "title": "SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers",
      "authors": [
        "Oded Schlesinger",
        "Amirhossein Farzam",
        "J. Matias Di Martino",
        "Guillermo Sapiro"
      ],
      "abstract": "While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process. SPOT informs token sparsification and facilitates the elimination of such tokens, improving computational efficiency without sacrificing performance. SPOT employs computationally lightweight predictors that can be plugged into various ViT architectures and learn to derive effective input-specific token prioritization across layers. Its versatile design supports a range of performance levels adaptable to varying resource constraints. Empirical evaluations demonstrate significant efficiency gains of up to 40% compared to standard ViTs, while maintaining or even improving accuracy. Code and models are available at https://github.com/odedsc/SPOT .",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10488v1",
        "pdf": "https://arxiv.org/pdf/2511.10488v1"
      },
      "arxiv_id": "2511.10488v1",
      "comment": "Project repository: https://github.com/odedsc/SPOT",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.10484v1",
      "title": "Utility of Pancreas Surface Lobularity as a CT Biomarker for Opportunistic Screening of Type 2 Diabetes",
      "authors": [
        "Tejas Sudharshan Mathai",
        "Anisa V. Prasad",
        "Xinya Wang",
        "Praveen T. S. Balamuralikrishna",
        "Yan Zhuang",
        "Abhinav Suri",
        "Jianfei Liu",
        "Perry J. Pickhardt",
        "Ronald M. Summers"
      ],
      "abstract": "Type 2 Diabetes Mellitus (T2DM) is a chronic metabolic disease that affects millions of people worldwide. Early detection is crucial as it can alter pancreas function through morphological changes and increased deposition of ectopic fat, eventually leading to organ damage. While studies have shown an association between T2DM and pancreas volume and fat content, the role of increased pancreatic surface lobularity (PSL) in patients with T2DM has not been fully investigated. In this pilot work, we propose a fully automated approach to delineate the pancreas and other abdominal structures, derive CT imaging biomarkers, and opportunistically screen for T2DM. Four deep learning-based models were used to segment the pancreas in an internal dataset of 584 patients (297 males, 437 non-diabetic, age: 45$\\pm$15 years). PSL was automatically detected and it was higher for diabetic patients (p=0.01) at 4.26 $\\pm$ 8.32 compared to 3.19 $\\pm$ 3.62 for non-diabetic patients. The PancAP model achieved the highest Dice score of 0.79 $\\pm$ 0.17 and lowest ASSD error of 1.94 $\\pm$ 2.63 mm (p$<$0.05). For predicting T2DM, a multivariate model trained with CT biomarkers attained 0.90 AUC, 66.7\\% sensitivity, and 91.9\\% specificity. Our results suggest that PSL is useful for T2DM screening and could potentially help predict the early onset of T2DM.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10484v1",
        "pdf": "https://arxiv.org/pdf/2511.10484v1"
      },
      "arxiv_id": "2511.10484v1",
      "comment": "Submitted to IEEE ISBI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10482v1",
      "title": "Proceedings of The third international workshop on eXplainable AI for the Arts (XAIxArts)",
      "authors": [
        "Corey Ford",
        "Elizabeth Wilson",
        "Shuoyang Zheng",
        "Gabriel Vigliensoni",
        "Jeba Rezwana",
        "Lanxi Xiao",
        "Michael Clemens",
        "Makayla Lewis",
        "Drew Hemment",
        "Alan Chamberlain",
        "Helen Kennedy",
        "Nick Bryan-Kinns"
      ],
      "abstract": "This third international workshop on explainable AI for the Arts (XAIxArts) brought together a community of researchers in HCI, Interaction Design, AI, explainable AI (XAI), and digital arts to explore the role of XAI for the Arts. Workshop held at the 17th ACM Conference on Creativity and Cognition (C&C 2025), online.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10482v1",
        "pdf": "https://arxiv.org/pdf/2511.10482v1"
      },
      "arxiv_id": "2511.10482v1",
      "comment": "Proceedings of The second international workshop on eXplainable AI for the Arts (XAIxArts)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10481v1",
      "title": "Panda: Test-Time Adaptation with Negative Data Augmentation",
      "authors": [
        "Ruxi Deng",
        "Wenxuan Bao",
        "Tianxin Wei",
        "Jingrui He"
      ],
      "abstract": "Pretrained VLMs exhibit strong zero-shot classification capabilities, but their predictions degrade significantly under common image corruptions. To improve robustness, many test-time adaptation (TTA) methods adopt positive data augmentation (PDA), which generates multiple views of each test sample to reduce prediction variance. However, these methods suffer from two key limitations. First, it introduces considerable computational overhead due to the large number of augmentations required per image. Second, it fails to mitigate prediction bias, where the model tends to predict certain classes disproportionately under corruption, as PDA operates on corrupted inputs and typically does not remove the corruption itself. To address these challenges, we propose Panda, a novel TTA method based on negative data augmentation (NDA). Unlike positive augmentations that preserve object semantics, Panda generates negative augmentations by disrupting semantic content. It divides images into patches and randomly assembles them from a shared patch pool. These negatively augmented images retain corruption-specific features while discarding object-relevant signals. We then subtract the mean feature of these negative samples from the original image feature, effectively suppressing corruption-related components while preserving class-relevant information. This mitigates prediction bias under distribution shifts. Panda allows augmentation to be shared across samples within a batch, resulting in minimal computational overhead. Panda can be seamlessly integrated into existing test-time adaptation frameworks and substantially improve their robustness. Our experiments indicate that Panda delivers superior performance compared to PDA methods, and a wide range of TTA methods exhibit significantly enhanced performance when integrated with Panda. Our code is available at https://github.com/ruxideng/Panda .",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10481v1",
        "pdf": "https://arxiv.org/pdf/2511.10481v1"
      },
      "arxiv_id": "2511.10481v1",
      "comment": "Accepted by AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10480v1",
      "title": "Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs",
      "authors": [
        "Changhai Man",
        "Joongun Park",
        "Hanjiang Wu",
        "Huan Xu",
        "Srinivas Sridharan",
        "Tushar Krishna"
      ],
      "abstract": "Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10480v1",
        "pdf": "https://arxiv.org/pdf/2511.10480v1"
      },
      "arxiv_id": "2511.10480v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10475v1",
      "title": "Intrinsic Dimensionality as a Model-Free Measure of Class Imbalance",
      "authors": [
        "Çağrı Eser",
        "Zeynep Sonat Baltacı",
        "Emre Akbaş",
        "Sinan Kalkan"
      ],
      "abstract": "Imbalance in classification tasks is commonly quantified by the cardinalities of examples across classes. This, however, disregards the presence of redundant examples and inherent differences in the learning difficulties of classes. Alternatively, one can use complex measures such as training loss and uncertainty, which, however, depend on training a machine learning model. Our paper proposes using data Intrinsic Dimensionality (ID) as an easy-to-compute, model-free measure of imbalance that can be seamlessly incorporated into various imbalance mitigation methods. Our results across five different datasets with a diverse range of imbalance ratios show that ID consistently outperforms cardinality-based re-weighting and re-sampling techniques used in the literature. Moreover, we show that combining ID with cardinality can further improve performance. Code: https://github.com/cagries/IDIM.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10475v1",
        "pdf": "https://arxiv.org/pdf/2511.10475v1"
      },
      "arxiv_id": "2511.10475v1",
      "comment": "45 pages, 11 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10465v1",
      "title": "Beyond Elicitation: Provision-based Prompt Optimization for Knowledge-Intensive Tasks",
      "authors": [
        "Yunzhe Xu",
        "Zhuosheng Zhang",
        "Zhe Liu"
      ],
      "abstract": "While prompt optimization has emerged as a critical technique for enhancing language model performance, existing approaches primarily focus on elicitation-based strategies that search for optimal prompts to activate models' capabilities. These methods exhibit fundamental limitations when addressing knowledge-intensive tasks, as they operate within fixed parametric boundaries rather than providing the factual knowledge, terminology precision, and reasoning patterns required in specialized domains. To address these limitations, we propose Knowledge-Provision-based Prompt Optimization (KPPO), a framework that reformulates prompt optimization as systematic knowledge integration rather than potential elicitation. KPPO introduces three key innovations: 1) a knowledge gap filling mechanism for knowledge gap identification and targeted remediation; 2) a batch-wise candidate evaluation approach that considers both performance improvement and distributional stability; 3) an adaptive knowledge pruning strategy that balances performance and token efficiency, reducing up to 29% token usage. Extensive evaluation on 15 knowledge-intensive benchmarks from various domains demonstrates KPPO's superiority over elicitation-based methods, with an average performance improvement of ~6% over the strongest baseline while achieving comparable or lower token consumption. Code at: https://github.com/xyz9911/KPPO.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10465v1",
        "pdf": "https://arxiv.org/pdf/2511.10465v1"
      },
      "arxiv_id": "2511.10465v1",
      "comment": "16 pages, 19 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10461v1",
      "title": "OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data",
      "authors": [
        "Simon Donike",
        "Cesar Aybar",
        "Julio Contreras",
        "Luis Gómez-Chova"
      ],
      "abstract": "We present OpenSR-SRGAN, an open and modular framework for single-image super-resolution in Earth Observation. The software provides a unified implementation of SRGAN-style models that is easy to configure, extend, and apply to multispectral satellite data such as Sentinel-2. Instead of requiring users to modify model code, OpenSR-SRGAN exposes generators, discriminators, loss functions, and training schedules through concise configuration files, making it straightforward to switch between architectures, scale factors, and band setups. The framework is designed as a practical tool and benchmark implementation rather than a state-of-the-art model. It ships with ready-to-use configurations for common remote sensing scenarios, sensible default settings for adversarial training, and built-in hooks for logging, validation, and large-scene inference. By turning GAN-based super-resolution into a configuration-driven workflow, OpenSR-SRGAN lowers the entry barrier for researchers and practitioners who wish to experiment with SRGANs, compare models in a reproducible way, and deploy super-resolution pipelines across diverse Earth-observation datasets.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10461v1",
        "pdf": "https://arxiv.org/pdf/2511.10461v1"
      },
      "arxiv_id": "2511.10461v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10459v1",
      "title": "LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning",
      "authors": [
        "Zihan Gao",
        "Yifei Xu",
        "Jacob Thebault-Spieker"
      ],
      "abstract": "Large language models (LLMs) have been widely evaluated on macro-scale geographic tasks, such as global factual recall, event summarization, and regional reasoning. Yet, their ability to handle hyper-local knowledge remains poorly understood. This gap is increasingly consequential as real-world applications, from civic platforms to community journalism, demand AI systems that can reason about neighborhood-specific dynamics, cultural narratives, and local governance. Existing benchmarks fall short in capturing this complexity, often relying on coarse-grained data or isolated references. We present LocalBench, the first benchmark designed to systematically evaluate LLMs on county-level local knowledge across the United States. Grounded in the Localness Conceptual Framework, LocalBench includes 14,782 validated question-answer pairs across 526 U.S. counties in 49 states, integrating diverse sources such as Census statistics, local subreddit discourse, and regional news. It spans physical, cognitive, and relational dimensions of locality. Using LocalBench, we evaluate 13 state-of-the-art LLMs under both closed-book and web-augmented settings. Our findings reveal critical limitations: even the best-performing models reach only 56.8% accuracy on narrative-style questions and perform below 15.5% on numerical reasoning. Moreover, larger model size and web augmentation do not guarantee better performance, for example, search improves Gemini's accuracy by +13.6%, but reduces GPT-series performance by -11.4%. These results underscore the urgent need for language models that can support equitable, place-aware AI systems: capable of engaging with the diverse, fine-grained realities of local communities across geographic and cultural contexts.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10459v1",
        "pdf": "https://arxiv.org/pdf/2511.10459v1"
      },
      "arxiv_id": "2511.10459v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10453v1",
      "title": "Reasoning About Intent for Ambiguous Requests",
      "authors": [
        "Irina Saparina",
        "Mirella Lapata"
      ],
      "abstract": "Large language models often respond to ambiguous requests by implicitly committing to one interpretation. Intent misunderstandings can frustrate users and create safety risks. To address this, we propose generating multiple interpretation-answer pairs in a single structured response to ambiguous requests. Our models are trained with reinforcement learning and customized reward functions using multiple valid answers as supervision. Experiments on conversational question answering and semantic parsing demonstrate that our method achieves higher coverage of valid answers than baseline approaches. Human evaluation confirms that predicted interpretations are highly aligned with their answers. Our approach promotes transparency with explicit interpretations, achieves efficiency by requiring only one generation step, and supports downstream applications through its structured output format.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10453v1",
        "pdf": "https://arxiv.org/pdf/2511.10453v1"
      },
      "arxiv_id": "2511.10453v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10449v1",
      "title": "Non-Monotonic S4F Standpoint Logic",
      "authors": [
        "Piotr Gorczyca",
        "Hannes Strass"
      ],
      "abstract": "Standpoint logics offer unified modal logic-based formalisms for representing multiple heterogeneous viewpoints. At the same time, many non-monotonic reasoning frameworks can be naturally captured using modal logics, in particular using the modal logic S4F. In this work, we propose a novel formalism called S4F Standpoint Logic, which generalises both S4F and standpoint propositional logic and is therefore capable of expressing multi-viewpoint, non-monotonic semantic commitments. We define its syntax and semantics and analyze its computational complexity, obtaining the result that S4F Standpoint Logic is not computationally harder than its constituent logics, whether in monotonic or non-monotonic form. We also outline mechanisms for credulous and sceptical acceptance and illustrate the framework with an example.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10449v1",
        "pdf": "https://arxiv.org/pdf/2511.10449v1"
      },
      "arxiv_id": "2511.10449v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10446v1",
      "title": "Continuum Dropout for Neural Differential Equations",
      "authors": [
        "Jonghun Lee",
        "YongKyung Oh",
        "Sungil Kim",
        "Dong-Young Lim"
      ],
      "abstract": "Neural Differential Equations (NDEs) excel at modeling continuous-time dynamics, effectively handling challenges such as irregular observations, missing values, and noise. Despite their advantages, NDEs face a fundamental challenge in adopting dropout, a cornerstone of deep learning regularization, making them susceptible to overfitting. To address this research gap, we introduce Continuum Dropout, a universally applicable regularization technique for NDEs built upon the theory of alternating renewal processes. Continuum Dropout formulates the on-off mechanism of dropout as a stochastic process that alternates between active (evolution) and inactive (paused) states in continuous time. This provides a principled approach to prevent overfitting and enhance the generalization capabilities of NDEs. Moreover, Continuum Dropout offers a structured framework to quantify predictive uncertainty via Monte Carlo sampling at test time. Through extensive experiments, we demonstrate that Continuum Dropout outperforms existing regularization methods for NDEs, achieving superior performance on various time series and image classification tasks. It also yields better-calibrated and more trustworthy probability estimates, highlighting its effectiveness for uncertainty-aware modeling.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10446v1",
        "pdf": "https://arxiv.org/pdf/2511.10446v1"
      },
      "arxiv_id": "2511.10446v1",
      "comment": "",
      "journal_ref": "The Association for the Advancement of Artificial Intelligence 2026",
      "has_code": false
    },
    {
      "id": "2511.10440v1",
      "title": "Completion of partial structures using Patterson maps with the CrysFormer machine learning model",
      "authors": [
        "Tom Pan",
        "Evan Dramko",
        "Mitchell D. Miller",
        "Anastasios Kyrillidis",
        "George N. Phillips"
      ],
      "abstract": "Protein structure determination has long been one of the primary challenges of structural biology, to which deep machine learning (ML)-based approaches have increasingly been applied. However, these ML models generally do not incorporate the experimental measurements directly, such as X-ray crystallographic diffraction data. To this end, we explore an approach that more tightly couples these traditional crystallographic and recent ML-based methods, by training a hybrid 3-d vision transformer and convolutional network on inputs from both domains. We make use of two distinct input constructs / Patterson maps, which are directly obtainable from crystallographic data, and ``partial structure'' template maps derived from predicted structures deposited in the AlphaFold Protein Structure Database with subsequently omitted residues. With these, we predict electron density maps that are then post-processed into atomic models through standard crystallographic refinement processes. Introducing an initial dataset of small protein fragments taken from Protein Data Bank entries and placing them in hypothetical crystal settings, we demonstrate that our method is effective at both improving the phases of the crystallographic structure factors and completing the regions missing from partial structure templates, as well as improving the agreement of the electron density maps with the ground truth atomic structures.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "physics.bio-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.bio-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10440v1",
        "pdf": "https://arxiv.org/pdf/2511.10440v1"
      },
      "arxiv_id": "2511.10440v1",
      "comment": "15 pages, accepted at Acta Crystallographic section D",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10439v1",
      "title": "Improving Perturbation-based Explanations by Understanding the Role of Uncertainty Calibration",
      "authors": [
        "Thomas Decker",
        "Volker Tresp",
        "Florian Buettner"
      ],
      "abstract": "Perturbation-based explanations are widely utilized to enhance the transparency of machine-learning models in practice. However, their reliability is often compromised by the unknown model behavior under the specific perturbations used. This paper investigates the relationship between uncertainty calibration - the alignment of model confidence with actual accuracy - and perturbation-based explanations. We show that models systematically produce unreliable probability estimates when subjected to explainability-specific perturbations and theoretically prove that this directly undermines global and local explanation quality. To address this, we introduce ReCalX, a novel approach to recalibrate models for improved explanations while preserving their original predictions. Empirical evaluations across diverse models and datasets demonstrate that ReCalX consistently reduces perturbation-specific miscalibration most effectively while enhancing explanation robustness and the identification of globally important input features.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10439v1",
        "pdf": "https://arxiv.org/pdf/2511.10439v1"
      },
      "arxiv_id": "2511.10439v1",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10436v1",
      "title": "Preference Elicitation for Step-Wise Explanations in Logic Puzzles",
      "authors": [
        "Marco Foschini",
        "Marianne Defresne",
        "Emilio Gamba",
        "Bart Bogaerts",
        "Tias Guns"
      ],
      "abstract": "Step-wise explanations can explain logic puzzles and other satisfaction problems by showing how to derive decisions step by step. Each step consists of a set of constraints that derive an assignment to one or more decision variables. However, many candidate explanation steps exist, with different sets of constraints and different decisions they derive. To identify the most comprehensible one, a user-defined objective function is required to quantify the quality of each step. However, defining a good objective function is challenging. Here, interactive preference elicitation methods from the wider machine learning community can offer a way to learn user preferences from pairwise comparisons. We investigate the feasibility of this approach for step-wise explanations and address several limitations that distinguish it from elicitation for standard combinatorial problems. First, because the explanation quality is measured using multiple sub-objectives that can vary a lot in scale, we propose two dynamic normalization techniques to rescale these features and stabilize the learning process. We also observed that many generated comparisons involve similar explanations. For this reason, we introduce MACHOP (Multi-Armed CHOice Perceptron), a novel query generation strategy that integrates non-domination constraints with upper confidence bound-based diversification. We evaluate the elicitation techniques on Sudokus and Logic-Grid puzzles using artificial users, and validate them with a real-user evaluation. In both settings, MACHOP consistently produces higher-quality explanations than the standard approach.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10436v1",
        "pdf": "https://arxiv.org/pdf/2511.10436v1"
      },
      "arxiv_id": "2511.10436v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10435v1",
      "title": "Neuronal Fluctuations: Learning Rates vs Participating Neurons",
      "authors": [
        "Darsh Pareek",
        "Umesh Kumar",
        "Ruthu Rao",
        "Ravi Janjam"
      ],
      "abstract": "Deep Neural Networks (DNNs) rely on inherent fluctuations in their internal parameters (weights and biases) to effectively navigate the complex optimization landscape and achieve robust performance. While these fluctuations are recognized as crucial for escaping local minima and improving generalization, their precise relationship with fundamental hyperparameters remains underexplored. A significant knowledge gap exists concerning how the learning rate, a critical parameter governing the training process, directly influences the dynamics of these neural fluctuations. This study systematically investigates the impact of varying learning rates on the magnitude and character of weight and bias fluctuations within a neural network. We trained a model using distinct learning rates and analyzed the corresponding parameter fluctuations in conjunction with the network's final accuracy. Our findings aim to establish a clear link between the learning rate's value, the resulting fluctuation patterns, and overall model performance. By doing so, we provide deeper insights into the optimization process, shedding light on how the learning rate mediates the crucial exploration-exploitation trade-off during training. This work contributes to a more nuanced understanding of hyperparameter tuning and the underlying mechanics of deep learning.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10435v1",
        "pdf": "https://arxiv.org/pdf/2511.10435v1"
      },
      "arxiv_id": "2511.10435v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10434v1",
      "title": "Unlocking Dynamic Inter-Client Spatial Dependencies: A Federated Spatio-Temporal Graph Learning Method for Traffic Flow Forecasting",
      "authors": [
        "Feng Wang",
        "Tianxiang Chen",
        "Shuyue Wei",
        "Qian Chu",
        "Yi Zhang",
        "Yifan Sun",
        "Zhiming Zheng"
      ],
      "abstract": "Spatio-temporal graphs are powerful tools for modeling complex dependencies in traffic time series. However, the distributed nature of real-world traffic data across multiple stakeholders poses significant challenges in modeling and reconstructing inter-client spatial dependencies while adhering to data locality constraints. Existing methods primarily address static dependencies, overlooking their dynamic nature and resulting in suboptimal performance. In response, we propose Federated Spatio-Temporal Graph with Dynamic Inter-Client Dependencies (FedSTGD), a framework designed to model and reconstruct dynamic inter-client spatial dependencies in federated learning. FedSTGD incorporates a federated nonlinear computation decomposition module to approximate complex graph operations. This is complemented by a graph node embedding augmentation module, which alleviates performance degradation arising from the decomposition. These modules are coordinated through a client-server collective learning protocol, which decomposes dynamic inter-client spatial dependency learning tasks into lightweight, parallelizable subtasks. Extensive experiments on four real-world datasets demonstrate that FedSTGD achieves superior performance over state-of-the-art baselines in terms of RMSE, MAE, and MAPE, approaching that of centralized baselines. Ablation studies confirm the contribution of each module in addressing dynamic inter-client spatial dependencies, while sensitivity analysis highlights the robustness of FedSTGD to variations in hyperparameters.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10434v1",
        "pdf": "https://arxiv.org/pdf/2511.10434v1"
      },
      "arxiv_id": "2511.10434v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10432v1",
      "title": "Histology-informed tiling of whole tissue sections improves the interpretability and predictability of cancer relapse and genetic alterations",
      "authors": [
        "Willem Bonnaffé",
        "Yang Hu",
        "Andrea Chatrian",
        "Mengran Fan",
        "Stefano Malacrino",
        "Sandy Figiel",
        "CRUK ICGC Prostate Group",
        "Srinivasa R. Rao",
        "Richard Colling",
        "Richard J. Bryant",
        "Freddie C. Hamdy",
        "Dan J. Woodcock",
        "Ian G. Mills",
        "Clare Verrill",
        "Jens Rittscher"
      ],
      "abstract": "Histopathologists establish cancer grade by assessing histological structures, such as glands in prostate cancer. Yet, digital pathology pipelines often rely on grid-based tiling that ignores tissue architecture. This introduces irrelevant information and limits interpretability. We introduce histology-informed tiling (HIT), which uses semantic segmentation to extract glands from whole slide images (WSIs) as biologically meaningful input patches for multiple-instance learning (MIL) and phenotyping. Trained on 137 samples from the ProMPT cohort, HIT achieved a gland-level Dice score of 0.83 +/- 0.17. By extracting 380,000 glands from 760 WSIs across ICGC-C and TCGA-PRAD cohorts, HIT improved MIL models AUCs by 10% for detecting copy number variation (CNVs) in genes related to epithelial-mesenchymal transitions (EMT) and MYC, and revealed 15 gland clusters, several of which were associated with cancer relapse, oncogenic mutations, and high Gleason. Therefore, HIT improved the accuracy and interpretability of MIL predictions, while streamlining computations by focussing on biologically meaningful structures during feature extraction.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "q-bio.QM",
        "q-bio.TO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10432v1",
        "pdf": "https://arxiv.org/pdf/2511.10432v1"
      },
      "arxiv_id": "2511.10432v1",
      "comment": "26 pages, 6 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10431v1",
      "title": "RodEpil: A Video Dataset of Laboratory Rodents for Seizure Detection and Benchmark Evaluation",
      "authors": [
        "Daniele Perlo",
        "Vladimir Despotovic",
        "Selma Boudissa",
        "Sang-Yoon Kim",
        "Petr Nazarov",
        "Yanrong Zhang",
        "Max Wintermark",
        "Olivier Keunen"
      ],
      "abstract": "We introduce a curated video dataset of laboratory rodents for automatic detection of convulsive events. The dataset contains short (10~s) top-down and side-view video clips of individual rodents, labeled at clip level as normal activity or seizure. It includes 10,101 negative samples and 2,952 positive samples collected from 19 subjects. We describe the data curation, annotation protocol and preprocessing pipeline, and report baseline experiments using a transformer-based video classifier (TimeSformer). Experiments employ five-fold cross-validation with strict subject-wise partitioning to prevent data leakage (no subject appears in more than one fold). Results show that the TimeSformer architecture enables discrimination between seizure and normal activity with an average F1-score of 97%. The dataset and baseline code are publicly released to support reproducible research on non-invasive, video-based monitoring in preclinical epilepsy research. RodEpil Dataset access - DOI: 10.5281/zenodo.17601357",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10431v1",
        "pdf": "https://arxiv.org/pdf/2511.10431v1"
      },
      "arxiv_id": "2511.10431v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10428v1",
      "title": "Using Certifying Constraint Solvers for Generating Step-wise Explanations",
      "authors": [
        "Ignace Bleukx",
        "Maarten Flippo",
        "Bart Bogaerts",
        "Emir Demirović",
        "Tias Guns"
      ],
      "abstract": "In the field of Explainable Constraint Solving, it is common to explain to a user why a problem is unsatisfiable. A recently proposed method for this is to compute a sequence of explanation steps. Such a step-wise explanation shows individual reasoning steps involving constraints from the original specification, that in the end explain a conflict. However, computing a step-wise explanation is computationally expensive, limiting the scope of problems for which it can be used. We investigate how we can use proofs generated by a constraint solver as a starting point for computing step-wise explanations, instead of computing them step-by-step. More specifically, we define a framework of abstract proofs, in which both proofs and step-wise explanations can be represented. We then propose several methods for converting a proof to a step-wise explanation sequence, with special attention to trimming and simplification techniques to keep the sequence and its individual steps small. Our results show our method significantly speeds up the generation of step-wise explanation sequences, while the resulting step-wise explanation has a quality similar to the current state-of-the-art.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10428v1",
        "pdf": "https://arxiv.org/pdf/2511.10428v1"
      },
      "arxiv_id": "2511.10428v1",
      "comment": "Accepted for publication at AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10424v1",
      "title": "Domain Adaptation for Camera-Specific Image Characteristics using Shallow Discriminators",
      "authors": [
        "Maximiliane Gruber",
        "Jürgen Seiler",
        "André Kaup"
      ],
      "abstract": "Each image acquisition setup leads to its own camera-specific image characteristics degrading the image quality. In learning-based perception algorithms, characteristics occurring during the application phase, but absent in the training data, lead to a domain gap impeding the performance. Previously, pixel-level domain adaptation through unpaired learning of the pristine-to-distorted mapping function has been proposed. In this work, we propose shallow discriminator architectures to address limitations of these approaches. We show that a smaller receptive field size improves learning of unknown image distortions by more accurately reproducing local distortion characteristics at a low network complexity. In a domain adaptation setup for instance segmentation, we achieve mean average precision increases over previous methods of up to 0.15 for individual distortions and up to 0.16 for camera-specific image characteristics in a simplified camera model. In terms of number of parameters, our approach matches the complexity of one state of the art method while reducing complexity by a factor of 20 compared to another, demonstrating superior efficiency without compromising performance.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "eess.IV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10424v1",
        "pdf": "https://arxiv.org/pdf/2511.10424v1"
      },
      "arxiv_id": "2511.10424v1",
      "comment": "5 pages, 7 figures, accepted for International Conference on Visual Communications and Image Processing (VCIP) 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10416v1",
      "title": "Generalizing Analogical Inference from Boolean to Continuous Domains",
      "authors": [
        "Francisco Cunha",
        "Yves Lepage",
        "Zied Bouraoui",
        "Miguel Couceiro"
      ],
      "abstract": "Analogical reasoning is a powerful inductive mechanism, widely used in human cognition and increasingly applied in artificial intelligence. Formal frameworks for analogical inference have been developed for Boolean domains, where inference is provably sound for affine functions and approximately correct for functions close to affine. These results have informed the design of analogy-based classifiers. However, they do not extend to regression tasks or continuous domains. In this paper, we revisit analogical inference from a foundational perspective. We first present a counterexample showing that existing generalization bounds fail even in the Boolean setting. We then introduce a unified framework for analogical reasoning in real-valued domains based on parameterized analogies defined via generalized means. This model subsumes both Boolean classification and regression, and supports analogical inference over continuous functions. We characterize the class of analogy-preserving functions in this setting and derive both worst-case and average-case error bounds under smoothness assumptions. Our results offer a general theory of analogical inference across discrete and continuous domains.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10416v1",
        "pdf": "https://arxiv.org/pdf/2511.10416v1"
      },
      "arxiv_id": "2511.10416v1",
      "comment": "11 pages, to appear in AAAI 2026, extended version",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10412v1",
      "title": "3DFETUS: Standardizing Fetal Facial Planes in 3D Ultrasound",
      "authors": [
        "Alomar Antonia",
        "Rubio Ricardo",
        "Albaiges Gerard",
        "Salort-Benejam Laura",
        "Caminal Julia",
        "Prat Maria",
        "Rueda Carolina",
        "Cortes Berta",
        "Piella Gemma",
        "Sukno Federico"
      ],
      "abstract": "Acquiring standard facial planes during routine fetal ultrasound (US) examinations is often challenging due to fetal movement, variability in orientation, and operator-dependent expertise. These factors contribute to inconsistencies, increased examination time, and potential diagnostic bias.\n  To address these challenges in the context of facial assessment, we present: 1) GT++, a robust algorithm that estimates standard facial planes from 3D US volumes using annotated anatomical landmarks; and 2) 3DFETUS, a deep learning model that automates and standardizes their localization in 3D fetal US volumes.\n  We evaluated our methods both qualitatively, through expert clinical review, and quantitatively. The proposed approach achieved a mean translation error of 4.13 mm and a mean rotation error of 7.93 degrees per plane, outperforming other state-of-the-art methods on 3D US volumes. Clinical assessments further confirmed the effectiveness of both GT++ and 3DFETUS, demonstrating statistically significant improvements in plane estimation accuracy.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10412v1",
        "pdf": "https://arxiv.org/pdf/2511.10412v1"
      },
      "arxiv_id": "2511.10412v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10409v1",
      "title": "Explaining Decentralized Multi-Agent Reinforcement Learning Policies",
      "authors": [
        "Kayla Boggess",
        "Sarit Kraus",
        "Lu Feng"
      ],
      "abstract": "Multi-Agent Reinforcement Learning (MARL) has gained significant interest in recent years, enabling sequential decision-making across multiple agents in various domains. However, most existing explanation methods focus on centralized MARL, failing to address the uncertainty and nondeterminism inherent in decentralized settings. We propose methods to generate policy summarizations that capture task ordering and agent cooperation in decentralized MARL policies, along with query-based explanations for When, Why Not, and What types of user queries about specific agent behaviors. We evaluate our approach across four MARL domains and two decentralized MARL algorithms, demonstrating its generalizability and computational efficiency. User studies show that our summarizations and explanations significantly improve user question-answering performance and enhance subjective ratings on metrics such as understanding and satisfaction.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10409v1",
        "pdf": "https://arxiv.org/pdf/2511.10409v1"
      },
      "arxiv_id": "2511.10409v1",
      "comment": "Accepted for oral presentation at AAAI-26",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10403v1",
      "title": "nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation",
      "authors": [
        "Mingxing Peng",
        "Ruoyu Yao",
        "Xusen Guo",
        "Jun Ma"
      ],
      "abstract": "Recent advances in closed-loop planning benchmarks have significantly improved the evaluation of autonomous vehicles. However, existing benchmarks still rely on rule-based reactive agents such as the Intelligent Driver Model (IDM), which lack behavioral diversity and fail to capture realistic human interactions, leading to oversimplified traffic dynamics. To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework. Our benchmark replaces the rule-based IDM agents with noise-decoupled diffusion-based reactive agents and introduces an interaction-aware agent selection mechanism to ensure both realism and computational efficiency. Furthermore, we extend the benchmark with two additional metrics to enable a more comprehensive assessment of planning performance. Extensive experiments demonstrate that our reactive agent model produces more realistic, diverse, and human-like traffic behaviors, leading to a benchmark environment that better reflects real-world interactive driving. We further reimplement a collection of rule-based, learning-based, and hybrid planning approaches within our nuPlan-R benchmark, providing a clearer reflection of planner performance in complex interactive scenarios and better highlighting the advantages of learning-based planners in handling complex and dynamic scenarios. These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation. We will open-source the code for the new benchmark.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10403v1",
        "pdf": "https://arxiv.org/pdf/2511.10403v1"
      },
      "arxiv_id": "2511.10403v1",
      "comment": "8 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10400v1",
      "title": "Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance",
      "authors": [
        "Lifan Zheng",
        "Jiawei Chen",
        "Qinghong Yin",
        "Jingyuan Zhang",
        "Xinyi Zeng",
        "Yu Tian"
      ],
      "abstract": "Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.MA",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10400v1",
        "pdf": "https://arxiv.org/pdf/2511.10400v1"
      },
      "arxiv_id": "2511.10400v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10395v1",
      "title": "AgentEvolver: Towards Efficient Self-Evolving Agent System",
      "authors": [
        "Yunpeng Zhai",
        "Shuchang Tao",
        "Cheng Chen",
        "Anni Zou",
        "Ziqian Chen",
        "Qingxu Fu",
        "Shinji Mai",
        "Li Yu",
        "Jiaji Deng",
        "Zouying Cao",
        "Zhaoyang Liu",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "abstract": "Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10395v1",
        "pdf": "https://arxiv.org/pdf/2511.10395v1"
      },
      "arxiv_id": "2511.10395v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10394v1",
      "title": "LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components",
      "authors": [
        "Yaru Li",
        "Yanxue Wang",
        "Meng Li",
        "Xinming Li",
        "Jianbo Feng"
      ],
      "abstract": "The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\\% and generates maintenance reports with an average accuracy of 89\\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10394v1",
        "pdf": "https://arxiv.org/pdf/2511.10394v1"
      },
      "arxiv_id": "2511.10394v1",
      "comment": "Journal resubmission",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10392v1",
      "title": "Enhancing Kernel Power K-means: Scalable and Robust Clustering with Random Fourier Features and Possibilistic Method",
      "authors": [
        "Yixi Chen",
        "Weixuan Liang",
        "Tianrui Liu",
        "Jun-Jie Huang",
        "Ao Li",
        "Xueling Zhu",
        "Xinwang Liu"
      ],
      "abstract": "Kernel power $k$-means (KPKM) leverages a family of means to mitigate local minima issues in kernel $k$-means. However, KPKM faces two key limitations: (1) the computational burden of the full kernel matrix restricts its use on extensive data, and (2) the lack of authentic centroid-sample assignment learning reduces its noise robustness. To overcome these challenges, we propose RFF-KPKM, introducing the first approximation theory for applying random Fourier features (RFF) to KPKM. RFF-KPKM employs RFF to generate efficient, low-dimensional feature maps, bypassing the need for the whole kernel matrix. Crucially, we are the first to establish strong theoretical guarantees for this combination: (1) an excess risk bound of $\\mathcal{O}(\\sqrt{k^3/n})$, (2) strong consistency with membership values, and (3) a $(1+\\varepsilon)$ relative error bound achievable using the RFF of dimension $\\mathrm{poly}(\\varepsilon^{-1}\\log k)$. Furthermore, to improve robustness and the ability to learn multiple kernels, we propose IP-RFF-MKPKM, an improved possibilistic RFF-based multiple kernel power $k$-means. IP-RFF-MKPKM ensures the scalability of MKPKM via RFF and refines cluster assignments by combining the merits of the possibilistic membership and fuzzy membership. Experiments on large-scale datasets demonstrate the superior efficiency and clustering accuracy of the proposed methods compared to the state-of-the-art alternatives.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10392v1",
        "pdf": "https://arxiv.org/pdf/2511.10392v1"
      },
      "arxiv_id": "2511.10392v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10391v1",
      "title": "GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models",
      "authors": [
        "Oussema Dhaouadi",
        "Johannes Meier",
        "Jacques Kaiser",
        "Daniel Cremers"
      ],
      "abstract": "Digital Terrain Models (DTMs) represent the bare-earth elevation and are important in numerous geospatial applications. Such data models cannot be directly measured by sensors and are typically generated from Digital Surface Models (DSMs) derived from LiDAR or photogrammetry. Traditional filtering approaches rely on manually tuned parameters, while learning-based methods require well-designed architectures, often combined with post-processing. To address these challenges, we introduce Ground Diffusion (GrounDiff), the first diffusion-based framework that iteratively removes non-ground structures by formulating the problem as a denoising task. We incorporate a gated design with confidence-guided generation that enables selective filtering. To increase scalability, we further propose Prior-Guided Stitching (PrioStitch), which employs a downsampled global prior automatically generated using GrounDiff to guide local high-resolution predictions. We evaluate our method on the DSM-to-DTM translation task across diverse datasets, showing that GrounDiff consistently outperforms deep learning-based state-of-the-art methods, reducing RMSE by up to 93% on ALS2DTM and up to 47% on USGS benchmarks. In the task of road reconstruction, which requires both high precision and smoothness, our method achieves up to 81% lower distance error compared to specialized techniques on the GeRoD benchmark, while maintaining competitive surface smoothness using only DSM inputs, without task-specific optimization. Our variant for road reconstruction, GrounDiff+, is specifically designed to produce even smoother surfaces, further surpassing state-of-the-art methods. The project page is available at https://deepscenario.github.io/GrounDiff/.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10391v1",
        "pdf": "https://arxiv.org/pdf/2511.10391v1"
      },
      "arxiv_id": "2511.10391v1",
      "comment": "Accepted at WACV 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10390v1",
      "title": "MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns",
      "authors": [
        "Jiarui Zhang",
        "Yuliang Liu",
        "Zijun Wu",
        "Guosheng Pang",
        "Zhili Ye",
        "Yupei Zhong",
        "Junteng Ma",
        "Tao Wei",
        "Haiyang Xu",
        "Weikai Chen",
        "Zeen Wang",
        "Qiangjun Ji",
        "Fanxi Zhou",
        "Qi Zhang",
        "Yuanrui Hu",
        "Jiahao Liu",
        "Zhang Li",
        "Ziyang Zhang",
        "Qiang Liu",
        "Xiang Bai"
      ],
      "abstract": "Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis. However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems. We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline. The first stage employs a large multimodal model to jointly predict document layout and reading order, leveraging visual information to ensure structural and sequential consistency. The second stage performs localized recognition of text, formulas, and tables within detected regions, maintaining high visual fidelity while reducing error propagation. To address complex table structures, we propose a visual consistency-based reinforcement learning scheme that evaluates recognition quality via render-and-compare alignment, improving structural accuracy without manual annotations. Additionally, two specialized modules, Image-Decoupled Table Parsing and Type-Guided Table Merging, are introduced to enable reliable parsing of tables containing embedded images and reconstruction of tables crossing pages or columns. Comprehensive experiments on OmniDocBench v1.5 demonstrate that MonkeyOCR v1.5 achieves state-of-the-art performance, outperforming PPOCR-VL and MinerU 2.5 while showing exceptional robustness in visually complex document scenarios.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10390v1",
        "pdf": "https://arxiv.org/pdf/2511.10390v1"
      },
      "arxiv_id": "2511.10390v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10387v1",
      "title": "Physics informed Transformer-VAE for biophysical parameter estimation: PROSAIL model inversion in Sentinel-2 imagery",
      "authors": [
        "Prince Mensah",
        "Pelumi Victor Aderinto",
        "Ibrahim Salihu Yusuf",
        "Arnu Pretorius"
      ],
      "abstract": "Accurate retrieval of vegetation biophysical variables from satellite imagery is crucial for ecosystem monitoring and agricultural management. In this work, we propose a physics-informed Transformer-VAE architecture to invert the PROSAIL radiative transfer model for simultaneous estimation of key canopy parameters from Sentinel-2 data. Unlike previous hybrid approaches that require real satellite images for self-supevised training. Our model is trained exclusively on simulated data, yet achieves performance on par with state-of-the-art methods that utilize real imagery. The Transformer-VAE incorporates the PROSAIL model as a differentiable physical decoder, ensuring that inferred latent variables correspond to physically plausible leaf and canopy properties. We demonstrate retrieval of leaf area index (LAI) and canopy chlorophyll content (CCC) on real-world field datasets (FRM4Veg and BelSAR) with accuracy comparable to models trained with real Sentinel-2 data. Our method requires no in-situ labels or calibration on real images, offering a cost-effective and self-supervised solution for global vegetation monitoring. The proposed approach illustrates how integrating physical models with advanced deep networks can improve the inversion of RTMs, opening new prospects for large-scale, physically-constrained remote sensing of vegetation traits.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10387v1",
        "pdf": "https://arxiv.org/pdf/2511.10387v1"
      },
      "arxiv_id": "2511.10387v1",
      "comment": "10 pages, 6 figures, uses fancyhdr.sty",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10385v1",
      "title": "SAMIRO: Spatial Attention Mutual Information Regularization with a Pre-trained Model as Oracle for Lane Detection",
      "authors": [
        "Hyunjong Lee",
        "Jangho Lee",
        "Jaekoo Lee"
      ],
      "abstract": "Lane detection is an important topic in the future mobility solutions. Real-world environmental challenges such as background clutter, varying illumination, and occlusions pose significant obstacles to effective lane detection, particularly when relying on data-driven approaches that require substantial effort and cost for data collection and annotation. To address these issues, lane detection methods must leverage contextual and global information from surrounding lanes and objects. In this paper, we propose a Spatial Attention Mutual Information Regularization with a pre-trained model as an Oracle, called SAMIRO. SAMIRO enhances lane detection performance by transferring knowledge from a pretrained model while preserving domain-agnostic spatial information. Leveraging SAMIRO's plug-and-play characteristic, we integrate it into various state-of-the-art lane detection approaches and conduct extensive experiments on major benchmarks such as CULane, Tusimple, and LLAMAS. The results demonstrate that SAMIRO consistently improves performance across different models and datasets. The code will be made available upon publication.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10385v1",
        "pdf": "https://arxiv.org/pdf/2511.10385v1"
      },
      "arxiv_id": "2511.10385v1",
      "comment": "7 pages, 4 figures, paper in press",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10384v1",
      "title": "Simulating Misinformation Propagation in Social Networks using Large Language Models",
      "authors": [
        "Raj Gaurav Maurya",
        "Vaibhav Shukla",
        "Raj Abhijit Dandekar",
        "Rajat Dandekar",
        "Sreedath Panat"
      ],
      "abstract": "Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.SI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10384v1",
        "pdf": "https://arxiv.org/pdf/2511.10384v1"
      },
      "arxiv_id": "2511.10384v1",
      "comment": "Accepted to CIKM 2025 Workshop LASS",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10383v1",
      "title": "Operator Models for Continuous-Time Offline Reinforcement Learning",
      "authors": [
        "Nicolas Hoischen",
        "Petar Bevanda",
        "Max Beier",
        "Stefan Sosnowski",
        "Boris Houska",
        "Sandra Hirche"
      ],
      "abstract": "Continuous-time stochastic processes underlie many natural and engineered systems. In healthcare, autonomous driving, and industrial control, direct interaction with the environment is often unsafe or impractical, motivating offline reinforcement learning from historical data. However, there is limited statistical understanding of the approximation errors inherent in learning policies from offline datasets. We address this by linking reinforcement learning to the Hamilton-Jacobi-Bellman equation and proposing an operator-theoretic algorithm based on a simple dynamic programming recursion. Specifically, we represent our world model in terms of the infinitesimal generator of controlled diffusion processes learned in a reproducing kernel Hilbert space. By integrating statistical learning methods and operator theory, we establish global convergence of the value function and derive finite-sample guarantees with bounds tied to system properties such as smoothness and stability. Our theoretical and numerical results indicate that operator-based approaches may hold promise in solving offline reinforcement learning using continuous-time optimal control.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "stat.ML",
        "cs.LG",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10383v1",
        "pdf": "https://arxiv.org/pdf/2511.10383v1"
      },
      "arxiv_id": "2511.10383v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10382v1",
      "title": "Fragile by Design: On the Limits of Adversarial Defenses in Personalized Generation",
      "authors": [
        "Zhen Chen",
        "Yi Zhang",
        "Xiangyu Yin",
        "Chengxuan Qin",
        "Xingyu Zhao",
        "Xiaowei Huang",
        "Wenjie Ruan"
      ],
      "abstract": "Personalized AI applications such as DreamBooth enable the generation of customized content from user images, but also raise significant privacy concerns, particularly the risk of facial identity leakage. Recent defense mechanisms like Anti-DreamBooth attempt to mitigate this risk by injecting adversarial perturbations into user photos to prevent successful personalization. However, we identify two critical yet overlooked limitations of these methods. First, the adversarial examples often exhibit perceptible artifacts such as conspicuous patterns or stripes, making them easily detectable as manipulated content. Second, the perturbations are highly fragile, as even a simple, non-learned filter can effectively remove them, thereby restoring the model's ability to memorize and reproduce user identity. To investigate this vulnerability, we propose a novel evaluation framework, AntiDB_Purify, to systematically evaluate existing defenses under realistic purification threats, including both traditional image filters and adversarial purification. Results reveal that none of the current methods maintains their protective effectiveness under such threats. These findings highlight that current defenses offer a false sense of security and underscore the urgent need for more imperceptible and robust protections to safeguard user identity in personalized generation.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10382v1",
        "pdf": "https://arxiv.org/pdf/2511.10382v1"
      },
      "arxiv_id": "2511.10382v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10376v1",
      "title": "MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation",
      "authors": [
        "Xun Huang",
        "Shijia Zhao",
        "Yunxiang Wang",
        "Xin Lu",
        "Wanfa Zhang",
        "Rongsheng Qu",
        "Weixin Li",
        "Yunhong Wang",
        "Chenglu Wen"
      ],
      "abstract": "Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images. Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning. Additionally, we further identify the last-mile problem in zero-shot navigation - determining the feasible target location with a suitable final viewpoint, and propose a Visibility-based Viewpoint Decision module to explicitly resolve it. Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets. The open-source code will be publicly available.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10376v1",
        "pdf": "https://arxiv.org/pdf/2511.10376v1"
      },
      "arxiv_id": "2511.10376v1",
      "comment": "10 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10370v1",
      "title": "SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation",
      "authors": [
        "Kai-Hendrik Cohrs",
        "Zuzanna Osika",
        "Maria Gonzalez-Calabuig",
        "Vishal Nedungadi",
        "Ruben Cartuyvels",
        "Steffen Knoblauch",
        "Joppe Massant",
        "Shruti Nath",
        "Patrick Ebel",
        "Vasileios Sitokonstantinou"
      ],
      "abstract": "Geospatial foundation models for Earth observation often fail to perform reliably in environments underrepresented during pretraining. We introduce SHRUG-FM, a framework for reliability-aware prediction that integrates three complementary signals: out-of-distribution (OOD) detection in the input space, OOD detection in the embedding space and task-specific predictive uncertainty. Applied to burn scar segmentation, SHRUG-FM shows that OOD scores correlate with lower performance in specific environmental conditions, while uncertainty-based flags help discard many poorly performing predictions. Linking these flags to land cover attributes from HydroATLAS shows that failures are not random but concentrated in certain geographies, such as low-elevation zones and large river areas, likely due to underrepresentation in pretraining data. SHRUG-FM provides a pathway toward safer and more interpretable deployment of GFMs in climate-sensitive applications, helping bridge the gap between benchmark performance and real-world reliability.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10370v1",
        "pdf": "https://arxiv.org/pdf/2511.10370v1"
      },
      "arxiv_id": "2511.10370v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10367v1",
      "title": "DermAI: Clinical dermatology acquisition through quality-driven image collection for AI classification in mobile",
      "authors": [
        "Thales Bezerra",
        "Emanoel Thyago",
        "Kelvin Cunha",
        "Rodrigo Abreu",
        "Fábio Papais",
        "Francisco Mauro",
        "Natália Lopes",
        "Érico Medeiros",
        "Jéssica Guido",
        "Shirley Cruz",
        "Paulo Borba",
        "Tsang Ing Ren"
      ],
      "abstract": "AI-based dermatology adoption remains limited by biased datasets, variable image quality, and limited validation. We introduce DermAI, a lightweight, smartphone-based application that enables real-time capture, annotation, and classification of skin lesions during routine consultations. Unlike prior dermoscopy-focused tools, DermAI performs on-device quality checks, and local model adaptation. The DermAI clinical dataset, encompasses a wide range of skin tones, ethinicity and source devices. In preliminary experiments, models trained on public datasets failed to generalize to our samples, while fine-tuning with local data improved performance. These results highlight the importance of standardized, diverse data collection aligned with healthcare needs and oriented to machine learning development.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10367v1",
        "pdf": "https://arxiv.org/pdf/2511.10367v1"
      },
      "arxiv_id": "2511.10367v1",
      "comment": "4 pages, 2 figures, 1 table, submitted on ISBI",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10366v1",
      "title": "Product distribution learning with imperfect advice",
      "authors": [
        "Arnab Bhattacharyya",
        "Davin Choo",
        "Philips George John",
        "Themis Gouleakis"
      ],
      "abstract": "Given i.i.d.~samples from an unknown distribution $P$, the goal of distribution learning is to recover the parameters of a distribution that is close to $P$. When $P$ belongs to the class of product distributions on the Boolean hypercube $\\{0,1\\}^d$, it is known that $Ω(d/\\varepsilon^2)$ samples are necessary to learn $P$ within total variation (TV) distance $\\varepsilon$. We revisit this problem when the learner is also given as advice the parameters of a product distribution $Q$. We show that there is an efficient algorithm to learn $P$ within TV distance $\\varepsilon$ that has sample complexity $\\tilde{O}(d^{1-η}/\\varepsilon^2)$, if $\\|\\mathbf{p} - \\mathbf{q}\\|_1 < \\varepsilon d^{0.5 - Ω(η)}$. Here, $\\mathbf{p}$ and $\\mathbf{q}$ are the mean vectors of $P$ and $Q$ respectively, and no bound on $\\|\\mathbf{p} - \\mathbf{q}\\|_1$ is known to the algorithm a priori.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10366v1",
        "pdf": "https://arxiv.org/pdf/2511.10366v1"
      },
      "arxiv_id": "2511.10366v1",
      "comment": "Full version (11 pages). To be published in NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10362v1",
      "title": "Gradient Flow Equations for Deep Linear Neural Networks: A Survey from a Network Perspective",
      "authors": [
        "Joel Wendin",
        "Claudio Altafini"
      ],
      "abstract": "The paper surveys recent progresses in understanding the dynamics and loss landscape of the gradient flow equations associated to deep linear neural networks, i.e., the gradient descent training dynamics (in the limit when the step size goes to 0) of deep neural networks missing the activation functions and subject to quadratic loss functions. When formulated in terms of the adjacency matrix of the neural network, as we do in the paper, these gradient flow equations form a class of converging matrix ODEs which is nilpotent, polynomial, isospectral, and with conservation laws. The loss landscape is described in detail. It is characterized by infinitely many global minima and saddle points, both strict and nonstrict, but lacks local minima and maxima. The loss function itself is a positive semidefinite Lyapunov function for the gradient flow, and its level sets are unbounded invariant sets of critical points, with critical values that correspond to the amount of singular values of the input-output data learnt by the gradient along a certain trajectory. The adjacency matrix representation we use in the paper allows to highlight the existence of a quotient space structure in which each critical value of the loss function is represented only once, while all other critical points with the same critical value belong to the fiber associated to the quotient space. It also allows to easily determine stable and unstable submanifolds at the saddle points, even when the Hessian fails to obtain them.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG",
        "eess.SY",
        "math.DS"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10362v1",
        "pdf": "https://arxiv.org/pdf/2511.10362v1"
      },
      "arxiv_id": "2511.10362v1",
      "comment": "Manuscript accepted for publication in SIAM Review (SIREV)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10356v1",
      "title": "SITA: A Framework for Structure-to-Instance Theorem Autoformalization",
      "authors": [
        "Chenyi Li",
        "Wanli Ma",
        "Zichen Wang",
        "Zaiwen Wen"
      ],
      "abstract": "While large language models (LLMs) have shown progress in mathematical reasoning, they still face challenges in formalizing theorems that arise from instantiating abstract structures in concrete settings. With the goal of auto-formalizing mathematical results at the research level, we develop a framework for structure-to-instance theorem autoformalization (SITA), which systematically bridges the gap between abstract mathematical theories and their concrete applications in Lean proof assistant. Formalized abstract structures are treated as modular templates that contain definitions, assumptions, operations, and theorems. These templates serve as reusable guides for the formalization of concrete instances. Given a specific instantiation, we generate corresponding Lean definitions and instance declarations, integrate them using Lean's typeclass mechanism, and construct verified theorems by checking structural assumptions. We incorporate LLM-based generation with feedback-guided refinement to ensure both automation and formal correctness. Experiments on a dataset of optimization problems demonstrate that SITA effectively formalizes diverse instances grounded in abstract structures.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10356v1",
        "pdf": "https://arxiv.org/pdf/2511.10356v1"
      },
      "arxiv_id": "2511.10356v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10352v1",
      "title": "FOUND: Fourier-based von Mises Distribution for Robust Single Domain Generalization in Object Detection",
      "authors": [
        "Mengzhu Wang",
        "Changyuan Deng",
        "Shanshan Wang",
        "Nan Yin",
        "Long Lan",
        "Liang Yang"
      ],
      "abstract": "Single Domain Generalization (SDG) for object detection aims to train a model on a single source domain that can generalize effectively to unseen target domains. While recent methods like CLIP-based semantic augmentation have shown promise, they often overlook the underlying structure of feature distributions and frequency-domain characteristics that are critical for robustness. In this paper, we propose a novel framework that enhances SDG object detection by integrating the von Mises-Fisher (vMF) distribution and Fourier transformation into a CLIP-guided pipeline. Specifically, we model the directional features of object representations using vMF to better capture domain-invariant semantic structures in the embedding space. Additionally, we introduce a Fourier-based augmentation strategy that perturbs amplitude and phase components to simulate domain shifts in the frequency domain, further improving feature robustness. Our method not only preserves the semantic alignment benefits of CLIP but also enriches feature diversity and structural consistency across domains. Extensive experiments on the diverse weather-driving benchmark demonstrate that our approach outperforms the existing state-of-the-art method.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10352v1",
        "pdf": "https://arxiv.org/pdf/2511.10352v1"
      },
      "arxiv_id": "2511.10352v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10344v1",
      "title": "Robust Decentralized Multi-armed Bandits: From Corruption-Resilience to Byzantine-Resilience",
      "authors": [
        "Zicheng Hu",
        "Yuchen Wang",
        "Cheng Chen"
      ],
      "abstract": "Decentralized cooperative multi-agent multi-armed bandits (DeCMA2B) considers how multiple agents collaborate in a decentralized multi-armed bandit setting. Though this problem has been extensively studied in previous work, most existing methods remain susceptible to various adversarial attacks. In this paper, we first study DeCMA2B with adversarial corruption, where an adversary can corrupt reward observations of all agents with a limited corruption budget. We propose a robust algorithm, called DeMABAR, which ensures that each agent's individual regret suffers only an additive term proportional to the corruption budget. Then we consider a more realistic scenario where the adversary can only attack a small number of agents. Our theoretical analysis shows that the DeMABAR algorithm can also almost completely eliminate the influence of adversarial attacks and is inherently robust in the Byzantine setting, where an unknown fraction of the agents can be Byzantine, i.e., may arbitrarily select arms and communicate wrong information. We also conduct numerical experiments to illustrate the robustness and effectiveness of the proposed method.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10344v1",
        "pdf": "https://arxiv.org/pdf/2511.10344v1"
      },
      "arxiv_id": "2511.10344v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10340v1",
      "title": "Equivariant Denoisers for Plug and Play Image Restoration",
      "authors": [
        "Marien Renaud",
        "Eliot Guez",
        "Arthur Leclaire",
        "Nicolas Papadakis"
      ],
      "abstract": "One key ingredient of image restoration is to define a realistic prior on clean images to complete the missing information in the observation. State-of-the-art restoration methods rely on a neural network to encode this prior. Typical image distributions are invariant to some set of transformations, such as rotations or flips. However, most deep architectures are not designed to represent an invariant image distribution. Recent works have proposed to overcome this difficulty by including equivariance properties within a Plug-and-Play paradigm. In this work, we propose two unified frameworks named Equivariant Regularization by Denoising (ERED) and Equivariant Plug-and-Play (EPnP) based on equivariant denoisers and stochastic optimization. We analyze the convergence of the proposed algorithms and discuss their practical benefit.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "eess.IV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10340v1",
        "pdf": "https://arxiv.org/pdf/2511.10340v1"
      },
      "arxiv_id": "2511.10340v1",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2412.05343",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10339v1",
      "title": "Massively Parallel Proof-Number Search for Impartial Games and Beyond",
      "authors": [
        "Tomáš Čížek",
        "Martin Balko",
        "Martin Schmid"
      ],
      "abstract": "Proof-Number Search is a best-first search algorithm with many successful applications, especially in game solving. As large-scale computing clusters become increasingly accessible, parallelization is a natural way to accelerate computation. However, existing parallel versions of Proof-Number Search are known to scale poorly on many CPU cores. Using two parallelized levels and shared information among workers, we present the first massively parallel version of Proof-Number Search that scales efficiently even on a large number of CPUs. We apply our solver, enhanced with Grundy numbers for reducing game trees, to the Sprouts game, a case study motivated by the long-standing Sprouts Conjecture. Our solver achieves a significantly improved 332.9$\\times$ speedup when run on 1024 cores, enabling it to outperform the state-of-the-art Sprouts solver GLOP by four orders of magnitude in runtime and to generate proofs 1,000$\\times$ more complex. Despite exponential growth in game tree size, our solver verified the Sprouts Conjecture for 42 new positions, nearly doubling the number of known outcomes.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10339v1",
        "pdf": "https://arxiv.org/pdf/2511.10339v1"
      },
      "arxiv_id": "2511.10339v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10338v1",
      "title": "BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages",
      "authors": [
        "Guduru Manoj",
        "Neel Prabhanjan Rachamalla",
        "Ashish Kulkarni",
        "Gautam Rajeev",
        "Jay Piplodiya",
        "Arul Menezes",
        "Shaharukh Khan",
        "Souvik Rana",
        "Manya Sah",
        "Chandra Khatri",
        "Shubham Agarwal"
      ],
      "abstract": "In the context of pretraining of Large Language Models (LLMs), synthetic data has emerged as an alternative for generating high-quality pretraining data at scale. This is particularly beneficial in low-resource language settings where the benefits of recent LLMs have been unevenly distributed across languages. In this work, we present a systematic study on the generation and evaluation of synthetic multilingual pretraining data for Indic languages, where we construct a large-scale synthetic dataset BhashaKritika, comprising 540B tokens using 5 different techniques for 10 languages. We explore the impact of grounding generation in documents, personas, and topics. We analyze how language choice, both in the prompt instructions and document grounding, affects data quality, and we compare translations of English content with native generation in Indic languages. To support scalable and language-sensitive evaluation, we introduce a modular quality evaluation pipeline that integrates script and language detection, metadata consistency checks, n-gram repetition analysis, and perplexity-based filtering using KenLM models. Our framework enables robust quality control across diverse scripts and linguistic contexts. Empirical results through model runs reveal key trade-offs in generation strategies and highlight best practices for constructing effective multilingual corpora.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10338v1",
        "pdf": "https://arxiv.org/pdf/2511.10338v1"
      },
      "arxiv_id": "2511.10338v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10334v1",
      "title": "Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment",
      "authors": [
        "Wenti Yin",
        "Huaxin Zhang",
        "Xiang Wang",
        "Yuqing Lu",
        "Yicheng Zhang",
        "Bingquan Gong",
        "Jialong Zuo",
        "Li Yu",
        "Changxin Gao",
        "Nong Sang"
      ],
      "abstract": "Recent advancements in weakly-supervised video anomaly detection have achieved remarkable performance by applying the multiple instance learning paradigm based on multimodal foundation models such as CLIP to highlight anomalous instances and classify categories. However, their objectives may tend to detect the most salient response segments, while neglecting to mine diverse normal patterns separated from anomalies, and are prone to category confusion due to similar appearance, leading to unsatisfactory fine-grained classification results. Therefore, we propose a novel Disentangled Semantic Alignment Network (DSANet) to explicitly separate abnormal and normal features from coarse-grained and fine-grained aspects, enhancing the distinguishability. Specifically, at the coarse-grained level, we introduce a self-guided normality modeling branch that reconstructs input video features under the guidance of learned normal prototypes, encouraging the model to exploit normality cues inherent in the video, thereby improving the temporal separation of normal patterns and anomalous events. At the fine-grained level, we present a decoupled contrastive semantic alignment mechanism, which first temporally decomposes each video into event-centric and background-centric components using frame-level anomaly scores and then applies visual-language contrastive learning to enhance class-discriminative representations. Comprehensive experiments on two standard benchmarks, namely XD-Violence and UCF-Crime, demonstrate that DSANet outperforms existing state-of-the-art methods.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10334v1",
        "pdf": "https://arxiv.org/pdf/2511.10334v1"
      },
      "arxiv_id": "2511.10334v1",
      "comment": "Accepted to AAAI 2026. Code is available at https://github.com/lessiYin/DSANet",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.10333v1",
      "title": "EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training",
      "authors": [
        "Qingao Yi",
        "Jiaang Duan",
        "Hanwen Hu",
        "Qin Hua",
        "Haiyan Zhao",
        "Shiyou Qian",
        "Dingyu Yang",
        "Jian Cao",
        "Jinghua Tang",
        "Yinghao Yu",
        "Chenzhi Liao",
        "Kangjin Wang",
        "Liping Zhang"
      ],
      "abstract": "Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10333v1",
        "pdf": "https://arxiv.org/pdf/2511.10333v1"
      },
      "arxiv_id": "2511.10333v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10320v1",
      "title": "PITE: Multi-Prototype Alignment for Individual Treatment Effect Estimation",
      "authors": [
        "Fuyuan Cao",
        "Jiaxuan Zhang",
        "Xiaoli Li"
      ],
      "abstract": "Estimating Individual Treatment Effects (ITE) from observational data is challenging due to confounding bias. Most studies tackle this bias by balancing distributions globally, but ignore individual heterogeneity and fail to capture the local structure that represents the natural clustering among individuals, which ultimately compromises ITE estimation. While instance-level alignment methods consider heterogeneity, they similarly overlook the local structure information. To address these issues, we propose an end-to-end Multi-\\textbf{P}rototype alignment method for \\textbf{ITE} estimation (\\textbf{PITE}). PITE effectively captures local structure within groups and enforces cross-group alignment, thereby achieving robust ITE estimation. Specifically, we first define prototypes as cluster centroids based on similar individuals under the same treatment. To identify local similarity and the distribution consistency, we perform instance-to-prototype matching to assign individuals to the nearest prototype within groups, and design a multi-prototype alignment strategy to encourage the matched prototypes to be close across treatment arms in the latent space. PITE not only reduces distribution shift through fine-grained, prototype-level alignment, but also preserves the local structures of treated and control groups, which provides meaningful constraints for ITE estimation. Extensive evaluations on benchmark datasets demonstrate that PITE outperforms 13 state-of-the-art methods, achieving more accurate and robust ITE estimation.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10320v1",
        "pdf": "https://arxiv.org/pdf/2511.10320v1"
      },
      "arxiv_id": "2511.10320v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10316v1",
      "title": "Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision",
      "authors": [
        "Yu Deng",
        "Baozhu Zhao",
        "Junyan Su",
        "Xiaohan Zhang",
        "Qi Liu"
      ],
      "abstract": "Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions. Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions. This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting. Our approach comprises two core components: (1) Depth-of-field Supervision employs a scale-recovered monocular depth estimator (e.g., Metric3D) to generate depth priors, leverages defocus convolution to synthesize physically accurate defocused images, and enforces geometric consistency through a novel depth-of-field loss, thereby enhancing depth fidelity in both far-field and near-field regions; (2) Multi-View Consistency Supervision employing LoFTR-based semi-dense feature matching to minimize cross-view geometric errors and enforce depth consistency via least squares optimization of reliable matched points. By unifying defocus physics with multi-view geometric constraints, our method achieves superior depth fidelity, demonstrating a 0.8 dB PSNR improvement over the state-of-the-art method on the Waymo Open Dataset. This framework bridges physical imaging principles and learning-based depth regularization, offering a scalable solution for complex depth stratification in urban environments.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10316v1",
        "pdf": "https://arxiv.org/pdf/2511.10316v1"
      },
      "arxiv_id": "2511.10316v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10309v1",
      "title": "CLIP4VI-ReID: Learning Modality-shared Representations via CLIP Semantic Bridge for Visible-Infrared Person Re-identification",
      "authors": [
        "Xiaomei Yang",
        "Xizhan Gao",
        "Sijie Niu",
        "Fa Zhu",
        "Guang Feng",
        "Xiaofeng Qu",
        "David Camacho"
      ],
      "abstract": "This paper proposes a novel CLIP-driven modality-shared representation learning network named CLIP4VI-ReID for VI-ReID task, which consists of Text Semantic Generation (TSG), Infrared Feature Embedding (IFE), and High-level Semantic Alignment (HSA). Specifically, considering the huge gap in the physical characteristics between natural images and infrared images, the TSG is designed to generate text semantics only for visible images, thereby enabling preliminary visible-text modality alignment. Then, the IFE is proposed to rectify the feature embeddings of infrared images using the generated text semantics. This process injects id-related semantics into the shared image encoder, enhancing its adaptability to the infrared modality. Besides, with text serving as a bridge, it enables indirect visible-infrared modality alignment. Finally, the HSA is established to refine the high-level semantic alignment. This process ensures that the fine-tuned text semantics only contain id-related information, thereby achieving more accurate cross-modal alignment and enhancing the discriminability of the learned modal-shared representations. Extensive experimental results demonstrate that the proposed CLIP4VI-ReID achieves superior performance than other state-of-the-art methods on some widely used VI-ReID datasets.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10309v1",
        "pdf": "https://arxiv.org/pdf/2511.10309v1"
      },
      "arxiv_id": "2511.10309v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10308v1",
      "title": "Revisiting Evaluation of Deep Neural Networks for Pedestrian Detection",
      "authors": [
        "Patrick Feifel",
        "Benedikt Franke",
        "Frank Bonarens",
        "Frank Köster",
        "Arne Raulf",
        "Friedhelm Schwenker"
      ],
      "abstract": "Reliable pedestrian detection represents a crucial step towards automated driving systems. However, the current performance benchmarks exhibit weaknesses. The currently applied metrics for various subsets of a validation dataset prohibit a realistic performance evaluation of a DNN for pedestrian detection. As image segmentation supplies fine-grained information about a street scene, it can serve as a starting point to automatically distinguish between different types of errors during the evaluation of a pedestrian detector. In this work, eight different error categories for pedestrian detection are proposed and new metrics are proposed for performance comparison along these error categories. We use the new metrics to compare various backbones for a simplified version of the APD, and show a more fine-grained and robust way to compare models with each other especially in terms of safety-critical performance. We achieve SOTA on CityPersons-reasonable (without extra training data) by using a rather simple architecture.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10308v1",
        "pdf": "https://arxiv.org/pdf/2511.10308v1"
      },
      "arxiv_id": "2511.10308v1",
      "comment": "",
      "journal_ref": "2022 Workshop on Artificial Intelligence Safety, AISafety 2022",
      "has_code": false
    },
    {
      "id": "2511.10301v1",
      "title": "Rethinking Visual Information Processing in Multimodal LLMs",
      "authors": [
        "Dongwan Kim",
        "Viresh Ranjan",
        "Takashi Nagata",
        "Arnab Dhua",
        "Amit Kumar K C"
      ],
      "abstract": "Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10301v1",
        "pdf": "https://arxiv.org/pdf/2511.10301v1"
      },
      "arxiv_id": "2511.10301v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.10300v1",
      "title": "Generalizable Slum Detection from Satellite Imagery with Mixture-of-Experts",
      "authors": [
        "Sumin Lee",
        "Sungwon Park",
        "Jeasurk Yang",
        "Jihee Kim",
        "Meeyoung Cha"
      ],
      "abstract": "Satellite-based slum segmentation holds significant promise in generating global estimates of urban poverty. However, the morphological heterogeneity of informal settlements presents a major challenge, hindering the ability of models trained on specific regions to generalize effectively to unseen locations. To address this, we introduce a large-scale high-resolution dataset and propose GRAM (Generalized Region-Aware Mixture-of-Experts), a two-phase test-time adaptation framework that enables robust slum segmentation without requiring labeled data from target regions. We compile a million-scale satellite imagery dataset from 12 cities across four continents for source training. Using this dataset, the model employs a Mixture-of-Experts architecture to capture region-specific slum characteristics while learning universal features through a shared backbone. During adaptation, prediction consistency across experts filters out unreliable pseudo-labels, allowing the model to generalize effectively to previously unseen regions. GRAM outperforms state-of-the-art baselines in low-resource settings such as African cities, offering a scalable and label-efficient solution for global slum mapping and data-driven urban planning.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10300v1",
        "pdf": "https://arxiv.org/pdf/2511.10300v1"
      },
      "arxiv_id": "2511.10300v1",
      "comment": "Accepted to AAAI 2026",
      "journal_ref": "",
      "has_code": false
    }
  ]
}