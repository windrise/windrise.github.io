{
  "fetched_at": "2025-11-20T00:25:08.966190",
  "total_papers": 100,
  "papers": [
    {
      "id": "2511.14761v1",
      "title": "ARC Is a Vision Problem!",
      "authors": [
        "Keya Hu",
        "Ali Cy",
        "Linlu Qiu",
        "Xiaoman Delores Ding",
        "Runqian Wang",
        "Yeyin Eva Zhu",
        "Jacob Andreas",
        "Kaiming He"
      ],
      "abstract": "The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a \"canvas\" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14761v1",
        "pdf": "https://arxiv.org/pdf/2511.14761v1"
      },
      "arxiv_id": "2511.14761v1",
      "comment": "Technical Report. Project webpage: https://github.com/lillian039/VARC",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.14760v1",
      "title": "UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning",
      "authors": [
        "Rui Tian",
        "Mingfei Gao",
        "Haiming Gang",
        "Jiasen Lu",
        "Zhe Gan",
        "Yinfei Yang",
        "Zuxuan Wu",
        "Afshin Dehghan"
      ],
      "abstract": "We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14760v1",
        "pdf": "https://arxiv.org/pdf/2511.14760v1"
      },
      "arxiv_id": "2511.14760v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14759v1",
      "title": "$π^{*}_{0.6}$: a VLA That Learns From Experience",
      "authors": [
        "Ali Amin",
        "Raichelle Aniceto",
        "Ashwin Balakrishna",
        "Kevin Black",
        "Ken Conley",
        "Grace Connors",
        "James Darpinian",
        "Karan Dhabalia",
        "Jared DiCarlo",
        "Danny Driess",
        "Michael Equi",
        "Adnan Esmail",
        "Yunhao Fang",
        "Chelsea Finn",
        "Catherine Glossop",
        "Thomas Godden",
        "Ivan Goryachev",
        "Lachy Groom",
        "Hunter Hancock",
        "Karol Hausman",
        "Gashon Hussein",
        "Brian Ichter",
        "Szymon Jakubczak",
        "Rowan Jen",
        "Tim Jones",
        "Ben Katz",
        "Liyiming Ke",
        "Chandra Kuchi",
        "Marinda Lamb",
        "Devin LeBlanc",
        "Sergey Levine",
        "Adrian Li-Bell",
        "Yao Lu",
        "Vishnu Mano",
        "Mohith Mothukuri",
        "Suraj Nair",
        "Karl Pertsch",
        "Allen Z. Ren",
        "Charvi Sharma",
        "Lucy Xiaoyang Shi",
        "Laura Smith",
        "Jost Tobias Springenberg",
        "Kyle Stachowicz",
        "Will Stoeckle",
        "Alex Swerdlow",
        "James Tanner",
        "Marcel Torne",
        "Quan Vuong",
        "Anna Walling",
        "Haohuan Wang",
        "Blake Williams",
        "Sukwon Yoo",
        "Lili Yu",
        "Ury Zhilinsky",
        "Zhiyuan Zhou"
      ],
      "abstract": "We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $π^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $π^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14759v1",
        "pdf": "https://arxiv.org/pdf/2511.14759v1"
      },
      "arxiv_id": "2511.14759v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14755v1",
      "title": "Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis",
      "authors": [
        "Albert Lin",
        "Alessandro Pinto",
        "Somil Bansal"
      ],
      "abstract": "As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.RO",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14755v1",
        "pdf": "https://arxiv.org/pdf/2511.14755v1"
      },
      "arxiv_id": "2511.14755v1",
      "comment": "Submitted to the 8th Annual Learning for Dynamics & Control Conference",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14753v1",
      "title": "SparseST: Exploiting Data Sparsity in Spatiotemporal Modeling and Prediction",
      "authors": [
        "Junfeng Wu",
        "Hadjer Benmeziane",
        "Kaoutar El Maghraoui",
        "Liu Liu",
        "Yinan Wang"
      ],
      "abstract": "Spatiotemporal data mining (STDM) has a wide range of applications in various complex physical systems (CPS), i.e., transportation, manufacturing, healthcare, etc. Among all the proposed methods, the Convolutional Long Short-Term Memory (ConvLSTM) has proved to be generalizable and extendable in different applications and has multiple variants achieving state-of-the-art performance in various STDM applications. However, ConvLSTM and its variants are computationally expensive, which makes them inapplicable in edge devices with limited computational resources. With the emerging need for edge computing in CPS, efficient AI is essential to reduce the computational cost while preserving the model performance. Common methods of efficient AI are developed to reduce redundancy in model capacity (i.e., model pruning, compression, etc.). However, spatiotemporal data mining naturally requires extensive model capacity, as the embedded dependencies in spatiotemporal data are complex and hard to capture, which limits the model redundancy. Instead, there is a fairly high level of data and feature redundancy that introduces an unnecessary computational burden, which has been largely overlooked in existing research. Therefore, we developed a novel framework SparseST, that pioneered in exploiting data sparsity to develop an efficient spatiotemporal model. In addition, we explore and approximate the Pareto front between model performance and computational efficiency by designing a multi-objective composite loss function, which provides a practical guide for practitioners to adjust the model according to computational resource constraints and the performance requirements of downstream tasks.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14753v1",
        "pdf": "https://arxiv.org/pdf/2511.14753v1"
      },
      "arxiv_id": "2511.14753v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14751v1",
      "title": "Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers",
      "authors": [
        "Yutian Chen",
        "Yuheng Qiu",
        "Ruogu Li",
        "Ali Agha",
        "Shayegan Omidshafiei",
        "Jay Patrikar",
        "Sebastian Scherer"
      ],
      "abstract": "We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\\times$ and $7.2\\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14751v1",
        "pdf": "https://arxiv.org/pdf/2511.14751v1"
      },
      "arxiv_id": "2511.14751v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14749v1",
      "title": "Vision Large Language Models Are Good Noise Handlers in Engagement Analysis",
      "authors": [
        "Alexander Vedernikov",
        "Puneet Kumar",
        "Haoyu Chen",
        "Tapio Seppänen",
        "Xiaobai Li"
      ],
      "abstract": "Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14749v1",
        "pdf": "https://arxiv.org/pdf/2511.14749v1"
      },
      "arxiv_id": "2511.14749v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14745v1",
      "title": "Look-Ahead Reasoning on Learning Platforms",
      "authors": [
        "Haiqing Zhu",
        "Tijana Zrnic",
        "Celestine Mendler-Dünner"
      ],
      "abstract": "On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes, effectively contesting the platform's predictions. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and -- at scale -- impact future predictions. Within this framework, we first formalize level-$k$ thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner's and the users' utilities emerges as a key concept. We discuss connections to several related mathematical frameworks, including strategic classification, performative prediction, and algorithmic collective action.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.GT",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14745v1",
        "pdf": "https://arxiv.org/pdf/2511.14745v1"
      },
      "arxiv_id": "2511.14745v1",
      "comment": "accepted to NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14744v1",
      "title": "Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the Tox21 Challenge",
      "authors": [
        "Antonia Ebner",
        "Christoph Bartmann",
        "Sonja Topf",
        "Sohvi Luukkonen",
        "Johannes Schimunek",
        "Günter Klambauer"
      ],
      "abstract": "Deep learning's rise since the early 2010s has transformed fields like computer vision and natural language processing and strongly influenced biomedical research. For drug discovery specifically, a key inflection - akin to vision's \"ImageNet moment\" - arrived in 2015, when deep neural networks surpassed traditional approaches on the Tox21 Data Challenge. This milestone accelerated the adoption of deep learning across the pharmaceutical industry, and today most major companies have integrated these methods into their research pipelines. After the Tox21 Challenge concluded, its dataset was included in several established benchmarks, such as MoleculeNet and the Open Graph Benchmark. However, during these integrations, the dataset was altered and labels were imputed or manufactured, resulting in a loss of comparability across studies. Consequently, the extent to which bioactivity and toxicity prediction methods have improved over the past decade remains unclear. To this end, we introduce a reproducible leaderboard, hosted on Hugging Face with the original Tox21 Challenge dataset, together with a set of baseline and representative methods. The current version of the leaderboard indicates that the original Tox21 winner - the ensemble-based DeepTox method - and the descriptor-based self-normalizing neural networks introduced in 2017, continue to perform competitively and rank among the top methods for toxicity prediction, leaving it unclear whether substantial progress in toxicity prediction has been achieved over the past decade. As part of this work, we make all baselines and evaluated models publicly accessible for inference via standardized API calls to Hugging Face Spaces.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14744v1",
        "pdf": "https://arxiv.org/pdf/2511.14744v1"
      },
      "arxiv_id": "2511.14744v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14743v1",
      "title": "Beyond Means: A Dynamic Framework for Predicting Customer Satisfaction",
      "authors": [
        "Christof Naumzik",
        "Abdurahman Maarouf",
        "Stefan Feuerriegel",
        "Markus Weinmann"
      ],
      "abstract": "Online ratings influence customer decision-making, yet standard aggregation methods, such as the sample mean, fail to adapt to quality changes over time and ignore review heterogeneity (e.g., review sentiment, a review's helpfulness). To address these challenges, we demonstrate the value of using the Gaussian process (GP) framework for rating aggregation. Specifically, we present a tailored GP model that captures the dynamics of ratings over time while additionally accounting for review heterogeneity. Based on 121,123 ratings from Yelp, we compare the predictive power of different rating aggregation methods in predicting future ratings, thereby finding that the GP model is considerably more accurate and reduces the mean absolute error by 10.2% compared to the sample mean. Our findings have important implications for marketing practitioners and customers. By moving beyond means, designers of online reputation systems can display more informative and adaptive aggregated rating scores that are accurate signals of expected customer satisfaction.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14743v1",
        "pdf": "https://arxiv.org/pdf/2511.14743v1"
      },
      "arxiv_id": "2511.14743v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14742v1",
      "title": "A Neural Field-Based Approach for View Computation & Data Exploration in 3D Urban Environments",
      "authors": [
        "Stefan Cobeli",
        "Kazi Shahrukh Omar",
        "Rodrigo Valença",
        "Nivan Ferreira",
        "Fabio Miranda"
      ],
      "abstract": "Despite the growing availability of 3D urban datasets, extracting insights remains challenging due to computational bottlenecks and the complexity of interacting with data. In fact, the intricate geometry of 3D urban environments results in high degrees of occlusion and requires extensive manual viewpoint adjustments that make large-scale exploration inefficient. To address this, we propose a view-based approach for 3D data exploration, where a vector field encodes views from the environment. To support this approach, we introduce a neural field-based method that constructs an efficient implicit representation of 3D environments. This representation enables both faster direct queries, which consist of the computation of view assessment indices, and inverse queries, which help avoid occlusion and facilitate the search for views that match desired data patterns. Our approach supports key urban analysis tasks such as visibility assessments, solar exposure evaluation, and assessing the visual impact of new developments. We validate our method through quantitative experiments, case studies informed by real-world urban challenges, and feedback from domain experts. Results show its effectiveness in finding desirable viewpoints, analyzing building facade visibility, and evaluating views from outdoor spaces. Code and data are publicly available at https://urbantk.org/neural-3d.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14742v1",
        "pdf": "https://arxiv.org/pdf/2511.14742v1"
      },
      "arxiv_id": "2511.14742v1",
      "comment": "Accepted at IEEE Transactions on Visualization and Computer Graphics. Code and data are publicly available at https://urbantk.org/neural-3d",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.14738v1",
      "title": "LAUD: Integrating Large Language Models with Active Learning for Unlabeled Data",
      "authors": [
        "Tzu-Hsuan Chou",
        "Chun-Nan Chou"
      ],
      "abstract": "Large language models (LLMs) have shown a remarkable ability to generalize beyond their pre-training data, and fine-tuning LLMs can elevate performance to human-level and beyond. However, in real-world scenarios, lacking labeled data often prevents practitioners from obtaining well-performing models, thereby forcing practitioners to highly rely on prompt-based approaches that are often tedious, inefficient, and driven by trial and error. To alleviate this issue of lacking labeled data, we present a learning framework integrating LLMs with active learning for unlabeled dataset (LAUD). LAUD mitigates the cold-start problem by constructing an initial label set with zero-shot learning. Experimental results show that LLMs derived from LAUD outperform LLMs with zero-shot or few-shot learning on commodity name classification tasks, demonstrating the effectiveness of LAUD.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14738v1",
        "pdf": "https://arxiv.org/pdf/2511.14738v1"
      },
      "arxiv_id": "2511.14738v1",
      "comment": "7 pages and one figure",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14730v1",
      "title": "Heterogeneous Multi-Agent Proximal Policy Optimization for Power Distribution System Restoration",
      "authors": [
        "Parya Dolatyabi",
        "Mahdi Khodayar"
      ],
      "abstract": "Restoring power distribution systems (PDS) after large-scale outages requires sequential switching operations that reconfigure feeder topology and coordinate distributed energy resources (DERs) under nonlinear constraints such as power balance, voltage limits, and thermal ratings. These challenges make conventional optimization and value-based RL approaches computationally inefficient and difficult to scale. This paper applies a Heterogeneous-Agent Reinforcement Learning (HARL) framework, instantiated through Heterogeneous-Agent Proximal Policy Optimization (HAPPO), to enable coordinated restoration across interconnected microgrids. Each agent controls a distinct microgrid with different loads, DER capacities, and switch counts, introducing practical structural heterogeneity. Decentralized actor policies are trained with a centralized critic to compute advantage values for stable on-policy updates. A physics-informed OpenDSS environment provides full power flow feedback and enforces operational limits via differentiable penalty signals rather than invalid action masking. The total DER generation is capped at 2400 kW, and each microgrid must satisfy local supply-demand feasibility. Experiments on the IEEE 123-bus and IEEE 8500-node systems show that HAPPO achieves faster convergence, higher restored power, and smoother multi-seed training than DQN, PPO, MAES, MAGDPG, MADQN, Mean-Field RL, and QMIX. Results demonstrate that incorporating microgrid-level heterogeneity within the HARL framework yields a scalable, stable, and constraint-aware solution for complex PDS restoration.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14730v1",
        "pdf": "https://arxiv.org/pdf/2511.14730v1"
      },
      "arxiv_id": "2511.14730v1",
      "comment": "6 pages, 4 figures, TPEC 2025 Conference",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14728v1",
      "title": "Automated proving in planar geometry based on the complex number identity method and elimination",
      "authors": [
        "Zoltán Kovács",
        "Xicheng Peng"
      ],
      "abstract": "We improve the complex number identity proving method to a fully automated procedure, based on elimination ideals. By using declarative equations or rewriting each real-relational hypothesis $h_i$ to $h_i-r_i$, and the thesis $t$ to $t-r$, clearing the denominators and introducing an extra expression with a slack variable, we eliminate all free and relational point variables. From the obtained ideal $I$ in $\\mathbb{Q}[r,r_1,r_2,\\ldots]$ we can find a conclusive result. It plays an important role that if $r_1,r_2,\\ldots$ are real, $r$ must also be real if there is a linear polynomial $p(r)\\in I$, unless division by zero occurs when expressing $r$. Our results are presented in Mathematica, Maple and in a new version of the Giac computer algebra system. Finally, we present a prototype of the automated procedure in an experimental version of the dynamic geometry software GeoGebra.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CG",
        "cs.AI"
      ],
      "primary_category": "cs.CG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14728v1",
        "pdf": "https://arxiv.org/pdf/2511.14728v1"
      },
      "arxiv_id": "2511.14728v1",
      "comment": "15 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14721v1",
      "title": "AdamHD: Decoupled Huber Decay Regularization for Language Model Pre-Training",
      "authors": [
        "Fu-Ming Guo",
        "Yingfang Fan"
      ],
      "abstract": "Adaptive optimizers with decoupled weight decay, such as AdamW, are the de facto standard for pre-training large transformer-based generative models. Yet the quadratic nature of the $\\ell_2$ penalty embedded in weight decay drives all parameters toward the origin at the same rate, making the update vulnerable to rare but extreme gradient directions and often over-penalizing well-conditioned coordinates. We propose AdamHuberDecay, a drop-in replacement for AdamW that substitutes the $\\ell_2$ penalty with a decoupled smooth Huber regularizer. The resulting update decays parameters quadratically while their magnitude remains below a threshold $δ$, and linearly ($\\ell_1$-like) once they exceed $δ$, yielding (i) bounded regularization gradients, (ii) invariance to per-coordinate second-moment rescaling, and (iii) stronger sparsity pressure on overgrown weights.\n  We derive the closed-form decoupled Huber decay step and show how to integrate it with any Adam-family optimizer at $O(1)$ extra cost. Extensive experiments on GPT-2 and GPT-3 pre-training demonstrate that AdamHuberDecay (a) converges 10-15% faster in wall-clock time, (b) reduces validation perplexity by up to 4 points, (c) delivers performance improvements of 2.5-4.7% across downstream tasks, and (d) yields visibly sparser weight histograms that translate into 20-30% memory savings after magnitude pruning, without tuning the decay coefficient beyond the default grid used for AdamW. Ablations confirm robustness to outlier gradients and large-batch regimes, together with theoretical analyses that bound the expected parameter norm under noisy updates. AdamHuberDecay therefore provides a simple, principled path toward more efficient and resilient training of next-generation foundational generative transformers.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14721v1",
        "pdf": "https://arxiv.org/pdf/2511.14721v1"
      },
      "arxiv_id": "2511.14721v1",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: GPU-Accelerated and Scalable Optimization (ScaleOpt)",
      "journal_ref": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: GPU-Accelerated and Scalable Optimization (ScaleOpt)",
      "has_code": false
    },
    {
      "id": "2511.14719v1",
      "title": "Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising",
      "authors": [
        "Yifan Wang",
        "Liya Ji",
        "Zhanghan Ke",
        "Harry Yang",
        "Ser-Nam Lim",
        "Qifeng Chen"
      ],
      "abstract": "We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. Our realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14719v1",
        "pdf": "https://arxiv.org/pdf/2511.14719v1"
      },
      "arxiv_id": "2511.14719v1",
      "comment": "Project Page: https://wyf0824.github.io/Video_Realism_Enhancement/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.14716v1",
      "title": "Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model",
      "authors": [
        "Xiyuan Wang",
        "Muhan Zhang"
      ],
      "abstract": "Standard Latent Diffusion Models rely on a complex, three-part architecture consisting of a separate encoder, decoder, and diffusion network, which are trained in multiple stages. This modular design is computationally inefficient, leads to suboptimal performance, and prevents the unification of diffusion with the single-network architectures common in vision foundation models. Our goal is to unify these three components into a single, end-to-end trainable network. We first demonstrate that a naive joint training approach fails catastrophically due to ``latent collapse'', where the diffusion training objective interferes with the network's ability to learn a good latent representation. We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method. Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space. This approach enables, for the first time, the stable end-to-end training of a single network that simultaneously learns to encode, decode, and perform diffusion. DSD achieves outstanding performance on the ImageNet $256\\times 256$ conditional generation task: FID=13.44/6.38/4.25 with only 42M/118M/205M parameters and 50 training epochs on ImageNet, without using classifier-free-guidance.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14716v1",
        "pdf": "https://arxiv.org/pdf/2511.14716v1"
      },
      "arxiv_id": "2511.14716v1",
      "comment": "Tech Report. 10 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14715v1",
      "title": "\\textit{FLARE}: Adaptive Multi-Dimensional Reputation for Robust Client Reliability in Federated Learning",
      "authors": [
        "Abolfazl Younesi",
        "Leon Kiss",
        "Zahra Najafabadi Samani",
        "Juan Aznar Poveda",
        "Thomas Fahringer"
      ],
      "abstract": "Federated learning (FL) enables collaborative model training while preserving data privacy. However, it remains vulnerable to malicious clients who compromise model integrity through Byzantine attacks, data poisoning, or adaptive adversarial behaviors. Existing defense mechanisms rely on static thresholds and binary classification, failing to adapt to evolving client behaviors in real-world deployments. We propose FLARE, an adaptive reputation-based framework that transforms client reliability assessment from binary decisions to a continuous, multi-dimensional trust evaluation. FLARE integrates: (i) a multi-dimensional reputation score capturing performance consistency, statistical anomaly indicators, and temporal behavior, (ii) a self-calibrating adaptive threshold mechanism that adjusts security strictness based on model convergence and recent attack intensity, (iii) reputation-weighted aggregation with soft exclusion to proportionally limit suspicious contributions rather than eliminating clients outright, and (iv) a Local Differential Privacy (LDP) mechanism enabling reputation scoring on privatized client updates. We further introduce a highly evasive Statistical Mimicry (SM) attack, a benchmark adversary that blends honest gradients with synthetic perturbations and persistent drift to remain undetected by traditional filters. Extensive experiments with 100 clients on MNIST, CIFAR-10, and SVHN demonstrate that FLARE maintains high model accuracy and converges faster than state-of-the-art Byzantine-robust methods under diverse attack types, including label flipping, gradient scaling, adaptive attacks, ALIE, and SM. FLARE improves robustness by up to 16% and preserves model convergence within 30% of the non-attacked baseline, while achieving strong malicious-client detection performance with minimal computational overhead. https://github.com/Anonymous0-0paper/FLARE",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.DC",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14715v1",
        "pdf": "https://arxiv.org/pdf/2511.14715v1"
      },
      "arxiv_id": "2511.14715v1",
      "comment": "Under Review",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14712v1",
      "title": "FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation",
      "authors": [
        "Yunfeng Wu",
        "Jiayi Song",
        "Zhenxiong Tan",
        "Zihao He",
        "Songhua Liu"
      ],
      "abstract": "The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14712v1",
        "pdf": "https://arxiv.org/pdf/2511.14712v1"
      },
      "arxiv_id": "2511.14712v1",
      "comment": "13 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14710v1",
      "title": "Towards a Unified Analysis of Neural Networks in Nonparametric Instrumental Variable Regression: Optimization and Generalization",
      "authors": [
        "Zonghao Chen",
        "Atsushi Nitanda",
        "Arthur Gretton",
        "Taiji Suzuki"
      ],
      "abstract": "We establish the first global convergence result of neural networks for two stage least squares (2SLS) approach in nonparametric instrumental variable regression (NPIV). This is achieved by adopting a lifted perspective through mean-field Langevin dynamics (MFLD), unlike standard MFLD, however, our setting of 2SLS entails a \\emph{bilevel} optimization problem in the space of probability measures. To address this challenge, we leverage the penalty gradient approach recently developed for bilevel optimization which formulates bilevel optimization as a Lagrangian problem. This leads to a novel fully first-order algorithm, termed \\texttt{F$^2$BMLD}. Apart from the convergence bound, we further provide a generalization bound, revealing an inherent trade-off in the choice of the Lagrange multiplier between optimization and statistical guarantees. Finally, we empirically validate the effectiveness of the proposed method on an offline reinforcement learning benchmark.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14710v1",
        "pdf": "https://arxiv.org/pdf/2511.14710v1"
      },
      "arxiv_id": "2511.14710v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14702v1",
      "title": "Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images",
      "authors": [
        "Farheen Ramzan",
        "Yusuf Kiberu",
        "Nikesh Jathanna",
        "Meryem Jabrane",
        "Vicente Grau",
        "Shahnaz Jamil-Copley",
        "Richard H. Clayton",
        "Chen",
        "Chen"
      ],
      "abstract": "Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to \"see beyond the image\", setting a new direction for robust and physiologically grounded cardiac scar segmentation.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14702v1",
        "pdf": "https://arxiv.org/pdf/2511.14702v1"
      },
      "arxiv_id": "2511.14702v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14698v1",
      "title": "HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring",
      "authors": [
        "Sriram Srinivasan",
        "Srinivasan Aruchamy",
        "Siva Ram Krisha Vadali"
      ],
      "abstract": "Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14698v1",
        "pdf": "https://arxiv.org/pdf/2511.14698v1"
      },
      "arxiv_id": "2511.14698v1",
      "comment": "Multi-label seismic signal classification using novel attention-based feature fusion. Submitting to cs.CV due to relevance to general pattern recognition and time-frequency (spectrogram) analysis",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14694v1",
      "title": "Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models",
      "authors": [
        "Rui Zhu",
        "Xiaopu Zhou",
        "Haixu Tang",
        "Stephen W. Scherer",
        "Lucila Ohno-Machado"
      ],
      "abstract": "Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental \"grammar\" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG",
        "q-bio.PE"
      ],
      "primary_category": "q-bio.GN",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14694v1",
        "pdf": "https://arxiv.org/pdf/2511.14694v1"
      },
      "arxiv_id": "2511.14694v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14691v1",
      "title": "Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer",
      "authors": [
        "Kallol Mondal",
        "Ankush Kumar"
      ],
      "abstract": "Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training and inference energy, leading to a large carbon footprint. While brain attention emerges from neural circuits, Transformer attention relies on dot-product similarity to weight elements in the input sequence. Neuromorphic computing, especially spiking neural networks (SNNs), offers a brain-inspired path to energy-efficient intelligence. Despite recent work on attention-based spiking Transformers, the core attention layer remains non-neuromorphic. Current spiking attention (i) relies on dot-product or element-wise similarity suited to floating-point operations, not event-driven spikes; (ii) keeps attention matrices that suffer from the von Neumann bottleneck, limiting in-memory computing; and (iii) still diverges from brain-like computation. To address these issues, we propose the Spiking STDP Transformer (S$^{2}$TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights. STDP, a core mechanism of memory and learning in the brain and widely studied in neuromorphic devices, naturally enables in-memory computing and supports non-von Neumann hardware. On CIFAR-10 and CIFAR-100, our model achieves 94.35\\% and 78.08\\% accuracy with only four timesteps and 0.49 mJ on CIFAR-100, an 88.47\\% energy reduction compared to a standard ANN Transformer. Grad-CAM shows that the model attends to semantically relevant regions, enhancing interpretability. Overall, S$^{2}$TDPT illustrates how biologically inspired attention can yield energy-efficient, hardware-friendly, and explainable neuromorphic models.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.ET",
        "stat.ML"
      ],
      "primary_category": "cs.NE",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14691v1",
        "pdf": "https://arxiv.org/pdf/2511.14691v1"
      },
      "arxiv_id": "2511.14691v1",
      "comment": "21 Pages, 5 Figures, 3 Table",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14689v1",
      "title": "Impact of Image Resolution on Age Estimation with DeepFace and InsightFace",
      "authors": [
        "Shiyar Jamo"
      ],
      "abstract": "Automatic age estimation is widely used for age verification, where input images often vary considerably in resolution. This study evaluates the effect of image resolution on age estimation accuracy using DeepFace and InsightFace. A total of 1000 images from the IMDB-Clean dataset were processed in seven resolutions, resulting in 7000 test samples. Performance was evaluated using Mean Absolute Error (MAE), Standard Deviation (SD), and Median Absolute Error (MedAE). Based on this study, we conclude that input image resolution has a clear and consistent impact on the accuracy of age estimation in both DeepFace and InsightFace. Both frameworks achieve optimal performance at 224x224 pixels, with an MAE of 10.83 years (DeepFace) and 7.46 years (InsightFace). At low resolutions, MAE increases substantially, while very high resolutions also degrade accuracy. InsightFace is consistently faster than DeepFace across all resolutions.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14689v1",
        "pdf": "https://arxiv.org/pdf/2511.14689v1"
      },
      "arxiv_id": "2511.14689v1",
      "comment": "6 pages, 7 figures, 7 tables. Evaluation of DeepFace and InsightFace age estimation across seven image resolutions (64 to 1080 px)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14688v1",
      "title": "Ground Truth Generation for Multilingual Historical NLP using LLMs",
      "authors": [
        "Clovis Gladstone",
        "Zhao Fang",
        "Spencer Dean Stewart"
      ],
      "abstract": "Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14688v1",
        "pdf": "https://arxiv.org/pdf/2511.14688v1"
      },
      "arxiv_id": "2511.14688v1",
      "comment": "13 pages, 5 tables, 1 figure",
      "journal_ref": "CHR2025",
      "has_code": false
    },
    {
      "id": "2511.14682v1",
      "title": "Machine Learning Models for Predicting Smoking-Related Health Decline and Disease Risk",
      "authors": [
        "Vaskar Chakma",
        "MD Jaheid Hasan Nerab",
        "Abdur Rouf",
        "Abu Sayed",
        "Hossem MD Saim",
        "Md. Nournabi Khan"
      ],
      "abstract": "Smoking continues to be a major preventable cause of death worldwide, affecting millions through damage to the heart, metabolism, liver, and kidneys. However, current medical screening methods often miss the early warning signs of smoking-related health problems, leading to late-stage diagnoses when treatment options become limited. This study presents a systematic comparative evaluation of machine learning approaches for smoking-related health risk assessment, emphasizing clinical interpretability and practical deployment over algorithmic innovation. We analyzed health screening data from 55,691 individuals, examining various health indicators, including body measurements, blood tests, and demographic information. We tested three advanced prediction algorithms - Random Forest, XGBoost, and LightGBM - to determine which could most accurately identify people at high risk. This study employed a cross-sectional design to classify current smoking status based on health screening biomarkers, not to predict future disease development. Our Random Forest model performed best, achieving an Area Under the Curve (AUC) of 0.926, meaning it could reliably distinguish between high-risk and lower-risk individuals. Using SHAP (SHapley Additive exPlanations) analysis to understand what the model was detecting, we found that key health markers played crucial roles in prediction: blood pressure levels, triglyceride concentrations, liver enzyme readings, and kidney function indicators (serum creatinine) were the strongest signals of declining health in smokers.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14682v1",
        "pdf": "https://arxiv.org/pdf/2511.14682v1"
      },
      "arxiv_id": "2511.14682v1",
      "comment": "This paper has been officially accepted for publication in the Journal of Intelligent Medicine and Healthcare. Once the final published version is available online, this document will be updated accordingly",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14680v1",
      "title": "NERD: Network-Regularized Diffusion Sampling For 3D Computed Tomography",
      "authors": [
        "Shijun Liang",
        "Ismail Alkhouri",
        "Qing Qu",
        "Rongrong Wang",
        "Saiprasad Ravishankar"
      ],
      "abstract": "Numerous diffusion model (DM)-based methods have been proposed for solving inverse imaging problems. Among these, a recent line of work has demonstrated strong performance by formulating sampling as an optimization procedure that enforces measurement consistency, forward diffusion consistency, and both step-wise and backward diffusion consistency. However, these methods have only considered 2D reconstruction tasks and do not directly extend to 3D image reconstruction problems, such as in Computed Tomography (CT). To bridge this gap, we propose NEtwork-Regularized diffusion sampling for 3D CT (NERD) by incorporating an L1 regularization into the optimization objective. This regularizer encourages spatial continuity across adjacent slices, reducing inter-slice artifacts and promoting coherent volumetric reconstructions. Additionally, we introduce two efficient optimization strategies to solve the resulting objective: one based on the Alternating Direction Method of Multipliers (ADMM) and another based on the Primal-Dual Hybrid Gradient (PDHG) method. Experiments on medical 3D CT data demonstrate that our approach achieves either state-of-the-art or highly competitive results.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "eess.IV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14680v1",
        "pdf": "https://arxiv.org/pdf/2511.14680v1"
      },
      "arxiv_id": "2511.14680v1",
      "comment": "",
      "journal_ref": "CAMSAP2025",
      "has_code": false
    },
    {
      "id": "2511.14670v1",
      "title": "SkillGen: Learning Domain Skills for In-Context Sequential Decision Making",
      "authors": [
        "Ruomeng Ding",
        "Wei Cheng",
        "Minglai Shao",
        "Chen Zhao"
      ],
      "abstract": "Large language models (LLMs) are increasingly applied to sequential decision-making through in-context learning (ICL), yet their effectiveness is highly sensitive to prompt quality. Effective prompts should meet three principles: focus on decision-critical information, provide step-level granularity, and minimize reliance on expert annotations through label efficiency. However, existing ICL methods often fail to satisfy all three criteria simultaneously. Motivated by these challenges, we introduce SkillGen, a skill-based ICL framework for structured sequential reasoning. It constructs an action-centric, domain-level graph from sampled trajectories, identifies high-utility actions via temporal-difference credit assignment, and retrieves step-wise skills to generate fine-grained, context-aware prompts. We further present a theoretical analysis showing that focusing on high-utility segments supports task identifiability and informs more effective ICL prompt design. Experiments on ALFWorld, BabyAI, and ScienceWorld, using both open-source and proprietary LLMs, show that SkillGen achieves consistent gains, improving progress rate by 5.9%-16.5% on average across models.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14670v1",
        "pdf": "https://arxiv.org/pdf/2511.14670v1"
      },
      "arxiv_id": "2511.14670v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14659v1",
      "title": "NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards",
      "authors": [
        "Chia-Yu Hung",
        "Navonil Majumder",
        "Haoyuan Deng",
        "Liu Renhang",
        "Yankang Ang",
        "Amir Zadeh",
        "Chuan Li",
        "Dorien Herremans",
        "Ziwei Wang",
        "Soujanya Poria"
      ],
      "abstract": "Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14659v1",
        "pdf": "https://arxiv.org/pdf/2511.14659v1"
      },
      "arxiv_id": "2511.14659v1",
      "comment": "https://declare-lab.github.io/nora-1.5",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.14654v1",
      "title": "Improving segmentation of retinal arteries and veins using cardiac signal in doppler holograms",
      "authors": [
        "Marius Dubosc",
        "Yann Fischer",
        "Zacharie Auray",
        "Nicolas Boutry",
        "Edwin Carlinet",
        "Michael Atlan",
        "Thierry Geraud"
      ],
      "abstract": "Doppler holography is an emerging retinal imaging technique that captures the dynamic behavior of blood flow with high temporal resolution, enabling quantitative assessment of retinal hemodynamics. This requires accurate segmentation of retinal arteries and veins, but traditional segmentation methods focus solely on spatial information and overlook the temporal richness of holographic data. In this work, we propose a simple yet effective approach for artery-vein segmentation in temporal Doppler holograms using standard segmentation architectures. By incorporating features derived from a dedicated pulse analysis pipeline, our method allows conventional U-Nets to exploit temporal dynamics and achieve performance comparable to more complex attention- or iteration-based models. These findings demonstrate that time-resolved preprocessing can unlock the full potential of deep learning for Doppler holography, opening new perspectives for quantitative exploration of retinal hemodynamics. The dataset is publicly available at https://huggingface.co/datasets/DigitalHolography/",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14654v1",
        "pdf": "https://arxiv.org/pdf/2511.14654v1"
      },
      "arxiv_id": "2511.14654v1",
      "comment": "5 pages, 3 figures, 1 table. Submitted to ISBI2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14651v1",
      "title": "Derivative of the truncated singular value and eigen decomposition",
      "authors": [
        "Jan Naumann"
      ],
      "abstract": "Recently developed applications in the field of machine learning and computational physics rely on automatic differentiation techniques, that require stable and efficient linear algebra gradient computations. This technical note provides a comprehensive and detailed discussion of the derivative of the truncated singular and eigenvalue decomposition. It summarizes previous work and builds on them with an extensive description of how to derive the relevant terms. A main focus is correctly expressing the derivative in terms of the truncated part, despite lacking knowledge of the full decomposition.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "math.NA",
        "cs.LG",
        "physics.comp-ph"
      ],
      "primary_category": "math.NA",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14651v1",
        "pdf": "https://arxiv.org/pdf/2511.14651v1"
      },
      "arxiv_id": "2511.14651v1",
      "comment": "Technical report",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14650v1",
      "title": "AutoTool: Efficient Tool Selection for Large Language Model Agents",
      "authors": [
        "Jingyi Jia",
        "Qinbin Li"
      ],
      "abstract": "Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14650v1",
        "pdf": "https://arxiv.org/pdf/2511.14650v1"
      },
      "arxiv_id": "2511.14650v1",
      "comment": "Accepted by AAAI 2026, 18 pages, 11 figures, Code: https://github.com/jiajingyyyyyy/AutoTool",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.14649v1",
      "title": "RepAir: A Framework for Airway Segmentation and Discontinuity Correction in CT",
      "authors": [
        "John M. Oyer",
        "Ali Namvar",
        "Benjamin A. Hoff",
        "Wassim W. Labaki",
        "Ella A. Kazerooni",
        "Charles R. Hatt",
        "Fernando J. Martinez",
        "MeiLan K. Han",
        "Craig J. Galbán",
        "Sundaresh Ram"
      ],
      "abstract": "Accurate airway segmentation from chest computed tomography (CT) scans is essential for quantitative lung analysis, yet manual annotation is impractical and many automated U-Net-based methods yield disconnected components that hinder reliable biomarker extraction. We present RepAir, a three-stage framework for robust 3D airway segmentation that combines an nnU-Net-based network with anatomically informed topology correction. The segmentation network produces an initial airway mask, after which a skeleton-based algorithm identifies potential discontinuities and proposes reconnections. A 1D convolutional classifier then determines which candidate links correspond to true anatomical branches versus false or obstructed paths. We evaluate RepAir on two distinct datasets: ATM'22, comprising annotated CT scans from predominantly healthy subjects and AeroPath, encompassing annotated scans with severe airway pathology. Across both datasets, RepAir outperforms existing 3D U-Net-based approaches such as Bronchinet and NaviAirway on both voxel-level and topological metrics, and produces more complete and anatomically consistent airway trees while maintaining high segmentation accuracy.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14649v1",
        "pdf": "https://arxiv.org/pdf/2511.14649v1"
      },
      "arxiv_id": "2511.14649v1",
      "comment": "4 pages, 3 figures, 1 table. Preprint submitted to SSIAI 2026 Conference on November 17, 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14640v1",
      "title": "Doppler Invariant CNN for Signal Classification",
      "authors": [
        "Avi Bagchi",
        "Dwight Hutchenson"
      ],
      "abstract": "Radio spectrum monitoring in contested environments motivates the need for reliable automatic signal classification technology. Prior work highlights deep learning as a promising approach, but existing models depend on brute-force Doppler augmentation to achieve real-world generalization, which undermines both training efficiency and interpretability. In this paper, we propose a convolutional neural network (CNN) architecture with complex-valued layers that exploits convolutional shift equivariance in the frequency domain. To establish provable frequency bin shift invariance, we use adaptive polyphase sampling (APS) as pooling layers followed by a global average pooling layer at the end of the network. Using a synthetic dataset of common interference signals, experimental results demonstrate that unlike a vanilla CNN, our model maintains consistent classification accuracy with and without random Doppler shifts despite being trained on no Doppler-shifted examples. Overall, our method establishes an invariance-driven framework for signal classification that offers provable robustness against real-world effects.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14640v1",
        "pdf": "https://arxiv.org/pdf/2511.14640v1"
      },
      "arxiv_id": "2511.14640v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14639v1",
      "title": "SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology",
      "authors": [
        "Marco Acerbis",
        "Swarnadip Chatterjee",
        "Christophe Avenel",
        "Joakim Lindblad"
      ],
      "abstract": "Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low. We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks. To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse. We integrate the pretrained encoder into an attention-based Multiple Instance Learning aggregator for bag-level prediction and attention-guided retrieval of the most abnormal instances in a bag. On a publicly available bone-marrow cytology dataset, with simulated witness rates from 10% down to 0.5%, SLAM-AGS improves bag-level F1-Score and Top 400 positive cell retrieval over other pretraining methods, with the largest gains at low witness rates, showing that resolving gradient interference enables stable pretraining and better performance on downstream tasks. To facilitate reproducibility, we share our complete implementation and evaluation framework as open source: https://github.com/Ace95/SLAM-AGS.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14639v1",
        "pdf": "https://arxiv.org/pdf/2511.14639v1"
      },
      "arxiv_id": "2511.14639v1",
      "comment": "5 pages, 2 figures, Submitted to ISBI2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14633v1",
      "title": "SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction",
      "authors": [
        "Meiying Gu",
        "Jiawei Zhang",
        "Jiahe Li",
        "Xiaohan Yu",
        "Haonan Luo",
        "Jin Zheng",
        "Xiao Bai"
      ],
      "abstract": "Recent advances in optimizing Gaussian Splatting for scene geometry have enabled efficient reconstruction of detailed surfaces from images. However, when input views are sparse, such optimization is prone to overfitting, leading to suboptimal reconstruction quality. Existing approaches address this challenge by employing flattened Gaussian primitives to better fit surface geometry, combined with depth regularization to alleviate geometric ambiguities under limited viewpoints. Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance. In this paper, we propose \\net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering. Our key insight is to introduce Stereo Geometry-Texture Alignment, which bridges rendering quality and geometry estimation, thereby jointly enhancing both surface reconstruction and view synthesis. In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision. Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14633v1",
        "pdf": "https://arxiv.org/pdf/2511.14633v1"
      },
      "arxiv_id": "2511.14633v1",
      "comment": "Accepted at AAAI 2026. Project page: https://miya-oi.github.io/SparseSurf-project",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.14632v1",
      "title": "Adapformer: Adaptive Channel Management for Multivariate Time Series Forecasting",
      "authors": [
        "Yuchen Luo",
        "Xinyu Li",
        "Liuhua Peng",
        "Mingming Gong"
      ],
      "abstract": "In multivariate time series forecasting (MTSF), accurately modeling the intricate dependencies among multiple variables remains a significant challenge due to the inherent limitations of traditional approaches. Most existing models adopt either \\textbf{channel-independent} (CI) or \\textbf{channel-dependent} (CD) strategies, each presenting distinct drawbacks. CI methods fail to leverage the potential insights from inter-channel interactions, resulting in models that may not fully exploit the underlying statistical dependencies present in the data. Conversely, CD approaches often incorporate too much extraneous information, risking model overfitting and predictive inefficiency. To address these issues, we introduce the Adaptive Forecasting Transformer (\\textbf{Adapformer}), an advanced Transformer-based framework that merges the benefits of CI and CD methodologies through effective channel management. The core of Adapformer lies in its dual-stage encoder-decoder architecture, which includes the \\textbf{A}daptive \\textbf{C}hannel \\textbf{E}nhancer (\\textbf{ACE}) for enriching embedding processes and the \\textbf{A}daptive \\textbf{C}hannel \\textbf{F}orecaster (\\textbf{ACF}) for refining the predictions. ACE enhances token representations by selectively incorporating essential dependencies, while ACF streamlines the decoding process by focusing on the most relevant covariates, substantially reducing noise and redundancy. Our rigorous testing on diverse datasets shows that Adapformer achieves superior performance over existing models, enhancing both predictive accuracy and computational efficiency, thus making it state-of-the-art in MTSF.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14632v1",
        "pdf": "https://arxiv.org/pdf/2511.14632v1"
      },
      "arxiv_id": "2511.14632v1",
      "comment": "",
      "journal_ref": "Neural Networks Volume 193 (2026) Article Number 107988",
      "has_code": false
    },
    {
      "id": "2511.14631v1",
      "title": "Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities",
      "authors": [
        "Kahaan Gandhi",
        "Boris Bolliet",
        "Inigo Zubeldia"
      ],
      "abstract": "We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14631v1",
        "pdf": "https://arxiv.org/pdf/2511.14631v1"
      },
      "arxiv_id": "2511.14631v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14630v1",
      "title": "Failure to Mix: Large language models struggle to answer according to desired probability distributions",
      "authors": [
        "Ivy Yuqian Yang",
        "David Yu Zhang"
      ],
      "abstract": "Scientific idea generation and selection requires exploration following a target probability distribution. In contrast, current AI benchmarks have objectively correct answers, and training large language models (LLMs) via reinforcement learning against these benchmarks discourages probabilistic exploration. Here, we conducted systematic experiments requesting LLMs to produce outputs following simple probabilistic distributions, and found that all modern LLMs tested grossly fail to follow the distributions. For example, requesting a binary output of \"1\" 49% of the time produces an answer of \"0\" nearly 100% of the time. This step function-like behavior of near-exclusively generating the output with marginally highest probability even overrules even strong in-built LLM biases.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14630v1",
        "pdf": "https://arxiv.org/pdf/2511.14630v1"
      },
      "arxiv_id": "2511.14630v1",
      "comment": "13 pages, 6 figures. Code and reproducibility package: https://github.com/BiostateAIresearch/failure-to-mix",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.14624v1",
      "title": "Active Matter as a framework for living systems-inspired Robophysics",
      "authors": [
        "Giulia Janzen",
        "Gaia Maselli",
        "Juan F. Jimenez",
        "Lia Garcia-Perez",
        "D A Matoz Fernandez",
        "Chantal Valeriani"
      ],
      "abstract": "Robophysics investigates the physical principles that govern living-like robots operating in complex, realworld environments. Despite remarkable technological advances, robots continue to face fundamental efficiency limitations. At the level of individual units, locomotion remains a challenge, while at the collective level, robot swarms struggle to achieve shared purpose, coordination, communication, and cost efficiency. This perspective article examines the key challenges faced by bio-inspired robotic collectives and highlights recent research efforts that incorporate principles from active-matter physics and biology into the modeling and design of robot swarms.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cond-mat.soft",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cond-mat.soft",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14624v1",
        "pdf": "https://arxiv.org/pdf/2511.14624v1"
      },
      "arxiv_id": "2511.14624v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14620v1",
      "title": "Fusing Biomechanical and Spatio-Temporal Features for Fall Prediction: Characterizing and Mitigating the Simulation-to-Reality Gap",
      "authors": [
        "Md Fokhrul Islam",
        "Sajeda Al-Hammouri",
        "Christopher J. Arellano",
        "Kavan Hazeli",
        "Heman Shakeri"
      ],
      "abstract": "Falls are a leading cause of injury and loss of independence among older adults. Vision-based fall prediction systems offer a non-invasive solution to anticipate falls seconds before impact, but their development is hindered by the scarcity of available fall data. Contributing to these efforts, this study proposes the Biomechanical Spatio-Temporal Graph Convolutional Network (BioST-GCN), a dual-stream model that combines both pose and biomechanical information using a cross-attention fusion mechanism. Our model outperforms the vanilla ST-GCN baseline by 5.32% and 2.91% F1-score on the simulated MCF-UA stunt-actor and MUVIM datasets, respectively. The spatio-temporal attention mechanisms in the ST-GCN stream also provide interpretability by identifying critical joints and temporal phases. However, a critical simulation-reality gap persists. While our model achieves an 89.0% F1-score with full supervision on simulated data, zero-shot generalization to unseen subjects drops to 35.9%. This performance decline is likely due to biases in simulated data, such as `intent-to-fall' cues. For older adults, particularly those with diabetes or frailty, this gap is exacerbated by their unique kinematic profiles. To address this, we propose personalization strategies and advocate for privacy-preserving data pipelines to enable real-world validation. Our findings underscore the urgent need to bridge the gap between simulated and real-world data to develop effective fall prediction systems for vulnerable elderly populations.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14620v1",
        "pdf": "https://arxiv.org/pdf/2511.14620v1"
      },
      "arxiv_id": "2511.14620v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14619v1",
      "title": "Expert-Guided POMDP Learning for Data-Efficient Modeling in Healthcare",
      "authors": [
        "Marco Locatelli",
        "Arjen Hommersom",
        "Roberto Clemens Cerioli",
        "Daniela Besozzi",
        "Fabio Stella"
      ],
      "abstract": "Learning the parameters of Partially Observable Markov Decision Processes (POMDPs) from limited data is a significant challenge. We introduce the Fuzzy MAP EM algorithm, a novel approach that incorporates expert knowledge into the parameter estimation process by enriching the Expectation Maximization (EM) framework with fuzzy pseudo-counts derived from an expert-defined fuzzy model. This integration naturally reformulates the problem as a Maximum A Posteriori (MAP) estimation, effectively guiding learning in environments with limited data. In synthetic medical simulations, our method consistently outperforms the standard EM algorithm under both low-data and high-noise conditions. Furthermore, a case study on Myasthenia Gravis illustrates the ability of the Fuzzy MAP EM algorithm to recover a clinically coherent POMDP, demonstrating its potential as a practical tool for data-efficient modeling in healthcare.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14619v1",
        "pdf": "https://arxiv.org/pdf/2511.14619v1"
      },
      "arxiv_id": "2511.14619v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14617v1",
      "title": "Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning",
      "authors": [
        "Ruoyu Qin",
        "Weiran He",
        "Weixiao Huang",
        "Yangkun Zhang",
        "Yikai Zhao",
        "Bo Pang",
        "Xinran Xu",
        "Yingdi Shan",
        "Yongwei Wu",
        "Mingxing Zhang"
      ],
      "abstract": "Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14617v1",
        "pdf": "https://arxiv.org/pdf/2511.14617v1"
      },
      "arxiv_id": "2511.14617v1",
      "comment": "16 pages, 12 figures, 6 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14613v1",
      "title": "3D-Guided Scalable Flow Matching for Generating Volumetric Tissue Spatial Transcriptomics from Serial Histology",
      "authors": [
        "Mohammad Vali Sanian",
        "Arshia Hemmat",
        "Amirhossein Vahidi",
        "Jonas Maaskola",
        "Jimmy Tsz Hang Lee",
        "Stanislaw Makarchuk",
        "Yeliz Demirci",
        "Nana-Jane Chipampe",
        "Omer Bayraktar",
        "Lassi Paavolainen",
        "Mohammad Lotfollahi"
      ],
      "abstract": "A scalable and robust 3D tissue transcriptomics profile can enable a holistic understanding of tissue organization and provide deeper insights into human biology and disease. Most predictive algorithms that infer ST directly from histology treat each section independently and ignore 3D structure, while existing 3D-aware approaches are not generative and do not scale well. We present Holographic Tissue Expression Inpainting and Analysis (HoloTea), a 3D-aware flow-matching framework that imputes spot-level gene expression from H&E while explicitly using information from adjacent sections. Our key idea is to retrieve morphologically corresponding spots on neighboring slides in a shared feature space and fuse this cross section context into a lightweight ControlNet, allowing conditioning to follow anatomical continuity. To better capture the count nature of the data, we introduce a 3D-consistent prior for flow matching that combines a learned zero-inflated negative binomial (ZINB) prior with a spatial-empirical prior constructed from neighboring sections. A global attention block introduces 3D H&E scaling linearly with the number of spots in the slide, enabling training and inference on large 3D ST datasets. Across three spatial transcriptomics datasets spanning different tissue types and resolutions, HoloTea consistently improves 3D expression accuracy and generalization compared to 2D and 3D baselines. We envision HoloTea advancing the creation of accurate 3D virtual tissues, ultimately accelerating biomarker discovery and deepening our understanding of disease.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14613v1",
        "pdf": "https://arxiv.org/pdf/2511.14613v1"
      },
      "arxiv_id": "2511.14613v1",
      "comment": "11 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14606v1",
      "title": "Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models",
      "authors": [
        "Shreya Adrita Banik",
        "Niaz Nafi Rahman",
        "Tahsina Moiukh",
        "Farig Sadeque"
      ],
      "abstract": "Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14606v1",
        "pdf": "https://arxiv.org/pdf/2511.14606v1"
      },
      "arxiv_id": "2511.14606v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14604v1",
      "title": "XAttn-BMD: Multimodal Deep Learning with Cross-Attention for Femoral Neck Bone Mineral Density Estimation",
      "authors": [
        "Yilin Zhang",
        "Leo D. Westbury",
        "Elaine M. Dennison",
        "Nicholas C. Harvey",
        "Nicholas R. Fuggle",
        "Rahman Attar"
      ],
      "abstract": "Poor bone health is a significant public health concern, and low bone mineral density (BMD) leads to an increased fracture risk, a key feature of osteoporosis. We present XAttn-BMD (Cross-Attention BMD), a multimodal deep learning framework that predicts femoral neck BMD from hip X-ray images and structured clinical metadata. It utilizes a novel bidirectional cross-attention mechanism to dynamically integrate image and metadata features for cross-modal mutual reinforcement. A Weighted Smooth L1 loss is tailored to address BMD imbalance and prioritize clinically significant cases. Extensive experiments on the data from the Hertfordshire Cohort Study show that our model outperforms the baseline models in regression generalization and robustness. Ablation studies confirm the effectiveness of both cross-attention fusion and the customized loss function. Experimental results show that the integration of multimodal data via cross-attention outperforms naive feature concatenation without cross-attention, reducing MSE by 16.7%, MAE by 6.03%, and increasing the R2 score by 16.4%, highlighting the effectiveness of the approach for femoral neck BMD estimation. Furthermore, screening performance was evaluated using binary classification at clinically relevant femoral neck BMD thresholds, demonstrating the model's potential in real-world scenarios.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14604v1",
        "pdf": "https://arxiv.org/pdf/2511.14604v1"
      },
      "arxiv_id": "2511.14604v1",
      "comment": "11 figures, 10 tables, 38 pages. Submitted to Artificial Intelligence in Medicine (currently with editor)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14603v1",
      "title": "A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease",
      "authors": [
        "Yilu Fang",
        "Jordan G. Nestor",
        "Casey N. Ta",
        "Jerard Z. Kneifati-Hayek",
        "Chunhua Weng"
      ],
      "abstract": "Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14603v1",
        "pdf": "https://arxiv.org/pdf/2511.14603v1"
      },
      "arxiv_id": "2511.14603v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14601v1",
      "title": "MRI Embeddings Complement Clinical Predictors for Cognitive Decline Modeling in Alzheimer's Disease Cohorts",
      "authors": [
        "Nathaniel Putera",
        "Daniel Vilet Rodríguez",
        "Noah Videcrantz",
        "Julia Machnio",
        "Mostafa Mehdipour Ghazi"
      ],
      "abstract": "Accurate modeling of cognitive decline in Alzheimer's disease is essential for early stratification and personalized management. While tabular predictors provide robust markers of global risk, their ability to capture subtle brain changes remains limited. In this study, we evaluate the predictive contributions of tabular and imaging-based representations, with a focus on transformer-derived Magnetic Resonance Imaging (MRI) embeddings. We introduce a trajectory-aware labeling strategy based on Dynamic Time Warping clustering to capture heterogeneous patterns of cognitive change, and train a 3D Vision Transformer (ViT) via unsupervised reconstruction on harmonized and augmented MRI data to obtain anatomy-preserving embeddings without progression labels. The pretrained encoder embeddings are subsequently assessed using both traditional machine learning classifiers and deep learning heads, and compared against tabular representations and convolutional network baselines. Results highlight complementary strengths across modalities. Clinical and volumetric features achieved the highest AUCs of around 0.70 for predicting mild and severe progression, underscoring their utility in capturing global decline trajectories. In contrast, MRI embeddings from the ViT model were most effective in distinguishing cognitively stable individuals with an AUC of 0.71. However, all approaches struggled in the heterogeneous moderate group. These findings indicate that clinical features excel in identifying high-risk extremes, whereas transformer-based MRI embeddings are more sensitive to subtle markers of stability, motivating multimodal fusion strategies for AD progression modeling.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14601v1",
        "pdf": "https://arxiv.org/pdf/2511.14601v1"
      },
      "arxiv_id": "2511.14601v1",
      "comment": "Accepted at SPIE - Medical Imaging Conference 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14599v1",
      "title": "CCSD: Cross-Modal Compositional Self-Distillation for Robust Brain Tumor Segmentation with Missing Modalities",
      "authors": [
        "Dongqing Xie",
        "Yonghuang Wu",
        "Zisheng Ai",
        "Jun Min",
        "Zhencun Jiang",
        "Shaojin Geng",
        "Lei Wang"
      ],
      "abstract": "The accurate segmentation of brain tumors from multi-modal MRI is critical for clinical diagnosis and treatment planning. While integrating complementary information from various MRI sequences is a common practice, the frequent absence of one or more modalities in real-world clinical settings poses a significant challenge, severely compromising the performance and generalizability of deep learning-based segmentation models. To address this challenge, we propose a novel Cross-Modal Compositional Self-Distillation (CCSD) framework that can flexibly handle arbitrary combinations of input modalities. CCSD adopts a shared-specific encoder-decoder architecture and incorporates two self-distillation strategies: (i) a hierarchical modality self-distillation mechanism that transfers knowledge across modality hierarchies to reduce semantic discrepancies, and (ii) a progressive modality combination distillation approach that enhances robustness to missing modalities by simulating gradual modality dropout during training. Extensive experiments on public brain tumor segmentation benchmarks demonstrate that CCSD achieves state-of-the-art performance across various missing-modality scenarios, with strong generalization and stability.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14599v1",
        "pdf": "https://arxiv.org/pdf/2511.14599v1"
      },
      "arxiv_id": "2511.14599v1",
      "comment": "9 pages, 5 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14595v1",
      "title": "Rate-Distortion Guided Knowledge Graph Construction from Lecture Notes Using Gromov-Wasserstein Optimal Transport",
      "authors": [
        "Yuan An",
        "Ruhma Hashmi",
        "Michelle Rogers",
        "Jane Greenberg",
        "Brian K. Smith"
      ],
      "abstract": "Task-oriented knowledge graphs (KGs) enable AI-powered learning assistant systems to automatically generate high-quality multiple-choice questions (MCQs). Yet converting unstructured educational materials, such as lecture notes and slides, into KGs that capture key pedagogical content remains difficult. We propose a framework for knowledge graph construction and refinement grounded in rate-distortion (RD) theory and optimal transport geometry. In the framework, lecture content is modeled as a metric-measure space, capturing semantic and relational structure, while candidate KGs are aligned using Fused Gromov-Wasserstein (FGW) couplings to quantify semantic distortion. The rate term, expressed via the size of KG, reflects complexity and compactness. Refinement operators (add, merge, split, remove, rewire) minimize the rate-distortion Lagrangian, yielding compact, information-preserving KGs. Our prototype applied to data science lectures yields interpretable RD curves and shows that MCQs generated from refined KGs consistently surpass those from raw notes on fifteen quality criteria. This study establishes a principled foundation for information-theoretic KG optimization in personalized and AI-assisted education.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14595v1",
        "pdf": "https://arxiv.org/pdf/2511.14595v1"
      },
      "arxiv_id": "2511.14595v1",
      "comment": "Accepted in the 5th Workshop on Knowledge Graphs and Big Data in Conjunction with IEEE Big Data 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14592v1",
      "title": "Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks",
      "authors": [
        "Xianhui Meng",
        "Yuchen Zhang",
        "Zhijian Huang",
        "Zheng Lu",
        "Ziling Ji",
        "Yaoyao Yin",
        "Hongyuan Zhang",
        "Guangfeng Jiang",
        "Yandan Lin",
        "Long Chen",
        "Hangjun Ye",
        "Li Zhang",
        "Jun Liu",
        "Xiaoshuai Hao"
      ],
      "abstract": "Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14592v1",
        "pdf": "https://arxiv.org/pdf/2511.14592v1"
      },
      "arxiv_id": "2511.14592v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14591v1",
      "title": "Biased Minds Meet Biased AI: How Class Imbalance Shapes Appropriate Reliance and Interacts with Human Base Rate Neglect",
      "authors": [
        "Nick von Felten",
        "Johannes Schöning",
        "Klaus Opwis",
        "Nicolas Scharowksi"
      ],
      "abstract": "Humans increasingly interact with artificial intelligence (AI) in decision-making. However, both AI and humans are prone to biases. While AI and human biases have been studied extensively in isolation, this paper examines their complex interaction. Specifically, we examined how class imbalance as an AI bias affects people's ability to appropriately rely on an AI-based decision-support system, and how it interacts with base rate neglect as a human bias. In a within-subject online study (N= 46), participants classified three diseases using an AI-based decision-support system trained on either a balanced or unbalanced dataset. We found that class imbalance disrupted participants' calibration of AI reliance. Moreover, we observed mutually reinforcing effects between class imbalance and base rate neglect, offering evidence of a compound human-AI bias. Based on these findings, we advocate for an interactionist perspective and further research into the mutually reinforcing effects of biases in human-AI interaction.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14591v1",
        "pdf": "https://arxiv.org/pdf/2511.14591v1"
      },
      "arxiv_id": "2511.14591v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14588v1",
      "title": "Deep Learning-Based Regional White Matter Hyperintensity Mapping as a Robust Biomarker for Alzheimer's Disease",
      "authors": [
        "Julia Machnio",
        "Mads Nielsen",
        "Mostafa Mehdipour Ghazi"
      ],
      "abstract": "White matter hyperintensities (WMH) are key imaging markers in cognitive aging, Alzheimer's disease (AD), and related dementias. Although automated methods for WMH segmentation have advanced, most provide only global lesion load and overlook their spatial distribution across distinct white matter regions. We propose a deep learning framework for robust WMH segmentation and localization, evaluated across public datasets and an independent Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort. Our results show that the predicted lesion loads are in line with the reference WMH estimates, confirming the robustness to variations in lesion load, acquisition, and demographics. Beyond accurate segmentation, we quantify WMH load within anatomically defined regions and combine these measures with brain structure volumes to assess diagnostic value. Regional WMH volumes consistently outperform global lesion burden for disease classification, and integration with brain atrophy metrics further improves performance, reaching area under the curve (AUC) values up to 0.97. Several spatially distinct regions, particularly within anterior white matter tracts, are reproducibly associated with diagnostic status, indicating localized vulnerability in AD. These results highlight the added value of regional WMH quantification. Incorporating localized lesion metrics alongside atrophy markers may enhance early diagnosis and stratification in neurodegenerative disorders.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14588v1",
        "pdf": "https://arxiv.org/pdf/2511.14588v1"
      },
      "arxiv_id": "2511.14588v1",
      "comment": "Accepted at SPIE - Medical Imaging Conference 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14584v1",
      "title": "ReflexGrad: Three-Way Synergistic Architecture for Zero-Shot Generalization in LLM Agents",
      "authors": [
        "Ankush Kadu",
        "Ashwanth Krishnan"
      ],
      "abstract": "Enabling agents to learn from experience and generalize across diverse tasks without task-specific training remains a fundamental challenge in reinforcement learning and decision-making. While recent approaches have explored episodic memory (Reflexion), gradient-based prompt optimization (TextGrad),and hierarchical task decomposition independently, their potential for synergistic integration remains unexplored. We introduce ReflexGrad, a novel architecture that tightly couples three complementary mechanisms: (1) LLM-based hierarchical TODO decomposition for strategic planning, (2) history-aware causal reflection that analyzes recent action patterns to identify failure root causes and enable within-trial learning, and (3) gradient-based optimization for systematic improvement. Unlike prior work relying on few-shot demonstrations, our system achieves true zero-shot generalization through pure LLM semantic reasoning,requiring no task-specific examples, fine-tuning, or hardcoded similarity metrics. Evaluated on ALFWorld benchmark tasks, ReflexGrad demonstrates 67% zero-shot success rate on Trial 0 without any prior task experience or demonstrations, establishing effective performance on first exposure. Through empirical analysis, we identify the architectural mechanisms underlying stable convergence (zero action loops) and effective cross-task transfer (67% to 78% improvement).Our work demonstrates that synergistic integration of complementary learning mechanisms enables robust zero-shot generalization that approaches few-shot baselines from prior work.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14584v1",
        "pdf": "https://arxiv.org/pdf/2511.14584v1"
      },
      "arxiv_id": "2511.14584v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14582v1",
      "title": "OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models",
      "authors": [
        "Keda Tao",
        "Kele Shao",
        "Bohan Yu",
        "Weiqiang Wang",
        "Jian liu",
        "Huan Wang"
      ],
      "abstract": "Omnimodal large language models (OmniLLMs) have attracted increasing research attention of late towards unified audio-video understanding, wherein processing audio-video token sequences creates a significant computational bottleneck, however. Existing token compression methods have yet to accommodate this emerging need of jointly compressing multimodal tokens. To bridge this gap, we present OmniZip, a training-free, audio-guided audio-visual token-compression framework that optimizes multimodal token representation and accelerates inference. Specifically, OmniZip first identifies salient audio tokens, then computes an audio retention score for each time group to capture information density, thereby dynamically guiding video token pruning and preserving cues from audio anchors enhanced by cross-modal similarity. For each time window, OmniZip compresses the video tokens using an interleaved spatio-temporal scheme. Extensive empirical results demonstrate the merits of OmniZip - it achieves 3.42X inference speedup and 1.4X memory reduction over other top-performing counterparts, while maintaining performance with no training.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14582v1",
        "pdf": "https://arxiv.org/pdf/2511.14582v1"
      },
      "arxiv_id": "2511.14582v1",
      "comment": "Code Link: https://github.com/KD-TAO/OmniZip",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.14581v1",
      "title": "Online learning of subgrid-scale models for quasi-geostrophic turbulence in planetary interiors",
      "authors": [
        "Hugo Frezat",
        "Thomas Gastine",
        "Alexandre Fournier"
      ],
      "abstract": "The use of machine learning to represent subgrid-scale (SGS) dynamics is now well established in weather forecasting and climate modelling. Recent advances have demonstrated that SGS models trained via ``online'' end-to-end learning -- where the dynamical solver operating on the filtered equations participates in the training -- can outperform traditional physics-based approaches. Most studies, however, have focused on idealised periodic domains, neglecting the mechanical boundaries present e.g. in planetary interiors. To address this issue, we consider two-dimensional quasi-geostrophic turbulent flow in an axisymmetric bounded domain that we model using a pseudo-spectral differentiable solver, thereby enabling online learning. We examine three configurations, varying the geometry (between an exponential container and a spherical shell) and the rotation rate. Flow is driven by a prescribed analytical forcing, allowing for precise control over the energy injection scale and an exact estimate of the power input. We evaluate the accuracy of the online-trained SGS model against the reference direct numerical simulation using integral quantities and spectral diagnostics. In all configurations, we show that an SGS model trained on data spanning only one turnover time remains stable and accurate over integrations at least a hundred times longer than the training period. Moreover, we demonstrate the model's remarkable ability to reproduce slow processes occurring on time scales far exceeding the training duration, such as the inward drift of jets in the spherical shell. These results suggest a promising path towards developing SGS models for planetary and stellar interior dynamics, including dynamo processes.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "physics.flu-dyn",
        "astro-ph.EP",
        "cs.LG"
      ],
      "primary_category": "physics.flu-dyn",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14581v1",
        "pdf": "https://arxiv.org/pdf/2511.14581v1"
      },
      "arxiv_id": "2511.14581v1",
      "comment": "33 pages, 11 figures, submitted for publication in Journal of Fluid Mechanics",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14569v1",
      "title": "Task Addition and Weight Disentanglement in Closed-Vocabulary Models",
      "authors": [
        "Adam Hazimeh",
        "Alessandro Favero",
        "Pascal Frossard"
      ],
      "abstract": "Task arithmetic has recently emerged as a promising method for editing pre-trained \\textit{open-vocabulary} models, offering a cost-effective alternative to standard multi-task fine-tuning. However, despite the abundance of \\textit{closed-vocabulary} models that are not pre-trained with language supervision, applying task arithmetic to these models remains unexplored. In this paper, we deploy and study task addition in closed-vocabulary image classification models. We consider different pre-training schemes and find that \\textit{weight disentanglement} -- the property enabling task arithmetic -- is a general consequence of pre-training, as it appears in different pre-trained closed-vocabulary models. In fact, we find that pre-trained closed-vocabulary vision transformers can also be edited with task arithmetic, achieving high task addition performance and enabling the efficient deployment of multi-task models. Finally, we demonstrate that simple linear probing is a competitive baseline to task addition. Overall, our findings expand the applicability of task arithmetic to a broader class of pre-trained models and open the way for more efficient use of pre-trained models in diverse settings.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14569v1",
        "pdf": "https://arxiv.org/pdf/2511.14569v1"
      },
      "arxiv_id": "2511.14569v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14567v1",
      "title": "SweeperBot: Making 3D Browsing Accessible through View Analysis and Visual Question Answering",
      "authors": [
        "Chen Chen",
        "Cuong Nguyen",
        "Alexa Siu",
        "Dingzeyu Li",
        "Nadir Weibel"
      ],
      "abstract": "Accessing 3D models remains challenging for Screen Reader (SR) users. While some existing 3D viewers allow creators to provide alternative text, they often lack sufficient detail about the 3D models. Grounded on a formative study, this paper introduces SweeperBot, a system that enables SR users to leverage visual question answering to explore and compare 3D models. SweeperBot answers SR users' visual questions by combining an optimal view selection technique with the strength of generative- and recognition-based foundation models. An expert review with 10 Blind and Low-Vision (BLV) users with SR experience demonstrated the feasibility of using SweeperBot to assist BLV users in exploring and comparing 3D models. The quality of the descriptions generated by SweeperBot was validated by a second survey study with 30 sighted participants.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14567v1",
        "pdf": "https://arxiv.org/pdf/2511.14567v1"
      },
      "arxiv_id": "2511.14567v1",
      "comment": "28 pages, 16 figures, this article has been accepted for publication in the International Journal of Human-Computer Interaction (IJHCI), published by Taylor and Francis",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14566v1",
      "title": "Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak",
      "authors": [
        "Lucia Makaiová",
        "Martin Fajčík",
        "Antonín Jarolím"
      ],
      "abstract": "Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14566v1",
        "pdf": "https://arxiv.org/pdf/2511.14566v1"
      },
      "arxiv_id": "2511.14566v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14565v1",
      "title": "Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language",
      "authors": [
        "Minyoung Hwang",
        "Alexandra Forsey-Smerek",
        "Nathaniel Dennler",
        "Andreea Bobu"
      ],
      "abstract": "Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: https://MIT-CLEAR-Lab.github.io/Masked-IRL and Code: https://github.com/MIT-CLEAR-Lab/Masked-IRL",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14565v1",
        "pdf": "https://arxiv.org/pdf/2511.14565v1"
      },
      "arxiv_id": "2511.14565v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14559v1",
      "title": "Apo2Mol: 3D Molecule Generation via Dynamic Pocket-Aware Diffusion Models",
      "authors": [
        "Xinzhe Zheng",
        "Shiyu Jiang",
        "Gustavo Seabra",
        "Chenglong Li",
        "Yanjun Li"
      ],
      "abstract": "Deep generative models are rapidly advancing structure-based drug design, offering substantial promise for generating small molecule ligands that bind to specific protein targets. However, most current approaches assume a rigid protein binding pocket, neglecting the intrinsic flexibility of proteins and the conformational rearrangements induced by ligand binding, limiting their applicability in practical drug discovery. Here, we propose Apo2Mol, a diffusion-based generative framework for 3D molecule design that explicitly accounts for conformational flexibility in protein binding pockets. To support this, we curate a dataset of over 24,000 experimentally resolved apo-holo structure pairs from the Protein Data Bank, enabling the characterization of protein structure changes associated with ligand binding. Apo2Mol employs a full-atom hierarchical graph-based diffusion model that simultaneously generates 3D ligand molecules and their corresponding holo pocket conformations from input apo states. Empirical studies demonstrate that Apo2Mol can achieve state-of-the-art performance in generating high-affinity ligands and accurately capture realistic protein pocket conformational changes.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG",
        "q-bio.QM"
      ],
      "primary_category": "q-bio.BM",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14559v1",
        "pdf": "https://arxiv.org/pdf/2511.14559v1"
      },
      "arxiv_id": "2511.14559v1",
      "comment": "Accepted by AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14558v1",
      "title": "Explaining Digital Pathology Models via Clustering Activations",
      "authors": [
        "Adam Bajger",
        "Jan Obdržálek",
        "Vojtěch Kůr",
        "Rudolf Nenutil",
        "Petr Holub",
        "Vít Musil",
        "Tomáš Brázdil"
      ],
      "abstract": "We present a clustering-based explainability technique for digital pathology models based on convolutional neural networks. Unlike commonly used methods based on saliency maps, such as occlusion, GradCAM, or relevance propagation, which highlight regions that contribute the most to the prediction for a single slide, our method shows the global behaviour of the model under consideration, while also providing more fine-grained information. The result clusters can be visualised not only to understand the model, but also to increase confidence in its operation, leading to faster adoption in clinical practice. We also evaluate the performance of our technique on an existing model for detecting prostate cancer, demonstrating its usefulness.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14558v1",
        "pdf": "https://arxiv.org/pdf/2511.14558v1"
      },
      "arxiv_id": "2511.14558v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14555v1",
      "title": "DecNefLab: A Modular and Interpretable Simulation Framework for Decoded Neurofeedback",
      "authors": [
        "Alexander Olza",
        "Roberto Santana",
        "David Soto"
      ],
      "abstract": "Decoded Neurofeedback (DecNef) is a flourishing non-invasive approach to brain modulation with wide-ranging applications in neuromedicine and cognitive neuroscience. However, progress in DecNef research remains constrained by subject-dependent learning variability, reliance on indirect measures to quantify progress, and the high cost and time demands of experimentation.\n  We present DecNefLab, a modular and interpretable simulation framework that formalizes DecNef as a machine learning problem. Beyond providing a virtual laboratory, DecNefLab enables researchers to model, analyze and understand neurofeedback dynamics. Using latent variable generative models as simulated participants, DecNefLab allows direct observation of internal cognitive states and systematic evaluation of how different protocol designs and subject characteristics influence learning.\n  We demonstrate how this approach can (i) reproduce empirical phenomena of DecNef learning, (ii) identify conditions under which DecNef feedback fails to induce learning, and (iii) guide the design of more robust and reliable DecNef protocols in silico before human implementation.\n  In summary, DecNefLab bridges computational modeling and cognitive neuroscience, offering a principled foundation for methodological innovation, robust protocol design, and ultimately, a deeper understanding of DecNef-based brain modulation.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14555v1",
        "pdf": "https://arxiv.org/pdf/2511.14555v1"
      },
      "arxiv_id": "2511.14555v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14554v1",
      "title": "ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection",
      "authors": [
        "Mohammad Romani"
      ],
      "abstract": "Deepfakes generated by advanced GANs and autoencoders severely threaten information integrity and societal stability. Single-stream CNNs fail to capture multi-scale forgery artifacts across spatial, texture, and frequency domains, limiting robustness and generalization. We introduce the ForensicFlow, a tri-modal forensic framework that synergistically fuses RGB, texture, and frequency evidence for video Deepfake detection. The RGB branch (ConvNeXt-tiny) extracts global visual inconsistencies; the texture branch (Swin Transformer-tiny) detects fine-grained blending artifacts; the frequency branch (CNN + SE) identifies periodic spectral noise. Attention-based temporal pooling dynamically prioritizes high-evidence frames, while adaptive attention fusion balances branch contributions.Trained on Celeb-DF (v2) with Focal Loss, ForensicFlow achieves AUC 0.9752, F1-Score 0.9408, and accuracy 0.9208, outperforming single-stream baselines. Ablation validates branch synergy; Grad-CAM confirms forensic focus. This comprehensive feature fusion provides superior resilience against subtle forgeries.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14554v1",
        "pdf": "https://arxiv.org/pdf/2511.14554v1"
      },
      "arxiv_id": "2511.14554v1",
      "comment": "11 pages, 4 figures, 2 tables. Preprint. Submitted on November 18, 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14545v1",
      "title": "DeepBlip: Estimating Conditional Average Treatment Effects Over Time",
      "authors": [
        "Haorui Ma",
        "Dennis Frauen",
        "Stefan Feuerriegel"
      ],
      "abstract": "Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14545v1",
        "pdf": "https://arxiv.org/pdf/2511.14545v1"
      },
      "arxiv_id": "2511.14545v1",
      "comment": "42 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14544v1",
      "title": "Mind the Gaps: Measuring Visual Artifacts in Dimensionality Reduction",
      "authors": [
        "Jaume Ros",
        "Alessio Arleo",
        "Fernando Paulovich"
      ],
      "abstract": "Dimensionality Reduction (DR) techniques are commonly used for the visual exploration and analysis of high-dimensional data due to their ability to project datasets of high-dimensional points onto the 2D plane. However, projecting datasets in lower dimensions often entails some distortion, which is not necessarily easy to recognize but can lead users to misleading conclusions. Several Projection Quality Metrics (PQMs) have been developed as tools to quantify the goodness-of-fit of a DR projection; however, they mostly focus on measuring how well the projection captures the global or local structure of the data, without taking into account the visual distortion of the resulting plots, thus often ignoring the presence of outliers or artifacts that can mislead a visual analysis of the projection. In this work, we introduce the Warping Index (WI), a new metric for measuring the quality of DR projections onto the 2D plane, based on the assumption that the correct preservation of empty regions between points is of crucial importance towards a faithful visual representation of the data.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14544v1",
        "pdf": "https://arxiv.org/pdf/2511.14544v1"
      },
      "arxiv_id": "2511.14544v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14543v1",
      "title": "MissHDD: Hybrid Deterministic Diffusion for Hetrogeneous Incomplete Data Imputation",
      "authors": [
        "Youran Zhou",
        "Mohamed Reda Bouadjenek",
        "Sunil Aryal"
      ],
      "abstract": "Incomplete data are common in real-world tabular applications, where numerical, categorical, and discrete attributes coexist within a single dataset. This heterogeneous structure presents significant challenges for existing diffusion-based imputation models, which typically assume a homogeneous feature space and rely on stochastic denoising trajectories. Such assumptions make it difficult to maintain conditional consistency, and they often lead to information collapse for categorical variables or instability when numerical variables require deterministic updates. These limitations indicate that a single diffusion process is insufficient for mixed-type tabular imputation.\n  We propose a hybrid deterministic diffusion framework that separates heterogeneous features into two complementary generative channels. A continuous DDIM-based channel provides efficient and stable deterministic denoising for numerical variables, while a discrete latent-path diffusion channel, inspired by loopholing-based discrete diffusion, models categorical and discrete features without leaving their valid sample manifolds. The two channels are trained under a unified conditional imputation objective, enabling coherent reconstruction of mixed-type incomplete data.\n  Extensive experiments on multiple real-world datasets show that the proposed framework achieves higher imputation accuracy, more stable sampling trajectories, and improved robustness across MCAR, MAR, and MNAR settings compared with existing diffusion-based and classical methods. These results demonstrate the importance of structure-aware diffusion processes for advancing deep learning approaches to incomplete tabular data.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14543v1",
        "pdf": "https://arxiv.org/pdf/2511.14543v1"
      },
      "arxiv_id": "2511.14543v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14540v1",
      "title": "Interaction-Aware 4D Gaussian Splatting for Dynamic Hand-Object Interaction Reconstruction",
      "authors": [
        "Hao Tian",
        "Chenyangguang Zhang",
        "Rui Liu",
        "Wen Shen",
        "Xiaolin Qin"
      ],
      "abstract": "This paper focuses on a challenging setting of simultaneously modeling geometry and appearance of hand-object interaction scenes without any object priors. We follow the trend of dynamic 3D Gaussian Splatting based methods, and address several significant challenges. To model complex hand-object interaction with mutual occlusion and edge blur, we present interaction-aware hand-object Gaussians with newly introduced optimizable parameters aiming to adopt piecewise linear hypothesis for clearer structural representation. Moreover, considering the complementarity and tightness of hand shape and object shape during interaction dynamics, we incorporate hand information into object deformation field, constructing interaction-aware dynamic fields to model flexible motions. To further address difficulties in the optimization process, we propose a progressive strategy that handles dynamic regions and static background step by step. Correspondingly, explicit regularizations are designed to stabilize the hand-object representations for smooth motion transition, physical interaction reality, and coherent lighting. Experiments show that our approach surpasses existing dynamic 3D-GS-based methods and achieves state-of-the-art performance in reconstructing dynamic hand-object interaction.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14540v1",
        "pdf": "https://arxiv.org/pdf/2511.14540v1"
      },
      "arxiv_id": "2511.14540v1",
      "comment": "11 pages, 6 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14539v1",
      "title": "Learning Compact Latent Space for Representing Neural Signed Distance Functions with High-fidelity Geometry Details",
      "authors": [
        "Qiang Bai",
        "Bojian Wu",
        "Xi Yang",
        "Zhizhong Han"
      ],
      "abstract": "Neural signed distance functions (SDFs) have been a vital representation to represent 3D shapes or scenes with neural networks. An SDF is an implicit function that can query signed distances at specific coordinates for recovering a 3D surface. Although implicit functions work well on a single shape or scene, they pose obstacles when analyzing multiple SDFs with high-fidelity geometry details, due to the limited information encoded in the latent space for SDFs and the loss of geometry details. To overcome these obstacles, we introduce a method to represent multiple SDFs in a common space, aiming to recover more high-fidelity geometry details with more compact latent representations. Our key idea is to take full advantage of the benefits of generalization-based and overfitting-based learning strategies, which manage to preserve high-fidelity geometry details with compact latent codes. Based on this framework, we also introduce a novel sampling strategy to sample training queries. The sampling can improve the training efficiency and eliminate artifacts caused by the influence of other SDFs. We report numerical and visual evaluations on widely used benchmarks to validate our designs and show advantages over the latest methods in terms of the representative ability and compactness.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14539v1",
        "pdf": "https://arxiv.org/pdf/2511.14539v1"
      },
      "arxiv_id": "2511.14539v1",
      "comment": "Accepted as an Poster paper at the AAAI Conference on Artificial Intelligence (AAAI-26)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14533v1",
      "title": "A Neuro-Symbolic Framework for Reasoning under Perceptual Uncertainty: Bridging Continuous Perception and Discrete Symbolic Planning",
      "authors": [
        "Jiahao Wu",
        "Shengwen Yu"
      ],
      "abstract": "Bridging continuous perceptual signals and discrete symbolic reasoning is a fundamental challenge in AI systems that must operate under uncertainty. We present a neuro-symbolic framework that explicitly models and propagates uncertainty from perception to planning, providing a principled connection between these two abstraction levels. Our approach couples a transformer-based perceptual front-end with graph neural network (GNN) relational reasoning to extract probabilistic symbolic states from visual observations, and an uncertainty-aware symbolic planner that actively gathers information when confidence is low. We demonstrate the framework's effectiveness on tabletop robotic manipulation as a concrete application: the translator processes 10,047 PyBullet-generated scenes (3--10 objects) and outputs probabilistic predicates with calibrated confidences (overall F1=0.68). When embedded in the planner, the system achieves 94\\%/90\\%/88\\% success on Simple Stack, Deep Stack, and Clear+Stack benchmarks (90.7\\% average), exceeding the strongest POMDP baseline by 10--14 points while planning within 15\\,ms. A probabilistic graphical-model analysis establishes a quantitative link between calibrated uncertainty and planning convergence, providing theoretical guarantees that are validated empirically. The framework is general-purpose and can be applied to any domain requiring uncertainty-aware reasoning from perceptual input to symbolic planning.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14533v1",
        "pdf": "https://arxiv.org/pdf/2511.14533v1"
      },
      "arxiv_id": "2511.14533v1",
      "comment": "29 pages, 10 figures, 12 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14530v1",
      "title": "DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation",
      "authors": [
        "Xiangchen Yin",
        "Jiahui Yuan",
        "Zhangchi Hu",
        "Wenzhang Sun",
        "Jie Chen",
        "Xiaozhen Qiao",
        "Hao Li",
        "Xiaoyan Sun"
      ],
      "abstract": "Existing video Variational Autoencoders (VAEs) generally overlook the similarity between frame contents, leading to redundant latent modeling. In this paper, we propose decoupled VAE (DeCo-VAE) to achieve compact latent representation. Instead of encoding RGB pixels directly, we decompose video content into distinct components via explicit decoupling: keyframe, motion and residual, and learn dedicated latent representation for each. To avoid cross-component interference, we design dedicated encoders for each decoupled component and adopt a shared 3D decoder to maintain spatiotemporal consistency during reconstruction. We further utilize a decoupled adaptation strategy that freezes partial encoders while training the others sequentially, ensuring stable training and accurate learning of both static and dynamic features. Extensive quantitative and qualitative experiments demonstrate that DeCo-VAE achieves superior video reconstruction performance.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14530v1",
        "pdf": "https://arxiv.org/pdf/2511.14530v1"
      },
      "arxiv_id": "2511.14530v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14521v1",
      "title": "A Generative Data Framework with Authentic Supervision for Underwater Image Restoration and Enhancement",
      "authors": [
        "Yufeng Tian",
        "Yifan Chen",
        "Zhe Sun",
        "Libang Chen",
        "Mingyu Dou",
        "Jijun Lu",
        "Ye Zheng",
        "Xuelong Li"
      ],
      "abstract": "Underwater image restoration and enhancement are crucial for correcting color distortion and restoring image details, thereby establishing a fundamental basis for subsequent underwater visual tasks. However, current deep learning methodologies in this area are frequently constrained by the scarcity of high-quality paired datasets. Since it is difficult to obtain pristine reference labels in underwater scenes, existing benchmarks often rely on manually selected results from enhancement algorithms, providing debatable reference images that lack globally consistent color and authentic supervision. This limits the model's capabilities in color restoration, image enhancement, and generalization. To overcome this limitation, we propose using in-air natural images as unambiguous reference targets and translating them into underwater-degraded versions, thereby constructing synthetic datasets that provide authentic supervision signals for model learning. Specifically, we establish a generative data framework based on unpaired image-to-image translation, producing a large-scale dataset that covers 6 representative underwater degradation types. The framework constructs synthetic datasets with precise ground-truth labels, which facilitate the learning of an accurate mapping from degraded underwater images to their pristine scene appearances. Extensive quantitative and qualitative experiments across 6 representative network architectures and 3 independent test sets show that models trained on our synthetic data achieve comparable or superior color restoration and generalization performance to those trained on existing benchmarks. This research provides a reliable and scalable data-driven solution for underwater image restoration and enhancement. The generated dataset is publicly available at: https://github.com/yftian2025/SynUIEDatasets.git.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14521v1",
        "pdf": "https://arxiv.org/pdf/2511.14521v1"
      },
      "arxiv_id": "2511.14521v1",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14518v1",
      "title": "D-PerceptCT: Deep Perceptual Enhancement for Low-Dose CT Images",
      "authors": [
        "Taifour Yousra Nabila",
        "Azeddine Beghdadi",
        "Marie Luong",
        "Zuheng Ming",
        "Habib Zaidi",
        "Faouzi Alaya Cheikh"
      ],
      "abstract": "Low Dose Computed Tomography (LDCT) is widely used as an imaging solution to aid diagnosis and other clinical tasks. However, this comes at the price of a deterioration in image quality due to the low dose of radiation used to reduce the risk of secondary cancer development. While some efficient methods have been proposed to enhance LDCT quality, many overestimate noise and perform excessive smoothing, leading to a loss of critical details. In this paper, we introduce D-PerceptCT, a novel architecture inspired by key principles of the Human Visual System (HVS) to enhance LDCT images. The objective is to guide the model to enhance or preserve perceptually relevant features, thereby providing radiologists with CT images where critical anatomical structures and fine pathological details are perceptu- ally visible. D-PerceptCT consists of two main blocks: 1) a Visual Dual-path Extractor (ViDex), which integrates semantic priors from a pretrained DINOv2 model with local spatial features, allowing the network to incorporate semantic-awareness during enhancement; (2) a Global-Local State-Space block that captures long-range information and multiscale features to preserve the important structures and fine details for diagnosis. In addition, we propose a novel deep perceptual loss, designated as the Deep Perceptual Relevancy Loss Function (DPRLF), which is inspired by human contrast sensitivity, to further emphasize perceptually important features. Extensive experiments on the Mayo2016 dataset demonstrate the effectiveness of D-PerceptCT method for LDCT enhancement, showing better preservation of structural and textural information within LDCT images compared to SOTA methods.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14518v1",
        "pdf": "https://arxiv.org/pdf/2511.14518v1"
      },
      "arxiv_id": "2511.14518v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14516v1",
      "title": "Full Atom Peptide Design via Riemannian Euclidean Bayesian Flow Networks",
      "authors": [
        "Hao Qian",
        "Shikui Tu",
        "Lei Xu"
      ],
      "abstract": "Diffusion and flow matching models have recently emerged as promising approaches for peptide binder design. Despite their progress, these models still face two major challenges. First, categorical sampling of discrete residue types collapses their continuous parameters into onehot assignments, while continuous variables (e.g., atom positions) evolve smoothly throughout the generation process. This mismatch disrupts the update dynamics and results in suboptimal performance. Second, current models assume unimodal distributions for side-chain torsion angles, which conflicts with the inherently multimodal nature of side chain rotameric states and limits prediction accuracy. To address these limitations, we introduce PepBFN, the first Bayesian flow network for full atom peptide design that directly models parameter distributions in fully continuous space. Specifically, PepBFN models discrete residue types by learning their continuous parameter distributions, enabling joint and smooth Bayesian updates with other continuous structural parameters. It further employs a novel Gaussian mixture based Bayesian flow to capture the multimodal side chain rotameric states and a Matrix Fisher based Riemannian flow to directly model residue orientations on the $\\mathrm{SO}(3)$ manifold. Together, these parameter distributions are progressively refined via Bayesian updates, yielding smooth and coherent peptide generation. Experiments on side chain packing, reverse folding, and binder design tasks demonstrate the strong potential of PepBFN in computational peptide design.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14516v1",
        "pdf": "https://arxiv.org/pdf/2511.14516v1"
      },
      "arxiv_id": "2511.14516v1",
      "comment": "7pages, 4 figures, AAAI2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14515v1",
      "title": "IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention",
      "authors": [
        "Xinxin Tang",
        "Bin Qin",
        "Yufang Li"
      ],
      "abstract": "Achieving a balance between lightweight design and high performance remains a significant challenge for speech enhancement (SE) tasks on resource-constrained devices. Existing state-of-the-art methods, such as MUSE, have established a strong baseline with only 0.51M parameters by introducing a Multi-path Enhanced Taylor (MET) transformer and Deformable Embedding (DE). However, an in-depth analysis reveals that MUSE still suffers from efficiency bottlenecks: the MET module relies on a complex \"approximate-compensate\" mechanism to mitigate the limitations of Taylor-expansion-based attention, while the offset calculation for deformable embedding introduces additional computational burden. This paper proposes IMSE, a systematically optimized and ultra-lightweight network. We introduce two core innovations: 1) Replacing the MET module with Amplitude-Aware Linear Attention (MALA). MALA fundamentally rectifies the \"amplitude-ignoring\" problem in linear attention by explicitly preserving the norm information of query vectors in the attention calculation, achieving efficient global modeling without an auxiliary compensation branch. 2) Replacing the DE module with Inception Depthwise Convolution (IDConv). IDConv borrows the Inception concept, decomposing large-kernel operations into efficient parallel branches (square, horizontal, and vertical strips), thereby capturing spectrogram features with extremely low parameter redundancy. Extensive experiments on the VoiceBank+DEMAND dataset demonstrate that, compared to the MUSE baseline, IMSE significantly reduces the parameter count by 16.8\\% (from 0.513M to 0.427M) while achieving competitive performance comparable to the state-of-the-art on the PESQ metric (3.373). This study sets a new benchmark for the trade-off between model size and speech quality in ultra-lightweight speech enhancement.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14515v1",
        "pdf": "https://arxiv.org/pdf/2511.14515v1"
      },
      "arxiv_id": "2511.14515v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14510v1",
      "title": "CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design",
      "authors": [
        "Jiawei Yi",
        "Ping Gong",
        "Youhui Bai",
        "Jiaqi Ruan",
        "Shengnan Wang",
        "Pengcheng Wang",
        "Haibo Wang",
        "Weiguang Wang",
        "Xia Zhu",
        "Feng Wu",
        "Cheng Li"
      ],
      "abstract": "The growth of million-token LLMs exposes the scalability limits of inference systems, where the KVCache dominates memory usage and data transfer overhead. Recent offloading systems migrate the KVCache to CPU memory and incorporate top-k attention to reduce the volume of data transferred from the CPU, while further applying system-level optimizations such as on-GPU caching and prefetching to lower transfer overhead. However, they overlook the CPU bottleneck in three aspects: (1) substantial overhead of fine-grained dynamic cache management performed on the CPU side, (2) significant transfer overhead from poor PCIe bandwidth utilization caused by heavy gathering operations at the CPU side, and (3) GPU runtime bubbles introduced by coarse-grained CPU-centric synchronization. To address these challenges, we propose CLO, a CPU-light KVCache offloading system via algorithm-system co-design. CLO features: (1) a coarse-grained head-wise approximate on-GPU caching strategy with negligible cache management cost, (2) seamless combination of data prefetching and on-GPU persistent caching for lower transfer overhead, (3) a zero-copy transfer engine to fully exploit PCIe bandwidth, and a GPU-centric synchronization method to eliminate GPU stalls. Evaluation on two widely-used LLMs demonstrates that CLO achieves comparable accuracy to state-of-the-art systems, while substantially minimizing CPU overhead, fully utilizing PCIe bandwidth, thus improving decoding throughput by 9.3%-66.6%. Our results highlight that algorithm-system co-design is essential for memory-constrained LLM inference on modern GPU platforms. We open source CLO at https://github.com/CommediaJW/CLO.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14510v1",
        "pdf": "https://arxiv.org/pdf/2511.14510v1"
      },
      "arxiv_id": "2511.14510v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14503v1",
      "title": "Parameter Aware Mamba Model for Multi-task Dense Prediction",
      "authors": [
        "Xinzhuo Yu",
        "Yunzhi Zhuge",
        "Sitong Gong",
        "Lu Zhang",
        "Pingping Zhang",
        "Huchuan Lu"
      ],
      "abstract": "Understanding the inter-relations and interactions between tasks is crucial for multi-task dense prediction. Existing methods predominantly utilize convolutional layers and attention mechanisms to explore task-level interactions. In this work, we introduce a novel decoder-based framework, Parameter Aware Mamba Model (PAMM), specifically designed for dense prediction in multi-task learning setting. Distinct from approaches that employ Transformers to model holistic task relationships, PAMM leverages the rich, scalable parameters of state space models to enhance task interconnectivity. It features dual state space parameter experts that integrate and set task-specific parameter priors, capturing the intrinsic properties of each task. This approach not only facilitates precise multi-task interactions but also allows for the global integration of task priors through the structured state space sequence model (S4). Furthermore, we employ the Multi-Directional Hilbert Scanning method to construct multi-angle feature sequences, thereby enhancing the sequence model's perceptual capabilities for 2D data. Extensive experiments on the NYUD-v2 and PASCAL-Context benchmarks demonstrate the effectiveness of our proposed method. Our code is available at https://github.com/CQC-gogopro/PAMM.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14503v1",
        "pdf": "https://arxiv.org/pdf/2511.14503v1"
      },
      "arxiv_id": "2511.14503v1",
      "comment": "Accepted to IEEE Transactions on Cybernetics",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14501v1",
      "title": "Improved Convergence in Parameter-Agnostic Error Feedback through Momentum",
      "authors": [
        "Abdurakhmon Sadiev",
        "Yury Demidovich",
        "Igor Sokolov",
        "Grigory Malinovsky",
        "Sarit Khirirat",
        "Peter Richtárik"
      ],
      "abstract": "Communication compression is essential for scalable distributed training of modern machine learning models, but it often degrades convergence due to the noise it introduces. Error Feedback (EF) mechanisms are widely adopted to mitigate this issue of distributed compression algorithms. Despite their popularity and training efficiency, existing distributed EF algorithms often require prior knowledge of problem parameters (e.g., smoothness constants) to fine-tune stepsizes. This limits their practical applicability especially in large-scale neural network training. In this paper, we study normalized error feedback algorithms that combine EF with normalized updates, various momentum variants, and parameter-agnostic, time-varying stepsizes, thus eliminating the need for problem-dependent tuning. We analyze the convergence of these algorithms for minimizing smooth functions, and establish parameter-agnostic complexity bounds that are close to the best-known bounds with carefully-tuned problem-dependent stepsizes. Specifically, we show that normalized EF21 achieve the convergence rate of near ${O}(1/T^{1/4})$ for Polyak's heavy-ball momentum, ${O}(1/T^{2/7})$ for Iterative Gradient Transport (IGT), and ${O}(1/T^{1/3})$ for STORM and Hessian-corrected momentum. Our results hold with decreasing stepsizes and small mini-batches. Finally, our empirical experiments confirm our theoretical insights.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14501v1",
        "pdf": "https://arxiv.org/pdf/2511.14501v1"
      },
      "arxiv_id": "2511.14501v1",
      "comment": "50 pages, 12 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14499v1",
      "title": "Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM",
      "authors": [
        "Jack Qin",
        "Zhitao Wang",
        "Yinan Zheng",
        "Keyu Chen",
        "Yang Zhou",
        "Yuanxin Zhong",
        "Siyuan Cheng"
      ],
      "abstract": "The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor configurations.Related works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention maps.This approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky objects.By focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14499v1",
        "pdf": "https://arxiv.org/pdf/2511.14499v1"
      },
      "arxiv_id": "2511.14499v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14488v1",
      "title": "Towards Stable and Structured Time Series Generation with Perturbation-Aware Flow Matching",
      "authors": [
        "Jintao Zhang",
        "Mingyue Cheng",
        "Zirui Liu",
        "Xianquan Wang",
        "Yitong Zhou",
        "Qi Liu"
      ],
      "abstract": "Time series generation is critical for a wide range of applications, which greatly supports downstream analytical and decision-making tasks. However, the inherent temporal heterogeneous induced by localized perturbations present significant challenges for generating structurally consistent time series. While flow matching provides a promising paradigm by modeling temporal dynamics through trajectory-level supervision, it fails to adequately capture abrupt transitions in perturbed time series, as the use of globally shared parameters constrains the velocity field to a unified representation. To address these limitations, we introduce \\textbf{PAFM}, a \\textbf{P}erturbation-\\textbf{A}ware \\textbf{F}low \\textbf{M}atching framework that models perturbed trajectories to ensure stable and structurally consistent time series generation. The framework incorporates perturbation-guided training to simulate localized disturbances and leverages a dual-path velocity field to capture trajectory deviations under perturbation, enabling refined modeling of perturbed behavior to enhance the structural coherence. In order to further improve sensitivity to trajectory perturbations while enhancing expressiveness, a mixture-of-experts decoder with flow routing dynamically allocates modeling capacity in response to different trajectory dynamics. Extensive experiments on both unconditional and conditional generation tasks demonstrate that PAFM consistently outperforms strong baselines. Code is available at https://anonymous.4open.science/r/PAFM-03B2.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14488v1",
        "pdf": "https://arxiv.org/pdf/2511.14488v1"
      },
      "arxiv_id": "2511.14488v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14485v1",
      "title": "Notes on Kernel Methods in Machine Learning",
      "authors": [
        "Diego Armando Pérez-Rosero",
        "Danna Valentina Salazar-Dubois",
        "Juan Camilo Lugo-Rojas",
        "Andrés Marino Álvarez-Meza",
        "Germán Castellanos-Dominguez"
      ],
      "abstract": "These notes provide a self-contained introduction to kernel methods and their geometric foundations in machine learning. Starting from the construction of Hilbert spaces, we develop the theory of positive definite kernels, reproducing kernel Hilbert spaces (RKHS), and Hilbert-Schmidt operators, emphasizing their role in statistical estimation and representation of probability measures. Classical concepts such as covariance, regression, and information measures are revisited through the lens of Hilbert space geometry. We also introduce kernel density estimation, kernel embeddings of distributions, and the Maximum Mean Discrepancy (MMD). The exposition is designed to serve as a foundation for more advanced topics, including Gaussian processes, kernel Bayesian inference, and functional analytic approaches to modern machine learning.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14485v1",
        "pdf": "https://arxiv.org/pdf/2511.14485v1"
      },
      "arxiv_id": "2511.14485v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14482v1",
      "title": "Gradient-Based Join Ordering",
      "authors": [
        "Tim Schwabe",
        "Maribel Acosta"
      ],
      "abstract": "Join ordering is the NP-hard problem of selecting the most efficient sequence in which to evaluate joins (conjunctive, binary operators) in a database query. As the performance of query execution critically depends on this choice, join ordering lies at the core of query optimization. Traditional approaches cast this problem as a discrete combinatorial search over binary trees guided by a cost model, but they often suffer from high computational complexity and limited scalability. We show that, when the cost model is differentiable, the query plans can be continuously relaxed into a soft adjacency matrix representing a superposition of plans. This continuous relaxation, together with a Gumbel-Softmax parameterization of the adjacency matrix and differentiable constraints enforcing plan validity, enables gradient-based search for plans within this relaxed space. Using a learned Graph Neural Network as the cost model, we demonstrate that this gradient-based approach can find comparable and even lower-cost plans compared to traditional discrete local search methods on two different graph datasets. Furthermore, we empirically show that the runtime of this approach scales linearly with query size, in contrast to quadratic or exponential runtimes of classical approaches. We believe this first step towards gradient-based join ordering can lead to more effective and efficient query optimizers in the future.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14482v1",
        "pdf": "https://arxiv.org/pdf/2511.14482v1"
      },
      "arxiv_id": "2511.14482v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14481v1",
      "title": "Segmentation-Aware Latent Diffusion for Satellite Image Super-Resolution: Enabling Smallholder Farm Boundary Delineation",
      "authors": [
        "Aditi Agarwal",
        "Anjali Jain",
        "Nikita Saxena",
        "Ishan Deshpande",
        "Michal Kazmierski",
        "Abigail Annkah",
        "Nadav Sherman",
        "Karthikeyan Shanmugam",
        "Alok Talekar",
        "Vaibhav Rajan"
      ],
      "abstract": "Delineating farm boundaries through segmentation of satellite images is a fundamental step in many agricultural applications. The task is particularly challenging for smallholder farms, where accurate delineation requires the use of high resolution (HR) imagery which are available only at low revisit frequencies (e.g., annually). To support more frequent (sub-) seasonal monitoring, HR images could be combined as references (ref) with low resolution (LR) images -- having higher revisit frequency (e.g., weekly) -- using reference-based super-resolution (Ref-SR) methods. However, current Ref-SR methods optimize perceptual quality and smooth over crucial features needed for downstream tasks, and are unable to meet the large scale-factor requirements for this task. Further, previous two-step approaches of SR followed by segmentation do not effectively utilize diverse satellite sources as inputs. We address these problems through a new approach, $\\textbf{SEED-SR}$, which uses a combination of conditional latent diffusion models and large-scale multi-spectral, multi-source geo-spatial foundation models. Our key innovation is to bypass the explicit SR task in the pixel space and instead perform SR in a segmentation-aware latent space. This unique approach enables us to generate segmentation maps at an unprecedented 20$\\times$ scale factor, and rigorous experiments on two large, real datasets demonstrate up to $\\textbf{25.5}$ and $\\textbf{12.9}$ relative improvement in instance and semantic segmentation metrics respectively over approaches based on state-of-the-art Ref-SR methods.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14481v1",
        "pdf": "https://arxiv.org/pdf/2511.14481v1"
      },
      "arxiv_id": "2511.14481v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14478v1",
      "title": "Agentic AI Systems in Electrical Power Systems Engineering: Current State-of-the-Art and Challenges",
      "authors": [
        "Soham Ghosh",
        "Gaurav Mittal"
      ],
      "abstract": "Agentic AI systems have recently emerged as a critical and transformative approach in artificial intelligence, offering capabilities that extend far beyond traditional AI agents and contemporary generative AI models. This rapid evolution necessitates a clear conceptual and taxonomical understanding to differentiate this new paradigm. Our paper addresses this gap by providing a comprehensive review that establishes a precise definition and taxonomy for \"agentic AI,\" with the aim of distinguishing it from previous AI paradigms. The concepts are gradually introduced, starting with a highlight of its diverse applications across the broader field of engineering. The paper then presents four detailed, state-of-the-art use case applications specifically within electrical engineering. These case studies demonstrate practical impact, ranging from an advanced agentic framework for streamlining complex power system studies and benchmarking to a novel system developed for survival analysis of dynamic pricing strategies in battery swapping stations. Finally, to ensure robust deployment, the paper provides detailed failure mode investigations. From these findings, we derive actionable recommendations for the design and implementation of safe, reliable, and accountable agentic AI systems, offering a critical resource for researchers and practitioners.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "eess.SY",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14478v1",
        "pdf": "https://arxiv.org/pdf/2511.14478v1"
      },
      "arxiv_id": "2511.14478v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14477v1",
      "title": "2D Gaussians Spatial Transport for Point-supervised Density Regression",
      "authors": [
        "Miao Shang",
        "Xiaopeng Hong"
      ],
      "abstract": "This paper introduces Gaussian Spatial Transport (GST), a novel framework that leverages Gaussian splatting to facilitate transport from the probability measure in the image coordinate space to the annotation map. We propose a Gaussian splatting-based method to estimate pixel-annotation correspondence, which is then used to compute a transport plan derived from Bayesian probability. To integrate the resulting transport plan into standard network optimization in typical computer vision tasks, we derive a loss function that measures discrepancy after transport. Extensive experiments on representative computer vision tasks, including crowd counting and landmark detection, validate the effectiveness of our approach. Compared to conventional optimal transport schemes, GST eliminates iterative transport plan computation during training, significantly improving efficiency. Code is available at https://github.com/infinite0522/GST.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14477v1",
        "pdf": "https://arxiv.org/pdf/2511.14477v1"
      },
      "arxiv_id": "2511.14477v1",
      "comment": "9 pages, 5 figures, accepted by AAAI, 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14476v1",
      "title": "Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior",
      "authors": [
        "Dalia Ali",
        "Dora Zhao",
        "Allison Koenecke",
        "Orestis Papakyriakopoulos"
      ],
      "abstract": "Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14476v1",
        "pdf": "https://arxiv.org/pdf/2511.14476v1"
      },
      "arxiv_id": "2511.14476v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14473v1",
      "title": "Learning Subglacial Bed Topography from Sparse Radar with Physics-Guided Residuals",
      "authors": [
        "Bayu Adhi Tama",
        "Jianwu Wang",
        "Vandana Janeja",
        "Mostafa Cham"
      ],
      "abstract": "Accurate subglacial bed topography is essential for ice sheet modeling, yet radar observations are sparse and uneven. We propose a physics-guided residual learning framework that predicts bed thickness residuals over a BedMachine prior and reconstructs bed from the observed surface. A DeepLabV3+ decoder over a standard encoder (e.g.,ResNet-50) is trained with lightweight physics and data terms: multi-scale mass conservation, flow-aligned total variation, Laplacian damping, non-negativity of thickness, a ramped prior-consistency term, and a masked Huber fit to radar picks modulated by a confidence map. To measure real-world generalization, we adopt leakage-safe blockwise hold-outs (vertical/horizontal) with safety buffers and report metrics only on held-out cores. Across two Greenland sub-regions, our approach achieves strong test-core accuracy and high structural fidelity, outperforming U-Net, Attention U-Net, FPN, and a plain CNN. The residual-over-prior design, combined with physics, yields spatially coherent, physically plausible beds suitable for operational mapping under domain shift.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14473v1",
        "pdf": "https://arxiv.org/pdf/2511.14473v1"
      },
      "arxiv_id": "2511.14473v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14469v1",
      "title": "CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring",
      "authors": [
        "Mingchen Zhong",
        "Xin Lu",
        "Dong Li",
        "Senyan Xu",
        "Ruixuan Jiang",
        "Xueyang Fu",
        "Baocai Yin"
      ],
      "abstract": "Low-light video deblurring poses significant challenges in applications like nighttime surveillance and autonomous driving due to dim lighting and long exposures. While event cameras offer potential solutions with superior low-light sensitivity and high temporal resolution, existing fusion methods typically employ staged strategies, limiting their effectiveness against combined low-light and motion blur degradations. To overcome this, we propose CompEvent, a complex neural network framework enabling holistic full-process fusion of event data and RGB frames for enhanced joint restoration. CompEvent features two core components: 1) Complex Temporal Alignment GRU, which utilizes complex-valued convolutions and processes video and event streams iteratively via GRU to achieve temporal alignment and continuous fusion; and 2) Complex Space-Frequency Learning module, which performs unified complex-valued signal processing in both spatial and frequency domains, facilitating deep fusion through spatial structures and system-level characteristics. By leveraging the holistic representation capability of complex-valued neural networks, CompEvent achieves full-process spatiotemporal fusion, maximizes complementary learning between modalities, and significantly strengthens low-light video deblurring capability. Extensive experiments demonstrate that CompEvent outperforms SOTA methods in addressing this challenging task. The code is available at https://github.com/YuXie1/CompEvent.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14469v1",
        "pdf": "https://arxiv.org/pdf/2511.14469v1"
      },
      "arxiv_id": "2511.14469v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14465v1",
      "title": "nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers",
      "authors": [
        "Clément Dumas"
      ],
      "abstract": "Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14465v1",
        "pdf": "https://arxiv.org/pdf/2511.14465v1"
      },
      "arxiv_id": "2511.14465v1",
      "comment": "7 pages, 1 figure, accepted at the mechanistic interpretability workshop of NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14461v1",
      "title": "Effective Diversification of Multi-Carousel Book Recommendation",
      "authors": [
        "Daniël Wilten",
        "Gideon Maillette de Buy Wenniger",
        "Arjen Hommersom",
        "Paul Lucassen",
        "Emiel Poortman"
      ],
      "abstract": "Using multiple carousels, lists that wrap around and can be scrolled, is the basis for offering content in most contemporary movie streaming platforms. Carousels allow for highlighting different aspects of users' taste, that fall in categories such as genres and authors. However, while carousels offer structure and greater ease of navigation, they alone do not increase diversity in recommendations, while this is essential to keep users engaged. In this work we propose several approaches to effectively increase item diversity within the domain of book recommendations, on top of a collaborative filtering algorithm. These approaches are intended to improve book recommendations in the web catalogs of public libraries. Furthermore, we introduce metrics to evaluate the resulting strategies, and show that the proposed system finds a suitable balance between accuracy and beyond-accuracy aspects.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14461v1",
        "pdf": "https://arxiv.org/pdf/2511.14461v1"
      },
      "arxiv_id": "2511.14461v1",
      "comment": "Accepted as a conference paper at BNAIC/BeNeLearn 2025; The 37th Benelux Conference on Artificial Intelligence and the 34th Belgian Dutch Conference on Machine Learning",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14456v1",
      "title": "Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning",
      "authors": [
        "Fabian Stricker",
        "David Bermbach",
        "Christian Zirpins"
      ],
      "abstract": "Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.\n  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14456v1",
        "pdf": "https://arxiv.org/pdf/2511.14456v1"
      },
      "arxiv_id": "2511.14456v1",
      "comment": "Accepted for publication in 3rd IEEE International Conference on Federated Learning Applications and Technologies (FLTA2025)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14455v1",
      "title": "Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks",
      "authors": [
        "Nicola Rares Franco",
        "Lorenzo Tedesco"
      ],
      "abstract": "We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\\varphi=\\varphi(x,u)$ such that $\\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14455v1",
        "pdf": "https://arxiv.org/pdf/2511.14455v1"
      },
      "arxiv_id": "2511.14455v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14452v1",
      "title": "Hybrid Modeling of Photoplethysmography for Non-invasive Monitoring of Cardiovascular Parameters",
      "authors": [
        "Emanuele Palumbo",
        "Sorawit Saengkyongam",
        "Maria R. Cervera",
        "Jens Behrmann",
        "Andrew C. Miller",
        "Guillermo Sapiro",
        "Christina Heinze-Deml",
        "Antoine Wehenkel"
      ],
      "abstract": "Continuous cardiovascular monitoring can play a key role in precision health. However, some fundamental cardiac biomarkers of interest, including stroke volume and cardiac output, require invasive measurements, e.g., arterial pressure waveforms (APW). As a non-invasive alternative, photoplethysmography (PPG) measurements are routinely collected in hospital settings. Unfortunately, the prediction of key cardiac biomarkers from PPG instead of APW remains an open challenge, further complicated by the scarcity of annotated PPG measurements. As a solution, we propose a hybrid approach that uses hemodynamic simulations and unlabeled clinical data to estimate cardiovascular biomarkers directly from PPG signals. Our hybrid model combines a conditional variational autoencoder trained on paired PPG-APW data with a conditional density estimator of cardiac biomarkers trained on labeled simulated APW segments. As a key result, our experiments demonstrate that the proposed approach can detect fluctuations of cardiac output and stroke volume and outperform a supervised baseline in monitoring temporal changes in these biomarkers.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14452v1",
        "pdf": "https://arxiv.org/pdf/2511.14452v1"
      },
      "arxiv_id": "2511.14452v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14449v1",
      "title": "DIR-TIR: Dialog-Iterative Refinement for Text-to-Image Retrieval",
      "authors": [
        "Zongwei Zhen",
        "Biqing Zeng"
      ],
      "abstract": "This paper addresses the task of interactive, conversational text-to-image retrieval.\n  Our DIR-TIR framework progressively refines the target image search through two specialized modules: the Dialog Refiner Module and the Image Refiner Module.\n  The Dialog Refiner actively queries users to extract essential information and generate increasingly precise descriptions of the target image.\n  Complementarily, the Image Refiner identifies perceptual gaps between generated images and user intentions, strategically reducing the visual-semantic discrepancy. By leveraging multi-turn dialogues, DIR-TIR provides superior controllability and fault tolerance compared to conventional single-query methods, significantly improving target image hit accuracy.\n  Comprehensive experiments across diverse image datasets demonstrate our dialogue-based approach substantially outperforms initial-description-only baselines, while the synergistic module integration achieves both higher retrieval precision and enhanced interactive experience.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14449v1",
        "pdf": "https://arxiv.org/pdf/2511.14449v1"
      },
      "arxiv_id": "2511.14449v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14446v1",
      "title": "Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration and Understanding",
      "authors": [
        "Hong Gao",
        "Yiming Bao",
        "Xuezhen Tu",
        "Yutong Xu",
        "Yue Jin",
        "Yiyang Mu",
        "Bin Zhong",
        "Linan Yue",
        "Min-Ling Zhang"
      ],
      "abstract": "Video understanding requires not only visual recognition but also complex reasoning. While Vision-Language Models (VLMs) demonstrate impressive capabilities, they typically process videos largely in a single-pass manner with limited support for evidence revisit and iterative refinement. While recently emerging agent-based methods enable long-horizon reasoning, they either depend heavily on expensive proprietary models or require extensive agentic RL training. To overcome these limitations, we propose Agentic Video Intelligence (AVI), a flexible and training-free framework that can mirror human video comprehension through system-level design and optimization. AVI introduces three key innovations: (1) a human-inspired three-phase reasoning process (Retrieve-Perceive-Review) that ensures both sufficient global exploration and focused local analysis, (2) a structured video knowledge base organized through entity graphs, along with multi-granularity integrated tools, constituting the agent's interaction environment, and (3) an open-source model ensemble combining reasoning LLMs with lightweight base CV models and VLM, eliminating dependence on proprietary APIs or RL training. Experiments on LVBench, VideoMME-Long, LongVideoBench, and Charades-STA demonstrate that AVI achieves competitive performance while offering superior interpretability.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14446v1",
        "pdf": "https://arxiv.org/pdf/2511.14446v1"
      },
      "arxiv_id": "2511.14446v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14445v1",
      "title": "Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning",
      "authors": [
        "Trishala Jayesh Ahalpara"
      ],
      "abstract": "We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14445v1",
        "pdf": "https://arxiv.org/pdf/2511.14445v1"
      },
      "arxiv_id": "2511.14445v1",
      "comment": "8 pages, 2 figures, 1 Table. Submitted to the Computation and Language (cs.CL) category. Uses the ACL-style template. Code and demo will be released at: https://github.com/trystine/Tell_Me_Mental_Wellbeing_System",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.14441v1",
      "title": "Skewness-Robust Causal Discovery in Location-Scale Noise Models",
      "authors": [
        "Daniel Klippert",
        "Alexander Marx"
      ],
      "abstract": "To distinguish Markov equivalent graphs in causal discovery, it is necessary to restrict the structural causal model. Crucially, we need to be able to distinguish cause $X$ from effect $Y$ in bivariate models, that is, distinguish the two graphs $X \\to Y$ and $Y \\to X$. Location-scale noise models (LSNMs), in which the effect $Y$ is modeled based on the cause $X$ as $Y = f(X) + g(X)N$, form a flexible class of models that is general and identifiable in most cases. Estimating these models for arbitrary noise terms $N$, however, is challenging. Therefore, practical estimators are typically restricted to symmetric distributions, such as the normal distribution. As we showcase in this paper, when $N$ is a skewed random variable, which is likely in real-world domains, the reliability of these approaches decreases. To approach this limitation, we propose SkewD, a likelihood-based algorithm for bivariate causal discovery under LSNMs with skewed noise distributions. SkewD extends the usual normal-distribution framework to the skew-normal setting, enabling reliable inference under symmetric and skewed noise. For parameter estimation, we employ a combination of a heuristic search and an expectation conditional maximization algorithm. We evaluate SkewD on novel synthetically generated datasets with skewed noise as well as established benchmark datasets. Throughout our experiments, SkewD exhibits a strong performance and, in comparison to prior work, remains robust under high skewness.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14441v1",
        "pdf": "https://arxiv.org/pdf/2511.14441v1"
      },
      "arxiv_id": "2511.14441v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14440v1",
      "title": "Learning to See Through a Baby's Eyes: Early Visual Diets Enable Robust Visual Intelligence in Humans and Machines",
      "authors": [
        "Yusen Cai",
        "Bhargava Satya Nunna",
        "Qing Lin",
        "Mengmi Zhang"
      ],
      "abstract": "Newborns perceive the world with low-acuity, color-degraded, and temporally continuous vision, which gradually sharpens as infants develop. To explore the ecological advantages of such staged \"visual diets\", we train self-supervised learning (SSL) models on object-centric videos under constraints that simulate infant vision: grayscale-to-color (C), blur-to-sharp (A), and preserved temporal continuity (T)-collectively termed CATDiet. For evaluation, we establish a comprehensive benchmark across ten datasets, covering clean and corrupted image recognition, texture-shape cue conflict tests, silhouette recognition, depth-order classification, and the visual cliff paradigm. All CATDiet variants demonstrate enhanced robustness in object recognition, despite being trained solely on object-centric videos. Remarkably, models also exhibit biologically aligned developmental patterns, including neural plasticity changes mirroring synaptic density in macaque V1 and behaviors resembling infants' visual cliff responses. Building on these insights, CombDiet initializes SSL with CATDiet before standard training while preserving temporal continuity. Trained on object-centric or head-mounted infant videos, CombDiet outperforms standard SSL on both in-domain and out-of-domain object recognition and depth perception. Together, these results suggest that the developmental progression of early infant visual experience offers a powerful reverse-engineering framework for understanding the emergence of robust visual intelligence in machines. All code, data, and models will be publicly released.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14440v1",
        "pdf": "https://arxiv.org/pdf/2511.14440v1"
      },
      "arxiv_id": "2511.14440v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.14435v1",
      "title": "Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems",
      "authors": [
        "Angelo Ferrando"
      ],
      "abstract": "Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.",
      "published": "2025-11-18",
      "updated": "2025-11-18",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2511.14435v1",
        "pdf": "https://arxiv.org/pdf/2511.14435v1"
      },
      "arxiv_id": "2511.14435v1",
      "comment": "In Proceedings FMAS 2025, arXiv:2511.13245",
      "journal_ref": "EPTCS 436, 2025, pp. 80-87",
      "has_code": false
    }
  ]
}