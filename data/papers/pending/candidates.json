{
  "fetched_at": "2025-11-21T00:25:40.098598",
  "total_papers": 100,
  "papers": [
    {
      "id": "2511.15709v1",
      "title": "Tokenisation over Bounded Alphabets is Hard",
      "authors": [
        "Violeta Kastreva",
        "Philip Whittington",
        "Dennis Komm",
        "Tiago Pimentel"
      ],
      "abstract": "Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CL",
        "cs.DS",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15709v1",
        "pdf": "https://arxiv.org/pdf/2511.15709v1"
      },
      "arxiv_id": "2511.15709v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15706v1",
      "title": "RoMa v2: Harder Better Faster Denser Feature Matching",
      "authors": [
        "Johan Edstedt",
        "David Nordström",
        "Yushan Zhang",
        "Georg Bökman",
        "Jonathan Astermark",
        "Viktor Larsson",
        "Anders Heyden",
        "Fredrik Kahl",
        "Mårten Wadenbäck",
        "Michael Felsberg"
      ],
      "abstract": "Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15706v1",
        "pdf": "https://arxiv.org/pdf/2511.15706v1"
      },
      "arxiv_id": "2511.15706v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15705v1",
      "title": "GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization",
      "authors": [
        "Yikun Wang",
        "Zuyan Liu",
        "Ziyi Wang",
        "Pengfei Liu",
        "Han Hu",
        "Yongming Rao"
      ],
      "abstract": "Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15705v1",
        "pdf": "https://arxiv.org/pdf/2511.15705v1"
      },
      "arxiv_id": "2511.15705v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15704v1",
      "title": "In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data",
      "authors": [
        "Xiongyi Cai",
        "Ri-Zhao Qiu",
        "Geng Chen",
        "Lai Wei",
        "Isabella Liu",
        "Tianshu Huang",
        "Xuxin Cheng",
        "Xiaolong Wang"
      ],
      "abstract": "Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: https://xiongyicai.github.io/In-N-On/",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15704v1",
        "pdf": "https://arxiv.org/pdf/2511.15704v1"
      },
      "arxiv_id": "2511.15704v1",
      "comment": "Project webpage: https://xiongyicai.github.io/In-N-On/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.15703v1",
      "title": "Think Visually, Reason Textually: Vision-Language Synergy in ARC",
      "authors": [
        "Beichen Zhang",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Haodong Duan",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "abstract": "Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15703v1",
        "pdf": "https://arxiv.org/pdf/2511.15703v1"
      },
      "arxiv_id": "2511.15703v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15700v1",
      "title": "First Frame Is the Place to Go for Video Content Customization",
      "authors": [
        "Jingxi Chen",
        "Zongxia Li",
        "Zhichao Liu",
        "Guangyao Shi",
        "Xiyang Wu",
        "Fuxiao Liu",
        "Cornelia Fermuller",
        "Brandon Y. Feng",
        "Yiannis Aloimonos"
      ],
      "abstract": "What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15700v1",
        "pdf": "https://arxiv.org/pdf/2511.15700v1"
      },
      "arxiv_id": "2511.15700v1",
      "comment": "Project Website: https://firstframego.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.15699v1",
      "title": "Joint Semantic-Channel Coding and Modulation for Token Communications",
      "authors": [
        "Jingkai Ying",
        "Zhijin Qin",
        "Yulong Feng",
        "Liejun Wang",
        "Xiaoming Tao"
      ],
      "abstract": "In recent years, the Transformer architecture has achieved outstanding performance across a wide range of tasks and modalities. Token is the unified input and output representation in Transformer-based models, which has become a fundamental information unit. In this work, we consider the problem of token communication, studying how to transmit tokens efficiently and reliably. Point cloud, a prevailing three-dimensional format which exhibits a more complex spatial structure compared to image or video, is chosen to be the information source. We utilize the set abstraction method to obtain point tokens. Subsequently, to get a more informative and transmission-friendly representation based on tokens, we propose a joint semantic-channel and modulation (JSCCM) scheme for the token encoder, mapping point tokens to standard digital constellation points (modulated tokens). Specifically, the JSCCM consists of two parallel Point Transformer-based encoders and a differential modulator which combines the Gumel-softmax and soft quantization methods. Besides, the rate allocator and channel adapter are developed, facilitating adaptive generation of high-quality modulated tokens conditioned on both semantic information and channel conditions. Extensive simulations demonstrate that the proposed method outperforms both joint semantic-channel coding and traditional separate coding, achieving over 1dB gain in reconstruction and more than 6x compression ratio in modulated symbols.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15699v1",
        "pdf": "https://arxiv.org/pdf/2511.15699v1"
      },
      "arxiv_id": "2511.15699v1",
      "comment": "14 pages, 14 figures, 2 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15698v1",
      "title": "RescueLens: LLM-Powered Triage and Action on Volunteer Feedback for Food Rescue",
      "authors": [
        "Naveen Raman",
        "Jingwu Tang",
        "Zhiyu Chen",
        "Zheyuan Ryan Shi",
        "Sean Hudson",
        "Ameesh Kapoor",
        "Fei Fang"
      ],
      "abstract": "Food rescue organizations simultaneously tackle food insecurity and waste by working with volunteers to redistribute food from donors who have excess to recipients who need it. Volunteer feedback allows food rescue organizations to identify issues early and ensure volunteer satisfaction. However, food rescue organizations monitor feedback manually, which can be cumbersome and labor-intensive, making it difficult to prioritize which issues are most important. In this work, we investigate how large language models (LLMs) assist food rescue organizers in understanding and taking action based on volunteer experiences. We work with 412 Food Rescue, a large food rescue organization based in Pittsburgh, Pennsylvania, to design RescueLens, an LLM-powered tool that automatically categorizes volunteer feedback, suggests donors and recipients to follow up with, and updates volunteer directions based on feedback. We evaluate the performance of RescueLens on an annotated dataset, and show that it can recover 96% of volunteer issues at 71% precision. Moreover, by ranking donors and recipients according to their rates of volunteer issues, RescueLens allows organizers to focus on 0.5% of donors responsible for more than 30% of volunteer issues. RescueLens is now deployed at 412 Food Rescue and through semi-structured interviews with organizers, we find that RescueLens streamlines the feedback process so organizers better allocate their time.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15698v1",
        "pdf": "https://arxiv.org/pdf/2511.15698v1"
      },
      "arxiv_id": "2511.15698v1",
      "comment": "Accepted at IAAI'26",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15694v1",
      "title": "The Impact of Quantization on Large Reasoning Model Reinforcement Learning",
      "authors": [
        "Medha Kumar",
        "Zifei Xu",
        "Xin Wang",
        "Tristan Webb"
      ],
      "abstract": "Strong reasoning capabilities can now be achieved by large-scale reinforcement learning (RL) without any supervised fine-tuning. Although post-training quantization (PTQ) and quantization-aware training (QAT) are well studied in the context of fine-tuning, how quantization impacts RL in large reasoning models (LRMs) remains an open question. To answer this question, we conducted systematic experiments and discovered a significant gap in reasoning performance on mathematical benchmarks between post-RL quantized models and their quantization-aware RL optimized counterparts. Our findings suggest that quantization-aware RL training negatively impacted the learning process, whereas PTQ and QLoRA led to greater performance.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15694v1",
        "pdf": "https://arxiv.org/pdf/2511.15694v1"
      },
      "arxiv_id": "2511.15694v1",
      "comment": "Accepted to the NeurIPS 2025 Efficient Reasoning Workshop",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15692v1",
      "title": "Hyperspectral Image Classification using Spectral-Spatial Mixer Network",
      "authors": [
        "Mohammed Q. Alkhatib"
      ],
      "abstract": "This paper introduces SS-MixNet, a lightweight and effective deep learning model for hyperspectral image (HSI) classification. The architecture integrates 3D convolutional layers for local spectral-spatial feature extraction with two parallel MLP-style mixer blocks that capture long-range dependencies in spectral and spatial dimensions. A depthwise convolution-based attention mechanism is employed to enhance discriminative capability with minimal computational overhead. The model is evaluated on the QUH-Tangdaowan and QUH-Qingyun datasets using only 1% of labeled data for training and validation. SS-MixNet achieves the highest performance among compared methods, including 2D-CNN, 3D-CNN, IP-SWIN, SimPoolFormer, and HybridKAN, reaching 95.68% and 93.86% overall accuracy on the Tangdaowan and Qingyun datasets, respectively. The results, supported by quantitative metrics and classification maps, confirm the model's effectiveness in delivering accurate and robust predictions with limited supervision. The code will be made publicly available at: https://github.com/mqalkhatib/SS-MixNet",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15692v1",
        "pdf": "https://arxiv.org/pdf/2511.15692v1"
      },
      "arxiv_id": "2511.15692v1",
      "comment": "Accepted for WHISPERS2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15690v1",
      "title": "MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping",
      "authors": [
        "Yushi Huang",
        "Zining Wang",
        "Zhihang Yuan",
        "Yifu Ding",
        "Ruihao Gong",
        "Jinyang Guo",
        "Xianglong Liu",
        "Jun Zhang"
      ],
      "abstract": "Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\\times$ and the decoding time by 1.26$\\times$.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15690v1",
        "pdf": "https://arxiv.org/pdf/2511.15690v1"
      },
      "arxiv_id": "2511.15690v1",
      "comment": "Code will be released upon acceptance",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.15684v1",
      "title": "Walrus: A Cross-Domain Foundation Model for Continuum Dynamics",
      "authors": [
        "Michael McCabe",
        "Payel Mukhopadhyay",
        "Tanya Marwah",
        "Bruno Regaldo-Saint Blancard",
        "Francois Rozet",
        "Cristiana Diaconu",
        "Lucas Meyer",
        "Kaze W. K. Wong",
        "Hadi Sotoudeh",
        "Alberto Bietti",
        "Irina Espejo",
        "Rio Fear",
        "Siavash Golkar",
        "Tom Hehir",
        "Keiya Hirashima",
        "Geraud Krawezik",
        "Francois Lanusse",
        "Rudy Morel",
        "Ruben Ohana",
        "Liam Parker",
        "Mariel Pettee",
        "Jeff Shen",
        "Kyunghyun Cho",
        "Miles Cranmer",
        "Shirley Ho"
      ],
      "abstract": "Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15684v1",
        "pdf": "https://arxiv.org/pdf/2511.15684v1"
      },
      "arxiv_id": "2511.15684v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15679v1",
      "title": "Front-door Reducibility: Reducing ADMGs to the Standard Front-door Setting via a Graphical Criterion",
      "authors": [
        "Jianqiao Mao",
        "Max A. Little"
      ],
      "abstract": "Front-door adjustment provides a simple closed-form identification formula under the classical front-door criterion, but its applicability is often viewed as narrow and strict. Although ID algorithm is very useful and is proved effective for causal relation identification in general causal graphs (if it is identifiable), performing ID algorithm does not guarantee to obtain a practical, easy-to-estimate interventional distribution expression. We argue that the applicability of the front-door criterion is not as limited as it seems: many more complicated causal graphs can be reduced to the front-door criterion. In this paper, We introduce front-door reducibility (FDR), a graphical condition on acyclic directed mixed graphs (ADMGs) that extends the applicability of the classic front-door criterion to reduce a large family of complicated causal graphs to a front-door setting by aggregating variables into super-nodes (FDR triple) $\\left(\\boldsymbol{X}^{*},\\boldsymbol{Y}^{*},\\boldsymbol{M}^{*}\\right)$. After characterizing FDR criterion, we prove a graph-level equivalence between the satisfication of FDR criterion and the applicability of FDR adjustment. Meanwhile, we then present FDR-TID, an exact algorithm that detects an admissible FDR triple, together with established the algorithm's correctness, completeness, and finite termination. Empirically-motivated examples illustrate that many graphs outside the textbook front-door setting are FDR, yielding simple, estimable adjustments where general ID expressions would be cumbersome. FDR thus complements existing identification method by prioritizing interpretability and computational simplicity without sacrificing generality across mixed graphs.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15679v1",
        "pdf": "https://arxiv.org/pdf/2511.15679v1"
      },
      "arxiv_id": "2511.15679v1",
      "comment": "16 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15675v1",
      "title": "MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features",
      "authors": [
        "Sejuti Rahman",
        "Swakshar Deb",
        "MD. Sameer Iqbal Chowdhury",
        "MD. Jubair Ahmed Sourov",
        "Mohammad Shamsuddin"
      ],
      "abstract": "Eye tracking data quantifies the attentional bias towards negative stimuli that is frequently observed in depressed groups. Audio and video data capture the affective flattening and psychomotor retardation characteristic of depression. Statistical validation confirmed their significant discriminative power in distinguishing depressed from non depressed groups. We address a critical limitation of existing graph-based models that focus on low-frequency information and propose a Multi-Frequency Graph Convolutional Network (MF-GCN). This framework consists of a novel Multi-Frequency Filter Bank Module (MFFBM), which can leverage both low and high frequency signals. Extensive evaluation against traditional machine learning algorithms and deep learning frameworks demonstrates that MF-GCN consistently outperforms baselines. In binary (depressed and non depressed) classification, the model achieved a sensitivity of 0.96 and F2 score of 0.94. For the 3 class (no depression, mild to moderate depression and severe depression) classification task, the proposed method achieved a sensitivity of 0.79 and specificity of 0.87 and siginificantly suprassed other models. To validate generalizability, the model was also evaluated on the Chinese Multimodal Depression Corpus (CMDC) dataset and achieved a sensitivity of 0.95 and F2 score of 0.96. These results confirm that our trimodal, multi frequency framework effectively captures cross modal interaction for accurate depression detection.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15675v1",
        "pdf": "https://arxiv.org/pdf/2511.15675v1"
      },
      "arxiv_id": "2511.15675v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15661v1",
      "title": "VisPlay: Self-Evolving Vision-Language Models from Images",
      "authors": [
        "Yicheng He",
        "Chengsong Huang",
        "Zongxia Li",
        "Jiaxin Huang",
        "Yonghui Yang"
      ],
      "abstract": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15661v1",
        "pdf": "https://arxiv.org/pdf/2511.15661v1"
      },
      "arxiv_id": "2511.15661v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15658v1",
      "title": "GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI",
      "authors": [
        "Naomi Simumba",
        "Nils Lehmann",
        "Paolo Fraccaro",
        "Hamed Alemohammad",
        "Geeth De Mel",
        "Salman Khan",
        "Manil Maskey",
        "Nicolas Longepe",
        "Xiao Xiang Zhu",
        "Hannah Kerner",
        "Juan Bernabe-Moreno",
        "Alexander Lacoste"
      ],
      "abstract": "Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common characteristics (e.g., resolution, bands, temporality). This enables users to identify which models excel in each capability and determine which areas need improvement in future work. To support both fair comparison and methodological innovation, we define a prescriptive yet flexible evaluation protocol. This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks.\n  Our experiments show that no single model dominates across all tasks, confirming the specificity of the choices made during architecture design and pretraining. While models pretrained on natural images (ConvNext ImageNet, DINO V3) excel on high-resolution tasks, EO-specific models (TerraMind, Prithvi, and Clay) outperform them on multispectral applications such as agriculture and disaster response. These findings demonstrate that optimal model choice depends on task requirements, data modalities, and constraints. This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research. GEO-Bench-2 enables informed, reproducible GeoFM evaluation tailored to specific use cases. Code, data, and leaderboard for GEO-Bench-2 are publicly released under a permissive license.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15658v1",
        "pdf": "https://arxiv.org/pdf/2511.15658v1"
      },
      "arxiv_id": "2511.15658v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15656v1",
      "title": "INQUIRE-Search: A Framework for Interactive Discovery in Large-Scale Biodiversity Databases",
      "authors": [
        "Edward Vendrow",
        "Julia Chae",
        "Rupa Kurinchi-Vendhan",
        "Isaac Eckert",
        "Jazlynn Hall",
        "Marta Jarzyna",
        "Reymond Miyajima",
        "Ruth Oliver",
        "Laura Pollock",
        "Lauren Schrack",
        "Scott Yanco",
        "Oisin Mac Aodha",
        "Sara Beery"
      ],
      "abstract": "Large community science platforms such as iNaturalist contain hundreds of millions of biodiversity images that often capture ecological context on behaviors, interactions, phenology, and habitat. Yet most ecological workflows rely on metadata filtering or manual inspection, leaving this secondary information inaccessible at scale. We introduce INQUIRE-Search, an open-source system that enables scientists to rapidly and interactively search within an ecological image database for specific concepts using natural language, verify and export relevant observations, and utilize this discovered data for novel scientific analysis. Compared to traditional methods, INQUIRE-Search takes a fraction of the time, opening up new possibilities for scientific questions that can be explored. Through five case studies, we show the diversity of scientific applications that a tool like INQUIRE-Search can support, from seasonal variation in behavior across species to forest regrowth after wildfires. These examples demonstrate a new paradigm for interactive, efficient, and scalable scientific discovery that can begin to unlock previously inaccessible scientific value in large-scale biodiversity datasets. Finally, we emphasize using such AI-enabled discovery tools for science call for experts to reframe the priorities of the scientific process and develop novel methods for experiment design, data collection, survey effort, and uncertainty analysis.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15656v1",
        "pdf": "https://arxiv.org/pdf/2511.15656v1"
      },
      "arxiv_id": "2511.15656v1",
      "comment": "EV, JC, RKV contributed equally",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15652v1",
      "title": "Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges",
      "authors": [
        "Kim N. Nolle",
        "Ivana Dusparic",
        "Rhodri Cusack",
        "Vinny Cahill"
      ],
      "abstract": "Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.\n  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.\n  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15652v1",
        "pdf": "https://arxiv.org/pdf/2511.15652v1"
      },
      "arxiv_id": "2511.15652v1",
      "comment": "5 pages, 5 figures, Accepted to RLDM 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15645v1",
      "title": "MambaIO: Global-Coordinate Inertial Odometry for Pedestrians via Multi-Scale Frequency-Decoupled Modeling",
      "authors": [
        "Shanshan Zhang"
      ],
      "abstract": "Inertial Odometry (IO) enables real-time localization using only acceleration and angular velocity measurements from an Inertial Measurement Unit (IMU), making it a promising solution for localization in consumer-grade applications. Traditionally, IMU measurements in IO have been processed under two coordinate system paradigms: the body coordinate frame and the global coordinate frame, with the latter being widely adopted. However, recent studies in drone scenarios have demonstrated that the body frame can significantly improve localization accuracy, prompting a re-evaluation of the suitability of the global frame for pedestrian IO. To address this issue, this paper systematically evaluates the effectiveness of the global coordinate frame in pedestrian IO through theoretical analysis, qualitative inspection, and quantitative experiments. Building upon these findings, we further propose MambaIO, which decomposes IMU measurements into high-frequency and low-frequency components using a Laplacian pyramid. The low-frequency component is processed by a Mamba architecture to extract implicit contextual motion cues, while the high-frequency component is handled by a convolutional structure to capture fine-grained local motion details. Experiments on multiple public datasets show that MambaIO substantially reduces localization error and achieves state-of-the-art (SOTA) performance. To the best of our knowledge, this is the first application of the Mamba architecture to the inertial odometry task.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15645v1",
        "pdf": "https://arxiv.org/pdf/2511.15645v1"
      },
      "arxiv_id": "2511.15645v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15640v1",
      "title": "Multi-Stage Residual-Aware Unsupervised Deep Learning Framework for Consistent Ultrasound Strain Elastography",
      "authors": [
        "Shourov Joarder",
        "Tushar Talukder Showrav",
        "Md. Kamrul Hasan"
      ],
      "abstract": "Ultrasound Strain Elastography (USE) is a powerful non-invasive imaging technique for assessing tissue mechanical properties, offering crucial diagnostic value across diverse clinical applications. However, its clinical application remains limited by tissue decorrelation noise, scarcity of ground truth, and inconsistent strain estimation under different deformation conditions. Overcoming these barriers, we propose MUSSE-Net, a residual-aware, multi-stage unsupervised sequential deep learning framework designed for robust and consistent strain estimation. At its backbone lies our proposed USSE-Net, an end-to-end multi-stream encoder-decoder architecture that parallelly processes pre- and post-deformation RF sequences to estimate displacement fields and axial strains. The novel architecture incorporates Context-Aware Complementary Feature Fusion (CACFF)-based encoder with Tri-Cross Attention (TCA) bottleneck with a Cross-Attentive Fusion (CAF)-based sequential decoder. To ensure temporal coherence and strain stability across varying deformation levels, this architecture leverages a tailored consistency loss. Finally, with the MUSSE-Net framework, a secondary residual refinement stage further enhances accuracy and suppresses noise. Extensive validation on simulation, in vivo, and private clinical datasets from Bangladesh University of Engineering and Technology (BUET) medical center, demonstrates MUSSE-Net's outperformed existing unsupervised approaches. On MUSSE-Net achieves state-of-the-art performance with a target SNR of 24.54, background SNR of 132.76, CNR of 59.81, and elastographic SNR of 9.73 on simulation data. In particular, on the BUET dataset, MUSSE-Net produces strain maps with enhanced lesion-to-background contrast and significant noise suppression yielding clinically interpretable strain patterns.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15640v1",
        "pdf": "https://arxiv.org/pdf/2511.15640v1"
      },
      "arxiv_id": "2511.15640v1",
      "comment": "13 pages, 9 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15634v1",
      "title": "Rényi Differential Privacy for Heavy-Tailed SDEs via Fractional Poincaré Inequalities",
      "authors": [
        "Benjamin Dupuis",
        "Mert Gürbüzbalaban",
        "Umut Şimşekli",
        "Jian Wang",
        "Sinan Yildirim",
        "Lingjiong Zhu"
      ],
      "abstract": "Characterizing the differential privacy (DP) of learning algorithms has become a major challenge in recent years. In parallel, many studies suggested investigating the behavior of stochastic gradient descent (SGD) with heavy-tailed noise, both as a model for modern deep learning models and to improve their performance. However, most DP bounds focus on light-tailed noise, where satisfactory guarantees have been obtained but the proposed techniques do not directly extend to the heavy-tailed setting. Recently, the first DP guarantees for heavy-tailed SGD were obtained. These results provide $(0,δ)$-DP guarantees without requiring gradient clipping. Despite casting new light on the link between DP and heavy-tailed algorithms, these results have a strong dependence on the number of parameters and cannot be extended to other DP notions like the well-established Rényi differential privacy (RDP). In this work, we propose to address these limitations by deriving the first RDP guarantees for heavy-tailed SDEs, as well as their discretized counterparts. Our framework is based on new Rényi flow computations and the use of well-established fractional Poincaré inequalities. Under the assumption that such inequalities are satisfied, we obtain DP guarantees that have a much weaker dependence on the dimension compared to prior art.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15634v1",
        "pdf": "https://arxiv.org/pdf/2511.15634v1"
      },
      "arxiv_id": "2511.15634v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15633v1",
      "title": "Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning",
      "authors": [
        "Tao Hu",
        "Lan Li",
        "Zhen-Hao Xie",
        "Da-Wei Zhou"
      ],
      "abstract": "Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like \"dog\" subsumes fine-grained categories such as \"Labrador\" and \"Golden Retriever,\" and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15633v1",
        "pdf": "https://arxiv.org/pdf/2511.15633v1"
      },
      "arxiv_id": "2511.15633v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15632v1",
      "title": "CODE-II: A large-scale dataset for artificial intelligence in ECG analysis",
      "authors": [
        "Petrus E. O. G. B. Abreu",
        "Gabriela M. M. Paixão",
        "Jiawei Li",
        "Paulo R. Gomes",
        "Peter W. Macfarlane",
        "Ana C. S. Oliveira",
        "Vinicius T. Carvalho",
        "Thomas B. Schön",
        "Antonio Luiz P. Ribeiro",
        "Antônio H. Ribeiro"
      ],
      "abstract": "Data-driven methods for electrocardiogram (ECG) interpretation are rapidly progressing. Large datasets have enabled advances in artificial intelligence (AI) based ECG analysis, yet limitations in annotation quality, size, and scope remain major challenges. Here we present CODE-II, a large-scale real-world dataset of 2,735,269 12-lead ECGs from 2,093,807 adult patients collected by the Telehealth Network of Minas Gerais (TNMG), Brazil. Each exam was annotated using standardized diagnostic criteria and reviewed by cardiologists. A defining feature of CODE-II is a set of 66 clinically meaningful diagnostic classes, developed with cardiologist input and routinely used in telehealth practice. We additionally provide an open available subset: CODE-II-open, a public subset of 15,000 patients, and the CODE-II-test, a non-overlapping set of 8,475 exams reviewed by multiple cardiologists for blinded evaluation. A neural network pre-trained on CODE-II achieved superior transfer performance on external benchmarks (PTB-XL and CPSC 2018) and outperformed alternatives trained on larger datasets.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15632v1",
        "pdf": "https://arxiv.org/pdf/2511.15632v1"
      },
      "arxiv_id": "2511.15632v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15623v1",
      "title": "Sufficient Explanations in Databases and their Connections to Necessary Explanations and Repairs",
      "authors": [
        "Leopoldo Bertossi",
        "Nina Pardal"
      ],
      "abstract": "The notion of cause, as formalized by Halpern and Pearl, has been recently applied to relational databases, to characterize and compute causal explanations for query answers. In this work we consider the alternative notion of sufficient explanation. We investigate its connections with database repairs as used for dealing with inconsistent databases, and with causality-based necessary explanations. We also obtain some computational results.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.DB",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15623v1",
        "pdf": "https://arxiv.org/pdf/2511.15623v1"
      },
      "arxiv_id": "2511.15623v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15622v1",
      "title": "The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification",
      "authors": [
        "Dante Francisco Wasmuht",
        "Otto Brookes",
        "Maximillian Schall",
        "Pablo Palencia",
        "Chris Beirne",
        "Tilo Burghardt",
        "Majid Mirmehdi",
        "Hjalmar Kühl",
        "Mimi Arandjelovic",
        "Sam Pottie",
        "Peter Bermant",
        "Brandon Asheim",
        "Yi Jin Toh",
        "Adam Elzinga",
        "Jason Holmberg",
        "Andrew Whitworth",
        "Eleanor Flatt",
        "Laura Gustafson",
        "Chaitanya Ryali",
        "Yuan-Ting Hu",
        "Baishan Guo",
        "Andrew Westbury",
        "Kate Saenko",
        "Didac Suris"
      ],
      "abstract": "Automated video analysis is critical for wildlife conservation. A foundational task in this domain is multi-animal tracking (MAT), which underpins applications such as individual re-identification and behavior recognition. However, existing datasets are limited in scale, constrained to a few species, or lack sufficient temporal and geographical diversity - leaving no suitable benchmark for training general-purpose MAT models applicable across wild animal populations. To address this, we introduce SA-FARI, the largest open-source MAT dataset for wild animals. It comprises 11,609 camera trap videos collected over approximately 10 years (2014-2024) from 741 locations across 4 continents, spanning 99 species categories. Each video is exhaustively annotated culminating in ~46 hours of densely annotated footage containing 16,224 masklet identities and 942,702 individual bounding boxes, segmentation masks, and species labels. Alongside the task-specific annotations, we publish anonymized camera trap locations for each video. Finally, we present comprehensive benchmarks on SA-FARI using state-of-the-art vision-language models for detection and tracking, including SAM 3, evaluated with both species-specific and generic animal prompts. We also compare against vision-only methods developed specifically for wildlife analysis. SA-FARI is the first large-scale dataset to combine high species diversity, multi-region coverage, and high-quality spatio-temporal annotations, offering a new foundation for advancing generalizable multianimal tracking in the wild. The dataset is available at $\\href{https://www.conservationxlabs.com/sa-fari}{\\text{conservationxlabs.com/SA-FARI}}$.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15622v1",
        "pdf": "https://arxiv.org/pdf/2511.15622v1"
      },
      "arxiv_id": "2511.15622v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15619v1",
      "title": "CODE: A global approach to ODE dynamics learning",
      "authors": [
        "Nils Wildt",
        "Daniel M. Tartakovsky",
        "Sergey Oladyshkin",
        "Wolfgang Nowak"
      ],
      "abstract": "Ordinary differential equations (ODEs) are a conventional way to describe the observed dynamics of physical systems. Scientists typically hypothesize about dynamical behavior, propose a mathematical model, and compare its predictions to data. However, modern computing and algorithmic advances now enable purely data-driven learning of governing dynamics directly from observations. In data-driven settings, one learns the ODE's right-hand side (RHS). Dense measurements are often assumed, yet high temporal resolution is typically both cumbersome and expensive. Consequently, one usually has only sparsely sampled data. In this work we introduce ChaosODE (CODE), a Polynomial Chaos ODE Expansion in which we use an arbitrary Polynomial Chaos Expansion (aPCE) for the ODE's right-hand side, resulting in a global orthonormal polynomial representation of dynamics. We evaluate the performance of CODE in several experiments on the Lotka-Volterra system, across varying noise levels, initial conditions, and predictions far into the future, even on previously unseen initial conditions. CODE exhibits remarkable extrapolation capabilities even when evaluated under novel initial conditions and shows advantages compared to well-examined methods using neural networks (NeuralODE) or kernel approximators (KernelODE) as the RHS representer. We observe that the high flexibility of NeuralODE and KernelODE degrades extrapolation capabilities under scarce data and measurement noise. Finally, we provide practical guidelines for robust optimization of dynamics-learning problems and illustrate them in the accompanying code.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15619v1",
        "pdf": "https://arxiv.org/pdf/2511.15619v1"
      },
      "arxiv_id": "2511.15619v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15618v1",
      "title": "FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation",
      "authors": [
        "Tingrui Shen",
        "Yiheng Zhang",
        "Chen Tang",
        "Chuan Ping",
        "Zixing Zhao",
        "Le Wan",
        "Yuwang Wang",
        "Ronggang Wang",
        "Shengfeng He"
      ],
      "abstract": "Autoregressive models can generate high-quality 3D meshes by sequentially producing vertices and faces, but their token-by-token decoding results in slow inference, limiting practical use in interactive and large-scale applications. We present FlashMesh, a fast and high-fidelity mesh generation framework that rethinks autoregressive decoding through a predict-correct-verify paradigm. The key insight is that mesh tokens exhibit strong structural and geometric correlations that enable confident multi-token speculation. FlashMesh leverages this by introducing a speculative decoding scheme tailored to the commonly used hourglass transformer architecture, enabling parallel prediction across face, point, and coordinate levels. Extensive experiments show that FlashMesh achieves up to a 2 x speedup over standard autoregressive models while also improving generation fidelity. Our results demonstrate that structural priors in mesh data can be systematically harnessed to accelerate and enhance autoregressive generation.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15618v1",
        "pdf": "https://arxiv.org/pdf/2511.15618v1"
      },
      "arxiv_id": "2511.15618v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15615v1",
      "title": "Near-optimal delta-convex estimation of Lipschitz functions",
      "authors": [
        "Gábor Balázs"
      ],
      "abstract": "This paper presents a tractable algorithm for estimating an unknown Lipschitz function from noisy observations and establishes an upper bound on its convergence rate. The approach extends max-affine methods from convex shape-restricted regression to the more general Lipschitz setting. A key component is a nonlinear feature expansion that maps max-affine functions into a subclass of delta-convex functions, which act as universal approximators of Lipschitz functions while preserving their Lipschitz constants. Leveraging this property, the estimator attains the minimax convergence rate (up to logarithmic factors) with respect to the intrinsic dimension of the data under squared loss and subgaussian distributions in the random design setting. The algorithm integrates adaptive partitioning to capture intrinsic dimension, a penalty-based regularization mechanism that removes the need to know the true Lipschitz constant, and a two-stage optimization procedure combining a convex initialization with local refinement. The framework is also straightforward to adapt to convex shape-restricted regression. Experiments demonstrate competitive performance relative to other theoretically justified methods, including nearest-neighbor and kernel-based regressors.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15615v1",
        "pdf": "https://arxiv.org/pdf/2511.15615v1"
      },
      "arxiv_id": "2511.15615v1",
      "comment": "41 pages, 7 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15614v1",
      "title": "Optimus-Q: Utilizing Federated Learning in Adaptive Robots for Intelligent Nuclear Power Plant Operations through Quantum Cryptography",
      "authors": [
        "Sai Puppala",
        "Ismail Hossain",
        "Jahangir Alam",
        "Sajedul Talukder"
      ],
      "abstract": "The integration of advanced robotics in nuclear power plants (NPPs) presents a transformative opportunity to enhance safety, efficiency, and environmental monitoring in high-stakes environments. Our paper introduces the Optimus-Q robot, a sophisticated system designed to autonomously monitor air quality and detect contamination while leveraging adaptive learning techniques and secure quantum communication. Equipped with advanced infrared sensors, the Optimus-Q robot continuously streams real-time environmental data to predict hazardous gas emissions, including carbon dioxide (CO$_2$), carbon monoxide (CO), and methane (CH$_4$). Utilizing a federated learning approach, the robot collaborates with other systems across various NPPs to improve its predictive capabilities without compromising data privacy. Additionally, the implementation of Quantum Key Distribution (QKD) ensures secure data transmission, safeguarding sensitive operational information. Our methodology combines systematic navigation patterns with machine learning algorithms to facilitate efficient coverage of designated areas, thereby optimizing contamination monitoring processes. Through simulations and real-world experiments, we demonstrate the effectiveness of the Optimus-Q robot in enhancing operational safety and responsiveness in nuclear facilities. This research underscores the potential of integrating robotics, machine learning, and quantum technologies to revolutionize monitoring systems in hazardous environments.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15614v1",
        "pdf": "https://arxiv.org/pdf/2511.15614v1"
      },
      "arxiv_id": "2511.15614v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15613v1",
      "title": "When to Think and When to Look: Uncertainty-Guided Lookback",
      "authors": [
        "Jing Bi",
        "Filippos Bellos",
        "Junjia Guo",
        "Yayuan Li",
        "Chao Huang",
        "Yunlong",
        "Tang",
        "Luchuan Song",
        "Susan Liang",
        "Zhongfei",
        "Zhang",
        "Jason J. Corso",
        "Chenliang Xu"
      ],
      "abstract": "Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15613v1",
        "pdf": "https://arxiv.org/pdf/2511.15613v1"
      },
      "arxiv_id": "2511.15613v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15605v1",
      "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
      "authors": [
        "Senyu Fei",
        "Siyin Wang",
        "Li Ji",
        "Ao Li",
        "Shiduo Zhang",
        "Liming Liu",
        "Jinlong Hou",
        "Jingjing Gong",
        "Xianzhong Zhao",
        "Xipeng Qiu"
      ],
      "abstract": "Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.RO",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15605v1",
        "pdf": "https://arxiv.org/pdf/2511.15605v1"
      },
      "arxiv_id": "2511.15605v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15603v1",
      "title": "MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation",
      "authors": [
        "Bin Xie",
        "Gady Agam"
      ],
      "abstract": "Medical image segmentation typically adopts a point-wise convolutional segmentation head to predict dense labels, where each output channel is heuristically tied to a specific class. This rigid design limits both feature sharing and semantic generalization. In this work, we propose a unified decoupled segmentation head that separates multi-class prediction into class-agnostic mask prediction and class label prediction using shared object queries. Furthermore, we introduce a Full-Scale Aware Deformable Transformer module that enables low-resolution encoder features to attend across full-resolution encoder features via deformable attention, achieving memory-efficient and spatially aligned full-scale fusion. Our proposed method, named MaskMed, achieves state-of-the-art performance, surpassing nnUNet by +2.0% Dice on AMOS 2022 and +6.9% Dice on BTCV.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15603v1",
        "pdf": "https://arxiv.org/pdf/2511.15603v1"
      },
      "arxiv_id": "2511.15603v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15600v1",
      "title": "US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery",
      "authors": [
        "Miruna-Alexandra Gafencu",
        "Yordanka Velikova",
        "Nassir Navab",
        "Mohammad Farid Azampour"
      ],
      "abstract": "Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15600v1",
        "pdf": "https://arxiv.org/pdf/2511.15600v1"
      },
      "arxiv_id": "2511.15600v1",
      "comment": "Accepted at the Workshop on Shape in Medical Imaging at MICCAI 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15597v1",
      "title": "Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition",
      "authors": [
        "Xufei Wang",
        "Junqiao Zhao",
        "Siyue Tao",
        "Qiwen Gu",
        "Wonbong Kim",
        "Tiantian Feng"
      ],
      "abstract": "LiDAR place recognition plays a crucial role in SLAM, robot navigation, and autonomous driving. However, existing LiDAR place recognition methods often struggle to adapt to new environments without forgetting previously learned knowledge, a challenge widely known as catastrophic forgetting. To address this issue, we propose KDF+, a novel continual learning framework for LiDAR place recognition that extends the KDF paradigm with a loss-aware sampling strategy and a rehearsal enhancement mechanism. The proposed sampling strategy estimates the learning difficulty of each sample via its loss value and selects samples for replay according to their estimated difficulty. Harder samples, which tend to encode more discriminative information, are sampled with higher probability while maintaining distributional coverage across the dataset. In addition, the rehearsal enhancement mechanism encourages memory samples to be further refined during new-task training by slightly reducing their loss relative to previous tasks, thereby reinforcing long-term knowledge retention. Extensive experiments across multiple benchmarks demonstrate that KDF+ consistently outperforms existing continual learning methods and can be seamlessly integrated into state-of-the-art continual learning for LiDAR place recognition frameworks to yield significant and stable performance gains. The code will be available at https://github.com/repo/KDF-plus.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15597v1",
        "pdf": "https://arxiv.org/pdf/2511.15597v1"
      },
      "arxiv_id": "2511.15597v1",
      "comment": "8 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15593v1",
      "title": "What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity",
      "authors": [
        "Alexis Audran-Reiss",
        "Jordi Armengol Estapé",
        "Karen Hambardzumyan",
        "Amar Budhiraja",
        "Martin Josifoski",
        "Edan Toledo",
        "Rishi Hazra",
        "Despoina Magka",
        "Michael Shvartsman",
        "Parth Pathak",
        "Justine T Kao",
        "Lucia Cipolina-Kun",
        "Bhavul Gauri",
        "Jean-Christophe Gagnon-Audet",
        "Emanuel Tewolde",
        "Jenny Zhang",
        "Taco Cohen",
        "Yossi Adi",
        "Tatiana Shavrina",
        "Yoram Bachrach"
      ],
      "abstract": "AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15593v1",
        "pdf": "https://arxiv.org/pdf/2511.15593v1"
      },
      "arxiv_id": "2511.15593v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15586v1",
      "title": "MHR: Momentum Human Rig",
      "authors": [
        "Aaron Ferguson",
        "Ahmed A. A. Osman",
        "Berta Bescos",
        "Carsten Stoll",
        "Chris Twigg",
        "Christoph Lassner",
        "David Otte",
        "Eric Vignola",
        "Federica Bogo",
        "Igor Santesteban",
        "Javier Romero",
        "Jenna Zarate",
        "Jeongseok Lee",
        "Jinhyung Park",
        "Jinlong Yang",
        "John Doublestein",
        "Kishore Venkateshan",
        "Kris Kitani",
        "Ladislav Kavan",
        "Marco Dal Farra",
        "Matthew Hu",
        "Matthew Cioffi",
        "Michael Fabris",
        "Michael Ranieri",
        "Mohammad Modarres",
        "Petr Kadlecek",
        "Rinat Abdrashitov",
        "Romain Prévost",
        "Roman Rajbhandari",
        "Ronald Mallet",
        "Russel Pearsall",
        "Sandy Kao",
        "Sanjeev Kumar",
        "Scott Parrish",
        "Te-Li Wang",
        "Tony Tung",
        "Yuan Dong",
        "Yuhua Chen",
        "Yuanlu Xu",
        "Yuting Ye",
        "Zhongshi Jiang"
      ],
      "abstract": "We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15586v1",
        "pdf": "https://arxiv.org/pdf/2511.15586v1"
      },
      "arxiv_id": "2511.15586v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15580v1",
      "title": "CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking",
      "authors": [
        "Sifan Zhou",
        "Yichao Cao",
        "Jiahao Nie",
        "Yuqian Fu",
        "Ziyu Zhao",
        "Xiaobo Lu",
        "Shuo Wang"
      ],
      "abstract": "3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15580v1",
        "pdf": "https://arxiv.org/pdf/2511.15580v1"
      },
      "arxiv_id": "2511.15580v1",
      "comment": "Accepted by AAAI 2026 (Oral)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15578v1",
      "title": "AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning",
      "authors": [
        "Urjitkumar Patel",
        "Fang-Chun Yeh",
        "Chinmay Gondhalekar"
      ],
      "abstract": "With the increasing prevalence of video content, effectively understanding and answering questions about long form videos has become essential for numerous applications. Although large vision language models (LVLMs) have enhanced performance, they often face challenges with nuanced queries that demand both a comprehensive understanding and detailed analysis. To overcome these obstacles, we introduce AVATAAR, a modular and interpretable framework that combines global and local video context, along with a Pre Retrieval Thinking Agent and a Rethink Module. AVATAAR creates a persistent global summary and establishes a feedback loop between the Rethink Module and the Pre Retrieval Thinking Agent, allowing the system to refine its retrieval strategies based on partial answers and replicate human-like iterative reasoning. On the CinePile benchmark, AVATAAR demonstrates significant improvements over a baseline, achieving relative gains of +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension. Our experiments confirm that each module contributes positively to the overall performance, with the feedback loop being crucial for adaptability. These findings highlight AVATAAR's effectiveness in enhancing video understanding capabilities. Ultimately, AVATAAR presents a scalable solution for long-form Video Question Answering (QA), merging accuracy, interpretability, and extensibility.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15578v1",
        "pdf": "https://arxiv.org/pdf/2511.15578v1"
      },
      "arxiv_id": "2511.15578v1",
      "comment": "Accepted in the 5th IEEE Big Data Workshop on Multimodal AI (MMAI 2025), Dec 8-11, Macau, China, 2025 (Preprint Copy)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15574v1",
      "title": "HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning",
      "authors": [
        "Qihao Yang",
        "Xuelin Wang",
        "Jiale Chen",
        "Xuelian Dong",
        "Yuxin Hao",
        "Tianyong Hao"
      ],
      "abstract": "Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15574v1",
        "pdf": "https://arxiv.org/pdf/2511.15574v1"
      },
      "arxiv_id": "2511.15574v1",
      "comment": "Accepted by AAAI-2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15572v1",
      "title": "From Low-Rank Features to Encoding Mismatch: Rethinking Feature Distillation in Vision Transformers",
      "authors": [
        "Huiyuan Tian",
        "Bonan Xu",
        "Shijian Li",
        "Xin Jin"
      ],
      "abstract": "Feature-map knowledge distillation (KD) is highly effective for convolutional networks but often fails for Vision Transformers (ViTs). To understand this failure and guide method design, we conduct a two-view representation analysis of ViTs. First, a layer-wise Singular Value Decomposition (SVD) of full feature matrices shows that final-layer representations are globally low-rank: for CaiT-S24, only $121/61/34/14$ dimensions suffice to capture $99\\%/95\\%/90\\%/80\\%$ of the energy. In principle, this suggests that a compact student plus a simple linear projector should be enough for feature alignment, contradicting the weak empirical performance of standard feature KD. To resolve this paradox, we introduce a token-level Spectral Energy Pattern (SEP) analysis that measures how each token uses channel capacity. SEP reveals that, despite the global low-rank structure, individual tokens distribute energy over most channels, forming a high-bandwidth encoding pattern. This results in an encoding mismatch between wide teachers and narrow students. Motivated by this insight, we propose two minimal, mismatch-driven strategies: (1) post-hoc feature lifting with a lightweight projector retained during inference, or (2) native width alignment that widens only the student's last block to the teacher's width. On ImageNet-1K, these strategies reactivate simple feature-map distillation in ViTs, raising DeiT-Tiny accuracy from $74.86\\%$ to $77.53\\%$ and $78.23\\%$ when distilling from CaiT-S24, while also improving standalone students trained without any teacher. Our analysis thus explains why ViT feature distillation fails and shows how exploiting low-rank structure yields effective, interpretable remedies and concrete design guidance for compact ViTs.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15572v1",
        "pdf": "https://arxiv.org/pdf/2511.15572v1"
      },
      "arxiv_id": "2511.15572v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15571v1",
      "title": "Transferable Dual-Domain Feature Importance Attack against AI-Generated Image Detector",
      "authors": [
        "Weiheng Zhu",
        "Gang Cao",
        "Jing Liu",
        "Lifang Yu",
        "Shaowei Weng"
      ],
      "abstract": "Recent AI-generated image (AIGI) detectors achieve impressive accuracy under clean condition. In view of antiforensics, it is significant to develop advanced adversarial attacks for evaluating the security of such detectors, which remains unexplored sufficiently. This letter proposes a Dual-domain Feature Importance Attack (DuFIA) scheme to invalidate AIGI detectors to some extent. Forensically important features are captured by the spatially interpolated gradient and frequency-aware perturbation. The adversarial transferability is enhanced by jointly modeling spatial and frequency-domain feature importances, which are fused to guide the optimization-based adversarial example generation. Extensive experiments across various AIGI detectors verify the cross-model transferability, transparency and robustness of DuFIA.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15571v1",
        "pdf": "https://arxiv.org/pdf/2511.15571v1"
      },
      "arxiv_id": "2511.15571v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15567v1",
      "title": "Computer-Use Agents as Judges for Generative User Interface",
      "authors": [
        "Kevin Qinghong Lin",
        "Siyuan Hu",
        "Linjie Li",
        "Zhengyuan Yang",
        "Lijuan Wang",
        "Philip Torr",
        "Mike Zheng Shou"
      ],
      "abstract": "Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15567v1",
        "pdf": "https://arxiv.org/pdf/2511.15567v1"
      },
      "arxiv_id": "2511.15567v1",
      "comment": "Project: https://showlab.github.io/AUI Github: https://github.com/showlab/AUI",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2511.15565v1",
      "title": "Scriboora: Rethinking Human Pose Forecasting",
      "authors": [
        "Daniel Bermuth",
        "Alexander Poeppel",
        "Wolfgang Reif"
      ],
      "abstract": "Human pose forecasting predicts future poses based on past observations, and has many significant applications in areas such as action recognition, autonomous driving or human-robot interaction. This paper evaluates a wide range of pose forecasting algorithms in the task of absolute pose forecasting, revealing many reproducibility issues, and provides a unified training and evaluation pipeline. After drawing a high-level analogy to the task of speech understanding, it is shown that recent speech models can be efficiently adapted to the task of pose forecasting, and improve current state-of-the-art performance. At last the robustness of the models is evaluated, using noisy joint coordinates obtained from a pose estimator model, to reflect a realistic type of noise, which is more close to real-world applications. For this a new dataset variation is introduced, and it is shown that estimated poses result in a substantial performance degradation, and how much of it can be recovered again by unsupervised finetuning.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15565v1",
        "pdf": "https://arxiv.org/pdf/2511.15565v1"
      },
      "arxiv_id": "2511.15565v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15557v1",
      "title": "B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index",
      "authors": [
        "Selim Furkan Tekin",
        "Rajesh Bordawekar"
      ],
      "abstract": "Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.DS"
      ],
      "primary_category": "cs.DB",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15557v1",
        "pdf": "https://arxiv.org/pdf/2511.15557v1"
      },
      "arxiv_id": "2511.15557v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15556v1",
      "title": "Event-based Data Format Standard (EVT+)",
      "authors": [
        "Jonah P. Sengupta",
        "Mohammad Imran Vakil",
        "Thanh M. Dang",
        "Ian Pardee",
        "Paul Coen",
        "Olivia Aul"
      ],
      "abstract": "Event-based Sensing (EBS) hardware is quickly proliferating while finding foothold in many commercial, industrial, and defense applications. At present, there are a handful of technologically mature systems which produce data streams with diverse output formats. In the near future it is anticipated there will be vendors who offer new sensor hardware which could also yield unique data schema that are not aligned to past efforts. Thus, due to the relative nascent nature of the technology and its potential for widespread use in a variety of applications, it is an opportune time to define a standard for this class of sensors' output data. The intent of this document is to identify and provide a standard for the collected EBS streaming data. The main objective of the standard is to be sensor agnostic, incorporate some of the current sensor configurations and modalities, and account for the developing configurations and modalities. The intent is also to leave enough place holders and space in the standard for future variations that may develop as EBS technology matures.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "eess.IV",
        "cs.CR"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15556v1",
        "pdf": "https://arxiv.org/pdf/2511.15556v1"
      },
      "arxiv_id": "2511.15556v1",
      "comment": "22 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15552v1",
      "title": "Multimodal Evaluation of Russian-language Architectures",
      "authors": [
        "Artem Chervyakov",
        "Ulyana Isaeva",
        "Anton Emelyanov",
        "Artem Safin",
        "Maria Tikhonova",
        "Alexander Kharitonov",
        "Yulia Lyakh",
        "Petr Surovtsev",
        "Denis Shevelev Vildan Saburov",
        "Vasily Konovalov",
        "Elisei Rykov",
        "Ivan Sviridov",
        "Amina Miftakhova",
        "Ilseyar Alimova",
        "Alexander Panchenko",
        "Alexander Kapitanov",
        "Alena Fenogenova"
      ],
      "abstract": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15552v1",
        "pdf": "https://arxiv.org/pdf/2511.15552v1"
      },
      "arxiv_id": "2511.15552v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15543v1",
      "title": "A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation",
      "authors": [
        "Georgios Venianakis",
        "Constantinos Theodoropoulos",
        "Michail Kavousanakis"
      ],
      "abstract": "Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15543v1",
        "pdf": "https://arxiv.org/pdf/2511.15543v1"
      },
      "arxiv_id": "2511.15543v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15535v1",
      "title": "A Hybrid CNN-ViT-GNN Framework with GAN-Based Augmentation for Intelligent Weed Detection in Precision Agriculture",
      "authors": [
        "Pandiyaraju V",
        "Abishek Karthik",
        "Sreya Mynampati",
        "Poovarasan L",
        "D. Saraswathi"
      ],
      "abstract": "The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment to edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15535v1",
        "pdf": "https://arxiv.org/pdf/2511.15535v1"
      },
      "arxiv_id": "2511.15535v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15534v1",
      "title": "Exploring the use of AI authors and reviewers at Agents4Science",
      "authors": [
        "Federico Bianchi",
        "Owen Queen",
        "Nitya Thakkar",
        "Eric Sun",
        "James Zou"
      ],
      "abstract": "There is growing interest in using AI agents for scientific research, yet fundamental questions remain about their capabilities as scientists and reviewers. To explore these questions, we organized Agents4Science, the first conference in which AI agents serve as both primary authors and reviewers, with humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and their implications for human-AI collaboration in science.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15534v1",
        "pdf": "https://arxiv.org/pdf/2511.15534v1"
      },
      "arxiv_id": "2511.15534v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15530v1",
      "title": "Convergence and Sketching-Based Efficient Computation of Neural Tangent Kernel Weights in Physics-Based Loss",
      "authors": [
        "Max Hirsch",
        "Federico Pichi"
      ],
      "abstract": "In multi-objective optimization, multiple loss terms are weighted and added together to form a single objective. These weights are chosen to properly balance the competing losses according to some meta-goal. For example, in physics-informed neural networks (PINNs), these weights are often adaptively chosen to improve the network's generalization error. A popular choice of adaptive weights is based on the neural tangent kernel (NTK) of the PINN, which describes the evolution of the network in predictor space during training. The convergence of such an adaptive weighting algorithm is not clear a priori. Moreover, these NTK-based weights would be updated frequently during training, further increasing the computational burden of the learning process. In this paper, we prove that under appropriate conditions, gradient descent enhanced with adaptive NTK-based weights is convergent in a suitable sense. We then address the problem of computational efficiency by developing a randomized algorithm inspired by a predictor-corrector approach and matrix sketching, which produces unbiased estimates of the NTK up to an arbitrarily small discretization error. Finally, we provide numerical experiments to support our theoretical findings and to show the efficacy of our randomized algorithm. Code Availability: https://github.com/maxhirsch/Efficient-NTK",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "math.NA",
        "cs.LG"
      ],
      "primary_category": "math.NA",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15530v1",
        "pdf": "https://arxiv.org/pdf/2511.15530v1"
      },
      "arxiv_id": "2511.15530v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15529v1",
      "title": "Decentralized Gaussian Process Classification and an Application in Subsea Robotics",
      "authors": [
        "Yifei Gao",
        "Hans J. He",
        "Daniel J. Stilwell",
        "James McMahon"
      ],
      "abstract": "Teams of cooperating autonomous underwater vehicles (AUVs) rely on acoustic communication for coordination, yet this communication medium is constrained by limited range, multi-path effects, and low bandwidth. One way to address the uncertainty associated with acoustic communication is to learn the communication environment in real-time. We address the challenge of a team of robots building a map of the probability of communication success from one location to another in real-time. This is a decentralized classification problem -- communication events are either successful or unsuccessful -- where AUVs share a subset of their communication measurements to build the map. The main contribution of this work is a rigorously derived data sharing policy that selects measurements to be shared among AUVs. We experimentally validate our proposed sharing policy using real acoustic communication data collected from teams of Virginia Tech 690 AUVs, demonstrating its effectiveness in underwater environments.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15529v1",
        "pdf": "https://arxiv.org/pdf/2511.15529v1"
      },
      "arxiv_id": "2511.15529v1",
      "comment": "8 pages, 8 figures, IROS 2025 conference",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15522v1",
      "title": "PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles",
      "authors": [
        "Yinan Yu",
        "Samuel Scheidegger"
      ],
      "abstract": "Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15522v1",
        "pdf": "https://arxiv.org/pdf/2511.15522v1"
      },
      "arxiv_id": "2511.15522v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15520v1",
      "title": "Theoretical Closed-loop Stability Bounds for Dynamical System Coupled with Diffusion Policies",
      "authors": [
        "Gabriel Lauzier",
        "Alexandre Girard",
        "François Ferland"
      ],
      "abstract": "Diffusion Policy has shown great performance in robotic manipulation tasks under stochastic perturbations, due to its ability to model multimodal action distributions. Nonetheless, its reliance on a computationally expensive reverse-time diffusion (denoising) process, for action inference, makes it challenging to use for real-time applications where quick decision-making is mandatory. This work studies the possibility of conducting the denoising process only partially before executing an action, allowing the plant to evolve according to its dynamics in parallel to the reverse-time diffusion dynamics ongoing on the computer. In a classical diffusion policy setting, the plant dynamics are usually slow and the two dynamical processes are uncoupled. Here, we investigate theoretical bounds on the stability of closed-loop systems using diffusion policies when the plant dynamics and the denoising dynamics are coupled. The contribution of this work gives a framework for faster imitation learning and a metric that yields if a controller will be stable based on the variance of the demonstrations.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15520v1",
        "pdf": "https://arxiv.org/pdf/2511.15520v1"
      },
      "arxiv_id": "2511.15520v1",
      "comment": "5 pages, 3 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15515v1",
      "title": "Multi-Text Guided Few-Shot Semantic Segmentation",
      "authors": [
        "Qiang Jiao",
        "Bin Yan",
        "Yi Yang",
        "Mengrui Shi",
        "Qiang Zhang"
      ],
      "abstract": "Recent CLIP-based few-shot semantic segmentation methods introduce class-level textual priors to assist segmentation by typically using a single prompt (e.g., a photo of class). However, these approaches often result in incomplete activation of target regions, as a single textual description cannot fully capture the semantic diversity of complex categories. Moreover, they lack explicit cross-modal interaction and are vulnerable to noisy support features, further degrading visual prior quality. To address these issues, we propose the Multi-Text Guided Few-Shot Semantic Segmentation Network (MTGNet), a dual-branch framework that enhances segmentation performance by fusing diverse textual prompts to refine textual priors and guide the cross-modal optimization of visual priors. Specifically, we design a Multi-Textual Prior Refinement (MTPR) module that suppresses interference and aggregates complementary semantic cues to enhance foreground activation and expand semantic coverage for structurally complex objects. We introduce a Text Anchor Feature Fusion (TAFF) module, which leverages multi-text embeddings as semantic anchors to facilitate the transfer of discriminative local prototypes from support images to query images, thereby improving semantic consistency and alleviating intra-class variations. Furthermore, a Foreground Confidence-Weighted Attention (FCWA) module is presented to enhance visual prior robustness by leveraging internal self-similarity within support foreground features. It adaptively down-weights inconsistent regions and effectively suppresses interference in the query segmentation process. Extensive experiments on standard FSS benchmarks validate the effectiveness of MTGNet. In the 1-shot setting, it achieves 76.8% mIoU on PASCAL-5i and 57.4% on COCO-20i, with notable improvements in folds exhibiting high intra-class variations.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15515v1",
        "pdf": "https://arxiv.org/pdf/2511.15515v1"
      },
      "arxiv_id": "2511.15515v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15509v1",
      "title": "Multimodal Optical Imaging Platform for Quantitative Burn Assessment",
      "authors": [
        "Nathaniel Hanson",
        "Mateusz Wolak",
        "Jonathan Richardson",
        "Patrick Walker",
        "David M. Burmeister",
        "Chakameh Jafari"
      ],
      "abstract": "Accurate assessment of burn severity at injury onset remains a major clinical challenge due to the lack of objective methods for detecting subsurface tissue damage. This limitation is critical in battlefield and mass-casualty settings, where rapid and reliable evaluation of burn depth is essential for triage and surgical decision-making. We present a multimodal optical imaging framework that establishes the foundation for a compact, low-size, weight, and power (low-SWaP) field-deployable device for quantitative burn assessment. The system integrates broadband hyperspectral imaging (VSWIR, 400 -- 2100 nm) with laser speckle contrast imaging to jointly evaluate biochemical composition and microvascular perfusion. Using short-wave infrared (SWIR, >1000 nm) wavelengths, we developed and validated novel deep-tissue parameters linked to water, lipid, and collagen absorption features that enhance burn-tissue separability and burn severity classification. We implemented and validated unsupervised learning methods for spectral feature extraction, band down-selection, and clustering against histology, establishing a foundation for a rugged, data-driven device for early quantitative burn evaluation in austere environments.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "eess.IV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15509v1",
        "pdf": "https://arxiv.org/pdf/2511.15509v1"
      },
      "arxiv_id": "2511.15509v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15507v1",
      "title": "Sample-Adaptivity Tradeoff in On-Demand Sampling",
      "authors": [
        "Nika Haghtalab",
        "Omar Montasser",
        "Mingda Qiao"
      ],
      "abstract": "We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\\widetilde O((d + k) / ε^2)$ within $\\widetilde O(\\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\\widetilde O(\\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG",
        "cs.DS",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15507v1",
        "pdf": "https://arxiv.org/pdf/2511.15507v1"
      },
      "arxiv_id": "2511.15507v1",
      "comment": "50 pages, to appear at NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15503v1",
      "title": "A Tensor Compiler for Processing-In-Memory Architectures",
      "authors": [
        "Peiming Yang",
        "Sankeerth Durvasula",
        "Ivan Fernandez",
        "Mohammad Sadrosadati",
        "Onur Mutlu",
        "Gennady Pekhimenko",
        "Christina Giannoula"
      ],
      "abstract": "Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.AR",
        "cs.DC",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.AR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15503v1",
        "pdf": "https://arxiv.org/pdf/2511.15503v1"
      },
      "arxiv_id": "2511.15503v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15499v1",
      "title": "Learning to Expand Images for Efficient Visual Autoregressive Modeling",
      "authors": [
        "Ruiqing Yang",
        "Kaixin Zhang",
        "Zheng Zhang",
        "Shan You",
        "Tao Huang"
      ],
      "abstract": "Autoregressive models have recently shown great promise in visual generation by leveraging discrete token sequences akin to language modeling. However, existing approaches often suffer from inefficiency, either due to token-by-token decoding or the complexity of multi-scale representations. In this work, we introduce Expanding Autoregressive Representation (EAR), a novel generation paradigm that emulates the human visual system's center-outward perception pattern. EAR unfolds image tokens in a spiral order from the center and progressively expands outward, preserving spatial continuity and enabling efficient parallel decoding. To further enhance flexibility and speed, we propose a length-adaptive decoding strategy that dynamically adjusts the number of tokens predicted at each step. This biologically inspired design not only reduces computational cost but also improves generation quality by aligning the generation order with perceptual relevance. Extensive experiments on ImageNet demonstrate that EAR achieves state-of-the-art trade-offs between fidelity and efficiency on single-scale autoregressive models, setting a new direction for scalable and cognitively aligned autoregressive image generation.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15499v1",
        "pdf": "https://arxiv.org/pdf/2511.15499v1"
      },
      "arxiv_id": "2511.15499v1",
      "comment": "16 pages, 18 figures, includes appendix with additional visualizations, submitted as arXiv preprint",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15496v1",
      "title": "Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels",
      "authors": [
        "Maria Pilligua",
        "David Serrano-Lozano",
        "Pai Peng",
        "Ramon Baldrich",
        "Michael S. Brown",
        "Javier Vazquez-Corral"
      ],
      "abstract": "Imaging in low-light environments is challenging due to reduced scene radiance, which leads to elevated sensor noise and reduced color saturation. Most learning-based low-light enhancement methods rely on paired training data captured under a single low-light condition and a well-lit reference. The lack of radiance diversity limits our understanding of how enhancement techniques perform across varying illumination intensities. We introduce the Multi-Illumination Low-Light (MILL) dataset, containing images captured at diverse light intensities under controlled conditions with fixed camera settings and precise illuminance measurements. MILL enables comprehensive evaluation of enhancement algorithms across variable lighting conditions. We benchmark several state-of-the-art methods and reveal significant performance variations across intensity levels. Leveraging the unique multi-illumination structure of our dataset, we propose improvements that enhance robustness across diverse illumination scenarios. Our modifications achieve up to 10 dB PSNR improvement for DSLR and 2 dB for the smartphone on Full HD images.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15496v1",
        "pdf": "https://arxiv.org/pdf/2511.15496v1"
      },
      "arxiv_id": "2511.15496v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15487v1",
      "title": "NTK-Guided Implicit Neural Teaching",
      "authors": [
        "Chen Zhang",
        "Wei Zuo",
        "Bingyang Cheng",
        "Yikun Wang",
        "Wei-Bin Kou",
        "Yik Chung WU",
        "Ngai Wong"
      ],
      "abstract": "Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15487v1",
        "pdf": "https://arxiv.org/pdf/2511.15487v1"
      },
      "arxiv_id": "2511.15487v1",
      "comment": "Preprint",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15485v1",
      "title": "A Novel CustNetGC Boosted Model with Spectral Features for Parkinson's Disease Prediction",
      "authors": [
        "Abishek Karthik",
        "Pandiyaraju V",
        "Dominic Savio M",
        "Rohit Swaminathan S"
      ],
      "abstract": "Parkinson's disease is a neurodegenerative disorder that can be very tricky to diagnose and treat. Such early symptoms can include tremors, wheezy breathing, and changes in voice quality as critical indicators of neural damage. Notably, there has been growing interest in utilizing changes in vocal attributes as markers for the detection of PD early on. Based on this understanding, the present paper was designed to focus on the acoustic feature analysis based on voice recordings of patients diagnosed with PD and healthy controls (HC). In this paper, we introduce a novel classification and visualization model known as CustNetGC, combining a Convolutional Neural Network (CNN) with Custom Network Grad-CAM and CatBoost to enhance the efficiency of PD diagnosis. We use a publicly available dataset from Figshare, including voice recordings of 81 participants: 40 patients with PD and 41 healthy controls. From these recordings, we extracted the key spectral features: L-mHP and Spectral Slopes. The L-mHP feature combines three spectrogram representations: Log-Mel spectrogram, harmonic spectrogram, and percussive spectrogram, which are derived using Harmonic-Percussive Source Separation (HPSS). Grad-CAM was used to highlight the important regions in the data, thus making the PD predictions interpretable and effective. Our proposed CustNetGC model achieved an accuracy of 99.06% and precision of 95.83%, with the area under the ROC curve (AUC) recorded at 0.90 for the PD class and 0.89 for the HC class. Additionally, the combination of CatBoost, a gradient boosting algorithm, enhanced the robustness and the prediction performance by properly classifying PD and non-PD samples. Therefore, the results provide the potential improvement in the CustNetGC system in enhancing diagnostic accuracy and the interpretability of the Parkinson's Disease prediction model.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.SD",
        "cs.CV"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15485v1",
        "pdf": "https://arxiv.org/pdf/2511.15485v1"
      },
      "arxiv_id": "2511.15485v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15481v1",
      "title": "FunnyNodules: A Customizable Medical Dataset Tailored for Evaluating Explainable AI",
      "authors": [
        "Luisa Gallée",
        "Yiheng Xiong",
        "Meinrad Beer",
        "Michael Götz"
      ],
      "abstract": "Densely annotated medical image datasets that capture not only diagnostic labels but also the underlying reasoning behind these diagnoses are scarce. Such reasoning-related annotations are essential for developing and evaluating explainable AI (xAI) models that reason similarly to radiologists: making correct predictions for the right reasons. To address this gap, we introduce FunnyNodules, a fully parameterized synthetic dataset designed for systematic analysis of attribute-based reasoning in medical AI models. The dataset generates abstract, lung nodule-like shapes with controllable visual attributes such as roundness, margin sharpness, and spiculation. Target class is derived from a predefined attribute combination, allowing full control over the decision rule that links attributes to the diagnostic class. We demonstrate how FunnyNodules can be used in model-agnostic evaluations to assess whether models learn correct attribute-target relations, to interpret over- or underperformance in attribute prediction, and to analyze attention alignment with attribute-specific regions of interest. The framework is fully customizable, supporting variations in dataset complexity, target definitions, class balance, and beyond. With complete ground truth information, FunnyNodules provides a versatile foundation for developing, benchmarking, and conducting in-depth analyses of explainable AI methods in medical image analysis.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15481v1",
        "pdf": "https://arxiv.org/pdf/2511.15481v1"
      },
      "arxiv_id": "2511.15481v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15476v1",
      "title": "RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection",
      "authors": [
        "Rashid Iqbal",
        "Saddam Hussain Khan"
      ],
      "abstract": "This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15476v1",
        "pdf": "https://arxiv.org/pdf/2511.15476v1"
      },
      "arxiv_id": "2511.15476v1",
      "comment": "33 Pages, 12 Figure, 4 Tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15468v1",
      "title": "Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners",
      "authors": [
        "Xabier Lekunberri",
        "Ahmad Kamal",
        "Izaro Goienetxea",
        "Jon Ruiz",
        "Iñaki Quincoces",
        "Jaime Valls Miro",
        "Ignacio Arganda-Carreras",
        "Jose A. Fernandes-Salvador"
      ],
      "abstract": "Purse seiners play a crucial role in tuna fishing, as approximately 69% of the world's tropical tuna is caught using this gear. All tuna Regional Fisheries Management Organizations have established minimum standards to use electronic monitoring (EM) in fisheries in addition to traditional observers. The EM systems produce a massive amount of video data that human analysts must process. Integrating artificial intelligence (AI) into their workflow can decrease that workload and improve the accuracy of the reports. However, species identification still poses significant challenges for AI, as achieving balanced performance across all species requires appropriate training data. Here, we quantify the difficulty experts face to distinguish bigeye tuna (BET, Thunnus Obesus) from yellowfin tuna (YFT, Thunnus Albacares) using images captured by EM systems. We found inter-expert agreements of 42.9% $\\pm$ 35.6% for BET and 57.1% $\\pm$ 35.6% for YFT. We then present a multi-stage pipeline to estimate the species composition of the catches using a reliable ground-truth dataset based on identifications made by observers on board. Three segmentation approaches are compared: Mask R-CNN, a combination of DINOv2 with SAM2, and a integration of YOLOv9 with SAM2. We found that the latest performs the best, with a validation mean average precision of 0.66 $\\pm$ 0.03 and a recall of 0.88 $\\pm$ 0.03. Segmented individuals are tracked using ByteTrack. For classification, we evaluate a standard multiclass classification model and a hierarchical approach, finding a superior generalization by the hierarchical. All our models were cross-validated during training and tested on fishing operations with fully known catch composition. Combining YOLOv9-SAM2 with the hierarchical classification produced the best estimations, with 84.8% of the individuals being segmented and classified with a mean average error of 4.5%.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15468v1",
        "pdf": "https://arxiv.org/pdf/2511.15468v1"
      },
      "arxiv_id": "2511.15468v1",
      "comment": "23 pages, 5 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15464v1",
      "title": "SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome",
      "authors": [
        "Dabin Jeong",
        "Amirhossein Vahidi",
        "Ciro Ramírez-Suástegui",
        "Marie Moullet",
        "Kevin Ly",
        "Mohammad Vali Sanian",
        "Sebastian Birk",
        "Yinshui Chang",
        "Adam Boxall",
        "Daniyal Jafree",
        "Lloyd Steele",
        "Vijaya Baskar MS",
        "Muzlifah Haniffa",
        "Mohammad Lotfollahi"
      ],
      "abstract": "Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\\% in the gene-expression prediction task and avg. 26.93\\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15464v1",
        "pdf": "https://arxiv.org/pdf/2511.15464v1"
      },
      "arxiv_id": "2511.15464v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15462v1",
      "title": "Insights from the ICLR Peer Review and Rebuttal Process",
      "authors": [
        "Amir Hossein Kargaran",
        "Nafiseh Nikeghbal",
        "Jing Yang",
        "Nedjma Ousidhoum"
      ],
      "abstract": "Peer review is a cornerstone of scientific publishing, including at premier machine learning conferences such as ICLR. As submission volumes increase, understanding the nature and dynamics of the review process is crucial for improving its efficiency, effectiveness, and the quality of published papers. We present a large-scale analysis of the ICLR 2024 and 2025 peer review processes, focusing on before- and after-rebuttal scores and reviewer-author interactions. We examine review scores, author-reviewer engagement, temporal patterns in review submissions, and co-reviewer influence effects. Combining quantitative analyses with LLM-based categorization of review texts and rebuttal discussions, we identify common strengths and weaknesses for each rating group, as well as trends in rebuttal strategies that are most strongly associated with score changes. Our findings show that initial scores and the ratings of co-reviewers are the strongest predictors of score changes during the rebuttal, pointing to a degree of reviewer influence. Rebuttals play a valuable role in improving outcomes for borderline papers, where thoughtful author responses can meaningfully shift reviewer perspectives. More broadly, our study offers evidence-based insights to improve the peer review process, guiding authors on effective rebuttal strategies and helping the community design fairer and more efficient review processes. Our code and score changes data are available at https://github.com/papercopilot/iclr-insights.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15462v1",
        "pdf": "https://arxiv.org/pdf/2511.15462v1"
      },
      "arxiv_id": "2511.15462v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15459v1",
      "title": "Driving in Spikes: An Entropy-Guided Object Detector for Spike Cameras",
      "authors": [
        "Ziyan Liu",
        "Qi Su",
        "Lulu Tang",
        "Zhaofei Yu",
        "Tiejun Huang"
      ],
      "abstract": "Object detection in autonomous driving suffers from motion blur and saturation under fast motion and extreme lighting. Spike cameras, offer microsecond latency and ultra high dynamic range for object detection by using per pixel asynchronous integrate and fire. However, their sparse, discrete output cannot be processed by standard image-based detectors, posing a critical challenge for end to end spike stream detection. We propose EASD, an end to end spike camera detector with a dual branch design: a Temporal Based Texture plus Feature Fusion branch for global cross slice semantics, and an Entropy Selective Attention branch for object centric details. To close the data gap, we introduce DSEC Spike, the first driving oriented simulated spike detection benchmark.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15459v1",
        "pdf": "https://arxiv.org/pdf/2511.15459v1"
      },
      "arxiv_id": "2511.15459v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15456v1",
      "title": "Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining",
      "authors": [
        "Qian'ang Mao",
        "Yuxuan Zhang",
        "Jiaman Chen",
        "Wenjun Zhou",
        "Jiaqi Yan"
      ],
      "abstract": "As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.AI",
        "q-fin.GN"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15456v1",
        "pdf": "https://arxiv.org/pdf/2511.15456v1"
      },
      "arxiv_id": "2511.15456v1",
      "comment": "Written in 2025 Q1",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15454v1",
      "title": "FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning",
      "authors": [
        "Ouiame Marnissi",
        "Hajar EL Hammouti",
        "El Houcine Bergou"
      ],
      "abstract": "Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\\% compared to baseline strategies.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15454v1",
        "pdf": "https://arxiv.org/pdf/2511.15454v1"
      },
      "arxiv_id": "2511.15454v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15447v1",
      "title": "TSFM in-context learning for time-series classification of bearing-health status",
      "authors": [
        "Michel Tokic",
        "Slobodan Djukanović",
        "Anja von Beuningen",
        "Cheng Feng"
      ],
      "abstract": "This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15447v1",
        "pdf": "https://arxiv.org/pdf/2511.15447v1"
      },
      "arxiv_id": "2511.15447v1",
      "comment": "Preprint submitted to ESANN 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15446v1",
      "title": "Gini Score under Ties and Case Weights",
      "authors": [
        "Alexej Brauer",
        "Mario V. Wüthrich"
      ],
      "abstract": "The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15446v1",
        "pdf": "https://arxiv.org/pdf/2511.15446v1"
      },
      "arxiv_id": "2511.15446v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15445v1",
      "title": "Neural network-driven domain decomposition for efficient solutions to the Helmholtz equation",
      "authors": [
        "Victorita Dolean",
        "Daria Hrebenshchykova",
        "Stéphane Lanteri",
        "Victor Michel-Dansac"
      ],
      "abstract": "Accurately simulating wave propagation is crucial in fields such as acoustics, electromagnetism, and seismic analysis. Traditional numerical methods, like finite difference and finite element approaches, are widely used to solve governing partial differential equations (PDEs) such as the Helmholtz equation. However, these methods face significant computational challenges when applied to high-frequency wave problems in complex two-dimensional domains. This work investigates Finite Basis Physics-Informed Neural Networks (FBPINNs) and their multilevel extensions as a promising alternative. These methods leverage domain decomposition, partitioning the computational domain into overlapping sub-domains, each governed by a local neural network. We assess their accuracy and computational efficiency in solving the Helmholtz equation for the homogeneous case, demonstrating their potential to mitigate the limitations of traditional approaches.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "math.NA",
        "cs.LG"
      ],
      "primary_category": "math.NA",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15445v1",
        "pdf": "https://arxiv.org/pdf/2511.15445v1"
      },
      "arxiv_id": "2511.15445v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15440v1",
      "title": "A Dataset and Baseline for Deep Learning-Based Visual Quality Inspection in Remanufacturing",
      "authors": [
        "Johannes C. Bauer",
        "Paul Geng",
        "Stephan Trattnig",
        "Petr Dokládal",
        "Rüdiger Daub"
      ],
      "abstract": "Remanufacturing describes a process where worn products are restored to like-new condition and it offers vast ecological and economic potentials. A key step is the quality inspection of disassembled components, which is mostly done manually due to the high variety of parts and defect patterns. Deep neural networks show great potential to automate such visual inspection tasks but struggle to generalize to new product variants, components, or defect patterns. To tackle this challenge, we propose a novel image dataset depicting typical gearbox components in good and defective condition from two automotive transmissions. Depending on the train-test split of the data, different distribution shifts are generated to benchmark the generalization ability of a classification model. We evaluate different models using the dataset and propose a contrastive regularization loss to enhance model robustness. The results obtained demonstrate the ability of the loss to improve generalisation to unseen types of components.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15440v1",
        "pdf": "https://arxiv.org/pdf/2511.15440v1"
      },
      "arxiv_id": "2511.15440v1",
      "comment": "",
      "journal_ref": "2025 IEEE 30th International Conference on Emerging Technologies and Factory Automation (ETFA)",
      "has_code": false
    },
    {
      "id": "2511.15435v1",
      "title": "HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation",
      "authors": [
        "Linyin Luo",
        "Yujuan Ding",
        "Yunshan Ma",
        "Wenqi Fan",
        "Hanjiang Lai"
      ],
      "abstract": "Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15435v1",
        "pdf": "https://arxiv.org/pdf/2511.15435v1"
      },
      "arxiv_id": "2511.15435v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15434v1",
      "title": "Small Language Models for Phishing Website Detection: Cost, Performance, and Privacy Trade-Offs",
      "authors": [
        "Georg Goldenits",
        "Philip Koenig",
        "Sebastian Raubitzek",
        "Andreas Ekelhart"
      ],
      "abstract": "Phishing websites pose a major cybersecurity threat, exploiting unsuspecting users and causing significant financial and organisational harm. Traditional machine learning approaches for phishing detection often require extensive feature engineering, continuous retraining, and costly infrastructure maintenance. At the same time, proprietary large language models (LLMs) have demonstrated strong performance in phishing-related classification tasks, but their operational costs and reliance on external providers limit their practical adoption in many business environments. This paper investigates the feasibility of small language models (SLMs) for detecting phishing websites using only their raw HTML code. A key advantage of these models is that they can be deployed on local infrastructure, providing organisations with greater control over data and operations. We systematically evaluate 15 commonly used Small Language Models (SLMs), ranging from 1 billion to 70 billion parameters, benchmarking their classification accuracy, computational requirements, and cost-efficiency. Our results highlight the trade-offs between detection performance and resource consumption, demonstrating that while SLMs underperform compared to state-of-the-art proprietary LLMs, they can still provide a viable and scalable alternative to external LLM services. By presenting a comparative analysis of costs and benefits, this work lays the foundation for future research on the adaptation, fine-tuning, and deployment of SLMs in phishing detection systems, aiming to balance security effectiveness and economic practicality.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15434v1",
        "pdf": "https://arxiv.org/pdf/2511.15434v1"
      },
      "arxiv_id": "2511.15434v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15433v1",
      "title": "Representation Space Constrained Learning with Modality Decoupling for Multimodal Object Detection",
      "authors": [
        "YiKang Shao",
        "Tao Shi"
      ],
      "abstract": "Multimodal object detection has attracted significant attention in both academia and industry for its enhanced robustness. Although numerous studies have focused on improving modality fusion strategies, most neglect fusion degradation, and none provide a theoretical analysis of its underlying causes. To fill this gap, this paper presents a systematic theoretical investigation of fusion degradation in multimodal detection and identifies two key optimization deficiencies: (1) the gradients of unimodal branch backbones are severely suppressed under multimodal architectures, resulting in under-optimization of the unimodal branches; (2) disparities in modality quality cause weaker modalities to experience stronger gradient suppression, which in turn results in imbalanced modality learning. To address these issues, this paper proposes a Representation Space Constrained Learning with Modality Decoupling (RSC-MD) method, which consists of two modules. The RSC module and the MD module are designed to respectively amplify the suppressed gradients and eliminate inter-modality coupling interference as well as modality imbalance, thereby enabling the comprehensive optimization of each modality-specific backbone. Extensive experiments conducted on the FLIR, LLVIP, M3FD, and MFAD datasets demonstrate that the proposed method effectively alleviates fusion degradation and achieves state-of-the-art performance across multiple benchmarks. The code and training procedures will be released at https://github.com/yikangshao/RSC-MD.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15433v1",
        "pdf": "https://arxiv.org/pdf/2511.15433v1"
      },
      "arxiv_id": "2511.15433v1",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15432v1",
      "title": "Towards Understanding Layer Contributions in Tabular In-Context Learning Models",
      "authors": [
        "Amir Rezaei Balef",
        "Mykhailo Koshil",
        "Katharina Eggensperger"
      ],
      "abstract": "Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the \"layers as painters\" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15432v1",
        "pdf": "https://arxiv.org/pdf/2511.15432v1"
      },
      "arxiv_id": "2511.15432v1",
      "comment": "Accepted at the EurIPS 2025 Workshop on AI for Tabular Data",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15429v1",
      "title": "WarNav: An Autonomous Driving Benchmark for Segmentation of Navigable Zones in War Scenes",
      "authors": [
        "Marc-Emmanuel Coupvent des Graviers",
        "Hejer Ammar",
        "Christophe Guettier",
        "Yann Dumortier",
        "Romaric Audigier"
      ],
      "abstract": "We introduce WarNav, a novel real-world dataset constructed from images of the open-source DATTALION repository, specifically tailored to enable the development and benchmarking of semantic segmentation models for autonomous ground vehicle navigation in unstructured, conflict-affected environments. This dataset addresses a critical gap between conventional urban driving resources and the unique operational scenarios encountered by unmanned systems in hazardous and damaged war-zones. We detail the methodological challenges encountered, ranging from data heterogeneity to ethical considerations, providing guidance for future efforts that target extreme operational contexts. To establish performance references, we report baseline results on WarNav using several state-of-the-art semantic segmentation models trained on structured urban scenes. We further analyse the impact of training data environments and propose a first step towards effective navigability in challenging environments with the constraint of having no annotation of the targeted images. Our goal is to foster impactful research that enhances the robustness and safety of autonomous vehicles in high-risk scenarios while being frugal in annotated data.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15429v1",
        "pdf": "https://arxiv.org/pdf/2511.15429v1"
      },
      "arxiv_id": "2511.15429v1",
      "comment": "Accepted at CAID (Conference on Artificial Intelligence for Defence)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15418v1",
      "title": "Building Robust and Scalable Multilingual ASR for Indian Languages",
      "authors": [
        "Arjun Gangwar",
        "Kaousheik Jayakumar",
        "S. Umesh"
      ],
      "abstract": "This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15418v1",
        "pdf": "https://arxiv.org/pdf/2511.15418v1"
      },
      "arxiv_id": "2511.15418v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15414v1",
      "title": "RRT*former: Environment-Aware Sampling-Based Motion Planning using Transformer",
      "authors": [
        "Mingyang Feng",
        "Shaoyuan Li",
        "Xiang Yin"
      ],
      "abstract": "We investigate the sampling-based optimal path planning problem for robotics in complex and dynamic environments. Most existing sampling-based algorithms neglect environmental information or the information from previous samples. Yet, these pieces of information are highly informative, as leveraging them can provide better heuristics when sampling the next state. In this paper, we propose a novel sampling-based planning algorithm, called \\emph{RRT*former}, which integrates the standard RRT* algorithm with a Transformer network in a novel way. Specifically, the Transformer is used to extract features from the environment and leverage information from previous samples to better guide the sampling process. Our extensive experiments demonstrate that, compared to existing sampling-based approaches such as RRT*, Neural RRT*, and their variants, our algorithm achieves considerable improvements in both the optimality of the path and sampling efficiency. The code for our implementation is available on https://github.com/fengmingyang666/RRTformer.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15414v1",
        "pdf": "https://arxiv.org/pdf/2511.15414v1"
      },
      "arxiv_id": "2511.15414v1",
      "comment": "Accepted to IROS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15411v1",
      "title": "D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models",
      "authors": [
        "Wenlun Zhang",
        "Yunshan Zhong",
        "Zihao Ding",
        "Xinyu Li",
        "Kentaro Yoshioka"
      ],
      "abstract": "Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15411v1",
        "pdf": "https://arxiv.org/pdf/2511.15411v1"
      },
      "arxiv_id": "2511.15411v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15409v1",
      "title": "Proximal Approximate Inference in State-Space Models",
      "authors": [
        "Hany Abdulsamad",
        "Ángel F. García-Fernández",
        "Simo Särkkä"
      ],
      "abstract": "We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15409v1",
        "pdf": "https://arxiv.org/pdf/2511.15409v1"
      },
      "arxiv_id": "2511.15409v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15408v1",
      "title": "NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework",
      "authors": [
        "Shanlin Zhou",
        "Xinpeng Wang",
        "Jianxun Lian",
        "Zhenghao Liu",
        "Laks V. S. Lakshmanan",
        "Xiaoyuan Yi",
        "Yongtao Hao"
      ],
      "abstract": "Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.MA",
        "cs.NE"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15408v1",
        "pdf": "https://arxiv.org/pdf/2511.15408v1"
      },
      "arxiv_id": "2511.15408v1",
      "comment": "13 pages,9 figures. This work has been submitted to the IEEE for possible publication",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15407v1",
      "title": "IPR-1: Interactive Physical Reasoner",
      "authors": [
        "Mingyu Zhang",
        "Lifeng Zhuo",
        "Tianxi Tan",
        "Guocan Xie",
        "Xian Nie",
        "Yan Li",
        "Renjie Zhao",
        "Zizhu He",
        "Ziyu Wang",
        "Jiting Cai",
        "Yong-Lu Li"
      ],
      "abstract": "Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15407v1",
        "pdf": "https://arxiv.org/pdf/2511.15407v1"
      },
      "arxiv_id": "2511.15407v1",
      "comment": "11 pages, 5 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15406v1",
      "title": "Controlling False Positives in Image Segmentation via Conformal Prediction",
      "authors": [
        "Luca Mossina",
        "Corentin Friedrich"
      ],
      "abstract": "Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15406v1",
        "pdf": "https://arxiv.org/pdf/2511.15406v1"
      },
      "arxiv_id": "2511.15406v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15396v1",
      "title": "ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation",
      "authors": [
        "Simon Boeder",
        "Fabian Gigengack",
        "Simon Roesler",
        "Holger Caesar",
        "Benjamin Risse"
      ],
      "abstract": "Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15396v1",
        "pdf": "https://arxiv.org/pdf/2511.15396v1"
      },
      "arxiv_id": "2511.15396v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15393v1",
      "title": "EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG",
      "authors": [
        "Kunyu Zhang",
        "Mingxuan Wang",
        "Xiangjie Shi",
        "Haoxing Xu",
        "Chao Zhang"
      ],
      "abstract": "The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15393v1",
        "pdf": "https://arxiv.org/pdf/2511.15393v1"
      },
      "arxiv_id": "2511.15393v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15392v1",
      "title": "DEPO: Dual-Efficiency Preference Optimization for LLM Agents",
      "authors": [
        "Sirui Chen",
        "Mengshi Zhao",
        "Lei Xu",
        "Yuying Zhao",
        "Beier Zhu",
        "Hanwang Zhang",
        "Shengjie Zhao",
        "Chaochao Lu"
      ],
      "abstract": "Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15392v1",
        "pdf": "https://arxiv.org/pdf/2511.15392v1"
      },
      "arxiv_id": "2511.15392v1",
      "comment": "Accepted to AAAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15390v1",
      "title": "Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models",
      "authors": [
        "Haidong Kang",
        "Lihong Lin",
        "Enneng Yang",
        "Hongning Dai",
        "Hao Wang"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable performance on a wide range of tasks, hindering real-world deployment due to their massive size. Existing pruning methods (e.g., Wanda) tailored for LLMs rely heavily on manual design pruning algorithms, thereby leading to \\textit{huge labor costs} and \\textit{requires expert knowledge}. Furthermore, we are the first to identify the serious \\textit{outlier value issue} behind dramatic performance degradation under high pruning ratios that are caused by uniform sparsity, raising an additional concern about how to design adaptive pruning sparsity ideal for LLMs. Can LLMs prune by themselves? In this work, we introduce an affirmative answer by proposing a novel pruning method called \\textbf{AutoPrune}, which first overcomes expert knowledge limits by leveraging LLMs to design optimal pruning algorithms for themselves automatically without any expert knowledge. Specifically, to mitigate the black-box nature of LLMs, we propose a Graph-driven Chain-of-Thought (GCoT) to optimize prompts, significantly enhancing the reasoning process in learning the pruning algorithm and enabling us to generate pruning algorithms with superior performance and interpretability in the next generation. Finally, grounded in insights of outlier value issue, we introduce Skew-aware Dynamic Sparsity Allocation (SDSA) to overcome the outlier value issue, mitigating performance degradation under high pruning ratios. We conduct extensive experiments on mainstream LLMs benchmarks, demonstrating the superiority of AutoPrune, which consistently excels state-of-the-art competitors. The code is available at: https://anonymous.4open.science/r/AutoPrune.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15390v1",
        "pdf": "https://arxiv.org/pdf/2511.15390v1"
      },
      "arxiv_id": "2511.15390v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15383v1",
      "title": "A Compliance-Preserving Retrieval System for Aircraft MRO Task Search",
      "authors": [
        "Byungho Jo"
      ],
      "abstract": "Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15383v1",
        "pdf": "https://arxiv.org/pdf/2511.15383v1"
      },
      "arxiv_id": "2511.15383v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15379v1",
      "title": "Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training",
      "authors": [
        "Yunjiao Zhou",
        "Xinyan Chen",
        "Junlang Qian",
        "Lihua Xie",
        "Jianfei Yang"
      ],
      "abstract": "Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7\\% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15379v1",
        "pdf": "https://arxiv.org/pdf/2511.15379v1"
      },
      "arxiv_id": "2511.15379v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15378v1",
      "title": "Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents",
      "authors": [
        "Trevor McInroe"
      ],
      "abstract": "We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15378v1",
        "pdf": "https://arxiv.org/pdf/2511.15378v1"
      },
      "arxiv_id": "2511.15378v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15375v1",
      "title": "Parameter Importance-Driven Continual Learning for Foundation Models",
      "authors": [
        "Lingxiang Wang",
        "Hainan Zhang",
        "Zhiming Zheng"
      ],
      "abstract": "Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15375v1",
        "pdf": "https://arxiv.org/pdf/2511.15375v1"
      },
      "arxiv_id": "2511.15375v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15371v1",
      "title": "CID: Measuring Feature Importance Through Counterfactual Distributions",
      "authors": [
        "Eddie Conti",
        "Álvaro Parafita",
        "Axel Brando"
      ],
      "abstract": "Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15371v1",
        "pdf": "https://arxiv.org/pdf/2511.15371v1"
      },
      "arxiv_id": "2511.15371v1",
      "comment": "Accepted at Northern Lights Deep Learning (NLDL) 2026 Conference",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15370v1",
      "title": "The Empowerment of Science of Science by Large Language Models: New Tools and Methods",
      "authors": [
        "Guoqiang Liang",
        "Jingqian Gong",
        "Mengxuan Li",
        "Gege Lin",
        "Shuo Zhang"
      ],
      "abstract": "Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15370v1",
        "pdf": "https://arxiv.org/pdf/2511.15370v1"
      },
      "arxiv_id": "2511.15370v1",
      "comment": "The manuscript is currently ongoing the underreview process of the journal of information science",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15369v1",
      "title": "IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers",
      "authors": [
        "Gihwan Kim",
        "Jemin Lee",
        "Hyungshin Kim"
      ],
      "abstract": "Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\\%p (avg. 1.78\\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15369v1",
        "pdf": "https://arxiv.org/pdf/2511.15369v1"
      },
      "arxiv_id": "2511.15369v1",
      "comment": "accepted in WACV 2026 (10 pages)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15357v1",
      "title": "Cost-Aware Prediction (CAP): An LLM-Enhanced Machine Learning Pipeline and Decision Support System for Heart Failure Mortality Prediction",
      "authors": [
        "Yinan Yu",
        "Falk Dippel",
        "Christina E. Lundberg",
        "Martin Lindgren",
        "Annika Rosengren",
        "Martin Adiels",
        "Helen Sjöland"
      ],
      "abstract": "Objective: Machine learning (ML) predictive models are often developed without considering downstream value trade-offs and clinical interpretability. This paper introduces a cost-aware prediction (CAP) framework that combines cost-benefit analysis assisted by large language model (LLM) agents to communicate the trade-offs involved in applying ML predictions. Materials and Methods: We developed an ML model predicting 1-year mortality in patients with heart failure (N = 30,021, 22% mortality) to identify those eligible for home care. We then introduced clinical impact projection (CIP) curves to visualize important cost dimensions - quality of life and healthcare provider expenses, further divided into treatment and error costs, to assess the clinical consequences of predictions. Finally, we used four LLM agents to generate patient-specific descriptions. The system was evaluated by clinicians for its decision support value. Results: The eXtreme gradient boosting (XGB) model achieved the best performance, with an area under the receiver operating characteristic curve (AUROC) of 0.804 (95% confidence interval (CI) 0.792-0.816), area under the precision-recall curve (AUPRC) of 0.529 (95% CI 0.502-0.558) and a Brier score of 0.135 (95% CI 0.130-0.140). Discussion: The CIP cost curves provided a population-level overview of cost composition across decision thresholds, whereas LLM-generated cost-benefit analysis at individual patient-levels. The system was well received according to the evaluation by clinicians. However, feedback emphasizes the need to strengthen the technical accuracy for speculative tasks. Conclusion: CAP utilizes LLM agents to integrate ML classifier outcomes and cost-benefit analysis for more transparent and interpretable decision support.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15357v1",
        "pdf": "https://arxiv.org/pdf/2511.15357v1"
      },
      "arxiv_id": "2511.15357v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15351v1",
      "title": "Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration",
      "authors": [
        "Yifu Guo",
        "Zishan Xu",
        "Zhiyuan Yao",
        "Yuquan Lu",
        "Jiaye Lin",
        "Sen Hu",
        "Zhenheng Tang",
        "Yingchao Li",
        "Huacan Wang",
        "Ronghao Chen"
      ],
      "abstract": "Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15351v1",
        "pdf": "https://arxiv.org/pdf/2511.15351v1"
      },
      "arxiv_id": "2511.15351v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15350v1",
      "title": "Multi-layer Stack Ensembles for Time Series Forecasting",
      "authors": [
        "Nathanael Bosch",
        "Oleksandr Shchur",
        "Nick Erickson",
        "Michael Bohlke-Schneider",
        "Caner Türkmen"
      ],
      "abstract": "Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15350v1",
        "pdf": "https://arxiv.org/pdf/2511.15350v1"
      },
      "arxiv_id": "2511.15350v1",
      "comment": "Published at AutoML Conference 2025 Methods Track",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2511.15343v1",
      "title": "Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection",
      "authors": [
        "Spyridon Loukovitis",
        "Vasileios Karampinis",
        "Athanasios Voulodimos"
      ],
      "abstract": "Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.",
      "published": "2025-11-19",
      "updated": "2025-11-19",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.15343v1",
        "pdf": "https://arxiv.org/pdf/2511.15343v1"
      },
      "arxiv_id": "2511.15343v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    }
  ]
}