{
  "fetched_at": "2025-12-24T00:26:04.454969",
  "total_papers": 100,
  "papers": [
    {
      "id": "2512.19693v1",
      "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
      "authors": [
        "Weichen Fan",
        "Haiwen Diao",
        "Quan Wang",
        "Dahua Lin",
        "Ziwei Liu"
      ],
      "abstract": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19693v1",
        "pdf": "https://arxiv.org/pdf/2512.19693v1"
      },
      "arxiv_id": "2512.19693v1",
      "comment": "Code link: https://github.com/WeichenFan/UAE",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19692v1",
      "title": "Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models",
      "authors": [
        "Pablo Ruiz-Ponce",
        "Sergio Escalera",
        "José García-Rodríguez",
        "Jiankang Deng",
        "Rolandos Alexandros Potamias"
      ],
      "abstract": "Generating realistic human-human interactions is a challenging task that requires not only high-quality individual body and hand motions, but also coherent coordination among all interactants. Due to limitations in available data and increased learning complexity, previous methods tend to ignore hand motions, limiting the realism and expressivity of the interactions. Additionally, current diffusion-based approaches generate entire motion sequences simultaneously, limiting their ability to capture the reactive and adaptive nature of human interactions. To address these limitations, we introduce Interact2Ar, the first end-to-end text-conditioned autoregressive diffusion model for generating full-body, human-human interactions. Interact2Ar incorporates detailed hand kinematics through dedicated parallel branches, enabling high-fidelity full-body generation. Furthermore, we introduce an autoregressive pipeline coupled with a novel memory technique that facilitates adaptation to the inherent variability of human interactions using efficient large context windows. The adaptability of our model enables a series of downstream applications, including temporal motion composition, real-time adaptation to disturbances, and extension beyond dyadic to multi-person scenarios. To validate the generated motions, we introduce a set of robust evaluators and extended metrics designed specifically for assessing full-body interactions. Through quantitative and qualitative experiments, we demonstrate the state-of-the-art performance of Interact2Ar.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19692v1",
        "pdf": "https://arxiv.org/pdf/2512.19692v1"
      },
      "arxiv_id": "2512.19692v1",
      "comment": "Project Page: https://pabloruizponce.com/papers/Interact2Ar",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19691v1",
      "title": "Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight",
      "authors": [
        "Junze Ye",
        "Daniel Tawfik",
        "Alex J. Goodell",
        "Nikhil V. Kotha",
        "Mark K. Buyyounouski",
        "Mohsen Bayati"
      ],
      "abstract": "Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19691v1",
        "pdf": "https://arxiv.org/pdf/2512.19691v1"
      },
      "arxiv_id": "2512.19691v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19687v1",
      "title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
      "authors": [
        "Apoorv Vyas",
        "Heng-Jui Chang",
        "Cheng-Fu Yang",
        "Po-Yao Huang",
        "Luya Gao",
        "Julius Richter",
        "Sanyuan Chen",
        "Matt Le",
        "Piotr Dollár",
        "Christoph Feichtenhofer",
        "Ann Lee",
        "Wei-Ning Hsu"
      ],
      "abstract": "We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19687v1",
        "pdf": "https://arxiv.org/pdf/2512.19687v1"
      },
      "arxiv_id": "2512.19687v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19686v1",
      "title": "Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models",
      "authors": [
        "Zixuan Ye",
        "Quande Liu",
        "Cong Wei",
        "Yuanxing Zhang",
        "Xintao Wang",
        "Pengfei Wan",
        "Kun Gai",
        "Wenhan Luo"
      ],
      "abstract": "Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \\textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19686v1",
        "pdf": "https://arxiv.org/pdf/2512.19686v1"
      },
      "arxiv_id": "2512.19686v1",
      "comment": "Project Page: https://zixuan-ye.github.io/VACoT/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19684v1",
      "title": "Zero-shot Reconstruction of In-Scene Object Manipulation from Video",
      "authors": [
        "Dixuan Lin",
        "Tianyou Wang",
        "Zhuoyang Pan",
        "Yufu Wang",
        "Lingjie Liu",
        "Kostas Daniilidis"
      ],
      "abstract": "We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses. We then apply a two-stage optimization that recovers a complete hand-object motion from grasping to interaction, which remains consistent with the scene information observed in the input video.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19684v1",
        "pdf": "https://arxiv.org/pdf/2512.19684v1"
      },
      "arxiv_id": "2512.19684v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19683v1",
      "title": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
      "authors": [
        "Mingrui Wu",
        "Zhaozhi Wang",
        "Fangjinhua Wang",
        "Jiaolong Yang",
        "Marc Pollefeys",
        "Tong Zhang"
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19683v1",
        "pdf": "https://arxiv.org/pdf/2512.19683v1"
      },
      "arxiv_id": "2512.19683v1",
      "comment": "Project page: https://harmlesssr.github.io/openbench/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19680v1",
      "title": "VA-$π$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
      "authors": [
        "Xinyao Liao",
        "Qiyuan He",
        "Kai Xu",
        "Xiaoye Qu",
        "Yicong Li",
        "Wei Wei",
        "Angela Yao"
      ],
      "abstract": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-$π$, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-$π$ formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-$π$ introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-$π$ enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19680v1",
        "pdf": "https://arxiv.org/pdf/2512.19680v1"
      },
      "arxiv_id": "2512.19680v1",
      "comment": "21 pages, 24 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19678v1",
      "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
      "authors": [
        "Hanyang Kong",
        "Xingyi Yang",
        "Xiaoxu Zheng",
        "Xinchao Wang"
      ],
      "abstract": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19678v1",
        "pdf": "https://arxiv.org/pdf/2512.19678v1"
      },
      "arxiv_id": "2512.19678v1",
      "comment": "Project page: https://hyokong.github.io/worldwarp-page/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19676v1",
      "title": "Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning",
      "authors": [
        "Mojtaba Safari",
        "Shansong Wang",
        "Vanessa L Wildman",
        "Mingzhe Hu",
        "Zach Eidex",
        "Chih-Wei Chang",
        "Erik H Middlebrooks",
        "Richard L. J Qiu",
        "Pretesh Patel",
        "Ashesh B. Jania",
        "Hui Mao",
        "Zhen Tian",
        "Xiaofeng Yang"
      ],
      "abstract": "Background: High-resolution MRI is critical for diagnosis, but long acquisition times limit clinical use. Super-resolution (SR) can enhance resolution post-scan, yet existing deep learning methods face fidelity-efficiency trade-offs. Purpose: To develop a computationally efficient and accurate deep learning framework for MRI SR that preserves anatomical detail for clinical integration. Materials and Methods: We propose a novel SR framework combining multi-head selective state-space models (MHSSM) with a lightweight channel MLP. The model uses 2D patch extraction with hybrid scanning to capture long-range dependencies. Each MambaFormer block integrates MHSSM, depthwise convolutions, and gated channel mixing. Evaluation used 7T brain T1 MP2RAGE maps (n=142) and 1.5T prostate T2w MRI (n=334). Comparisons included Bicubic interpolation, GANs (CycleGAN, Pix2pix, SPSR), transformers (SwinIR), Mamba (MambaIR), and diffusion models (I2SB, Res-SRDiff). Results: Our model achieved superior performance with exceptional efficiency. For 7T brain data: SSIM=0.951+-0.021, PSNR=26.90+-1.41 dB, LPIPS=0.076+-0.022, GMSD=0.083+-0.017, significantly outperforming all baselines (p<0.001). For prostate data: SSIM=0.770+-0.049, PSNR=27.15+-2.19 dB, LPIPS=0.190+-0.095, GMSD=0.087+-0.013. The framework used only 0.9M parameters and 57 GFLOPs, reducing parameters by 99.8% and computation by 97.5% versus Res-SRDiff, while outperforming SwinIR and MambaIR in accuracy and efficiency. Conclusion: The proposed framework provides an efficient, accurate MRI SR solution, delivering enhanced anatomical detail across datasets. Its low computational demand and state-of-the-art performance show strong potential for clinical translation.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV",
        "physics.med-ph"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19676v1",
        "pdf": "https://arxiv.org/pdf/2512.19676v1"
      },
      "arxiv_id": "2512.19676v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19675v1",
      "title": "Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)",
      "authors": [
        "Niclas Griesshaber",
        "Jochen Streb"
      ],
      "abstract": "We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "econ.GN",
        "cs.CV",
        "cs.DL"
      ],
      "primary_category": "econ.GN",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19675v1",
        "pdf": "https://arxiv.org/pdf/2512.19675v1"
      },
      "arxiv_id": "2512.19675v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19673v1",
      "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
      "authors": [
        "Yuqiao Tan",
        "Minzheng Wang",
        "Shizhu He",
        "Huanxuan Liao",
        "Chengfeng Zhao",
        "Qiunan Lu",
        "Tian Liang",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19673v1",
        "pdf": "https://arxiv.org/pdf/2512.19673v1"
      },
      "arxiv_id": "2512.19673v1",
      "comment": "Preprint. Our code is available at https://github.com/Trae1ounG/BuPO",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19663v1",
      "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
      "authors": [
        "Argha Kamal Samanta",
        "Harshika Goyal",
        "Vasudha Joshi",
        "Tushar Mungle",
        "Pabitra Mitra"
      ],
      "abstract": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19663v1",
        "pdf": "https://arxiv.org/pdf/2512.19663v1"
      },
      "arxiv_id": "2512.19663v1",
      "comment": "14 pages, 14 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19661v1",
      "title": "Over++: Generative Video Compositing for Layer Interaction Effects",
      "authors": [
        "Luchao Qi",
        "Jiaye Wu",
        "Jun Myeong Choi",
        "Cary Phillips",
        "Roni Sengupta",
        "Dan B Goldman"
      ],
      "abstract": "In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19661v1",
        "pdf": "https://arxiv.org/pdf/2512.19661v1"
      },
      "arxiv_id": "2512.19661v1",
      "comment": "Project page: https://overplusplus.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19654v1",
      "title": "Clustering with Label Consistency",
      "authors": [
        "Diptarka Chakraborty",
        "Hendrik Fichtenberger",
        "Bernhard Haeupler",
        "Silvio Lattanzi",
        "Ashkan Norouzi-Fard",
        "Ola Svensson"
      ],
      "abstract": "Designing efficient, effective, and consistent metric clustering algorithms is a significant challenge attracting growing attention. Traditional approaches focus on the stability of cluster centers; unfortunately, this neglects the real-world need for stable point labels, i.e., stable assignments of points to named sets (clusters). In this paper, we address this gap by initiating the study of label-consistent metric clustering. We first introduce a new notion of consistency, measuring the label distance between two consecutive solutions. Then, armed with this new definition, we design new consistent approximation algorithms for the classical $k$-center and $k$-median problems.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19654v1",
        "pdf": "https://arxiv.org/pdf/2512.19654v1"
      },
      "arxiv_id": "2512.19654v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19649v1",
      "title": "Deep Legendre Transform",
      "authors": [
        "Aleksey Minabutdinov",
        "Patrick Cheridito"
      ],
      "abstract": "We introduce a novel deep learning algorithm for computing convex conjugates of differentiable convex functions, a fundamental operation in convex analysis with various applications in different fields such as optimization, control theory, physics and economics. While traditional numerical methods suffer from the curse of dimensionality and become computationally intractable in high dimensions, more recent neural network-based approaches scale better, but have mostly been studied with the aim of solving optimal transport problems and require the solution of complicated optimization or max-min problems. Using an implicit Fenchel formulation of convex conjugation, our approach facilitates an efficient gradient-based framework for the minimization of approximation errors and, as a byproduct, also provides a posteriori error estimates for the approximation quality. Numerical experiments demonstrate our method's ability to deliver accurate results across different high-dimensional examples. Moreover, by employing symbolic regression with Kolmogorov--Arnold networks, it is able to obtain the exact convex conjugates of specific convex functions.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19649v1",
        "pdf": "https://arxiv.org/pdf/2512.19649v1"
      },
      "arxiv_id": "2512.19649v1",
      "comment": "Accepted at NeurIPS 2025 (poster). NeurIPS page: https://neurips.cc/virtual/2025/loc/san-diego/poster/120307",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19648v1",
      "title": "4D Gaussian Splatting as a Learned Dynamical System",
      "authors": [
        "Arnold Caleb Asiimwe",
        "Carl Vondrick"
      ],
      "abstract": "We reinterpret 4D Gaussian Splatting as a continuous-time dynamical system, where scene motion arises from integrating a learned neural dynamical field rather than applying per-frame deformations. This formulation, which we call EvoGS, treats the Gaussian representation as an evolving physical system whose state evolves continuously under a learned motion law. This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis. Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19648v1",
        "pdf": "https://arxiv.org/pdf/2512.19648v1"
      },
      "arxiv_id": "2512.19648v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19643v1",
      "title": "The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference",
      "authors": [
        "Rajyasri Roy",
        "Dibyajyoti Nayak",
        "Somdatta Goswami"
      ],
      "abstract": "Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across parametric and functional inputs; however, most autoregressive NO frameworks remain vulnerable to compounding errors, and ensemble-averaged metrics provide limited guarantees for individual inference trajectories. In practice, error accumulation can become unacceptable beyond the training horizon, and existing methods lack mechanisms for online monitoring or correction. To address this gap, we propose ANCHOR (Adaptive Numerical Correction for High-fidelity Operator Rollouts), an online, instance-aware hybrid inference framework for stable long-horizon prediction of nonlinear, time-dependent PDEs. ANCHOR treats a pretrained NO as the primary inference engine and adaptively couples it with a classical numerical solver using a physics-informed, residual-based error estimator. Inspired by adaptive time-stepping in numerical analysis, ANCHOR monitors an exponential moving average (EMA) of the normalized PDE residual to detect accumulating error and trigger corrective solver interventions without requiring access to ground-truth solutions. We show that the EMA-based estimator correlates strongly with the true relative L2 error, enabling data-free, instance-aware error control during inference. Evaluations on four canonical PDEs: 1D and 2D Burgers', 2D Allen-Cahn, and 3D heat conduction, demonstrate that ANCHOR reliably bounds long-horizon error growth, stabilizes extrapolative rollouts, and significantly improves robustness over standalone neural operators, while remaining substantially more efficient than high-fidelity numerical solvers.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19643v1",
        "pdf": "https://arxiv.org/pdf/2512.19643v1"
      },
      "arxiv_id": "2512.19643v1",
      "comment": "18 pages, 7 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19632v1",
      "title": "Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment",
      "authors": [
        "Da Tan",
        "Michael Beck",
        "Christopher P. Bidinosti",
        "Robert H. Gulden",
        "Christopher J. Henry"
      ],
      "abstract": "The success of agricultural artificial intelligence depends heavily on large, diverse, and high-quality plant image datasets, yet collecting such data in real field conditions is costly, labor intensive, and seasonally constrained. This paper investigates diffusion-based generative modeling to address these challenges through plant image synthesis, indoor-to-outdoor translation, and expert preference aligned fine tuning. First, a Stable Diffusion model is fine tuned on captioned indoor and outdoor plant imagery to generate realistic, text conditioned images of canola and soybean. Evaluation using Inception Score, Frechet Inception Distance, and downstream phenotype classification shows that synthetic images effectively augment training data and improve accuracy. Second, we bridge the gap between high resolution indoor datasets and limited outdoor imagery using DreamBooth-based text inversion and image guided diffusion, generating translated images that enhance weed detection and classification with YOLOv8. Finally, a preference guided fine tuning framework trains a reward model on expert scores and applies reward weighted updates to produce more stable and expert aligned outputs. Together, these components demonstrate a practical pathway toward data efficient generative pipelines for agricultural AI.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19632v1",
        "pdf": "https://arxiv.org/pdf/2512.19632v1"
      },
      "arxiv_id": "2512.19632v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19629v1",
      "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
      "authors": [
        "Jiaqi Peng",
        "Wenzhe Cai",
        "Yuqiang Yang",
        "Tai Wang",
        "Yuan Shen",
        "Jiangmiao Pang"
      ],
      "abstract": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \\href{https://steinate.github.io/logoplanner.github.io/}{project page}.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19629v1",
        "pdf": "https://arxiv.org/pdf/2512.19629v1"
      },
      "arxiv_id": "2512.19629v1",
      "comment": "Project page:https://steinate.github.io/logoplanner.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19620v1",
      "title": "Exploring the features used for summary evaluation by Human and GPT",
      "authors": [
        "Zahra Sadeghi",
        "Evangelos Milios",
        "Frank Rudzicz"
      ],
      "abstract": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19620v1",
        "pdf": "https://arxiv.org/pdf/2512.19620v1"
      },
      "arxiv_id": "2512.19620v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19609v1",
      "title": "MapTrace: Scalable Data Generation for Route Tracing on Maps",
      "authors": [
        "Artemis Panagopoulou",
        "Aveek Purohit",
        "Achin Kulshrestha",
        "Soroosh Yazdani",
        "Mohit Goyal"
      ],
      "abstract": "While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19609v1",
        "pdf": "https://arxiv.org/pdf/2512.19609v1"
      },
      "arxiv_id": "2512.19609v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19605v1",
      "title": "KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning",
      "authors": [
        "Eric Zimmermann",
        "Harley Wiltzer",
        "Justin Szeto",
        "David Alvarez-Melis",
        "Lester Mackey"
      ],
      "abstract": "Recent breakthroughs in self-supervised Joint-Embedding Predictive Architectures (JEPAs) have established that regularizing Euclidean representations toward isotropic Gaussian priors yields provable gains in training stability and downstream generalization. We introduce a new, flexible family of KerJEPAs, self-supervised learning algorithms with kernel-based regularizers. One instance of this family corresponds to the recently-introduced LeJEPA Epps-Pulley regularizer which approximates a sliced maximum mean discrepancy (MMD) with a Gaussian prior and Gaussian kernel. By expanding the class of viable kernels and priors and computing the closed-form high-dimensional limit of sliced MMDs, we develop alternative KerJEPAs with a number of favorable properties including improved training stability and design flexibility.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19605v1",
        "pdf": "https://arxiv.org/pdf/2512.19605v1"
      },
      "arxiv_id": "2512.19605v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19602v1",
      "title": "No Data? No Problem: Robust Vision-Tabular Learning with Missing Values",
      "authors": [
        "Marta Hasny",
        "Laura Daza",
        "Keno Bressem",
        "Maxime Di Folco",
        "Julia Schnabel"
      ],
      "abstract": "Large-scale medical biobanks provide imaging data complemented by extensive tabular information, such as demographics or clinical measurements. However, this abundance of tabular attributes does not reflect real-world datasets, where only a subset of attributes may be available. This discrepancy calls for methods that can leverage all the tabular data during training while remaining robust to missing values at inference. To address this challenge, we propose RoVTL (Robust Vision-Tabular Learning), a framework designed to handle any level of tabular data availability, from 0% to 100%. RoVTL comprises two key stages: contrastive pretraining, where we introduce tabular attribute missingness as data augmentation to promote robustness, and downstream task tuning using a gated cross-attention module for multimodal fusion. During fine-tuning, we employ a novel Tabular More vs. Fewer loss that ranks performance based on the amount of available tabular data. Combined with disentangled gradient learning, this enables consistent performance across all tabular data completeness scenarios. We evaluate RoVTL on cardiac MRI scans from the UK Biobank, demonstrating superior robustness to missing tabular data compared to prior methods. Furthermore, RoVTL successfully generalizes to an external cardiac MRI dataset for multimodal disease classification, and extends to the natural images domain, achieving robust performance on a car advertisements dataset. The code is available at https://github.com/marteczkah/RoVTL.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19602v1",
        "pdf": "https://arxiv.org/pdf/2512.19602v1"
      },
      "arxiv_id": "2512.19602v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19584v1",
      "title": "Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior",
      "authors": [
        "Ziqian Huang",
        "Boxiao Yu",
        "Siqi Li",
        "Savas Ozdemir",
        "Sangjin Bae",
        "Jae Sung Lee",
        "Guobao Wang",
        "Kuang Gong"
      ],
      "abstract": "Dynamic PET enables the quantitative estimation of physiology-related parameters and is widely utilized in research and increasingly adopted in clinical settings. Parametric imaging in dynamic PET requires kinetic modeling to estimate voxel-wise physiological parameters based on specific kinetic models. However, parametric images estimated through kinetic model fitting often suffer from low image quality due to the inherently ill-posed nature of the fitting process and the limited counts resulting from non-continuous data acquisition across multiple bed positions in whole-body PET. In this work, we proposed a diffusion model-based kinetic modeling framework for parametric image estimation, using the Patlak model as an example. The score function of the diffusion model was pre-trained on static total-body PET images and served as a prior for both Patlak slope and intercept images by leveraging their patch-wise similarity. During inference, the kinetic model was incorporated as a data-consistency constraint to guide the parametric image estimation. The proposed framework was evaluated on total-body dynamic PET datasets with different dose levels, demonstrating the feasibility and promising performance of the proposed framework in improving parametric image quality.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "eess.IV",
        "cs.CV",
        "physics.med-ph"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19584v1",
        "pdf": "https://arxiv.org/pdf/2512.19584v1"
      },
      "arxiv_id": "2512.19584v1",
      "comment": "10 pages, 9 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19577v1",
      "title": "Deep Learning for Primordial $B$-mode Extraction",
      "authors": [
        "Eric Guzman",
        "Joel Meyers"
      ],
      "abstract": "The search for primordial gravitational waves is a central goal of cosmic microwave background (CMB) surveys. Isolating the characteristic $B$-mode polarization signal sourced by primordial gravitational waves is challenging for several reasons: the amplitude of the signal is inherently small; astrophysical foregrounds produce $B$-mode polarization contaminating the signal; and secondary $B$-mode polarization fluctuations are produced via the conversion of $E$ modes. Current and future low-noise, multi-frequency observations enable sufficient precision to address the first two of these challenges such that secondary $B$ modes will become the bottleneck for improved constraints on the amplitude of primordial gravitational waves. The dominant source of secondary $B$-mode polarization is gravitational lensing by large scale structure. Various strategies have been developed to estimate the lensing deflection and to reverse its effects the CMB, thus reducing confusion from lensing $B$ modes in the search for primordial gravitational waves. However, a few complications remain. First, there may be additional sources of secondary $B$-mode polarization, for example from patchy reionization or from cosmic polarization rotation. Second, the statistics of delensed CMB maps can become complicated and non-Gaussian, especially when advanced lensing reconstruction techniques are applied. We previously demonstrated how a deep learning network, ResUNet-CMB, can provide nearly optimal simultaneous estimates of multiple sources of secondary $B$-mode polarization. In this paper, we show how deep learning can be applied to estimate and remove multiple sources of secondary $B$-mode polarization, and we further show how this technique can be used in a likelihood analysis to produce nearly optimal, unbiased estimates of the amplitude of primordial gravitational waves.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "astro-ph.CO",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "astro-ph.CO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19577v1",
        "pdf": "https://arxiv.org/pdf/2512.19577v1"
      },
      "arxiv_id": "2512.19577v1",
      "comment": "12 pages, 8 figures. Code available from https://github.com/EEmGuzman/resunet-cmb",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19576v1",
      "title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller",
      "authors": [
        "Kirill Djebko",
        "Tom Baumann",
        "Erik Dilger",
        "Frank Puppe",
        "Sergio Montenegro"
      ],
      "abstract": "Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universität Würzburg in cooperation with the Technische Universität Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19576v1",
        "pdf": "https://arxiv.org/pdf/2512.19576v1"
      },
      "arxiv_id": "2512.19576v1",
      "comment": "55 pages, 27 figures, 29 tables. The maneuver telemetry datasets generated and analyzed during this work are available in the GitHub repository https://github.com/kdjebko/lelar-in-orbit-data",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19570v1",
      "title": "The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge",
      "authors": [
        "Angjelin Hila"
      ],
      "abstract": "We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justification produces reflective knowledge. We specify reflective knowledge as follows: agents understand the evaluative basis of a claim, when that basis is unavailable agents consistently assess the reliability of truth sources, and agents have a duty to apply these standards within their domains of competence. We argue that LLMs approximate externalist reliabilism because they can reliably transmit information whose justificatory basis is established elsewhere, but they do not themselves possess reflective justification. Widespread outsourcing of reflective work to reliable LLM outputs can weaken reflective standards of justification, disincentivize comprehension, and reduce agents' capacity to meet professional and civic epistemic duties. To mitigate these risks, we propose a three tier norm program that includes an epistemic interaction model for individual use, institutional and organizational frameworks that seed and enforce norms for epistemically optimal outcomes, and deontic constraints at organizational and or legislative levels that instantiate discursive norms and curb epistemic vices.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19570v1",
        "pdf": "https://arxiv.org/pdf/2512.19570v1"
      },
      "arxiv_id": "2512.19570v1",
      "comment": "AI & Soc (2025)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19569v1",
      "title": "Owning the Intelligence: Global AI Patents Landscape and Europe's Quest for Technological Sovereignty",
      "authors": [
        "Lapo Santarlasci",
        "Armando Rungi",
        "Loredana Fattorini",
        "Nestor Maslej"
      ],
      "abstract": "Artificial intelligence has become a key arena of global technological competition and a central concern for Europe's quest for technological sovereignty. This paper analyzes global AI patenting from 2010 to 2023 to assess Europe's position in an increasingly bipolar innovation landscape dominated by the United States and China. Using linked patent, firm, ownership, and citation data, we examine the geography, specialization, and international diffusion of AI innovation. We find a highly concentrated patent landscape: China leads in patent volumes, while the United States dominates in citation impact and technological influence. Europe accounts for a limited share of AI patents but exhibits signals of relatively high patent quality. Technological proximity reveals global convergence toward U.S. innovation trajectories, with Europe remaining fragmented rather than forming an autonomous pole. Gravity-model estimates show that cross-border AI knowledge flows are driven primarily by technological capability and specialization, while geographic and institutional factors play a secondary role. EU membership does not significantly enhance intra-European knowledge diffusion, suggesting that technological capacity, rather than political integration, underpins participation in global AI innovation networks.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "econ.GN",
        "cs.AI"
      ],
      "primary_category": "econ.GN",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19569v1",
        "pdf": "https://arxiv.org/pdf/2512.19569v1"
      },
      "arxiv_id": "2512.19569v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19564v1",
      "title": "Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles",
      "authors": [
        "Yanliang Huang",
        "Xia Yan",
        "Peiran Yin",
        "Zhenduo Zhang",
        "Zeyan Shao",
        "Youran Wang",
        "Haoliang Huang",
        "Matthias Althoff"
      ],
      "abstract": "Over the past decade, a wide range of motion planning approaches for autonomous vehicles has been developed to handle increasingly complex traffic scenarios. However, these approaches are rarely compared on standardized benchmarks, limiting the assessment of relative strengths and weaknesses. To address this gap, we present the setup and results of the 4th CommonRoad Motion Planning Competition held in 2024, conducted using the CommonRoad benchmark suite. This annual competition provides an open-source and reproducible framework for benchmarking motion planning algorithms. The benchmark scenarios span highway and urban environments with diverse traffic participants, including passenger cars, buses, and bicycles. Planner performance is evaluated along four dimensions: efficiency, safety, comfort, and compliance with selected traffic rules. This report introduces the competition format and provides a comparison of representative high-performing planners from the 2023 and 2024 editions.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19564v1",
        "pdf": "https://arxiv.org/pdf/2512.19564v1"
      },
      "arxiv_id": "2512.19564v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19562v1",
      "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
      "authors": [
        "Martin Sedlacek",
        "Pavlo Yefanov",
        "Georgy Ponimatkin",
        "Jai Bardhan",
        "Simon Pilc",
        "Mederic Fourmy",
        "Evangelos Kazakos",
        "Cees G. M. Snoek",
        "Josef Sivic",
        "Vladimir Petrik"
      ],
      "abstract": "Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the π_{0}, π_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19562v1",
        "pdf": "https://arxiv.org/pdf/2512.19562v1"
      },
      "arxiv_id": "2512.19562v1",
      "comment": "9 pages, 10 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19560v1",
      "title": "BabyFlow: 3D modeling of realistic and expressive infant faces",
      "authors": [
        "Antonia Alomar",
        "Mireia Masias",
        "Marius George Linguraru",
        "Federico M. Sukno",
        "Gemma Piella"
      ],
      "abstract": "Early detection of developmental disorders can be aided by analyzing infant craniofacial morphology, but modeling infant faces is challenging due to limited data and frequent spontaneous expressions. We introduce BabyFlow, a generative AI model that disentangles facial identity and expression, enabling independent control over both. Using normalizing flows, BabyFlow learns flexible, probabilistic representations that capture the complex, non-linear variability of expressive infant faces without restrictive linear assumptions. To address scarce and uncontrolled expressive data, we perform cross-age expression transfer, adapting expressions from adult 3D scans to enrich infant datasets with realistic and systematic expressive variants. As a result, BabyFlow improves 3D reconstruction accuracy, particularly in highly expressive regions such as the mouth, eyes, and nose, and supports synthesis and modification of infant expressions while preserving identity. Additionally, by integrating with diffusion models, BabyFlow generates high-fidelity 2D infant images with consistent 3D geometry, providing powerful tools for data augmentation and early facial analysis.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19560v1",
        "pdf": "https://arxiv.org/pdf/2512.19560v1"
      },
      "arxiv_id": "2512.19560v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19557v1",
      "title": "Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations",
      "authors": [
        "Lawrence Krukrubo",
        "Julius Odede",
        "Olawande Olusegun"
      ],
      "abstract": "Current approaches to Explainable AI (XAI) face a \"Scalability-Stability Dilemma.\" Post-hoc methods (e.g., LIME, SHAP) may scale easily but suffer from instability, while supervised explanation frameworks (e.g., TED) offer stability but require prohibitive human effort to label every training instance. This paper proposes a Hybrid LRR-TED framework that addresses this dilemma through a novel \"Asymmetry of Discovery.\" When applied to customer churn prediction, we demonstrate that automated rule learners (GLRM) excel at identifying broad \"Safety Nets\" (retention patterns) but struggle to capture specific \"Risk Traps\" (churn triggers)-a phenomenon we term the Anna Karenina Principle of Churn. By initialising the explanation matrix with automated safety rules and augmenting it with a Pareto-optimal set of just four human-defined risk rules, our approach achieves 94.00% predictive accuracy. This configuration outperforms the full 8-rule manual expert baseline while reducing human annotation effort by 50%, proposing a shift in the paradigm for Human-in-the-Loop AI: moving experts from the role of \"Rule Writers\" to \"Exception Handlers.\"",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19557v1",
        "pdf": "https://arxiv.org/pdf/2512.19557v1"
      },
      "arxiv_id": "2512.19557v1",
      "comment": "5 pages, 2 figures, 2 tables. Code and experiments available at https://github.com/Lawrence-Krukrubo/IBM-Learn-XAI",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19554v1",
      "title": "CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal",
      "authors": [
        "Yongxin Wang",
        "Zhicheng Yang",
        "Meng Cao",
        "Mingfei Han",
        "Haokun Lin",
        "Yingying Zhu",
        "Xiaojun Chang",
        "Xiaodan Liang"
      ],
      "abstract": "Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19554v1",
        "pdf": "https://arxiv.org/pdf/2512.19554v1"
      },
      "arxiv_id": "2512.19554v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19551v1",
      "title": "Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios",
      "authors": [
        "Jiawen Wang",
        "Jingjing Wang Tianyang Chen",
        "Min Zhang",
        "Guodong Zhou"
      ],
      "abstract": "In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19551v1",
        "pdf": "https://arxiv.org/pdf/2512.19551v1"
      },
      "arxiv_id": "2512.19551v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19550v1",
      "title": "DFORD: Directional Feedback based Online Ordinal Regression Learning",
      "authors": [
        "Naresh Manwani",
        "M Elamparithy",
        "Tanish Taneja"
      ],
      "abstract": "In this paper, we introduce directional feedback in the ordinal regression setting, in which the learner receives feedback on whether the predicted label is on the left or the right side of the actual label. This is a weak supervision setting for ordinal regression compared to the full information setting, where the learner can access the labels. We propose an online algorithm for ordinal regression using directional feedback. The proposed algorithm uses an exploration-exploitation scheme to learn from directional feedback efficiently. Furthermore, we introduce its kernel-based variant to learn non-linear ordinal regression models in an online setting. We use a truncation trick to make the kernel implementation more memory efficient. The proposed algorithm maintains the ordering of the thresholds in the expected sense. Moreover, it achieves the expected regret of $\\mathcal{O}(\\log T)$. We compare our approach with a full information and a weakly supervised algorithm for ordinal regression on synthetic and real-world datasets. The proposed approach, which learns using directional feedback, performs comparably (sometimes better) to its full information counterpart.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19550v1",
        "pdf": "https://arxiv.org/pdf/2512.19550v1"
      },
      "arxiv_id": "2512.19550v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19546v1",
      "title": "ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars",
      "authors": [
        "Ziqiao Peng",
        "Yi Chen",
        "Yifeng Ma",
        "Guozhen Zhang",
        "Zhiyao Sun",
        "Zixiang Zhou",
        "Youliang Zhang",
        "Zhengguang Zhou",
        "Zhaoxin Fan",
        "Hongyan Liu",
        "Yuan Zhou",
        "Qinglin Lu",
        "Jun He"
      ],
      "abstract": "Despite significant advances in talking avatar generation, existing methods face critical challenges: insufficient text-following capability for diverse actions, lack of temporal alignment between actions and audio content, and dependency on additional control signals such as pose skeletons. We present ActAvatar, a framework that achieves phase-level precision in action control through textual guidance by capturing both action semantics and temporal context. Our approach introduces three core innovations: (1) Phase-Aware Cross-Attention (PACA), which decomposes prompts into a global base block and temporally-anchored phase blocks, enabling the model to concentrate on phase-relevant tokens for precise temporal-semantic alignment; (2) Progressive Audio-Visual Alignment, which aligns modality influence with the hierarchical feature learning process-early layers prioritize text for establishing action structure while deeper layers emphasize audio for refining lip movements, preventing modality interference; (3) A two-stage training strategy that first establishes robust audio-visual correspondence on diverse data, then injects action control through fine-tuning on structured annotations, maintaining both audio-visual alignment and the model's text-following capabilities. Extensive experiments demonstrate that ActAvatar significantly outperforms state-of-the-art methods in both action control and visual quality.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19546v1",
        "pdf": "https://arxiv.org/pdf/2512.19546v1"
      },
      "arxiv_id": "2512.19546v1",
      "comment": "Project Page: https://ziqiaopeng.github.io/ActAvatar/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19540v1",
      "title": "Active Convolved Illumination with Deep Transfer Learning for Complex Beam Transmission through Atmospheric Turbulence",
      "authors": [
        "Adrian A. Moazzam",
        "Anindya Ghoshroy",
        "Breeanne Heusdens",
        "Durdu O. Guney",
        "Roohollah Askari"
      ],
      "abstract": "Atmospheric turbulence imposes a fundamental limitation across a broad range of applications, including optical imaging, remote sensing, and free-space optical communication. Recent advances in adaptive optics, wavefront shaping, and machine learning, driven by synergistic progress in fundamental theories, optoelectronic hardware, and computational algorithms, have demonstrated substantial potential in mitigating turbulence-induced distortions. Recently, active convolved illumination (ACI) was proposed as a versatile and physics-driven technique for transmitting structured light beams with minimal distortion through highly challenging turbulent regimes. While distinct in its formulation, ACI shares conceptual similarities with other physics-driven distortion correction approaches and stands to benefit from complementary integration with data-driven deep learning (DL) models. Inspired by recent work coupling deep learning with traditional turbulence mitigation strategies, the present work investigates the feasibility of integrating ACI with neural network-based methods. We outline a conceptual framework for coupling ACI with data-driven models and identify conditions under which learned representations can meaningfully support ACI's correlation-injection mechanism. As a representative example, we employ a convolutional neural network (CNN) together with a transfer-learning approach to examine how a learned model may operate in tandem with ACI. This exploratory study demonstrates feasible implementation pathways and establishes an early foundation for assessing the potential of future ACI-DL hybrid architectures, representing a step toward evaluating broader synergistic interactions between ACI and modern DL models.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "physics.optics",
        "cs.LG"
      ],
      "primary_category": "physics.optics",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19540v1",
        "pdf": "https://arxiv.org/pdf/2512.19540v1"
      },
      "arxiv_id": "2512.19540v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19539v1",
      "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
      "authors": [
        "Kaiwen Zhang",
        "Liming Jiang",
        "Angtian Wang",
        "Jacob Zhiyuan Fang",
        "Tiancheng Zhi",
        "Qing Yan",
        "Hao Kang",
        "Xin Lu",
        "Xingang Pan"
      ],
      "abstract": "Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19539v1",
        "pdf": "https://arxiv.org/pdf/2512.19539v1"
      },
      "arxiv_id": "2512.19539v1",
      "comment": "Project page: https://kevin-thu.github.io/StoryMem",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19535v1",
      "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
      "authors": [
        "Moritz Böhle",
        "Amélie Royer",
        "Juliette Marrie",
        "Edouard Grave",
        "Patrick Pérez"
      ],
      "abstract": "Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19535v1",
        "pdf": "https://arxiv.org/pdf/2512.19535v1"
      },
      "arxiv_id": "2512.19535v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19534v1",
      "title": "SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates",
      "authors": [
        "Chi Zhang",
        "Braedon Gunn",
        "Andrew M. Read-Fuller"
      ],
      "abstract": "Poor adaptation of orbital implants remains a major contributor to postoperative complications and revision surgery. Although preformed orbital plates are widely used to reduce cost and operative time compared with customized implants, surgeons currently lack publicly available tools and standardized metrics to quantitatively compare plate fit across vendors, sizes, and patient anatomy. We developed SlicerOrbitSurgerySim, an open-source extension for the 3D Slicer platform that enables interactive virtual registration, evaluation, and comparison of multiple preformed orbital plates in a patient-specific virtual planning environment. The software generates reproducible quantitative plate-to-orbit distance metrics and visualization tools that support both patient-specific planning and population-level statistical analysis of plate adaptability. By facilitating objective comparison of implant designs and placement strategies, this tool aims to improve preoperative decision-making, reduce intraoperative plate modification, and promote collaborative research and surgical education. Pilot studies, sample datasets, and detailed tutorials are provided to support testing, transparency, and reproducibility.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19534v1",
        "pdf": "https://arxiv.org/pdf/2512.19534v1"
      },
      "arxiv_id": "2512.19534v1",
      "comment": "12 pages, 8 figures. Submitted to Journal of Oral and Maxillofacial Surgery. Code: https://github.com/chz31/SlicerOrbitSurgerySim/tree/main",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19530v1",
      "title": "Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement",
      "authors": [
        "Hongsheng Xing",
        "Qiuxin Si"
      ],
      "abstract": "Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \\textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.\n  Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \\textbf{MSE of 0.0039} ($\\pm$ 0.0003), representing a 60\\% error reduction over competitive baselines and a $>25\\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19530v1",
        "pdf": "https://arxiv.org/pdf/2512.19530v1"
      },
      "arxiv_id": "2512.19530v1",
      "comment": "13 pages, 6 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19528v1",
      "title": "Multi-Modal Soccer Scene Analysis with Masked Pre-Training",
      "authors": [
        "Marc Peral",
        "Guillem Capellera",
        "Luis Ferraz",
        "Antonio Rubio",
        "Antonio Agudo"
      ],
      "abstract": "In this work we propose a multi-modal architecture for analyzing soccer scenes from tactical camera footage, with a focus on three core tasks: ball trajectory inference, ball state classification, and ball possessor identification. To this end, our solution integrates three distinct input modalities (player trajectories, player types and image crops of individual players) into a unified framework that processes spatial and temporal dynamics using a cascade of sociotemporal transformer blocks. Unlike prior methods, which rely heavily on accurate ball tracking or handcrafted heuristics, our approach infers the ball trajectory without direct access to its past or future positions, and robustly identifies the ball state and ball possessor under noisy or occluded conditions from real top league matches. We also introduce CropDrop, a modality-specific masking pre-training strategy that prevents over-reliance on image features and encourages the model to rely on cross-modal patterns during pre-training. We show the effectiveness of our approach on a large-scale dataset providing substantial improvements over state-of-the-art baselines in all tasks. Our results highlight the benefits of combining structured and visual cues in a transformer-based architecture, and the importance of realistic masking strategies in multi-modal learning.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19528v1",
        "pdf": "https://arxiv.org/pdf/2512.19528v1"
      },
      "arxiv_id": "2512.19528v1",
      "comment": "10 pages, 2 figures. WACV 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19527v1",
      "title": "Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions",
      "authors": [
        "Diego Hitzges",
        "Guillaume Sagnol"
      ],
      "abstract": "Deep learning has been effectively applied to many discrete optimization problems. However, learning-based scheduling on unrelated parallel machines remains particularly difficult to design. Not only do the numbers of jobs and machines vary, but each job-machine pair has a unique processing time, dynamically altering feature dimensions. We propose a novel approach with a neural network tailored for offline deterministic scheduling of arbitrary sizes on unrelated machines. The goal is to minimize a complex objective function that includes the makespan and the weighted tardiness of jobs and machines. Unlike existing online approaches, which process jobs sequentially, our method generates a complete schedule considering the entire input at once. The key contribution of this work lies in the sophisticated architecture of our model. By leveraging various NLP-inspired architectures, it effectively processes any number of jobs and machines with varying feature dimensions imposed by unrelated processing times. Our approach enables supervised training on small problem instances while demonstrating strong generalization to much larger scheduling environments. Trained and tested on instances with 8 jobs and 4 machines, costs were only 2.51% above optimal. Across all tested configurations of up to 100 jobs and 10 machines, our network consistently outperformed an advanced dispatching rule, which incurred 22.22% higher costs on average. As our method allows fast retraining with simulated data and adaptation to various scheduling conditions, we believe it has the potential to become a standard approach for learning-based scheduling on unrelated machines and similar problem environments.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.DM"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19527v1",
        "pdf": "https://arxiv.org/pdf/2512.19527v1"
      },
      "arxiv_id": "2512.19527v1",
      "comment": "24th IEEE International Conference on Machine Learning and Applications (ICMLA 2025) in Boca Raton, USA. Project page: https://github.com/DiegoHitzges/Deep-Learning-for-Unrelated-Machines-Scheduling . 8 pages, 4 figures, 3 tables",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19526v1",
      "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
      "authors": [
        "Li Puyin",
        "Tiange Xiang",
        "Ella Mao",
        "Shirley Wei",
        "Xinye Chen",
        "Adnan Masood",
        "Li Fei-fei",
        "Ehsan Adeli"
      ],
      "abstract": "Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19526v1",
        "pdf": "https://arxiv.org/pdf/2512.19526v1"
      },
      "arxiv_id": "2512.19526v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19524v1",
      "title": "Initialization of a Polyharmonic Cascade, Launch and Testing",
      "authors": [
        "Yuriy N. Bakhvalov"
      ],
      "abstract": "This paper concludes a series of studies on the polyharmonic cascade, a deep machine learning architecture theoretically derived from indifference principles and the theory of random functions. A universal initialization procedure is proposed, based on symmetric constellations in the form of hyperoctahedra with a central point. This initialization not only ensures stable training of cascades with tens and hundreds of layers (up to 500 layers without skip connections), but also radically simplifies the computations. Scalability and robustness are demonstrated on MNIST (98.3% without convolutions or augmentations), HIGGS (AUC approximately 0.885 on 11M examples), and Epsilon (AUC approximately 0.963 with 2000 features). All linear algebra is reduced to 2D operations and is efficiently executed on GPUs. A public repository and an archived snapshot are provided for full reproducibility.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19524v1",
        "pdf": "https://arxiv.org/pdf/2512.19524v1"
      },
      "arxiv_id": "2512.19524v1",
      "comment": "Part 4 of 4 in the \"Polyharmonic Cascade\" cycle. Contains initialization algorithms and experimental results (MNIST, HIGGS, Epsilon). Previous papers: arXiv:2512.12731, arXiv:2512.16718, arXiv:2512.17671. Source code: https://github.com/xolod7/polyharmonic-cascade",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19522v1",
      "title": "A Convolutional Neural Deferred Shader for Physics Based Rendering",
      "authors": [
        "Zhuo He",
        "Yingdong Ru",
        "Qianying Liu",
        "Paul Henderson",
        "Nicolas Pugeault"
      ],
      "abstract": "Recent advances in neural rendering have achieved impressive results on photorealistic shading and relighting, by using a multilayer perceptron (MLP) as a regression model to learn the rendering equation from a real-world dataset. Such methods show promise for photorealistically relighting real-world objects, which is difficult to classical rendering, as there is no easy-obtained material ground truth. However, significant challenges still remain the dense connections in MLPs result in a large number of parameters, which requires high computation resources, complicating the training, and reducing performance during rendering. Data driven approaches require large amounts of training data for generalization; unbalanced data might bias the model to ignore the unusual illumination conditions, e.g. dark scenes. This paper introduces pbnds+: a novel physics-based neural deferred shading pipeline utilizing convolution neural networks to decrease the parameters and improve the performance in shading and relighting tasks; Energy regularization is also proposed to restrict the model reflection during dark illumination. Extensive experiments demonstrate that our approach outperforms classical baselines, a state-of-the-art neural shading model, and a diffusion-based method.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19522v1",
        "pdf": "https://arxiv.org/pdf/2512.19522v1"
      },
      "arxiv_id": "2512.19522v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19516v1",
      "title": "LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning",
      "authors": [
        "Xueming Yan",
        "Bo Yin",
        "Yaochu Jin"
      ],
      "abstract": "Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19516v1",
        "pdf": "https://arxiv.org/pdf/2512.19516v1"
      },
      "arxiv_id": "2512.19516v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19512v1",
      "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
      "authors": [
        "Ziyang Song",
        "Zelin Zang",
        "Zuyao Chen",
        "Xusheng Liang",
        "Dong Yi",
        "Jinlin Wu",
        "Hongbin Liu",
        "Jiebo Luo"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19512v1",
        "pdf": "https://arxiv.org/pdf/2512.19512v1"
      },
      "arxiv_id": "2512.19512v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19510v1",
      "title": "Toward Scalable and Valid Conditional Independence Testing with Spectral Representations",
      "authors": [
        "Alek Frohlich",
        "Vladimir Kostic",
        "Karim Lounici",
        "Daniel Perazzo",
        "Massimiliano Pontil"
      ],
      "abstract": "Conditional independence (CI) is central to causal inference, feature selection, and graphical modeling, yet it is untestable in many settings without additional assumptions. Existing CI tests often rely on restrictive structural conditions, limiting their validity on real-world data. Kernel methods using the partial covariance operator offer a more principled approach but suffer from limited adaptivity, slow convergence, and poor scalability. In this work, we explore whether representation learning can help address these limitations. Specifically, we focus on representations derived from the singular value decomposition of the partial covariance operator and use them to construct a simple test statistic, reminiscent of the Hilbert-Schmidt Independence Criterion (HSIC). We also introduce a practical bi-level contrastive algorithm to learn these representations. Our theory links representation learning error to test performance and establishes asymptotic validity and power guarantees. Preliminary experiments suggest that this approach offers a practical and statistically grounded path toward scalable CI testing, bridging kernel-based theory with modern representation learning.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19510v1",
        "pdf": "https://arxiv.org/pdf/2512.19510v1"
      },
      "arxiv_id": "2512.19510v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19506v1",
      "title": "DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast",
      "authors": [
        "Hongliang Li",
        "Nong Zhang",
        "Zhewen Xu",
        "Xiang Li",
        "Changzheng Liu",
        "Chongbo Zhao",
        "Jie Wu"
      ],
      "abstract": "Understanding and predicting the Madden-Julian Oscillation (MJO) is fundamental for precipitation forecasting and disaster prevention. To date, long-term and accurate MJO prediction has remained a challenge for researchers. Conventional MJO prediction methods using Numerical Weather Prediction (NWP) are resource-intensive, time-consuming, and highly unstable (most NWP methods are sensitive to seasons, with better MJO forecast results in winter). While existing Artificial Neural Network (ANN) methods save resources and speed forecasting, their accuracy never reaches the 28 days predicted by the state-of-the-art NWP method, i.e., the operational forecasts from ECMWF, since neural networks cannot handle climate data effectively. In this paper, we present a Domain Knowledge Embedded Spatio-Temporal Network (DK-STN), a stable neural network model for accurate and efficient MJO forecasting. It combines the benefits of NWP and ANN methods and successfully improves the forecast accuracy of ANN methods while maintaining a high level of efficiency and stability. We begin with a spatial-temporal network (STN) and embed domain knowledge in it using two key methods: (i) applying a domain knowledge enhancement method and (ii) integrating a domain knowledge processing method into network training. We evaluated DK-STN with the 5th generation of ECMWF reanalysis (ERA5) data and compared it with ECMWF. Given 7 days of climate data as input, DK-STN can generate reliable forecasts for the following 28 days in 1-2 seconds, with an error of only 2-3 days in different seasons. DK-STN significantly exceeds ECMWF in that its forecast accuracy is equivalent to ECMWF's, while its efficiency and stability are significantly superior.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19506v1",
        "pdf": "https://arxiv.org/pdf/2512.19506v1"
      },
      "arxiv_id": "2512.19506v1",
      "comment": "18 pages, 10 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19504v1",
      "title": "FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors",
      "authors": [
        "Georgios Voulgaris"
      ],
      "abstract": "Modern deep learning models operating on multi-modal visual signals often rely on inductive biases that are poorly aligned with the physical processes governing signal formation, leading to brittle performance under cross-spectral and real-world conditions. In particular, approaches that prioritise direct thermal cues struggle to capture indirect yet persistent environmental alterations induced by sustained heat emissions.\n  This work introduces a physics-aware representation learning framework that leverages multi-spectral information to model stable signatures of long-term physical processes. Specifically, a geological Short Wave Infrared (SWIR) ratio sensitive to soil property changes is integrated with Thermal Infrared (TIR) data through an intermediate fusion architecture, instantiated as FusionNet. The proposed backbone embeds trainable differential signal-processing priors within convolutional layers, combines mixed pooling strategies, and employs wider receptive fields to enhance robustness across spectral modalities.\n  Systematic ablations show that each architectural component contributes to performance gains, with DGCNN achieving 88.7% accuracy on the SWIR ratio and FusionNet reaching 90.6%, outperforming state-of-the-art baselines across five spectral configurations. Transfer learning experiments further show that ImageNet pretraining degrades TIR performance, highlighting the importance of modality-aware training for cross-spectral learning.\n  Evaluated on real-world data, the results demonstrate that combining physics-aware feature selection with principled deep learning architectures yields robust and generalisable representations, illustrating how first-principles signal modelling can improve multi-spectral learning under challenging conditions.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19504v1",
        "pdf": "https://arxiv.org/pdf/2512.19504v1"
      },
      "arxiv_id": "2512.19504v1",
      "comment": "Preprint. Under review at IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (JSTARS)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19494v1",
      "title": "Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset",
      "authors": [
        "Nikita Volzhin",
        "Soowhan Yoon"
      ],
      "abstract": "The recent development of Kolmogorov-Arnold Networks (KANs) introduced new discoveries in the field of Graph Neural Networks (GNNs), expanding the existing set of models with KAN-based versions of GNNs, which often surpass the accuracy of MultiLayer Perceptron (MLP)-based GNNs. These models were widely tested on the graph datasets consisting of organic molecules; however, those studies disregarded the inorganic nanomaterials datasets. In this work, we close this gap by applying Kolmogorov-Arnold Graph Neural Networks (KAGNNs) to a recently published large inorganic nanomaterials dataset called CHILI. For this, we adapt and test KAGNNs appropriate for this dataset. Our experiments reveal that on the CHILI datasets, particularly on the CHILI-3K, KAGNNs substantially surpass conventional GNNs in classification, achieving state-of-the-art results.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19494v1",
        "pdf": "https://arxiv.org/pdf/2512.19494v1"
      },
      "arxiv_id": "2512.19494v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19491v1",
      "title": "Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico",
      "authors": [
        "Martí Medina-Hern ández",
        "Janos Kertész",
        "Mihály Fazekas"
      ],
      "abstract": "Detecting fraud and corruption in public procurement remains a major challenge for governments worldwide. Most research to-date builds on domain-knowledge-based corruption risk indicators of individual contract-level features and some also analyzes contracting network patterns. A critical barrier for supervised machine learning is the absence of confirmed non-corrupt, negative, examples, which makes conventional machine learning inappropriate for this task. Using publicly available data on federally funded procurement in Mexico and company sanction records, this study implements positive-unlabeled (PU) learning algorithms that integrate domain-knowledge-based red flags with network-derived features to identify likely corrupt and fraudulent contracts. The best-performing PU model on average captures 32 percent more known positives and performs on average 2.3 times better than random guessing, substantially outperforming approaches based solely on traditional red flags. The analysis of the Shapley Additive Explanations reveals that network-derived features, particularly those associated with contracts in the network core or suppliers with high eigenvector centrality, are the most important. Traditional red flags further enhance model performance in line with expectations, albeit mainly for contracts awarded through competitive tenders. This methodology can support law enforcement in Mexico, and it can be adapted to other national contexts too.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19491v1",
        "pdf": "https://arxiv.org/pdf/2512.19491v1"
      },
      "arxiv_id": "2512.19491v1",
      "comment": "15 pages of main text with 6 figures and 31 pages of supplementary information",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19489v1",
      "title": "Rethinking Coupled Tensor Analysis for Hyperspectral Super-Resolution: Recoverable Modeling Under Endmember Variability",
      "authors": [
        "Meng Ding",
        "Xiao Fu"
      ],
      "abstract": "This work revisits the hyperspectral super-resolution (HSR) problem, i.e., fusing a pair of spatially co-registered hyperspectral (HSI) and multispectral (MSI) images to recover a super-resolution image (SRI) that enhances the spatial resolution of the HSI. Coupled tensor decomposition (CTD)-based methods have gained traction in this domain, offering recoverability guarantees under various assumptions. Existing models such as canonical polyadic decomposition (CPD) and Tucker decomposition provide strong expressive power but lack physical interpretability. The block-term decomposition model with rank-$(L_r, L_r, 1)$ terms (the LL1 model) yields interpretable factors under the linear mixture model (LMM) of spectral images, but LMM assumptions are often violated in practice -- primarily due to nonlinear effects such as endmember variability (EV). To address this, we propose modeling spectral images using a more flexible block-term tensor decomposition with rank-$(L_r, M_r, N_r)$ terms (the LMN model). This modeling choice retains interpretability, subsumes CPD, Tucker, and LL1 as special cases, and robustly accounts for non-ideal effects such as EV, offering a balanced tradeoff between expressiveness and interpretability for HSR. Importantly, under the LMN model for HSI and MSI, recoverability of the SRI can still be established under proper conditions -- providing strong theoretical support. Extensive experiments on synthetic and real datasets further validate the effectiveness and robustness of the proposed method compared with existing CTD-based approaches.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19489v1",
        "pdf": "https://arxiv.org/pdf/2512.19489v1"
      },
      "arxiv_id": "2512.19489v1",
      "comment": "The paper was accepted by SIAM Journal on Imaging Sciences",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19488v1",
      "title": "Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks",
      "authors": [
        "Hafsa Benaddi",
        "Mohammed Jouhari",
        "Nouha Laamech",
        "Anas Motii",
        "Khalil Ibrahimi"
      ],
      "abstract": "The widespread deployment of Internet of Things (IoT) devices requires intrusion detection systems (IDS) with high accuracy while operating under strict resource constraints. Conventional deep learning IDS are often too large and computationally intensive for edge deployment. We propose a lightweight IDS that combines SHAP-guided feature pruning with knowledge-distilled Kronecker networks. A high-capacity teacher model identifies the most relevant features through SHAP explanations, and a compressed student leverages Kronecker-structured layers to minimize parameters while preserving discriminative inputs. Knowledge distillation transfers softened decision boundaries from teacher to student, improving generalization under compression. Experiments on the TON\\_IoT dataset show that the student is nearly three orders of magnitude smaller than the teacher yet sustains macro-F1 above 0.986 with millisecond-level inference latency. The results demonstrate that explainability-driven pruning and structured compression can jointly enable scalable, low-latency, and energy-efficient IDS for heterogeneous IoT environments.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19488v1",
        "pdf": "https://arxiv.org/pdf/2512.19488v1"
      },
      "arxiv_id": "2512.19488v1",
      "comment": "This work has been published in the proceedings of the 2025 8th International Conference on Advanced Communication Technologies and Networking (CommNet)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19486v1",
      "title": "Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration",
      "authors": [
        "Shaochen Bi",
        "Yuting He",
        "Weiming Wang",
        "Hao Chen"
      ],
      "abstract": "Combinatorial explosion problem caused by dual inputs presents a critical challenge in Deformable Medical Image Registration (DMIR). Since DMIR processes two images simultaneously as input, the combination relationships between features has grown exponentially, ultimately the model considers more interfering features during the feature modeling process. Introducing dynamics in the receptive fields and weights of the network enable the model to eliminate the interfering features combination and model the potential feature combination relationships. In this paper, we propose the Dynamic Stream Network (DySNet), which enables the receptive fields and weights to be dynamically adjusted. This ultimately enables the model to ignore interfering feature combinations and model the potential feature relationships. With two key innovations: 1) Adaptive Stream Basin (AdSB) module dynamically adjusts the shape of the receptive field, thereby enabling the model to focus on the feature relationships with greater correlation. 2) Dynamic Stream Attention (DySA) mechanism generates dynamic weights to search for more valuable feature relationships. Extensive experiments have shown that DySNet consistently outperforms the most advanced DMIR methods, highlighting its outstanding generalization ability. Our code will be released on the website: https://github.com/ShaochenBi/DySNet.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19486v1",
        "pdf": "https://arxiv.org/pdf/2512.19486v1"
      },
      "arxiv_id": "2512.19486v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19481v1",
      "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis",
      "authors": [
        "Katharina Stengg",
        "Christian Macho",
        "Martin Pinzger"
      ],
      "abstract": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19481v1",
        "pdf": "https://arxiv.org/pdf/2512.19481v1"
      },
      "arxiv_id": "2512.19481v1",
      "comment": "6 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19479v1",
      "title": "Emotion-Director: Bridging Affective Shortcut in Emotion-Oriented Image Generation",
      "authors": [
        "Guoli Jia",
        "Junyao Hu",
        "Xinwei Long",
        "Kai Tian",
        "Kaiyan Zhang",
        "KaiKai Zhao",
        "Ning Ding",
        "Bowen Zhou"
      ],
      "abstract": "Image generation based on diffusion models has demonstrated impressive capability, motivating exploration into diverse and specialized applications. Owing to the importance of emotion in advertising, emotion-oriented image generation has attracted increasing attention. However, current emotion-oriented methods suffer from an affective shortcut, where emotions are approximated to semantics. As evidenced by two decades of research, emotion is not equivalent to semantics. To this end, we propose Emotion-Director, a cross-modal collaboration framework consisting of two modules. First, we propose a cross-Modal Collaborative diffusion model, abbreviated as MC-Diffusion. MC-Diffusion integrates visual prompts with textual prompts for guidance, enabling the generation of emotion-oriented images beyond semantics. Further, we improve the DPO optimization by a negative visual prompt, enhancing the model's sensitivity to different emotions under the same semantics. Second, we propose MC-Agent, a cross-Modal Collaborative Agent system that rewrites textual prompts to express the intended emotions. To avoid template-like rewrites, MC-Agent employs multi-agents to simulate human subjectivity toward emotions, and adopts a chain-of-concept workflow that improves the visual expressiveness of the rewritten prompts. Extensive qualitative and quantitative experiments demonstrate the superiority of Emotion-Director in emotion-oriented image generation.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19479v1",
        "pdf": "https://arxiv.org/pdf/2512.19479v1"
      },
      "arxiv_id": "2512.19479v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19472v1",
      "title": "Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications",
      "authors": [
        "Lorenzo Capelli",
        "Leandro de Souza Rosa",
        "Gianluca Setti",
        "Mauro Mangia",
        "Riccardo Rovatti"
      ],
      "abstract": "The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19472v1",
        "pdf": "https://arxiv.org/pdf/2512.19472v1"
      },
      "arxiv_id": "2512.19472v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19469v1",
      "title": "GLUE: Generative Latent Unification of Expertise-Informed Engineering Models",
      "authors": [
        "Tim Aebersold",
        "Soheyl Massoudi",
        "Mark D. Fuge"
      ],
      "abstract": "Engineering complex systems (aircraft, buildings, vehicles) requires accounting for geometric and performance couplings across subsystems. As generative models proliferate for specialized domains (wings, structures, engines), a key research gap is how to coordinate frozen, pre-trained submodels to generate full-system designs that are feasible, diverse, and high-performing. We introduce Generative Latent Unification of Expertise-Informed Engineering Models (GLUE), which orchestrates pre-trained, frozen subsystem generators while enforcing system-level feasibility, optimality, and diversity. We propose and benchmark (i) data-driven GLUE models trained on pre-generated system-level designs and (ii) a data-free GLUE model trained online on a differentiable geometry layer. On a UAV design problem with five coupling constraints, we find that data-driven approaches yield diverse, high-performing designs but require large datasets to satisfy constraints reliably. The data-free approach is competitive with Bayesian optimization and gradient-based optimization in performance and feasibility while training a full generative model in only 10 min on a RTX 4090 GPU, requiring more than two orders of magnitude fewer geometry evaluations and FLOPs than the data-driven method. Ablations focused on data-free training show that subsystem output continuity affects coordination, and equality constraints can trigger mode collapse unless mitigated. By integrating unmodified, domain-informed submodels into a modular generative workflow, this work provides a viable path for scaling generative design to complex, real-world engineering systems.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.CE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19469v1",
        "pdf": "https://arxiv.org/pdf/2512.19469v1"
      },
      "arxiv_id": "2512.19469v1",
      "comment": "11 pages, 10 figures. Preprint. Submitted to Computer-Aided Engineering (Elsevier)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19458v1",
      "title": "An Agentic Framework for Autonomous Materials Computation",
      "authors": [
        "Zeyu Xia",
        "Jinzhe Ma",
        "Congjie Zheng",
        "Shufei Zhang",
        "Yuqiang Li",
        "Hang Su",
        "P. Hu",
        "Changshui Zhang",
        "Xingao Gong",
        "Wanli Ouyang",
        "Lei Bai",
        "Dongzhan Zhou",
        "Mao Su"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19458v1",
        "pdf": "https://arxiv.org/pdf/2512.19458v1"
      },
      "arxiv_id": "2512.19458v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19456v1",
      "title": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations",
      "authors": [
        "Jinwei Chi",
        "Ke Wang",
        "Yu Chen",
        "Xuanye Lin",
        "Qiang Xu"
      ],
      "abstract": "Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19456v1",
        "pdf": "https://arxiv.org/pdf/2512.19456v1"
      },
      "arxiv_id": "2512.19456v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19451v1",
      "title": "Sign Language Recognition using Parallel Bidirectional Reservoir Computing",
      "authors": [
        "Nitin Kumar Singh",
        "Arie Rachmad Syulistyo",
        "Yuichiro Tanaka",
        "Hakaru Tamukoh"
      ],
      "abstract": "Sign language recognition (SLR) facilitates communication between deaf and hearing communities. Deep learning based SLR models are commonly used but require extensive computational resources, making them unsuitable for deployment on edge devices. To address these limitations, we propose a lightweight SLR system that combines parallel bidirectional reservoir computing (PBRC) with MediaPipe. MediaPipe enables real-time hand tracking and precise extraction of hand joint coordinates, which serve as input features for the PBRC architecture. The proposed PBRC architecture consists of two echo state network (ESN) based bidirectional reservoir computing (BRC) modules arranged in parallel to capture temporal dependencies, thereby creating a rich feature representation for classification. We trained our PBRC-based SLR system on the Word-Level American Sign Language (WLASL) video dataset, achieving top-1, top-5, and top-10 accuracies of 60.85%, 85.86%, and 91.74%, respectively. Training time was significantly reduced to 18.67 seconds due to the intrinsic properties of reservoir computing, compared to over 55 minutes for deep learning based methods such as Bi-GRU. This approach offers a lightweight, cost-effective solution for real-time SLR on edge devices.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19451v1",
        "pdf": "https://arxiv.org/pdf/2512.19451v1"
      },
      "arxiv_id": "2512.19451v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19443v1",
      "title": "D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning",
      "authors": [
        "Evelyn Zhang",
        "Fufu Yu",
        "Aoqi Wu",
        "Zichen Wen",
        "Ke Yan",
        "Shouhong Ding",
        "Biqing Qi",
        "Linfeng Zhang"
      ],
      "abstract": "Processing long visual token sequences poses a significant computational burden on Multimodal Large Language Models (MLLMs). While token pruning offers a path to acceleration, we find that current methods, while adequate for general understanding, catastrophically fail on fine-grained localization tasks. We attribute this failure to the inherent flaws of the two prevailing strategies: importance-based methods suffer from a strong positional bias, an inherent model artifact that distracts from semantic content, while diversity-based methods exhibit structural blindness, disregarding the user's prompt and spatial redundancy. To address this, we introduce D2Pruner, a framework that rectifies these issues by uniquely combining debiased importance with a structural pruning mechanism. Our method first secures a core set of the most critical tokens as pivots based on a debiased attention score. It then performs a Maximal Independent Set (MIS) selection on the remaining tokens, which are modeled on a hybrid graph where edges signify spatial proximity and semantic similarity. This process iteratively preserves the most important and available token while removing its neighbors, ensuring that the supplementary tokens are chosen to maximize importance and diversity. Extensive experiments demonstrate that D2Pruner has exceptional efficiency and fidelity. Applied to LLaVA-1.5-7B for general understanding tasks, it reduces FLOPs by 74.2\\% while retaining 99.2\\% of its original performance. Furthermore, in challenging localization benchmarks with InternVL-2.5-8B, it maintains 85.7\\% performance at a 90\\% token reduction rate, marking a significant advancement with up to 63. 53\\% improvement over existing methods.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19443v1",
        "pdf": "https://arxiv.org/pdf/2512.19443v1"
      },
      "arxiv_id": "2512.19443v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19442v1",
      "title": "Real-Time Streamable Generative Speech Restoration with Flow Matching",
      "authors": [
        "Simon Welker",
        "Bunlong Lay",
        "Maris Hillemann",
        "Tal Peer",
        "Timo Gerkmann"
      ],
      "abstract": "Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs.\n  Here, we present Stream.FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task.\n  Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream.FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream.FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "eess.SP",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.SP",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19442v1",
        "pdf": "https://arxiv.org/pdf/2512.19442v1"
      },
      "arxiv_id": "2512.19442v1",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19440v1",
      "title": "Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm",
      "authors": [
        "Antonio Consolo",
        "Andrea Manno",
        "Edoardo Amaldi"
      ],
      "abstract": "Kernel logistic regression (KLR) is a widely used supervised learning method for binary and multi-class classification, which provides estimates of the conditional probabilities of class membership for the data points. Unlike other kernel methods such as Support Vector Machines (SVMs), KLRs are generally not sparse. Previous attempts to deal with sparsity in KLR include a heuristic method referred to as the Import Vector Machine (IVM) and ad hoc regularizations such as the $\\ell_{1/2}$-based one. Achieving a good trade-off between prediction accuracy and sparsity is still a challenging issue with a potential significant impact from the application point of view. In this work, we revisit binary KLR and propose an extension of the training formulation proposed by Keerthi et al., which is able to induce sparsity in the trained model, while maintaining good testing accuracy. To efficiently solve the dual of this formulation, we devise a decomposition algorithm of Sequential Minimal Optimization type which exploits second-order information, and for which we establish global convergence. Numerical experiments conducted on 12 datasets from the literature show that the proposed binary KLR approach achieves a competitive trade-off between accuracy and sparsity with respect to IVM, $\\ell_{1/2}$-based regularization for KLR, and SVM while retaining the advantages of providing informative estimates of the class membership probabilities.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19440v1",
        "pdf": "https://arxiv.org/pdf/2512.19440v1"
      },
      "arxiv_id": "2512.19440v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19439v1",
      "title": "An Inverse Scattering Inspired Fourier Neural Operator for Time-Dependent PDE Learning",
      "authors": [
        "Rixin Yu"
      ],
      "abstract": "Learning accurate and stable time-advancement operators for nonlinear partial differential equations (PDEs) remains challenging, particularly for chaotic, stiff, and long-horizon dynamical systems. While neural operator methods such as the Fourier Neural Operator (FNO) and Koopman-inspired extensions achieve good short-term accuracy, their long-term stability is often limited by unconstrained latent representations and cumulative rollout errors. In this work, we introduce an inverse scattering inspired Fourier Neural Operator(IS-FNO), motivated by the reversibility and spectral evolution structure underlying the classical inverse scattering transform. The proposed architecture enforces a near-reversible pairing between lifting and projection maps through an explicitly invertible neural transformation, and models latent temporal evolution using exponential Fourier layers that naturally encode linear and nonlinear spectral dynamics. We systematically evaluate IS-FNO against baseline FNO and Koopman-based models on a range of benchmark PDEs, including the Michelson-Sivashinsky and Kuramoto-Sivashinsky equations (in one and two dimensions), as well as the integrable Korteweg-de Vries and Kadomtsev-Petviashvili equations. The results demonstrate that IS-FNO achieves lower short-term errors and substantially improved long-horizon stability in non-stiff regimes. For integrable systems, reduced IS-FNO variants that embed analytical scattering structure retain competitive long-term accuracy despite limited model capacity. Overall, this work shows that incorporating physical structure -- particularly reversibility and spectral evolution -- into neural operator design significantly enhances robustness and long-term predictive fidelity for nonlinear PDE dynamics.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19439v1",
        "pdf": "https://arxiv.org/pdf/2512.19439v1"
      },
      "arxiv_id": "2512.19439v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19438v1",
      "title": "MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation",
      "authors": [
        "Fei Ge",
        "Ying Huang",
        "Jie Liu",
        "Guixuan Zhang",
        "Zhi Zeng",
        "Shuwu Zhang",
        "Hu Guan"
      ],
      "abstract": "Existing deep image watermarking methods follow a fixed embedding-distortion-extraction pipeline, where the embedder and extractor are weakly coupled through a final loss and optimized in isolation. This design lacks explicit collaboration, leaving no structured mechanism for the embedder to incorporate decoding-aware cues or for the extractor to guide embedding during training. To address this architectural limitation, we rethink deep image watermarking by reformulating embedding and extraction as explicitly collaborative components. To realize this reformulation, we introduce a Collaborative Interaction Mechanism (CIM) that establishes direct, bidirectional communication between the embedder and extractor, enabling a mutual-teacher training paradigm and coordinated optimization. Built upon this explicitly collaborative architecture, we further propose an Adaptive Feature Modulation Module (AFMM) to support effective interaction. AFMM enables content-aware feature regulation by decoupling modulation structure and strength, guiding watermark embedding toward stable image features while suppressing host interference during extraction. Under CIM, the AFMMs on both sides form a closed-loop collaboration that aligns embedding behavior with extraction objectives. This architecture-level redesign changes how robustness is learned in watermarking systems. Rather than relying on exhaustive distortion simulation, robustness emerges from coordinated representation learning between embedding and extraction. Experiments on real-world and AI-generated datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in watermark extraction accuracy while maintaining high perceptual quality, showing strong robustness and generalization.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19438v1",
        "pdf": "https://arxiv.org/pdf/2512.19438v1"
      },
      "arxiv_id": "2512.19438v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19433v1",
      "title": "dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models",
      "authors": [
        "Yi Xin",
        "Siqi Luo",
        "Qi Qin",
        "Haoxing Chen",
        "Kaiwen Zhu",
        "Zhiwei Zhang",
        "Yangfan He",
        "Rongchao Zhang",
        "Jinbin Bai",
        "Shuo Cao",
        "Bin Fu",
        "Junjun He",
        "Yihao Liu",
        "Yuewen Cao",
        "Xiaohong Liu"
      ],
      "abstract": "Diffusion Multi-modal Large Language Models (dMLLMs) have recently emerged as a novel architecture unifying image generation and understanding. However, developing effective and efficient Test-Time Scaling (TTS) methods to unlock their full generative potential remains an underexplored challenge. To address this, we propose dMLLM-TTS, a novel framework operating on two complementary scaling axes: (1) trajectory exploration scaling to enhance the diversity of generated hypotheses, and (2) iterative refinement scaling for stable generation. Conventional TTS approaches typically perform linear search across these two dimensions, incurring substantial computational costs of O(NT) and requiring an external verifier for best-of-N selection. To overcome these limitations, we propose two innovations. First, we design an efficient hierarchical search algorithm with O(N+T) complexity that adaptively expands and prunes sampling trajectories. Second, we introduce a self-verified feedback mechanism that leverages the dMLLMs' intrinsic image understanding capabilities to assess text-image alignment, eliminating the need for external verifier. Extensive experiments on the GenEval benchmark across three representative dMLLMs (e.g., Lumina-DiMOO, MMaDA, Muddit) show that our framework substantially improves generation quality while achieving up to 6x greater efficiency than linear search. Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19433v1",
        "pdf": "https://arxiv.org/pdf/2512.19433v1"
      },
      "arxiv_id": "2512.19433v1",
      "comment": "Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19428v1",
      "title": "Attention Is Not What You Need",
      "authors": [
        "Zhang Chong"
      ],
      "abstract": "We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.\n  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.\n  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.AG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19428v1",
        "pdf": "https://arxiv.org/pdf/2512.19428v1"
      },
      "arxiv_id": "2512.19428v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19415v1",
      "title": "Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis",
      "authors": [
        "Xiaoming Zhang",
        "Chunli Li",
        "Jiacheng Hao",
        "Yuan Gao",
        "Danyang Tu",
        "Jianyi Qiao",
        "Xiaoli Yin",
        "Le Lu",
        "Ling Zhang",
        "Ke Yan",
        "Yang Hou",
        "Yu Shi"
      ],
      "abstract": "Esophageal varices (EV) represent a critical complication of portal hypertension, affecting approximately 60% of cirrhosis patients with a significant bleeding risk of ~30%. While traditionally diagnosed through invasive endoscopy, non-contrast computed tomography (NCCT) presents a potential non-invasive alternative that has yet to be fully utilized in clinical practice. We present Multi-Organ-COhesion Network++ (MOON++), a novel multimodal framework that enhances EV assessment through comprehensive analysis of NCCT scans. Inspired by clinical evidence correlating organ volumetric relationships with liver disease severity, MOON++ synthesizes imaging characteristics of the esophagus, liver, and spleen through multimodal learning. We evaluated our approach using 1,631 patients, those with endoscopically confirmed EV were classified into four severity grades. Validation in 239 patient cases and independent testing in 289 cases demonstrate superior performance compared to conventional single organ methods, achieving an AUC of 0.894 versus 0.803 for the severe grade EV classification (G3 versus <G3) and 0.921 versus 0.793 for the differentiation of moderate to severe grades (>=G2 versus <G2). We conducted a reader study involving experienced radiologists to further validate the performance of MOON++. To our knowledge, MOON++ represents the first comprehensive multi-organ NCCT analysis framework incorporating clinical knowledge priors for EV assessment, potentially offering a promising non-invasive diagnostic alternative.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19415v1",
        "pdf": "https://arxiv.org/pdf/2512.19415v1"
      },
      "arxiv_id": "2512.19415v1",
      "comment": "Medical Image Analysis",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19410v1",
      "title": "Research Program: Theory of Learning in Dynamical Systems",
      "authors": [
        "Elad Hazan",
        "Shai Shalev Shwartz",
        "Nathan Srebro"
      ],
      "abstract": "Modern learning systems increasingly interact with data that evolve over time and depend on hidden internal state. We ask a basic question: when is such a dynamical system learnable from observations alone? This paper proposes a research program for understanding learnability in dynamical systems through the lens of next-token prediction. We argue that learnability in dynamical systems should be studied as a finite-sample question, and be based on the properties of the underlying dynamics rather than the statistical properties of the resulting sequence. To this end, we give a formulation of learnability for stochastic processes induced by dynamical systems, focusing on guarantees that hold uniformly at every time step after a finite burn-in period. This leads to a notion of dynamic learnability which captures how the structure of a system, such as stability, mixing, observability, and spectral properties, governs the number of observations required before reliable prediction becomes possible. We illustrate the framework in the case of linear dynamical systems, showing that accurate prediction can be achieved after finite observation without system identification, by leveraging improper methods based on spectral filtering. We survey the relationship between learning in dynamical systems and classical PAC, online, and universal prediction theories, and suggest directions for studying nonlinear and controlled systems.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19410v1",
        "pdf": "https://arxiv.org/pdf/2512.19410v1"
      },
      "arxiv_id": "2512.19410v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19409v1",
      "title": "Symplectic Reservoir Representation of Legendre Dynamics",
      "authors": [
        "Robert Simon Fong",
        "Gouhei Tanaka",
        "Kazuyuki Aihara"
      ],
      "abstract": "Modern learning systems act on internal representations of data, yet how these representations encode underlying physical or statistical structure is often left implicit. In physics, conservation laws of Hamiltonian systems such as symplecticity guarantee long-term stability, and recent work has begun to hard-wire such constraints into learning models at the loss or output level. Here we ask a different question: what would it mean for the representation itself to obey a symplectic conservation law in the sense of Hamiltonian mechanics?\n  We express this symplectic constraint through Legendre duality: the pairing between primal and dual parameters, which becomes the structure that the representation must preserve. We formalize Legendre dynamics as stochastic processes whose trajectories remain on Legendre graphs, so that the evolving primal-dual parameters stay Legendre dual. We show that this class includes linear time-invariant Gaussian process regression and Ornstein-Uhlenbeck dynamics.\n  Geometrically, we prove that the maps that preserve all Legendre graphs are exactly symplectomorphisms of cotangent bundles of the form \"cotangent lift of a base diffeomorphism followed by an exact fibre translation\". Dynamically, this characterization leads to the design of a Symplectic Reservoir (SR), a reservoir-computing architecture that is a special case of recurrent neural network and whose recurrent core is generated by Hamiltonian systems that are at most linear in the momentum.\n  Our main theorem shows that every SR update has this normal form and therefore transports Legendre graphs to Legendre graphs, preserving Legendre duality at each time step. Overall, SR implements a geometrically constrained, Legendre-preserving representation map, injecting symplectic geometry and Hamiltonian mechanics directly at the representational level.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19409v1",
        "pdf": "https://arxiv.org/pdf/2512.19409v1"
      },
      "arxiv_id": "2512.19409v1",
      "comment": "39 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19402v1",
      "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
      "authors": [
        "Yujie Zhao",
        "Hongwei Fan",
        "Di Chen",
        "Shengcong Chen",
        "Liliang Chen",
        "Xiaoqi Li",
        "Guanghui Ren",
        "Hao Dong"
      ],
      "abstract": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19402v1",
        "pdf": "https://arxiv.org/pdf/2512.19402v1"
      },
      "arxiv_id": "2512.19402v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19399v1",
      "title": "Brain-Grounded Axes for Reading and Steering LLM States",
      "authors": [
        "Sandro Andric"
      ],
      "abstract": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19399v1",
        "pdf": "https://arxiv.org/pdf/2512.19399v1"
      },
      "arxiv_id": "2512.19399v1",
      "comment": "10 pages, 4 figures. Code: https://github.com/sandroandric/Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2512.19396v1",
      "title": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration",
      "authors": [
        "Runze Li",
        "Yuwen Zhai",
        "Bo Xu",
        "LiWu Xu",
        "Nian Shi",
        "Wei Zhang",
        "Ran Lin",
        "Liang Wang"
      ],
      "abstract": "Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19396v1",
        "pdf": "https://arxiv.org/pdf/2512.19396v1"
      },
      "arxiv_id": "2512.19396v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19390v1",
      "title": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation",
      "authors": [
        "Hongwei Fan",
        "Hang Dai",
        "Jiyao Zhang",
        "Jinzhou Li",
        "Qiyang Yan",
        "Yujie Zhao",
        "Mingju Gao",
        "Jinghang Wu",
        "Hao Tang",
        "Hao Dong"
      ],
      "abstract": "The robotics field is evolving towards data-driven, end-to-end learning, inspired by multimodal large models. However, reliance on expensive real-world data limits progress. Simulators offer cost-effective alternatives, but the gap between simulation and reality challenges effective policy transfer. This paper introduces TwinAligner, a novel Real2Sim2Real system that addresses both visual and dynamic gaps. The visual alignment module achieves pixel-level alignment through SDF reconstruction and editable 3DGS rendering, while the dynamic alignment module ensures dynamic consistency by identifying rigid physics from robot-object interaction. TwinAligner improves robot learning by providing scalable data collection and establishing a trustworthy iterative cycle, accelerating algorithm development. Quantitative evaluations highlight TwinAligner's strong capabilities in visual and dynamic real-to-sim alignment. This system enables policies trained in simulation to achieve strong zero-shot generalization to the real world. The high consistency between real-world and simulated policy performance underscores TwinAligner's potential to advance scalable robot learning. Code and data will be released on https://twin-aligner.github.io",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19390v1",
        "pdf": "https://arxiv.org/pdf/2512.19390v1"
      },
      "arxiv_id": "2512.19390v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19387v1",
      "title": "DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition",
      "authors": [
        "Yueyao Chen",
        "Kai-Ni Wang",
        "Dario Tayupo",
        "Arnaud Huaulm'e",
        "Krystel Nyangoh Timoh",
        "Pierre Jannin",
        "Qi Dou"
      ],
      "abstract": "Purpose: Surgical workflow recognition enables context-aware assistance and skill assessment in computer-assisted interventions. Despite recent advances, current methods suffer from two critical challenges: prediction jitter across consecutive frames and poor discrimination of ambiguous phases. This paper aims to develop a stable framework by selectively propagating reliable historical information and explicitly modeling uncertainty for hard sample enhancement.\n  Methods: We propose a dual-pathway framework DSTED with Reliable Memory Propagation (RMP) and Uncertainty-Aware Prototype Retrieval (UPR). RMP maintains temporal coherence by filtering and fusing high-confidence historical features through multi-criteria reliability assessment. UPR constructs learnable class-specific prototypes from high-uncertainty samples and performs adaptive prototype matching to refine ambiguous frame representations. Finally, a confidence-driven gate dynamically balances both pathways based on prediction certainty.\n  Results: Our method achieves state-of-the-art performance on AutoLaparo-hysterectomy with 84.36% accuracy and 65.51% F1-score, surpassing the second-best method by 3.51% and 4.88% respectively. Ablations reveal complementary gains from RMP (2.19%) and UPR (1.93%), with synergistic effects when combined. Extensive analysis confirms substantial reduction in temporal jitter and marked improvement on challenging phase transitions.\n  Conclusion: Our dual-pathway design introduces a novel paradigm for stable workflow recognition, demonstrating that decoupling the modeling of temporal consistency and phase ambiguity yields superior performance and clinical applicability.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19387v1",
        "pdf": "https://arxiv.org/pdf/2512.19387v1"
      },
      "arxiv_id": "2512.19387v1",
      "comment": "Early accepted to IPCAI 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19383v1",
      "title": "Real-Time Machine Learning for Embedded Anomaly Detection",
      "authors": [
        "Abdelmadjid Benmachiche",
        "Khadija Rais",
        "Hamda Slimi"
      ],
      "abstract": "The spread of a resource-constrained Internet of Things (IoT) environment and embedded devices has put pressure on the real-time detection of anomalies occurring at the edge. This survey presents an overview of machine-learning methods aimed specifically at on-device anomaly detection with extremely strict constraints for latency, memory, and power consumption. Lightweight algorithms such as Isolation Forest, One-Class SVM, recurrent architectures, and statistical techniques are compared here according to the realities of embedded implementation. Our survey brings out significant trade-offs of accuracy and computational efficiency of detection, as well as how hardware constraints end up fundamentally redefining algorithm choice. The survey is completed with a set of practical recommendations on the choice of the algorithm depending on the equipment profiles and new trends in TinyML, which can help close the gap between detection capabilities and embedded reality. The paper serves as a strategic roadmap for engineers deploying anomaly detection in edge environments that are constrained by bandwidth and may be safety-critical.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19383v1",
        "pdf": "https://arxiv.org/pdf/2512.19383v1"
      },
      "arxiv_id": "2512.19383v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19379v1",
      "title": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
      "authors": [
        "Xueming Yan",
        "Boyan Xu",
        "Yaochu Jin",
        "Lixian Xiao",
        "Wenlong Ye",
        "Runyang Cai",
        "Zeqi Zheng",
        "Jingfa Liu",
        "Aimin Yang"
      ],
      "abstract": "Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19379v1",
        "pdf": "https://arxiv.org/pdf/2512.19379v1"
      },
      "arxiv_id": "2512.19379v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19376v1",
      "title": "A Critical Assessment of Pattern Comparisons Between POD and Autoencoders in Intraventricular Flows",
      "authors": [
        "Eneko Lazpita",
        "Andrés Bell-Navas",
        "Jesús Garicano-Mena",
        "Petros Koumoutsakos",
        "Soledad Le Clainche"
      ],
      "abstract": "Understanding intraventricular hemodynamics requires compact and physically interpretable representations of the underlying flow structures, as characteristic flow patterns are closely associated with cardiovascular conditions and can support early detection of cardiac deterioration. Conventional visualization of velocity or pressure fields, however, provides limited insight into the coherent mechanisms driving these dynamics. Reduced-order modeling techniques, like Proper Orthogonal Decomposition (POD) and Autoencoder (AE) architectures, offer powerful alternatives to extract dominant flow features from complex datasets. This study systematically compares POD with several AE variants (Linear, Nonlinear, Convolutional, and Variational) using left ventricular flow fields obtained from computational fluid dynamics simulations. We show that, for a suitably chosen latent dimension, AEs produce modes that become nearly orthogonal and qualitatively resemble POD modes that capture a given percentage of kinetic energy. As the number of latent modes increases, AE modes progressively lose orthogonality, leading to linear dependence, spatial redundancy, and the appearance of repeated modes with substantial high-frequency content. This degradation reduces interpretability and introduces noise-like components into AE-based reduced-order models, potentially complicating their integration with physics-based formulations or neural-network surrogates. The extent of interpretability loss varies across the AEs, with nonlinear, convolutional, and variational models exhibiting distinct behaviors in orthogonality preservation and feature localization. Overall, the results indicate that AEs can reproduce POD-like coherent structures under specific latent-space configurations, while highlighting the need for careful mode selection to ensure physically meaningful representations of cardiac flow dynamics.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "physics.flu-dyn",
        "cs.LG"
      ],
      "primary_category": "physics.flu-dyn",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19376v1",
        "pdf": "https://arxiv.org/pdf/2512.19376v1"
      },
      "arxiv_id": "2512.19376v1",
      "comment": "27 pages, 9 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19373v1",
      "title": "Cluster-Based Generalized Additive Models Informed by Random Fourier Features",
      "authors": [
        "Xin Huang",
        "Jia Li",
        "Jun Yu"
      ],
      "abstract": "Explainable machine learning aims to strike a balance between prediction accuracy and model transparency, particularly in settings where black-box predictive models, such as deep neural networks or kernel-based methods, achieve strong empirical performance but remain difficult to interpret. This work introduces a mixture of generalized additive models (GAMs) in which random Fourier feature (RFF) representations are leveraged to uncover locally adaptive structure in the data. In the proposed method, an RFF-based embedding is first learned and then compressed via principal component analysis. The resulting low-dimensional representations are used to perform soft clustering of the data through a Gaussian mixture model. These cluster assignments are then applied to construct a mixture-of-GAMs framework, where each local GAM captures nonlinear effects through interpretable univariate smooth functions. Numerical experiments on real-world regression benchmarks, including the California Housing, NASA Airfoil Self-Noise, and Bike Sharing datasets, demonstrate improved predictive performance relative to classical interpretable models. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19373v1",
        "pdf": "https://arxiv.org/pdf/2512.19373v1"
      },
      "arxiv_id": "2512.19373v1",
      "comment": "25 pages, 13 figures, 4 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19367v1",
      "title": "Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture",
      "authors": [
        "Christian Hägg",
        "Kathlén Kohn",
        "Giovanni Luca Marchetti",
        "Boris Shapiro"
      ],
      "abstract": "We present Sprecher Networks (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov-Arnold-Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (monotonic and general) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 sum of shifted splines formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19367v1",
        "pdf": "https://arxiv.org/pdf/2512.19367v1"
      },
      "arxiv_id": "2512.19367v1",
      "comment": "37 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19366v1",
      "title": "Learning General Policies with Policy Gradient Methods",
      "authors": [
        "Simon Ståhlberg",
        "Blai Bonet",
        "Hector Geffner"
      ],
      "abstract": "While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19366v1",
        "pdf": "https://arxiv.org/pdf/2512.19366v1"
      },
      "arxiv_id": "2512.19366v1",
      "comment": "In Proceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning (KR 2023)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19365v1",
      "title": "Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization",
      "authors": [
        "Zhongwei Chen",
        "Hai-Jun Rong",
        "Zhao-Xu Yang",
        "Guoqi Li"
      ],
      "abstract": "Traditional drone-view geo-localization (DVGL) methods based on artificial neural networks (ANNs) have achieved remarkable performance. However, ANNs rely on dense computation, which results in high power consumption. In contrast, spiking neural networks (SNNs), which benefit from spike-driven computation, inherently provide low power consumption. Regrettably, the potential of SNNs for DVGL has yet to be thoroughly investigated. Meanwhile, the inherent sparsity of spike-driven computation for representation learning scenarios also results in loss of critical information and difficulties in learning long-range dependencies when aligning heterogeneous visual data sources. To address these, we propose SpikeViMFormer, the first SNN framework designed for DVGL. In this framework, a lightweight spike-driven transformer backbone is adopted to extract coarse-grained features. To mitigate the loss of critical information, the spike-driven selective attention (SSA) block is designed, which uses a spike-driven gating mechanism to achieve selective feature enhancement and highlight discriminative regions. Furthermore, a spike-driven hybrid state space (SHS) block is introduced to learn long-range dependencies using a hybrid state space. Moreover, only the backbone is utilized during the inference stage to reduce computational cost. To ensure backbone effectiveness, a novel hierarchical re-ranking alignment learning (HRAL) strategy is proposed. It refines features via neighborhood re-ranking and maintains cross-batch consistency to directly optimize the backbone. Experimental results demonstrate that SpikeViMFormer outperforms state-of-the-art SNNs. Compared with advanced ANNs, it also achieves competitive performance.Our code is available at https://github.com/ISChenawei/SpikeViMFormer",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19365v1",
        "pdf": "https://arxiv.org/pdf/2512.19365v1"
      },
      "arxiv_id": "2512.19365v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19364v1",
      "title": "ForeSpeed: A real-world video dataset of CCTV cameras with different settings for vehicle speed estimation",
      "authors": [
        "Massimo Iuliani",
        "Blake Sawyer",
        "Marco Fontani",
        "David Spreadborough",
        "Martino Jerian"
      ],
      "abstract": "The need to estimate the speed of road vehicles has become increasingly important in the field of video forensics, particularly with the widespread deployment of CCTV cameras worldwide. Despite the development of various approaches, the accuracy of forensic speed estimation from real-world footage remains highly dependent on several factors, including camera specifications, acquisition methods, spatial and temporal resolution, compression methods, and scene perspective, which can significantly influence performance.\n  In this paper, we introduce ForeSpeed, a comprehensive dataset designed to support the evaluation of speed estimation techniques in real-world scenarios using CCTV footage. The dataset includes recordings of a vehicle traveling at known speeds, captured by three digital and three analog cameras from two distinct perspectives. Real-world road metrics are provided to enable the restoration of the scene geometry. Videos were stored with multiple compression factors and settings, to simulate real world scenarios in which export procedures are not always performed according to forensic standards. Overall, ForeSpeed, includes a collection of 322 videos.\n  As a case study, we employed the ForeSpeed dataset to benchmark a speed estimation algorithm available in a commercial product (Amped FIVE). Results demonstrate that while the method reliably estimates average speed across various conditions, its uncertainty range significantly increases when the scene involves strong perspective distortion. The ForeSpeed dataset is publicly available to the forensic community, with the aim of facilitating the evaluation of current methodologies and inspiring the development of new, robust solutions tailored to collision investigation and forensic incident analysis.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "eess.IV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19364v1",
        "pdf": "https://arxiv.org/pdf/2512.19364v1"
      },
      "arxiv_id": "2512.19364v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19363v1",
      "title": "From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples",
      "authors": [
        "Canran Xiao",
        "Jiabao Dou",
        "Zhiming Lin",
        "Zong Ke",
        "Liwei Hou"
      ],
      "abstract": "How should we quantify the value of each training example when datasets are large, heterogeneous, and geometrically structured? Classical Data-Shapley answers in principle, but its O(n!) complexity and point-wise perspective are ill-suited to modern scales. We propose Hierarchical Contrastive Data Valuation (HCDV), a three-stage framework that (i) learns a contrastive, geometry-preserving representation, (ii) organizes the data into a balanced coarse-to-fine hierarchy of clusters, and (iii) assigns Shapley-style payoffs to coalitions via local Monte-Carlo games whose budgets are propagated downward. HCDV collapses the factorial burden to O(T sum_{l} K_{l}) = O(T K_max log n), rewards examples that sharpen decision boundaries, and regularizes outliers through curvature-based smoothness. We prove that HCDV approximately satisfies the four Shapley axioms with surplus loss O(eta log n), enjoys sub-Gaussian coalition deviation tilde O(1/sqrt{T}), and incurs at most k epsilon_infty regret for top-k selection. Experiments on four benchmarks--tabular, vision, streaming, and a 45M-sample CTR task--plus the OpenDataVal suite show that HCDV lifts accuracy by up to +5 pp, slashes valuation time by up to 100x, and directly supports tasks such as augmentation filtering, low-latency streaming updates, and fair marketplace payouts.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19363v1",
        "pdf": "https://arxiv.org/pdf/2512.19363v1"
      },
      "arxiv_id": "2512.19363v1",
      "comment": "AAAI'26 Oral",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19361v1",
      "title": "Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation",
      "authors": [
        "Isshaan Singh",
        "Divyansh Chawla",
        "Anshu Garg",
        "Shivin Mangal",
        "Pallavi Gupta",
        "Khushi Agarwal",
        "Nimrat Singh Khalsa",
        "Nandan Patel"
      ],
      "abstract": "The need for an intelligent, real-time spoilage prediction system has become critical in modern IoT-driven food supply chains, where perishable goods are highly susceptible to environmental conditions. Existing methods often lack adaptability to dynamic conditions and fail to optimize decision making in real time. To address these challenges, we propose a hybrid reinforcement learning framework integrating Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNN) for enhanced spoilage prediction. This hybrid architecture captures temporal dependencies within sensor data, enabling robust and adaptive decision making. In alignment with interpretable artificial intelligence principles, a rule-based classifier environment is employed to provide transparent ground truth labeling of spoilage levels based on domain-specific thresholds. This structured design allows the agent to operate within clearly defined semantic boundaries, supporting traceable and interpretable decisions. Model behavior is monitored using interpretability-driven metrics, including spoilage accuracy, reward-to-step ratio, loss reduction rate, and exploration decay. These metrics provide both quantitative performance evaluation and insights into learning dynamics. A class-wise spoilage distribution visualization is used to analyze the agents decision profile and policy behavior. Extensive evaluations on simulated and real-time hardware data demonstrate that the LSTM and RNN based agent outperforms alternative reinforcement learning approaches in prediction accuracy and decision efficiency while maintaining interpretability. The results highlight the potential of hybrid deep reinforcement learning with integrated interpretability for scalable IoT-based food monitoring systems.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19361v1",
        "pdf": "https://arxiv.org/pdf/2512.19361v1"
      },
      "arxiv_id": "2512.19361v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19355v1",
      "title": "First-Order Representation Languages for Goal-Conditioned RL",
      "authors": [
        "Simon Ståhlberg",
        "Hector Geffner"
      ],
      "abstract": "First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19355v1",
        "pdf": "https://arxiv.org/pdf/2512.19355v1"
      },
      "arxiv_id": "2512.19355v1",
      "comment": "In Proceedings of the 40th AAAI Conference on Artificial Intelligence (AAAI 2026)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19354v1",
      "title": "ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining",
      "authors": [
        "Zhenyang Huang",
        "Xiao Yu",
        "Yi Zhang",
        "Decheng Wang",
        "Hang Ruan"
      ],
      "abstract": "Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users' CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users' implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users' implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19354v1",
        "pdf": "https://arxiv.org/pdf/2512.19354v1"
      },
      "arxiv_id": "2512.19354v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19350v1",
      "title": "PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models",
      "authors": [
        "A. B. M. Ashikur Rahman",
        "Saeed Anwar",
        "Muhammad Usman",
        "Irfan Ahmad",
        "Ajmal Mian"
      ],
      "abstract": "Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \\textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19350v1",
        "pdf": "https://arxiv.org/pdf/2512.19350v1"
      },
      "arxiv_id": "2512.19350v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19349v1",
      "title": "VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop",
      "authors": [
        "JiaWei Zhu",
        "ZiHeng Liu"
      ],
      "abstract": "Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19349v1",
        "pdf": "https://arxiv.org/pdf/2512.19349v1"
      },
      "arxiv_id": "2512.19349v1",
      "comment": "7 pages,1 figure,4 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19342v1",
      "title": "Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives",
      "authors": [
        "Kiril Dichev",
        "Filip Pawlowski",
        "Albert-Jan Yzelman"
      ],
      "abstract": "Recommender systems are enablers of personalized content delivery, and therefore revenue, for many large companies. In the last decade, deep learning recommender models (DLRMs) are the de-facto standard in this field. The main bottleneck in DLRM inference is the lookup of sparse features across huge embedding tables, which are usually partitioned across the aggregate RAM of many nodes. In state-of-the-art recommender systems, the distributed lookup is implemented via irregular all-to-all (alltoallv) communication, and often presents the main bottleneck. Today, most related work sees this operation as a given; in addition, every collective is synchronous in nature. In this work, we propose a novel bounded lag synchronous (BLS) version of the alltoallv operation. The bound can be a parameter allowing slower processes to lag behind entire iterations before the fastest processes block. In special applications such as inference-only DLRM, the accuracy of the application is fully preserved. We implement BLS alltoallv in a new PyTorch Distributed backend and evaluate it with a BLS version of the reference DLRM code. We show that for well balanced, homogeneous-access DLRM runs our BLS technique does not offer notable advantages. But for unbalanced runs, e.g. runs with strongly irregular embedding table accesses or with delays across different processes, our BLS technique improves both the latency and throughput of inference-only DLRM. In the best-case scenario, the proposed reduced synchronisation can mask the delays across processes altogether.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19342v1",
        "pdf": "https://arxiv.org/pdf/2512.19342v1"
      },
      "arxiv_id": "2512.19342v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19336v1",
      "title": "GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis",
      "authors": [
        "Siyuan Mei",
        "Yan Xia",
        "Fuxin Fan"
      ],
      "abstract": "The synthesis of computed tomography (CT) from magnetic resonance imaging (MRI) and cone-beam CT (CBCT) plays a critical role in clinical treatment planning by enabling accurate anatomical representation in adaptive radiotherapy. In this work, we propose GANeXt, a 3D patch-based, fully ConvNeXt-powered generative adversarial network for unified CT synthesis across different modalities and anatomical regions. Specifically, GANeXt employs an efficient U-shaped generator constructed from stacked 3D ConvNeXt blocks with compact convolution kernels, while the discriminator adopts a conditional PatchGAN. To improve synthesis quality, we incorporate a combination of loss functions, including mean absolute error (MAE), perceptual loss, segmentation-based masked MAE, and adversarial loss and a combination of Dice loss and cross-entropy for multi-head segmentation discriminator. For both tasks, training is performed with a batch size of 8 using two separate AdamW optimizers for the generator and discriminator, each equipped with a warmup and cosine decay scheduler, with learning rates of $5\\times10^{-4}$ and $1\\times10^{-3}$, respectively. Data preprocessing includes deformable registration, foreground cropping, percentile normalization for the input modality, and linear normalization of the CT to the range $[-1024, 1000]$. Data augmentation involves random zooming within $(0.8, 1.3)$ (for MRI-to-CT only), fixed-size cropping to $32\\times160\\times192$ for MRI-to-CT and $32\\times128\\times128$ for CBCT-to-CT, and random flipping. During inference, we apply a sliding-window approach with $0.8$ overlap and average folding to reconstruct the full-size sCT, followed by inversion of the CT normalization. After joint training on all regions without any fine-tuning, the final models are selected at the end of 3000 epochs for MRI-to-CT and 1000 epochs for CBCT-to-CT using the full training dataset.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19336v1",
        "pdf": "https://arxiv.org/pdf/2512.19336v1"
      },
      "arxiv_id": "2512.19336v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19334v1",
      "title": "Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models",
      "authors": [
        "Haohua Chen",
        "Songbin Liu",
        "Junjie Ma"
      ],
      "abstract": "We propose an orthogonal approximate message passing (OAMP) algorithm for signal estimation in the rectangular spiked matrix model with general rotationally invariant (RI) noise. We establish a rigorous state evolution that precisely characterizes the algorithm's high-dimensional dynamics and enables the construction of iteration-wise optimal denoisers. Within this framework, we accommodate spectral initializations under minimal assumptions on the empirical noise spectrum. In the rectangular setting, where a single rank-one component typically generates multiple informative outliers, we further propose a procedure for combining these outliers under mild non-Gaussian signal assumptions. For general RI noise models, the predicted performance of the proposed optimal OAMP algorithm agrees with replica-symmetric predictions for the associated Bayes-optimal estimator, and we conjecture that it is statistically optimal within a broad class of iterative estimation methods.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.IT",
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "cs.IT",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19334v1",
        "pdf": "https://arxiv.org/pdf/2512.19334v1"
      },
      "arxiv_id": "2512.19334v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19332v1",
      "title": "A Logical View of GNN-Style Computation and the Role of Activation Functions",
      "authors": [
        "Pablo Barceló",
        "Floris Geerts",
        "Matthias Lanzinger",
        "Klara Pakhomenko",
        "Jan Van den Bussche"
      ],
      "abstract": "We study the numerical and Boolean expressiveness of MPLang, a declarative language that captures the computation of graph neural networks (GNNs) through linear message passing and activation functions. We begin with A-MPLang, the fragment without activation functions, and give a characterization of its expressive power in terms of walk-summed features. For bounded activation functions, we show that (under mild conditions) all eventually constant activations yield the same expressive power - numerical and Boolean - and that it subsumes previously established logics for GNNs with eventually constant activation functions but without linear layers. Finally, we prove the first expressive separation between unbounded and bounded activations in the presence of linear layers: MPLang with ReLU is strictly more powerful for numerical queries than MPLang with eventually constant activation functions, e.g., truncated ReLU. This hinges on subtle interactions between linear aggregation and eventually constant non-linearities, and it establishes that GNNs using ReLU are more expressive than those restricted to eventually constant activations and linear layers.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19332v1",
        "pdf": "https://arxiv.org/pdf/2512.19332v1"
      },
      "arxiv_id": "2512.19332v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19331v1",
      "title": "DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis",
      "authors": [
        "Yueting Zhu",
        "Yuehao Song",
        "Shuai Zhang",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "abstract": "Whole Slide Images (WSIs) are typically analyzed using multiple instance learning (MIL) methods. However, the scale and heterogeneity of WSIs generate highly redundant and dispersed information, making it difficult to identify and integrate discriminative signals. Existing MIL methods either fail to discard uninformative cues effectively or have limited ability to consolidate relevant features from multiple patches, which restricts their performance on large and heterogeneous WSIs. To address this issue, we propose DeltaMIL, a novel MIL framework that explicitly selects semantically relevant regions and integrates the discriminative information from WSIs. Our method leverages the gated delta rule to efficiently filter and integrate information through a block combining forgetting and memory mechanisms. The delta mechanism dynamically updates the memory by removing old values and inserting new ones according to their correlation with the current patch. The gating mechanism further enables rapid forgetting of irrelevant signals. Additionally, DeltaMIL integrates a complementary local pattern mixing mechanism to retain fine-grained pathological locality. Our design enhances the extraction of meaningful cues and suppresses redundant or noisy information, which improves the model's robustness and discriminative power. Experiments demonstrate that DeltaMIL achieves state-of-the-art performance. Specifically, for survival prediction, DeltaMIL improves performance by 3.69\\% using ResNet-50 features and 2.36\\% using UNI features. For slide-level classification, it increases accuracy by 3.09\\% with ResNet-50 features and 3.75\\% with UNI features. These results demonstrate the strong and consistent performance of DeltaMIL across diverse WSI tasks.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19331v1",
        "pdf": "https://arxiv.org/pdf/2512.19331v1"
      },
      "arxiv_id": "2512.19331v1",
      "comment": "11 pages,7 figures,8 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19327v1",
      "title": "Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome",
      "authors": [
        "Moamal Fadhil Abdul",
        "Jonas Bruun Hubrechts",
        "Thomas Martini Jørgensen",
        "Emil Hovad"
      ],
      "abstract": "Automatically detecting and classifying strokes in table tennis video can streamline training workflows, enrich broadcast overlays, and enable fine-grained performance analytics. For this to be possible, annotated video data of table tennis is needed. We extend the public OpenTTGames dataset with highly detailed, frame-accurate shot type annotations (forehand, backhand with subtypes), player posture labels (body lean and leg stance), and rally outcome tags at point end. OpenTTGames is a set of recordings from the side of the table with official labels for bounces, when the ball is above the net, or hitting the net. The dataset already contains ball coordinates near events, which are either \"bounce\", \"net\", or \"empty_event\" in the original OpenTTGames dataset, and semantic masks (humans, table, scoreboard). Our extension adds the types of stroke to the events and a per-player taxonomy so models can move beyond event spotting toward tactical understanding (e.g., whether a stroke is likely to win the point or set up an advantage). We provide a compact coding scheme and code-assisted labeling procedure to support reproducible annotations and baselines for fine-grained stroke understanding in racket sports. This fills a practical gap in the community, where many prior video resources are either not publicly released or carry restrictive/unclear licenses that hinder reuse and benchmarking. Our annotations are released under the same CC BY-NC-SA 4.0 license as OpenTTGames, allowing free non-commercial use, modification, and redistribution, with appropriate attribution.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19327v1",
        "pdf": "https://arxiv.org/pdf/2512.19327v1"
      },
      "arxiv_id": "2512.19327v1",
      "comment": "Thomas Martini Jørgensen and Emil Hovad contributed equally and share last authorship",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2512.19323v1",
      "title": "Alternative positional encoding functions for neural transformers",
      "authors": [
        "Ezequiel Lopez-Rubio",
        "Macoris Decena-Gimenez",
        "Rafael Marcos Luque-Baena"
      ],
      "abstract": "A key module in neural transformer-based deep architectures is positional encoding. This module enables a suitable way to encode positional information as input for transformer neural layers. This success has been rooted in the use of sinusoidal functions of various frequencies, in order to capture recurrent patterns of differing typical periods. In this work, an alternative set of periodic functions is proposed for positional encoding. These functions preserve some key properties of sinusoidal ones, while they depart from them in fundamental ways. Some tentative experiments are reported, where the original sinusoidal version is substantially outperformed. This strongly suggests that the alternative functions may have a wider use in other transformer architectures.",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.19323v1",
        "pdf": "https://arxiv.org/pdf/2512.19323v1"
      },
      "arxiv_id": "2512.19323v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    }
  ]
}