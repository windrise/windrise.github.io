{
  "fetched_at": "2026-02-04T00:31:01.402026",
  "total_papers": 100,
  "papers": [
    {
      "id": "2602.02495v1",
      "title": "Reward-free Alignment for Conflicting Objectives",
      "authors": [
        "Peter Chen",
        "Xiaopeng Li",
        "Xi Chen",
        "Tianyi Lin"
      ],
      "abstract": "Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02495v1",
        "pdf": "https://arxiv.org/pdf/2602.02495v1"
      },
      "arxiv_id": "2602.02495v1",
      "comment": "27 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02494v1",
      "title": "MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training",
      "authors": [
        "Dulhan Jayalath",
        "Oiwi Parker Jones"
      ],
      "abstract": "Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02494v1",
        "pdf": "https://arxiv.org/pdf/2602.02494v1"
      },
      "arxiv_id": "2602.02494v1",
      "comment": "19 pages, 8 figures, 5 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02493v1",
      "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
      "authors": [
        "Zehong Ma",
        "Ruihan Xu",
        "Shiliang Zhang"
      ],
      "abstract": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02493v1",
        "pdf": "https://arxiv.org/pdf/2602.02493v1"
      },
      "arxiv_id": "2602.02493v1",
      "comment": "Project Pages: https://zehong-ma.github.io/PixelGen/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2602.02488v1",
      "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",
      "authors": [
        "Yinjie Wang",
        "Tianbao Xie",
        "Ke Shen",
        "Mengdi Wang",
        "Ling Yang"
      ],
      "abstract": "We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02488v1",
        "pdf": "https://arxiv.org/pdf/2602.02488v1"
      },
      "arxiv_id": "2602.02488v1",
      "comment": "Code: https://github.com/Gen-Verse/Open-AgentRL",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2602.02486v1",
      "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
      "authors": [
        "Jialiang Zhu",
        "Gongrui Zhang",
        "Xiaolong Ma",
        "Lin Xu",
        "Miaosen Zhang",
        "Ruiqi Yang",
        "Song Wang",
        "Kai Qiu",
        "Zhirong Wu",
        "Qi Dai",
        "Ruichun Ma",
        "Bei Liu",
        "Yifan Yang",
        "Chong Luo",
        "Zhengyuan Yang",
        "Linjie Li",
        "Lijuan Wang",
        "Weizhu Chen",
        "Xin Geng",
        "Baining Guo"
      ],
      "abstract": "LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02486v1",
        "pdf": "https://arxiv.org/pdf/2602.02486v1"
      },
      "arxiv_id": "2602.02486v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02482v1",
      "title": "Expanding the Capabilities of Reinforcement Learning via Text Feedback",
      "authors": [
        "Yuda Song",
        "Lili Chen",
        "Fahim Tajwar",
        "Remi Munos",
        "Deepak Pathak",
        "J. Andrew Bagnell",
        "Aarti Singh",
        "Andrea Zanette"
      ],
      "abstract": "The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02482v1",
        "pdf": "https://arxiv.org/pdf/2602.02482v1"
      },
      "arxiv_id": "2602.02482v1",
      "comment": "43 pages, 6 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02481v1",
      "title": "Flow Policy Gradients for Robot Control",
      "authors": [
        "Brent Yi",
        "Hongsuk Choi",
        "Himanshu Gaurav Singh",
        "Xiaoyu Huang",
        "Takara E. Truong",
        "Carmelo Sferrazza",
        "Yi Ma",
        "Rocky Duan",
        "Pieter Abbeel",
        "Guanya Shi",
        "Karen Liu",
        "Angjoo Kanazawa"
      ],
      "abstract": "Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02481v1",
        "pdf": "https://arxiv.org/pdf/2602.02481v1"
      },
      "arxiv_id": "2602.02481v1",
      "comment": "Project webpage: https://hongsukchoi.github.io/fpo-control",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2602.02475v1",
      "title": "AgentRx: Diagnosing AI Agent Failures from Execution Trajectories",
      "authors": [
        "Shraddha Barke",
        "Arnav Goyal",
        "Alind Khare",
        "Avaljot Singh",
        "Suman Nath",
        "Chetan Bansal"
      ],
      "abstract": "AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02475v1",
        "pdf": "https://arxiv.org/pdf/2602.02475v1"
      },
      "arxiv_id": "2602.02475v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02474v1",
      "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents",
      "authors": [
        "Haozhen Zhang",
        "Quanyu Long",
        "Jianzhu Bao",
        "Tao Feng",
        "Weizhi Zhang",
        "Haodong Yue",
        "Wenya Wang"
      ],
      "abstract": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \\emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \\emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \\emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02474v1",
        "pdf": "https://arxiv.org/pdf/2602.02474v1"
      },
      "arxiv_id": "2602.02474v1",
      "comment": "Code is available at https://github.com/ViktorAxelsen/MemSkill",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2602.02473v1",
      "title": "HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos",
      "authors": [
        "Yinhuai Wang",
        "Qihan Zhao",
        "Yuen Fui Lau",
        "Runyi Yu",
        "Hok Wai Tsui",
        "Qifeng Chen",
        "Jingbo Wang",
        "Jiangmiao Pang",
        "Ping Tan"
      ],
      "abstract": "Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02473v1",
        "pdf": "https://arxiv.org/pdf/2602.02473v1"
      },
      "arxiv_id": "2602.02473v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02472v1",
      "title": "SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning",
      "authors": [
        "Qifan Yu",
        "Xinyu Ma",
        "Zhijian Zhuo",
        "Minrui Wang",
        "Deyi Liu",
        "Shiyi Zhan",
        "Yiyuan Ma",
        "Liang Xiang",
        "Xingyan Bin",
        "Di He"
      ],
      "abstract": "Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\\times$ width expansion.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02472v1",
        "pdf": "https://arxiv.org/pdf/2602.02472v1"
      },
      "arxiv_id": "2602.02472v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02471v1",
      "title": "Multi-head automated segmentation by incorporating detection head into the contextual layer neural network",
      "authors": [
        "Edwin Kys",
        "Febian Febian"
      ],
      "abstract": "Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \\pm 0.036$ versus $0.732 \\pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV",
        "cs.AI",
        "physics.med-ph"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02471v1",
        "pdf": "https://arxiv.org/pdf/2602.02471v1"
      },
      "arxiv_id": "2602.02471v1",
      "comment": "8 pages, 3 figures, 1 table",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02470v1",
      "title": "Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge",
      "authors": [
        "Xutao Ma",
        "Yixiao Huang",
        "Hanlin Zhu",
        "Somayeh Sojoudi"
      ],
      "abstract": "Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the \"reversal curse\" -- when trained on forward knowledge data of the form \"$A \\rightarrow B$\" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge \"$B \\leftarrow A$\" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form \"$A \\to A$\" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02470v1",
        "pdf": "https://arxiv.org/pdf/2602.02470v1"
      },
      "arxiv_id": "2602.02470v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02469v1",
      "title": "Age-Aware Edge-Blind Federated Learning via Over-the-Air Aggregation",
      "authors": [
        "Ahmed M. Elshazly",
        "Ahmed Arafa"
      ],
      "abstract": "We study federated learning (FL) over wireless fading channels where multiple devices simultaneously send their model updates. We propose an efficient \\emph{age-aware edge-blind over-the-air FL} approach that does not require channel state information (CSI) at the devices. Instead, the parameter server (PS) uses multiple antennas and applies maximum-ratio combining (MRC) based on its estimated sum of the channel gains to detect the parameter updates. A key challenge is that the number of orthogonal subcarriers is limited; thus, transmitting many parameters requires multiple Orthogonal Frequency Division Multiplexing (OFDM) symbols, which increases latency. To address this, the PS selects only a small subset of model coordinates each round using \\emph{AgeTop-\\(k\\)}, which first picks the largest-magnitude entries and then chooses the \\(k\\) coordinates with the longest waiting times since they were last selected. This ensures that all selected parameters fit into a single OFDM symbol, reducing latency. We provide a convergence bound that highlights the advantages of using a higher number of antenna array elements and demonstrates a key trade-off: increasing \\(k\\) decreases compression error at the cost of increasing the effect of channel noise. Experimental results show that (i) more PS antennas greatly improve accuracy and convergence speed; (ii) AgeTop-\\(k\\) outperforms random selection under relatively good channel conditions; and (iii) the optimum \\(k\\) depends on the channel, with smaller \\(k\\) being better in noisy settings.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.IT",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.IT",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02469v1",
        "pdf": "https://arxiv.org/pdf/2602.02469v1"
      },
      "arxiv_id": "2602.02469v1",
      "comment": "To appear in IEEE ICC 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02468v1",
      "title": "Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts",
      "authors": [
        "Aiden Yiliu Li",
        "Xinyue Hao",
        "Shilong Liu",
        "Mengdi Wang"
      ],
      "abstract": "Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02468v1",
        "pdf": "https://arxiv.org/pdf/2602.02468v1"
      },
      "arxiv_id": "2602.02468v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02465v1",
      "title": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
      "authors": [
        "Jana Zeller",
        "Thaddäus Wiedemer",
        "Fanfei Li",
        "Thomas Klein",
        "Prasanna Mayilvahanan",
        "Matthias Bethge",
        "Felix Wichmann",
        "Ryan Cotterell",
        "Wieland Brendel"
      ],
      "abstract": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02465v1",
        "pdf": "https://arxiv.org/pdf/2602.02465v1"
      },
      "arxiv_id": "2602.02465v1",
      "comment": "9 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02462v1",
      "title": "Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models",
      "authors": [
        "Gabriele Maraia",
        "Marco Valentino",
        "Fabio Massimo Zanzotto",
        "Leonardo Ranaldi"
      ],
      "abstract": "Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02462v1",
        "pdf": "https://arxiv.org/pdf/2602.02462v1"
      },
      "arxiv_id": "2602.02462v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02458v1",
      "title": "Conflict-Aware Client Selection for Multi-Server Federated Learning",
      "authors": [
        "Mingwei Hong",
        "Zheng Lin",
        "Zehang Lin",
        "Lin Li",
        "Miao Yang",
        "Xia Du",
        "Zihan Fang",
        "Zhaolu Kang",
        "Dianxin Luan",
        "Shunzhi Zhu"
      ],
      "abstract": "Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02458v1",
        "pdf": "https://arxiv.org/pdf/2602.02458v1"
      },
      "arxiv_id": "2602.02458v1",
      "comment": "6 pages, 4 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02455v1",
      "title": "Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction",
      "authors": [
        "Han Bao",
        "Zheyuan Zhang",
        "Pengcheng Jing",
        "Zhengqing Yuan",
        "Kaiwen Shi",
        "Yanfang Ye"
      ],
      "abstract": "As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \\textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \\textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \\textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \\MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02455v1",
        "pdf": "https://arxiv.org/pdf/2602.02455v1"
      },
      "arxiv_id": "2602.02455v1",
      "comment": "65 pages, 40 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02454v1",
      "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model",
      "authors": [
        "Ansh Kumar Sharma",
        "Yixiang Sun",
        "Ninghao Lu",
        "Yunzhe Zhang",
        "Jiarao Liu",
        "Sherry Yang"
      ],
      "abstract": "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02454v1",
        "pdf": "https://arxiv.org/pdf/2602.02454v1"
      },
      "arxiv_id": "2602.02454v1",
      "comment": "https://world-gymnast.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2602.02453v1",
      "title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling",
      "authors": [
        "Andong Chen",
        "Wenxin Zhu",
        "Qiuyu Ding",
        "Yuchen Song",
        "Muyun Yang",
        "Tiejun Zhao"
      ],
      "abstract": "Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02453v1",
        "pdf": "https://arxiv.org/pdf/2602.02453v1"
      },
      "arxiv_id": "2602.02453v1",
      "comment": "Working paper",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02451v1",
      "title": "Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization",
      "authors": [
        "Patrick Cooper",
        "Alvaro Velasquez"
      ],
      "abstract": "Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02451v1",
        "pdf": "https://arxiv.org/pdf/2602.02451v1"
      },
      "arxiv_id": "2602.02451v1",
      "comment": "9 pages, 5 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02445v1",
      "title": "Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation",
      "authors": [
        "Seo Taek Kong",
        "R. Srikant"
      ],
      "abstract": "This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.\n  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $γ_n^{1/6}$, where $γ_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov's inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02445v1",
        "pdf": "https://arxiv.org/pdf/2602.02445v1"
      },
      "arxiv_id": "2602.02445v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02444v1",
      "title": "RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval",
      "authors": [
        "Tyler Skow",
        "Alexander Martin",
        "Benjamin Van Durme",
        "Rama Chellappa",
        "Reno Kriz"
      ],
      "abstract": "Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.IR",
        "cs.CV"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02444v1",
        "pdf": "https://arxiv.org/pdf/2602.02444v1"
      },
      "arxiv_id": "2602.02444v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02443v1",
      "title": "Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE",
      "authors": [
        "Yuanteng Chen",
        "Peisong Wang",
        "Nanxin Zeng",
        "Yuantian Shao",
        "Gang Li",
        "Jing Liu",
        "Jian Cheng"
      ],
      "abstract": "Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02443v1",
        "pdf": "https://arxiv.org/pdf/2602.02443v1"
      },
      "arxiv_id": "2602.02443v1",
      "comment": "24 pages, 13 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02439v1",
      "title": "Energy-Efficient Neuromorphic Computing for Edge AI: A Framework with Adaptive Spiking Neural Networks and Hardware-Aware Optimization",
      "authors": [
        "Olaf Yunus Laitinen Imanov",
        "Derya Umut Kulali",
        "Taner Yilmaz",
        "Duygu Erisken",
        "Rana Irem Turhan"
      ],
      "abstract": "Edge AI applications increasingly require ultra-low-power, low-latency inference. Neuromorphic computing based on event-driven spiking neural networks (SNNs) offers an attractive path, but practical deployment on resource-constrained devices is limited by training difficulty, hardware-mapping overheads, and sensitivity to temporal dynamics. We present NeuEdge, a framework that combines adaptive SNN models with hardware-aware optimization for edge deployment. NeuEdge uses a temporal coding scheme that blends rate and spike-timing patterns to reduce spike activity while preserving accuracy, and a hardware-aware training procedure that co-optimizes network structure and on-chip placement to improve utilization on neuromorphic processors. An adaptive threshold mechanism adjusts neuron excitability from input statistics, reducing energy consumption without degrading performance. Across standard vision and audio benchmarks, NeuEdge achieves 91-96% accuracy with up to 2.3 ms inference latency on edge hardware and an estimated 847 GOp/s/W energy efficiency. A case study on an autonomous-drone workload shows up to 312x energy savings relative to conventional deep neural networks while maintaining real-time operation.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.NE",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02439v1",
        "pdf": "https://arxiv.org/pdf/2602.02439v1"
      },
      "arxiv_id": "2602.02439v1",
      "comment": "8 pages, 4 figures, 4 tables. Submitted to IEEE Transactions on Neural Networks and Learning Systems (TNNLS)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02437v1",
      "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
      "authors": [
        "Dianyi Wang",
        "Chaofan Ma",
        "Feng Han",
        "Size Wu",
        "Wei Song",
        "Yibin Wang",
        "Zhixiong Zhang",
        "Tianhang Wang",
        "Siyuan Wang",
        "Zhongyu Wei",
        "Jiaqi Wang"
      ],
      "abstract": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02437v1",
        "pdf": "https://arxiv.org/pdf/2602.02437v1"
      },
      "arxiv_id": "2602.02437v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02432v1",
      "title": "Maximizing Reliability with Bayesian Optimization",
      "authors": [
        "Jack M. Buckingham",
        "Ivo Couckuyt",
        "Juergen Branke"
      ],
      "abstract": "Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02432v1",
        "pdf": "https://arxiv.org/pdf/2602.02432v1"
      },
      "arxiv_id": "2602.02432v1",
      "comment": "25 pages, 9 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02431v1",
      "title": "Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation in Single-Index Learning",
      "authors": [
        "Filip Kovačević",
        "Hong Chang Ji",
        "Denny Wu",
        "Mahdi Soltanolkotabi",
        "Marco Mondelli"
      ],
      "abstract": "It is folklore that reusing training data more than once can improve the statistical efficiency of gradient-based learning. However, beyond linear regression, the theoretical advantage of full-batch gradient descent (GD, which always reuses all the data) over one-pass stochastic gradient descent (online SGD, which uses each data point only once) remains unclear. In this work, we consider learning a $d$-dimensional single-index model with a quadratic activation, for which it is known that one-pass SGD requires $n\\gtrsim d\\log d$ samples to achieve weak recovery. We first show that this $\\log d$ factor in the sample complexity persists for full-batch spherical GD on the correlation loss; however, by simply truncating the activation, full-batch GD exhibits a favorable optimization landscape at $n \\simeq d$ samples, thereby outperforming one-pass SGD (with the same activation) in statistical efficiency. We complement this result with a trajectory analysis of full-batch GD on the squared loss from small initialization, showing that $n \\gtrsim d$ samples and $T \\gtrsim\\log d$ gradient steps suffice to achieve strong (exact) recovery.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02431v1",
        "pdf": "https://arxiv.org/pdf/2602.02431v1"
      },
      "arxiv_id": "2602.02431v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02427v1",
      "title": "Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning",
      "authors": [
        "Qihao Wen",
        "Jiahao Wang",
        "Yang Nan",
        "Pengfei He",
        "Ravi Tandon",
        "Han Xu"
      ],
      "abstract": "Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02427v1",
        "pdf": "https://arxiv.org/pdf/2602.02427v1"
      },
      "arxiv_id": "2602.02427v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02426v1",
      "title": "SelvaMask: Segmenting Trees in Tropical Forests and Beyond",
      "authors": [
        "Simon-Olivier Duguay",
        "Hugo Baudchon",
        "Etienne Laliberté",
        "Helene Muller-Landau",
        "Gonzalo Rivas-Torres",
        "Arthur Ouaknine"
      ],
      "abstract": "Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02426v1",
        "pdf": "https://arxiv.org/pdf/2602.02426v1"
      },
      "arxiv_id": "2602.02426v1",
      "comment": "22 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02425v1",
      "title": "Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization",
      "authors": [
        "Amaru Caceres Arroyo",
        "Lea Bogensperger",
        "Ahmed Allam",
        "Michael Krauthammer",
        "Konrad Schindler",
        "Dominik Narnhofer"
      ],
      "abstract": "Protein fitness optimization is challenged by a vast combinatorial landscape where high-fitness variants are extremely sparse. Many current methods either underperform or require computationally expensive gradient-based sampling. We present CHASE, a framework that repurposes the evolutionary knowledge of pretrained protein language models by compressing their embeddings into a compact latent space. By training a conditional flow-matching model with classifier-free guidance, we enable the direct generation of high-fitness variants without predictor-based guidance during the ODE sampling steps. CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks. Finally, we show that bootstrapping with synthetic data can further enhance performance in data-constrained settings.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02425v1",
        "pdf": "https://arxiv.org/pdf/2602.02425v1"
      },
      "arxiv_id": "2602.02425v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02422v1",
      "title": "Poly-attention: a general scheme for higher-order self-attention",
      "authors": [
        "Sayak Chakrabarti",
        "Toniann Pitassi",
        "Josh Alman"
      ],
      "abstract": "The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.\n  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.\n  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02422v1",
        "pdf": "https://arxiv.org/pdf/2602.02422v1"
      },
      "arxiv_id": "2602.02422v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02419v1",
      "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration",
      "authors": [
        "Qingni Wang",
        "Yue Fan",
        "Xin Eric Wang"
      ],
      "abstract": "Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\\% percentage points over Gemini-only inference.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02419v1",
        "pdf": "https://arxiv.org/pdf/2602.02419v1"
      },
      "arxiv_id": "2602.02419v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02417v1",
      "title": "Trust Region Continual Learning as an Implicit Meta-Learner",
      "authors": [
        "Zekun Wang",
        "Anant Gupta",
        "Christopher J. MacLellan"
      ],
      "abstract": "Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \\emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \\emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02417v1",
        "pdf": "https://arxiv.org/pdf/2602.02417v1"
      },
      "arxiv_id": "2602.02417v1",
      "comment": "19 pages, 23 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02416v1",
      "title": "Structure Enables Effective Self-Localization of Errors in LLMs",
      "authors": [
        "Ankur Samanta",
        "Akshayaa Magesh",
        "Ayush Jain",
        "Kavosh Asadi",
        "Youliang Yu",
        "Daniel Jiang",
        "Boris Vidolov",
        "Kaveh Hassani",
        "Paul Sajda",
        "Jalaj Bhandari",
        "Yonathan Efroni"
      ],
      "abstract": "Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02416v1",
        "pdf": "https://arxiv.org/pdf/2602.02416v1"
      },
      "arxiv_id": "2602.02416v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02415v1",
      "title": "Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models",
      "authors": [
        "Vivienne Pelletier",
        "Daniel J. Rivera",
        "Obinna Nwokonkwo",
        "Steven A. Wilson",
        "Christopher L. Muhich"
      ],
      "abstract": "Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02415v1",
        "pdf": "https://arxiv.org/pdf/2602.02415v1"
      },
      "arxiv_id": "2602.02415v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02414v1",
      "title": "Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank",
      "authors": [
        "Joshua Mitton",
        "Prarthana Bhattacharyya",
        "Digory Smith",
        "Thomas Christie",
        "Ralph Abboud",
        "Simon Woodhead"
      ],
      "abstract": "Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02414v1",
        "pdf": "https://arxiv.org/pdf/2602.02414v1"
      },
      "arxiv_id": "2602.02414v1",
      "comment": "21 pages, 8 figures, 8 tables. Joshua Mitton and Prarthana Bhattacharyya contributed equally to this paper",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02413v1",
      "title": "Masked Autoencoders as Universal Speech Enhancer",
      "authors": [
        "Rajalaxmi Rajagopalan",
        "Ritwik Giri",
        "Zhiqiang Tang",
        "Kyu Han"
      ],
      "abstract": "Supervised speech enhancement methods have been very successful. However, in practical scenarios, there is a lack of clean speech, and self-supervised learning-based (SSL) speech enhancement methods that offer comparable enhancement performance and can be applied to other speech-related downstream applications are desired. In this work, we develop a masked autoencoder based universal speech enhancer that is agnostic to the type of distortion affecting speech, can handle multiple distortions simultaneously, and is trained in a self-supervised manner. An augmentation stack adds further distortions to the noisy input data. The masked autoencoder model learns to remove the added distortions along with reconstructing the masked regions of the spectrogram during pre-training. The pre-trained embeddings are then used by fine-tuning models trained on a small amount of paired data for specific downstream tasks. We evaluate the pre-trained features for denoising and dereverberation downstream tasks. We explore different augmentations (like single or multi-speaker) in the pre-training augmentation stack and the effect of different noisy input feature representations (like $log1p$ compression) on pre-trained embeddings and downstream fine-tuning enhancement performance. We show that the proposed method not only outperforms the baseline but also achieves state-of-the-art performance for both in-domain and out-of-domain evaluation datasets.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02413v1",
        "pdf": "https://arxiv.org/pdf/2602.02413v1"
      },
      "arxiv_id": "2602.02413v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02409v1",
      "title": "Catalyst: Out-of-Distribution Detection via Elastic Scaling",
      "authors": [
        "Abid Hassan",
        "Tuan Ngo",
        "Saad Shafiq",
        "Nenad Medvidovic"
      ],
      "abstract": "Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($γ$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $γ$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02409v1",
        "pdf": "https://arxiv.org/pdf/2602.02409v1"
      },
      "arxiv_id": "2602.02409v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02408v1",
      "title": "ReasonEdit: Editing Vision-Language Models using Human Reasoning",
      "authors": [
        "Jiaxing Qiu",
        "Kaihua Hou",
        "Roxana Daneshjou",
        "Ahmed Alaa",
        "Thomas Hartvigsen"
      ],
      "abstract": "Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02408v1",
        "pdf": "https://arxiv.org/pdf/2602.02408v1"
      },
      "arxiv_id": "2602.02408v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02406v1",
      "title": "Provably Data-driven Multiple Hyper-parameter Tuning with Structured Loss Function",
      "authors": [
        "Tung Quoc Le",
        "Anh Tuan Nguyen",
        "Viet Anh Nguyen"
      ],
      "abstract": "Data-driven algorithm design automates hyperparameter tuning, but its statistical foundations remain limited because model performance can depend on hyperparameters in implicit and highly non-smooth ways. Existing guarantees focus on the simple case of a one-dimensional (scalar) hyperparameter. This leaves the practically important, multi-dimensional hyperparameter tuning setting unresolved. We address this open question by establishing the first general framework for establishing generalization guarantees for tuning multi-dimensional hyperparameters in data-driven settings. Our approach strengthens the generalization guarantee framework for semi-algebraic function classes by exploiting tools from real algebraic geometry, yielding sharper, more broadly applicable guarantees. We then extend the analysis to hyperparameter tuning using the validation loss under minimal assumptions, and derive improved bounds when additional structure is available. Finally, we demonstrate the scope of the framework with new learnability results, including data-driven weighted group lasso and weighted fused lasso.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02406v1",
        "pdf": "https://arxiv.org/pdf/2602.02406v1"
      },
      "arxiv_id": "2602.02406v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02405v1",
      "title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning",
      "authors": [
        "Ethan Mendes",
        "Jungsoo Park",
        "Alan Ritter"
      ],
      "abstract": "Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02405v1",
        "pdf": "https://arxiv.org/pdf/2602.02405v1"
      },
      "arxiv_id": "2602.02405v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02402v1",
      "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation",
      "authors": [
        "Mu Huang",
        "Hui Wang",
        "Kerui Ren",
        "Linning Xu",
        "Yunsong Zhou",
        "Mulin Yu",
        "Bo Dai",
        "Jiangmiao Pang"
      ],
      "abstract": "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "physics.app-ph"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02402v1",
        "pdf": "https://arxiv.org/pdf/2602.02402v1"
      },
      "arxiv_id": "2602.02402v1",
      "comment": "Project page: https://city-super.github.io/SoMA/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2602.02401v1",
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "authors": [
        "Xinshun Wang",
        "Peiming Li",
        "Ziyi Wang",
        "Zhongbin Fang",
        "Zhichao Deng",
        "Songtao Wu",
        "Jason Li",
        "Mengyuan Liu"
      ],
      "abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception'' models that understand motion from video but only output text, and ``generation'' models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02401v1",
        "pdf": "https://arxiv.org/pdf/2602.02401v1"
      },
      "arxiv_id": "2602.02401v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02400v1",
      "title": "An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence",
      "authors": [
        "Qizhen Zhang",
        "Ankush Garg",
        "Jakob Foerster",
        "Niladri Chatterji",
        "Kshitiz Malik",
        "Mike Lewis"
      ],
      "abstract": "Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02400v1",
        "pdf": "https://arxiv.org/pdf/2602.02400v1"
      },
      "arxiv_id": "2602.02400v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02396v1",
      "title": "PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning",
      "authors": [
        "Amisha Bhaskar",
        "Pratap Tokekar",
        "Stefano Di Cairano",
        "Alexander Schperberg"
      ],
      "abstract": "Robotic imitation learning typically requires models that capture multimodal action distributions while operating at real-time control rates and accommodating multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation (IMLE) have achieved promising results, they often satisfy only a subset of these requirements. To address this, we introduce PRISM, a single-pass policy based on a batch-global rejection-sampling variant of IMLE. PRISM couples a temporal multisensory encoder (integrating RGB, depth, tactile, audio, and proprioception) with a linear-attention generator using a Performer architecture. We demonstrate the efficacy of PRISM on a diverse real-world hardware suite, including loco-manipulation using a Unitree Go2 with a 7-DoF arm D1 and tabletop manipulation with a UR5 manipulator. Across challenging physical tasks such as pre-manipulation parking, high-precision insertion, and multi-object pick-and-place, PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency (30-50 Hz) closed-loop control. We further validate our approach on large-scale simulation benchmarks, including CALVIN, MetaWorld, and Robomimic. In CALVIN (10% data split), PRISM improves success rates by approximately 25% over diffusion and approximately 20% over flow matching, while simultaneously reducing trajectory jerk by 20x-50x. These results position PRISM as a fast, accurate, and multisensory imitation policy that retains multimodal action coverage without the latency of iterative sampling.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02396v1",
        "pdf": "https://arxiv.org/pdf/2602.02396v1"
      },
      "arxiv_id": "2602.02396v1",
      "comment": "10 pages main text and 4 figures, and 11 pages appendix and 10 figures, total 21 pages and 14 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02395v1",
      "title": "David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning",
      "authors": [
        "Samuel Nellessen",
        "Tal Kachman"
      ],
      "abstract": "The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary \"tags along\" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02395v1",
        "pdf": "https://arxiv.org/pdf/2602.02395v1"
      },
      "arxiv_id": "2602.02395v1",
      "comment": "Under review. 8 main pages, 2 figures, 2 tables. Appendix included",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02393v1",
      "title": "Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory",
      "authors": [
        "Ruiqi Wu",
        "Xuanhua He",
        "Meng Cheng",
        "Tianyu Yang",
        "Yong Zhang",
        "Zhuoliang Kang",
        "Xunliang Cai",
        "Xiaoming Wei",
        "Chunle Guo",
        "Chongyi Li",
        "Ming-Ming Cheng"
      ],
      "abstract": "We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02393v1",
        "pdf": "https://arxiv.org/pdf/2602.02393v1"
      },
      "arxiv_id": "2602.02393v1",
      "comment": "14 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02388v1",
      "title": "Personalized Image Generation via Human-in-the-loop Bayesian Optimization",
      "authors": [
        "Rajalaxmi Rajagopalan",
        "Debottam Dutta",
        "Yu-Lin Wei",
        "Romit Roy Choudhury"
      ],
      "abstract": "Imagine Alice has a specific image $x^\\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\\ast$, even though the generative model has no information about $x^\\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02388v1",
        "pdf": "https://arxiv.org/pdf/2602.02388v1"
      },
      "arxiv_id": "2602.02388v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02386v1",
      "title": "Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing",
      "authors": [
        "Mika Okamoto",
        "Ansel Kaplan Erol",
        "Glenn Matlin"
      ],
      "abstract": "How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02386v1",
        "pdf": "https://arxiv.org/pdf/2602.02386v1"
      },
      "arxiv_id": "2602.02386v1",
      "comment": "Appeared at MLSys YPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02385v1",
      "title": "Transformers learn factored representations",
      "authors": [
        "Adam Shai",
        "Loren Amdahl-Culleton",
        "Casper L. Christensen",
        "Henry R. Bigelow",
        "Fernando E. Rosas",
        "Alexander B. Boyd",
        "Eric A. Alt",
        "Kyle J. Ray",
        "Paul M. Riechers"
      ],
      "abstract": "Transformers pretrained via next token prediction learn to factor their world into parts, representing these factors in orthogonal subspaces of the residual stream. We formalize two representational hypotheses: (1) a representation in the product space of all factors, whose dimension grows exponentially with the number of parts, or (2) a factored representation in orthogonal subspaces, whose dimension grows linearly. The factored representation is lossless when factors are conditionally independent, but sacrifices predictive fidelity otherwise, creating a tradeoff between dimensional efficiency and accuracy. We derive precise predictions about the geometric structure of activations for each, including the number of subspaces, their dimensionality, and the arrangement of context embeddings within them. We test between these hypotheses on transformers trained on synthetic processes with known latent structure. Models learn factored representations when factors are conditionally independent, and continue to favor them early in training even when noise or hidden dependencies undermine conditional independence, reflecting an inductive bias toward factoring at the cost of fidelity. This provides a principled explanation for why transformers decompose the world into parts, and suggests that interpretable low dimensional structure may persist even in models trained on complex data.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02385v1",
        "pdf": "https://arxiv.org/pdf/2602.02385v1"
      },
      "arxiv_id": "2602.02385v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02383v1",
      "title": "SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization",
      "authors": [
        "Maksim Afanasyev",
        "Illarion Iov"
      ],
      "abstract": "Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02383v1",
        "pdf": "https://arxiv.org/pdf/2602.02383v1"
      },
      "arxiv_id": "2602.02383v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02381v1",
      "title": "Self-Supervised Learning from Structural Invariance",
      "authors": [
        "Yipeng Zhang",
        "Hafez Ghaemi",
        "Jungyoon Lee",
        "Shahab Bakhtiari",
        "Eilif B. Muller",
        "Laurent Charlin"
      ],
      "abstract": "Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02381v1",
        "pdf": "https://arxiv.org/pdf/2602.02381v1"
      },
      "arxiv_id": "2602.02381v1",
      "comment": "ICLR 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02380v1",
      "title": "Unified Personalized Reward Model for Vision Generation",
      "authors": [
        "Yibin Wang",
        "Yuhang Zang",
        "Feng Han",
        "Jiazi Bu",
        "Yujie Zhou",
        "Cheng Jin",
        "Jiaqi Wang"
      ],
      "abstract": "Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02380v1",
        "pdf": "https://arxiv.org/pdf/2602.02380v1"
      },
      "arxiv_id": "2602.02380v1",
      "comment": "Website: https://codegoat24.github.io/UnifiedReward/flex",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2602.02378v1",
      "title": "From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making",
      "authors": [
        "Raunak Jain",
        "Mudita Khurana",
        "John Stephens",
        "Srinivas Dharmasanam",
        "Shankar Venkataraman"
      ],
      "abstract": "As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02378v1",
        "pdf": "https://arxiv.org/pdf/2602.02378v1"
      },
      "arxiv_id": "2602.02378v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02371v1",
      "title": "C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference",
      "authors": [
        "Jing Wang",
        "Jie Shen",
        "Qiaomin Xie",
        "Jeremy C Weiss"
      ],
      "abstract": "Estimating causal effects from longitudinal trajectories is central to understanding the progression of complex conditions and optimizing clinical decision-making, such as comorbidities and long COVID recovery. We introduce \\emph{C-kNN--LSH}, a nearest-neighbor framework for sequential causal inference designed to handle such high-dimensional, confounded situations. By utilizing locality-sensitive hashing, we efficiently identify ``clinical twins'' with similar covariate histories, enabling local estimation of conditional treatment effects across evolving disease states. To mitigate bias from irregular sampling and shifting patient recovery profiles, we integrate neighborhood estimator with a doubly-robust correction.\n  Theoretical analysis guarantees our estimator is consistent and second-order robust to nuisance error.\n  Evaluated on a real-world Long COVID cohort with 13,511 participants, \\emph{C-kNN-LSH} demonstrates superior performance in capturing recovery heterogeneity and estimating policy values compared to existing baselines.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02371v1",
        "pdf": "https://arxiv.org/pdf/2602.02371v1"
      },
      "arxiv_id": "2602.02371v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02370v1",
      "title": "Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes",
      "authors": [
        "Uma Meleti",
        "Jeffrey J. Nirschl"
      ],
      "abstract": "Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02370v1",
        "pdf": "https://arxiv.org/pdf/2602.02370v1"
      },
      "arxiv_id": "2602.02370v1",
      "comment": "Accepted for publication at the IEEE International Symposium on Biomedical Imaging (ISBI) 2026",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02369v1",
      "title": "Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback",
      "authors": [
        "Yaolun Zhang",
        "Yiran Wu",
        "Yijiong Yu",
        "Qingyun Wu",
        "Huazheng Wang"
      ],
      "abstract": "Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \\emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \\textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \\textsc{Live-Evo} decouples \\emph{what happened} from \\emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \\textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \\textit{Prophet Arena} benchmark over a 10-week horizon, \\textsc{Live-Evo} improves Brier score by 20.8\\% and increases market returns by 12.9\\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02369v1",
        "pdf": "https://arxiv.org/pdf/2602.02369v1"
      },
      "arxiv_id": "2602.02369v1",
      "comment": "13 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02366v1",
      "title": "ReasonCACHE: Teaching LLMs To Reason Without Weight Updates",
      "authors": [
        "Sharut Gupta",
        "Phillip Isola",
        "Stefanie Jegelka",
        "David Lopez-Paz",
        "Kartik Ahuja",
        "Mark Ibrahim",
        "Mohammad Pezeshki"
      ],
      "abstract": "Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02366v1",
        "pdf": "https://arxiv.org/pdf/2602.02366v1"
      },
      "arxiv_id": "2602.02366v1",
      "comment": "26 pages, 17 Figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02361v1",
      "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
      "authors": [
        "Mouxiang Chen",
        "Lei Zhang",
        "Yunlong Feng",
        "Xuwu Wang",
        "Wenting Zhao",
        "Ruisheng Cao",
        "Jiaxi Yang",
        "Jiawei Chen",
        "Mingze Li",
        "Zeyao Ma",
        "Hao Ge",
        "Zongmeng Zhang",
        "Zeyu Cui",
        "Dayiheng Liu",
        "Jingren Zhou",
        "Jianling Sun",
        "Junyang Lin",
        "Binyuan Hui"
      ],
      "abstract": "We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02361v1",
        "pdf": "https://arxiv.org/pdf/2602.02361v1"
      },
      "arxiv_id": "2602.02361v1",
      "comment": "13 pages",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02358v1",
      "title": "Transfer Learning Through Conditional Quantile Matching",
      "authors": [
        "Yikun Zhang",
        "Steven Wilkins-Reeves",
        "Wesley Lee",
        "Aude Hofleitner"
      ],
      "abstract": "We introduce a transfer learning framework for regression that leverages heterogeneous source domains to improve predictive performance in a data-scarce target domain. Our approach learns a conditional generative model separately for each source domain and calibrates the generated responses to the target domain via conditional quantile matching. This distributional alignment step corrects general discrepancies between source and target domains without imposing restrictive assumptions such as covariate or label shift. The resulting framework provides a principled and flexible approach to high-quality data augmentation for downstream learning tasks in the target domain. From a theoretical perspective, we show that an empirical risk minimizer (ERM) trained on the augmented dataset achieves a tighter excess risk bound than the target-only ERM under mild conditions. In particular, we establish new convergence rates for the quantile matching estimator that governs the transfer bias-variance tradeoff. From a practical perspective, extensive simulations and real data applications demonstrate that the proposed method consistently improves prediction accuracy over target-only learning and competing transfer learning methods.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02358v1",
        "pdf": "https://arxiv.org/pdf/2602.02358v1"
      },
      "arxiv_id": "2602.02358v1",
      "comment": "24 pages (8 pages for the main paper), 3 figures, 3 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02356v1",
      "title": "NAB: Neural Adaptive Binning for Sparse-View CT reconstruction",
      "authors": [
        "Wangduo Xie",
        "Matthew B. Blaschko"
      ],
      "abstract": "Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \\textbf{N}eural \\textbf{A}daptive \\textbf{B}inning (\\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02356v1",
        "pdf": "https://arxiv.org/pdf/2602.02356v1"
      },
      "arxiv_id": "2602.02356v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02355v1",
      "title": "Hierarchical Federated Learning with SignSGD: A Highly Communication-Efficient Approach",
      "authors": [
        "Amirreza Kazemi",
        "Seyed Mohammad Azimi-Abarghouyi",
        "Gabor Fodor",
        "Carlo Fischione"
      ],
      "abstract": "Hierarchical federated learning (HFL) has emerged as a key architecture for large-scale wireless and Internet of Things systems, where devices communicate with nearby edge servers before reaching the cloud. In these environments, uplink bandwidth and latency impose strict communication limits, thereby making aggressive gradient compression essential. One-bit methods such as sign-based stochastic gradient descent (SignSGD) offer an attractive solution in flat federated settings, but existing theory and algorithms do not naturally extend to hierarchical settings. In particular, the interaction between majority-vote aggregation at the edge layer and model aggregation at the cloud layer, and its impact on end-to-end performance, remains unknown. To bridge this gap, we propose a highly communication-efficient sign-based HFL framework and develop its corresponding formulation for nonconvex learning, where devices send only signed stochastic gradients, edge servers combine them through majority-vote, and the cloud periodically averages the obtained edge models, while utilizing downlink quantization to broadcast the global model. We introduce the resulting scalable HFL algorithm, HierSignSGD, and provide the convergence analysis for SignSGD in a hierarchical setting. Our core technical contribution is a characterization of how biased sign compression, two-level aggregation intervals, and inter-cluster heterogeneity collectively affect convergence. Numerical experiments under homogeneous and heterogeneous data splits show that HierSignSGD, despite employing extreme compression, achieves accuracy comparable to or better than full-precision stochastic gradient descent while reducing communication cost in the process, and remains robust under aggressive downlink sparsification.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.DC",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02355v1",
        "pdf": "https://arxiv.org/pdf/2602.02355v1"
      },
      "arxiv_id": "2602.02355v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02354v1",
      "title": "Implicit neural representation of textures",
      "authors": [
        "Albert Kwok",
        "Zheyuan Hu",
        "Dounia Hammou"
      ],
      "abstract": "Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02354v1",
        "pdf": "https://arxiv.org/pdf/2602.02354v1"
      },
      "arxiv_id": "2602.02354v1",
      "comment": "Albert Kwok and Zheyuan Hu contributed equally to this work",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02351v1",
      "title": "Artificial Intelligence and Symmetries: Learning, Encoding, and Discovering Structure in Physical Data",
      "authors": [
        "Veronica Sanz"
      ],
      "abstract": "Symmetries play a central role in physics, organizing dynamics, constraining interactions, and determining the effective number of physical degrees of freedom. In parallel, modern artificial intelligence methods have demonstrated a remarkable ability to extract low-dimensional structure from high-dimensional data through representation learning. This review examines the interplay between these two perspectives, focusing on the extent to which symmetry-induced constraints can be identified, encoded, or diagnosed using machine learning techniques.\n  Rather than emphasizing architectures that enforce known symmetries by construction, we concentrate on data-driven approaches and latent representation learning, with particular attention to variational autoencoders. We discuss how symmetries and conservation laws reduce the intrinsic dimensionality of physical datasets, and how this reduction may manifest itself through self-organization of latent spaces in generative models trained to balance reconstruction and compression. We review recent results, including case studies from simple geometric systems and particle physics processes, and analyze the theoretical and practical limitations of inferring symmetry structure without explicit inductive bias.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "hep-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "hep-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02351v1",
        "pdf": "https://arxiv.org/pdf/2602.02351v1"
      },
      "arxiv_id": "2602.02351v1",
      "comment": "25 pages, 9 figures. This manuscript is an invited review at the International Journal of Modern Physics A",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02350v1",
      "title": "Context Learning for Multi-Agent Discussion",
      "authors": [
        "Xingyuan Hua",
        "Sheng Yue",
        "Xinyi Li",
        "Yizhe Zhao",
        "Jinrui Zhang",
        "Ju Ren"
      ],
      "abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02350v1",
        "pdf": "https://arxiv.org/pdf/2602.02350v1"
      },
      "arxiv_id": "2602.02350v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02343v1",
      "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
      "authors": [
        "Ziwen Xu",
        "Chenyan Wu",
        "Hengyu Sun",
        "Haiwen Hong",
        "Mengru Wang",
        "Yunzhi Yao",
        "Longtao Huang",
        "Hui Xue",
        "Shumin Deng",
        "Zhixuan Chu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02343v1",
        "pdf": "https://arxiv.org/pdf/2602.02343v1"
      },
      "arxiv_id": "2602.02343v1",
      "comment": "Work in progress",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02341v1",
      "title": "LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization",
      "authors": [
        "Zhenpeng Huang",
        "Jiaqi Li",
        "Zihan Jia",
        "Xinhao Li",
        "Desen Meng",
        "Lingxue Song",
        "Xi Chen",
        "Liang Li",
        "Limin Wang"
      ],
      "abstract": "We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02341v1",
        "pdf": "https://arxiv.org/pdf/2602.02341v1"
      },
      "arxiv_id": "2602.02341v1",
      "comment": "NeurIPS 2025",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02338v1",
      "title": "Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs",
      "authors": [
        "Yu Liang",
        "Zhongjin Zhang",
        "Yuxuan Zhu",
        "Kerui Zhang",
        "Zhiluohan Guo",
        "Wenhang Zhou",
        "Zonqi Yang",
        "Kangle Wu",
        "Yabo Ni",
        "Anxiang Zeng",
        "Cong Fu",
        "Jianxin Wang",
        "Jiazhi Xia"
      ],
      "abstract": "Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems, but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction, and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling. To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability, without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02338v1",
        "pdf": "https://arxiv.org/pdf/2602.02338v1"
      },
      "arxiv_id": "2602.02338v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02335v1",
      "title": "Building a Correct-by-Design Lakehouse. Data Contracts, Versioning, and Transactional Pipelines for Humans and Agents",
      "authors": [
        "Weiming Sheng",
        "Jinlang Wang",
        "Manuel Barros",
        "Aldrin Montana",
        "Jacopo Tagliabue",
        "Luca Bigon"
      ],
      "abstract": "Lakehouses are the default cloud platform for analytics and AI, but they become unsafe when untrusted actors concurrently operate on production data: upstream-downstream mismatches surface only at runtime, and multi-table pipelines can leak partial effects. Inspired by software engineering, we design Bauplan, a code-first lakehouse that aims to make (most) illegal states unrepresentable using familiar abstractions. Bauplan acts along three axes: typed table contracts to make pipeline boundaries checkable, Git-like data versioning for review and reproducibility, and transactional runs that guarantee pipeline-level atomicity. We report early results from a lightweight formal transaction model and discuss future work motivated by counterexamples.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.DC",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02335v1",
        "pdf": "https://arxiv.org/pdf/2602.02335v1"
      },
      "arxiv_id": "2602.02335v1",
      "comment": "Pre-print (PaPoC 2026)",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02334v1",
      "title": "VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations",
      "authors": [
        "Fatemeh Zargarbashi",
        "Dhruv Agrawal",
        "Jakob Buhmann",
        "Martin Guay",
        "Stelian Coros",
        "Robert W. Sumner"
      ],
      "abstract": "Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02334v1",
        "pdf": "https://arxiv.org/pdf/2602.02334v1"
      },
      "arxiv_id": "2602.02334v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02331v1",
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "authors": [
        "Shaoting Zhu",
        "Baijun Ye",
        "Jiaxuan Wang",
        "Jiakang Chen",
        "Ziwen Zhuang",
        "Linzhan Mou",
        "Runhan Huang",
        "Hang Zhao"
      ],
      "abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02331v1",
        "pdf": "https://arxiv.org/pdf/2602.02331v1"
      },
      "arxiv_id": "2602.02331v1",
      "comment": "Project Page: https://ttt-parkour.github.io/",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2602.02320v1",
      "title": "A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method",
      "authors": [
        "Feiyang Cai",
        "Guijuan He",
        "Yi Hu",
        "Jingjing Wang",
        "Joshua Luo",
        "Tianyu Zhu",
        "Srikanth Pilla",
        "Gang Li",
        "Ling Liu",
        "Feng Luo"
      ],
      "abstract": "Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02320v1",
        "pdf": "https://arxiv.org/pdf/2602.02320v1"
      },
      "arxiv_id": "2602.02320v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02318v1",
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "authors": [
        "Xiang Li",
        "Yupeng Zheng",
        "Pengfei Li",
        "Yilun Chen",
        "Ya-Qin Zhang",
        "Wenchao Ding"
      ],
      "abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02318v1",
        "pdf": "https://arxiv.org/pdf/2602.02318v1"
      },
      "arxiv_id": "2602.02318v1",
      "comment": "Accepted by RA-L",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02313v1",
      "title": "Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient",
      "authors": [
        "Changming Li",
        "Kaixing Zhang",
        "Haoyun Xu",
        "Yingdong Shi",
        "Zheng Zhang",
        "Kaitao Song",
        "Kan Ren"
      ],
      "abstract": "Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02313v1",
        "pdf": "https://arxiv.org/pdf/2602.02313v1"
      },
      "arxiv_id": "2602.02313v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02310v1",
      "title": "FragmentFlow: Scalable Transition State Generation for Large Molecules",
      "authors": [
        "Ron Shprints",
        "Peter Holderrieth",
        "Juno Nam",
        "Rafael Gómez-Bombarelli",
        "Tommi Jaakkola"
      ],
      "abstract": "Transition states (TSs) are central to understanding and quantitatively predicting chemical reactivity and reaction mechanisms. Although traditional TS generation methods are computationally expensive, recent generative modeling approaches have enabled chemically meaningful TS prediction for relatively small molecules. However, these methods fail to generalize to practically relevant reaction substrates because of distribution shifts induced by increasing molecular sizes. Furthermore, TS geometries for larger molecules are not available at scale, making it infeasible to train generative models from scratch on such molecules. To address these challenges, we introduce FragmentFlow: a divide-and-conquer approach that trains a generative model to predict TS geometries for the reactive core atoms, which define the reaction mechanism. The full TS structure is then reconstructed by re-attaching substituent fragments to the predicted core. By operating on reactive cores, whose size and composition remain relatively invariant across molecular contexts, FragmentFlow mitigates distribution shifts in generative modeling. Evaluated on a new curated dataset of reactions involving reactants with up to 33 heavy atoms, FragmentFlow correctly identifies 90% of TSs while requiring 30% fewer saddle-point optimization steps than classical initialization schemes. These results point toward scalable TS generation for high-throughput reactivity studies.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "physics.chem-ph",
        "cs.AI"
      ],
      "primary_category": "physics.chem-ph",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02310v1",
        "pdf": "https://arxiv.org/pdf/2602.02310v1"
      },
      "arxiv_id": "2602.02310v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02306v1",
      "title": "Spark: Modular Spiking Neural Networks",
      "authors": [
        "Mario Franco",
        "Carlos Gershenson"
      ],
      "abstract": "Nowadays, neural networks act as a synonym for artificial intelligence. Present neural network models, although remarkably powerful, are inefficient both in terms of data and energy. Several alternative forms of neural networks have been proposed to address some of these problems. Specifically, spiking neural networks are suitable for efficient hardware implementations. However, effective learning algorithms for spiking networks remain elusive, although it is suspected that effective plasticity mechanisms could alleviate the problem of data efficiency. Here, we present a new framework for spiking neural networks - Spark - built upon the idea of modular design, from simple components to entire models. The aim of this framework is to provide an efficient and streamlined pipeline for spiking neural networks. We showcase this framework by solving the sparse-reward cartpole problem with simple plasticity mechanisms. We hope that a framework compatible with traditional ML pipelines may accelerate research in the area, specifically for continuous and unbatched learning, akin to the one animals exhibit.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02306v1",
        "pdf": "https://arxiv.org/pdf/2602.02306v1"
      },
      "arxiv_id": "2602.02306v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02304v1",
      "title": "Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach",
      "authors": [
        "Martino Ciaperoni",
        "Marzio Di Vece",
        "Luca Pappalardo",
        "Fosca Giannotti",
        "Francesco Giannini"
      ],
      "abstract": "Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02304v1",
        "pdf": "https://arxiv.org/pdf/2602.02304v1"
      },
      "arxiv_id": "2602.02304v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02301v1",
      "title": "Advancing General-Purpose Reasoning Models with Modular Gradient Surgery",
      "authors": [
        "Min Cai",
        "Yu Liang",
        "Longzheng Wang",
        "Yan Wang",
        "Yueyang Zhang",
        "Long Xia",
        "Zhiyuan Sun",
        "Xi Ye",
        "Daiting Shi"
      ],
      "abstract": "Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\\%) and 4.5 (11.1\\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02301v1",
        "pdf": "https://arxiv.org/pdf/2602.02301v1"
      },
      "arxiv_id": "2602.02301v1",
      "comment": "Preprint; Code: https://github.com/StringNLPLAB/MGS; Website: https://modular-gradient-surgery.github.io",
      "journal_ref": "",
      "has_code": true
    },
    {
      "id": "2602.02296v1",
      "title": "Decoupling Generalizability and Membership Privacy Risks in Neural Networks",
      "authors": [
        "Xingli Fang",
        "Jung-Eun Kim"
      ],
      "abstract": "A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02296v1",
        "pdf": "https://arxiv.org/pdf/2602.02296v1"
      },
      "arxiv_id": "2602.02296v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02295v1",
      "title": "EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models",
      "authors": [
        "Shaima Ahmad Freja",
        "Ferhat Ozgur Catak",
        "Betul Yurdem",
        "Chunming Rong"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in critical applications requiring reliable reasoning, yet their internal reasoning processes remain difficult to evaluate systematically. Existing methods focus on final-answer correctness, providing limited insight into how reasoning unfolds across intermediate steps. We present EvalQReason, a framework that quantifies LLM reasoning quality through step-level probability distribution analysis without requiring human annotation. The framework introduces two complementary algorithms: Consecutive Step Divergence (CSD), which measures local coherence between adjacent reasoning steps, and Step-to-Final Convergence (SFC), which assesses global alignment with final answers. Each algorithm employs five statistical metrics to capture reasoning dynamics. Experiments across mathematical and medical datasets with open-source 7B-parameter models demonstrate that CSD-based features achieve strong predictive performance for correctness classification, with classical machine learning models reaching F1=0.78 and ROC-AUC=0.82, and sequential neural models substantially improving performance (F1=0.88, ROC-AUC=0.97). CSD consistently outperforms SFC, and sequential architectures outperform classical machine learning approaches. Critically, reasoning dynamics prove domain-specific: mathematical reasoning exhibits clear divergence-based discrimination patterns between correct and incorrect solutions, while medical reasoning shows minimal discriminative signals, revealing fundamental differences in how LLMs process different reasoning types. EvalQReason enables scalable, process-aware evaluation of reasoning reliability, establishing probability-based divergence analysis as a principled approach for trustworthy AI deployment.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02295v1",
        "pdf": "https://arxiv.org/pdf/2602.02295v1"
      },
      "arxiv_id": "2602.02295v1",
      "comment": "15 pages (including appendix), 11 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02290v1",
      "title": "Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?",
      "authors": [
        "Alex Argese",
        "Pasquale Lisena",
        "Raphaël Troncy"
      ],
      "abstract": "Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, factual hallucinations are critical in scientific contexts, yet, detectors often misclassify legitimate narrative reformulations or prove unstable when creativity is involved. In this work, we propose StoryScore, a composite metric for evaluating AI-generated scientific stories. StoryScore integrates semantic alignment, lexical grounding, narrative control, structural fidelity, redundancy avoidance, and entity-level hallucination detection into a unified framework. Our analysis also reveals why many hallucination detection methods fail to distinguish pedagogical creativity from factual errors, highlighting a key limitation: while automatic metrics can effectively assess semantic similarity with original content, they struggle to evaluate how it is narrated and controlled.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02290v1",
        "pdf": "https://arxiv.org/pdf/2602.02290v1"
      },
      "arxiv_id": "2602.02290v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02288v1",
      "title": "An Optimization Method for Autoregressive Time Series Forecasting",
      "authors": [
        "Zheng Li",
        "Jerry Cheng",
        "Huanying Gu"
      ],
      "abstract": "Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/AROpt",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02288v1",
        "pdf": "https://arxiv.org/pdf/2602.02288v1"
      },
      "arxiv_id": "2602.02288v1",
      "comment": "10 pages, 2 figures, 2 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02286v1",
      "title": "DFKI-Speech System for WildSpoof Challenge: A robust framework for SASV In-the-Wild",
      "authors": [
        "Arnab Das",
        "Yassine El Kheir",
        "Enes Erdem Erdogan",
        "Feidi Kallel",
        "Tim Polzehl",
        "Sebastian Moeller"
      ],
      "abstract": "This paper presents the DFKI-Speech system developed for the WildSpoof Challenge under the Spoofing aware Automatic Speaker Verification (SASV) track. We propose a robust SASV framework in which a spoofing detector and a speaker verification (SV) network operate in tandem. The spoofing detector employs a self-supervised speech embedding extractor as the frontend, combined with a state-of-the-art graph neural network backend. In addition, a top-3 layer based mixture-of-experts (MoE) is used to fuse high-level and low-level features for effective spoofed utterance detection. For speaker verification, we adapt a low-complexity convolutional neural network that fuses 2D and 1D features at multiple scales, trained with the SphereFace loss. Additionally, contrastive circle loss is applied to adaptively weight positive and negative pairs within each training batch, enabling the network to better distinguish between hard and easy sample pairs. Finally, fixed imposter cohort based AS Norm score normalization and model ensembling are used to further enhance the discriminative capability of the speaker verification system.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02286v1",
        "pdf": "https://arxiv.org/pdf/2602.02286v1"
      },
      "arxiv_id": "2602.02286v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02285v1",
      "title": "Statistical Learning Theory in Lean 4: Empirical Processes from Scratch",
      "authors": [
        "Yuanhe Zhang",
        "Jason D. Lee",
        "Fanghui Liu"
      ],
      "abstract": "We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.CL",
        "math.ST"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02285v1",
        "pdf": "https://arxiv.org/pdf/2602.02285v1"
      },
      "arxiv_id": "2602.02285v1",
      "comment": "19 pages, 2 figures. Comments are welcome",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02283v1",
      "title": "Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management",
      "authors": [
        "Owen Shen",
        "Patrick Jaillet"
      ],
      "abstract": "We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \\emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\\varepsilon/(1-γ))$ neighborhood of the optimal Q-function, where $\\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02283v1",
        "pdf": "https://arxiv.org/pdf/2602.02283v1"
      },
      "arxiv_id": "2602.02283v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02282v1",
      "title": "MoLF: Mixture-of-Latent-Flow for Pan-Cancer Spatial Gene Expression Prediction from Histology",
      "authors": [
        "Susu Hu",
        "Stefanie Speidel"
      ],
      "abstract": "Inferring spatial transcriptomics (ST) from histology enables scalable histogenomic profiling, yet current methods are largely restricted to single-tissue models. This fragmentation fails to leverage biological principles shared across cancer types and hinders application to data-scarce scenarios. While pan-cancer training offers a solution, the resulting heterogeneity challenges monolithic architectures. To bridge this gap, we introduce MoLF (Mixture-of-Latent-Flow), a generative model for pan-cancer histogenomic prediction. MoLF leverages a conditional Flow Matching objective to map noise to the gene latent manifold, parameterized by a Mixture-of-Experts (MoE) velocity field. By dynamically routing inputs to specialized sub-networks, this architecture effectively decouples the optimization of diverse tissue patterns. Our experiments demonstrate that MoLF establishes a new state-of-the-art, consistently outperforming both specialized and foundation model baselines on pan-cancer benchmarks. Furthermore, MoLF exhibits zero-shot generalization to cross-species data, suggesting it captures fundamental, conserved histo-molecular mechanisms.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02282v1",
        "pdf": "https://arxiv.org/pdf/2602.02282v1"
      },
      "arxiv_id": "2602.02282v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02281v1",
      "title": "Backpropagation as Physical Relaxation: Exact Gradients in Finite Time",
      "authors": [
        "Antonino Emanuele Scurria"
      ],
      "abstract": "Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "physics.class-ph",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02281v1",
        "pdf": "https://arxiv.org/pdf/2602.02281v1"
      },
      "arxiv_id": "2602.02281v1",
      "comment": "15 pages, 8 figures",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02280v1",
      "title": "RACA: Representation-Aware Coverage Criteria for LLM Safety Testing",
      "authors": [
        "Zeming Wei",
        "Zhixin Zhang",
        "Chengcan Wu",
        "Yihao Zhang",
        "Xiaokun Luan",
        "Meng Sun"
      ],
      "abstract": "Recent advancements in LLMs have led to significant breakthroughs in various AI applications. However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks. Current safety testing for LLMs often relies on static datasets and lacks systematic criteria to evaluate the quality and adequacy of these tests. While coverage criteria have been effective for smaller neural networks, they are not directly applicable to LLMs due to scalability issues and differing objectives. To address these challenges, this paper introduces RACA, a novel set of coverage criteria specifically designed for LLM safety testing. RACA leverages representation engineering to focus on safety-critical concepts within LLMs, thereby reducing dimensionality and filtering out irrelevant information. The framework operates in three stages: first, it identifies safety-critical representations using a small, expert-curated calibration set of jailbreak prompts. Second, it calculates conceptual activation scores for a given test suite based on these representations. Finally, it computes coverage results using six sub-criteria that assess both individual and compositional safety concepts. We conduct comprehensive experiments to validate RACA's effectiveness, applicability, and generalization, where the results demonstrate that RACA successfully identifies high-quality jailbreak prompts and is superior to traditional neuron-level criteria. We also showcase its practical application in real-world scenarios, such as test set prioritization and attack prompt sampling. Furthermore, our findings confirm RACA's generalization to various scenarios and its robustness across various configurations. Overall, RACA provides a new framework for evaluating the safety of LLMs, contributing a valuable technique to the field of testing for AI.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02280v1",
        "pdf": "https://arxiv.org/pdf/2602.02280v1"
      },
      "arxiv_id": "2602.02280v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02269v1",
      "title": "Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems",
      "authors": [
        "Jon Škerlj",
        "Seongjin Bien",
        "Abdeldjallil Naceri",
        "Sami Haddadin"
      ],
      "abstract": "We present $multipanda\\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SE",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02269v1",
        "pdf": "https://arxiv.org/pdf/2602.02269v1"
      },
      "arxiv_id": "2602.02269v1",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02268v1",
      "title": "HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control",
      "authors": [
        "Sanggeon Yun",
        "Raheeb Hassan",
        "Ryozo Masukawa",
        "Sungheon Jeong",
        "Mohsen Imani"
      ],
      "abstract": "Graph Transformers typically rely on explicit positional or structural encodings and dense global attention to incorporate graph topology. In this work, we show that neither is essential. We introduce HopFormer, a graph Transformer that injects structure exclusively through head-specific n-hop masked sparse attention, without the use of positional encodings or architectural modifications. This design provides explicit and interpretable control over receptive fields while enabling genuinely sparse attention whose computational cost scales linearly with mask sparsity. Through extensive experiments on both node-level and graph-level benchmarks, we demonstrate that our approach achieves competitive or superior performance across diverse graph structures. Our results further reveal that dense global attention is often unnecessary: on graphs with strong small-world properties, localized attention yields more stable and consistently high performance, while on graphs with weaker small-world effects, global attention offers diminishing returns. Together, these findings challenge prevailing assumptions in graph Transformer design and highlight sparsity-controlled attention as a principled and efficient alternative.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02268v1",
        "pdf": "https://arxiv.org/pdf/2602.02268v1"
      },
      "arxiv_id": "2602.02268v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02266v1",
      "title": "OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data",
      "authors": [
        "Tan Sang Nguyen",
        "Muhammad Reza Qorib",
        "Hwee Tou Ng"
      ],
      "abstract": "Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been developed, but none are truly open source, as they do not publicly disclose their training data. Truly open-source models are important for transparency and for enabling a deeper and more precise understanding of LLM internals and development, including biases, generalization, and multilinguality. Motivated by recent advances demonstrating the effectiveness of parallel data in improving multilingual performance, we conduct controlled and comprehensive experiments to study the effectiveness of parallel data in continual pretraining of LLMs. Our findings show that using only parallel data is the most effective way to extend an LLM to new languages. Using just 34.7B tokens of parallel data and 180 hours on 8x NVIDIA H200 GPUs, we built OpenSeal, the first truly open Southeast Asian LLM that rivals the performance of existing models of similar size.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02266v1",
        "pdf": "https://arxiv.org/pdf/2602.02266v1"
      },
      "arxiv_id": "2602.02266v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02264v1",
      "title": "Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training",
      "authors": [
        "Paolo Marcandelli",
        "Natansh Mathur",
        "Stefano Markidis",
        "Martina Siena",
        "Stefano Mariani"
      ],
      "abstract": "Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural networks address this limitation through unsupervised training with physical constraints but often suffer from unstable convergence and limited generalization capability. To overcome these issues, we introduce a multi-stage physics-informed training strategy that achieves convergence by progressively enforcing boundary conditions in the loss landscape and subsequently incorporating interior residuals. At each stage the optimizer is re-initialized, acting as a continuation mechanism that restores stability and prevents gradient stagnation. We further propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO), combining Fourier layers with Hermite spline kernels for smooth residual evaluation. Across canonical benchmarks, PhIS-FNO attains a level of accuracy comparable to that of supervised learning, using labeled information only along a narrow boundary region, establishing staged, spline-based optimization as a robust paradigm for physics-informed operator learning.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02264v1",
        "pdf": "https://arxiv.org/pdf/2602.02264v1"
      },
      "arxiv_id": "2602.02264v1",
      "comment": "51 pages, 15 figures, 6 tables",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02262v1",
      "title": "OmniCode: A Benchmark for Evaluating Software Engineering Agents",
      "authors": [
        "Atharv Sonwane",
        "Eng-Shen Tu",
        "Wei-Chung Lu",
        "Claas Beger",
        "Carter Larsen",
        "Debjit Dhar",
        "Rachel Chen",
        "Ronit Pattanayak",
        "Tuan Anh Dang",
        "Guohao Chen",
        "Gloria Geng",
        "Kevin Ellis",
        "Saikat Dutta"
      ],
      "abstract": "LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02262v1",
        "pdf": "https://arxiv.org/pdf/2602.02262v1"
      },
      "arxiv_id": "2602.02262v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02261v1",
      "title": "Unlocking the Duality between Flow and Field Matching",
      "authors": [
        "Daniil Shlenskii",
        "Alexander Varlamov",
        "Nazar Buzun",
        "Alexander Korotin"
      ],
      "abstract": "Conditional Flow Matching (CFM) unifies conventional generative paradigms such as diffusion models and flow matching. Interaction Field Matching (IFM) is a newer framework that generalizes Electrostatic Field Matching (EFM) rooted in Poisson Flow Generative Models (PFGM). While both frameworks define generative dynamics, they start from different objects: CFM specifies a conditional probability path in data space, whereas IFM specifies a physics-inspired interaction field in an augmented data space. This raises a basic question: are CFM and IFM genuinely different, or are they two descriptions of the same underlying dynamics? We show that they coincide for a natural subclass of IFM that we call forward-only IFM. Specifically, we construct a bijection between CFM and forward-only IFM. We further show that general IFM is strictly more expressive: it includes EFM and other interaction fields that cannot be realized within the standard CFM formulation. Finally, we highlight how this duality can benefit both frameworks: it provides a probabilistic interpretation of forward-only IFM and yields novel, IFM-driven techniques for CFM.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02261v1",
        "pdf": "https://arxiv.org/pdf/2602.02261v1"
      },
      "arxiv_id": "2602.02261v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02260v1",
      "title": "Learning Markov Decision Processes under Fully Bandit Feedback",
      "authors": [
        "Zhengjia Zhuo",
        "Anupam Gupta",
        "Viswanath Nagarajan"
      ],
      "abstract": "A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $Θ(\\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \\emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\\widetilde{O}(\\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02260v1",
        "pdf": "https://arxiv.org/pdf/2602.02260v1"
      },
      "arxiv_id": "2602.02260v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02259v1",
      "title": "Segment to Focus: Guiding Latent Action Models in the Presence of Distractors",
      "authors": [
        "Hamza Adnan",
        "Matthew T. Jackson",
        "Alexey Zakharov"
      ],
      "abstract": "Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02259v1",
        "pdf": "https://arxiv.org/pdf/2602.02259v1"
      },
      "arxiv_id": "2602.02259v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02258v1",
      "title": "Alignment-Aware Model Adaptation via Feedback-Guided Optimization",
      "authors": [
        "Gaurav Bhatt",
        "Aditya Chinchure",
        "Jiawei Zhou",
        "Leonid Sigal"
      ],
      "abstract": "Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02258v1",
        "pdf": "https://arxiv.org/pdf/2602.02258v1"
      },
      "arxiv_id": "2602.02258v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false
    },
    {
      "id": "2602.02250v1",
      "title": "Well-Posed KL-Regularized Control via Wasserstein and Kalman-Wasserstein KL Divergences",
      "authors": [
        "Viktor Stein",
        "Adwait Datar",
        "Nihat Ay"
      ],
      "abstract": "Kullback-Leibler divergence (KL) regularization is widely used in reinforcement learning, but it becomes infinite under support mismatch and can degenerate in low-noise limits. Utilizing a unified information-geometric framework, we introduce (Kalman)-Wasserstein-based KL analogues by replacing the Fisher-Rao geometry in the dynamical formulation of the KL with transport-based geometries, and we derive closed-form values for common distribution families. These divergences remain finite under support mismatch and yield a geometric interpretation of regularization heuristics used in Kalman ensemble methods. We demonstrate the utility of these divergences in KL-regularized optimal control. In the fully tractable setting of linear time-invariant systems with Gaussian process noise, the classical KL reduces to a quadratic control penalty that becomes singular as process noise vanishes. Our variants remove this singularity, yielding well-posed problems. On a double integrator and a cart-pole example, the resulting controls outperform KL-based regularization.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02250v1",
        "pdf": "https://arxiv.org/pdf/2602.02250v1"
      },
      "arxiv_id": "2602.02250v1",
      "comment": "37 pages, 9 figures, comments welcome",
      "journal_ref": "",
      "has_code": false
    }
  ]
}