{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2602.04755v1",
      "title": "When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?",
      "authors": [
        "Xinyu Zhou",
        "Chang Jin",
        "Carsten Eickhoff",
        "Zhijiang Guo",
        "Seyed Ali Bahrainian"
      ],
      "abstract": "Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers, rather than abstaining (i.e., refusing to answer). This weakness is even evident in temporal question answering, where models frequently ignore time-sensitive evidence and conflate facts across different time-periods. In this paper, we present the first empirical study of training LLMs with an abstention ability while reasoning about temporal QA. Existing approaches such as calibration might be unreliable in capturing uncertainty in complex reasoning. We instead frame abstention as a teachable skill and introduce a pipeline that couples Chain-of-Thought (CoT) supervision with Reinforcement Learning (RL) guided by abstention-aware rewards. Our goal is to systematically analyze how different information types and training techniques affect temporal reasoning with abstention behavior in LLMs. Through extensive experiments studying various methods, we find that RL yields strong empirical gains on reasoning: a model initialized by Qwen2.5-1.5B-Instruct surpasses GPT-4o by $3.46\\%$ and $5.80\\%$ in Exact Match on TimeQA-Easy and Hard, respectively. Moreover, it improves the True Positive rate on unanswerable questions by $20\\%$ over a pure supervised fine-tuned (SFT) variant. Beyond performance, our analysis shows that SFT induces overconfidence and harms reliability, while RL improves prediction accuracy but exhibits similar risks. Finally, by comparing implicit reasoning cues (e.g., original context, temporal sub-context, knowledge graphs) with explicit CoT supervision, we find that implicit information provides limited benefit for reasoning with abstention. Our study provides new insights into how abstention and reasoning can be jointly optimized, providing a foundation for building more reliable LLMs.",
      "published": "2026-02-04",
      "updated": "2026-02-04",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.04755v1",
        "pdf": "https://arxiv.org/pdf/2602.04755v1"
      },
      "arxiv_id": "2602.04755v1",
      "comment": "Accepted to ICLR2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.5,
      "score_breakdown": {
        "total_score": 4.5,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      }
    },
    {
      "id": "2602.04877v1",
      "title": "CoWTracker: Tracking by Warping instead of Correlation",
      "authors": [
        "Zihang Lai",
        "Eldar Insafutdinov",
        "Edgar Sucar",
        "Andrea Vedaldi"
      ],
      "abstract": "Dense point tracking is a fundamental problem in computer vision, with applications ranging from video analysis to robotic manipulation. State-of-the-art trackers typically rely on cost volumes to match features across frames, but this approach incurs quadratic complexity in spatial resolution, limiting scalability and efficiency. In this paper, we propose \\method, a novel dense point tracker that eschews cost volumes in favor of warping. Inspired by recent advances in optical flow, our approach iteratively refines track estimates by warping features from the target frame to the query frame based on the current estimate. Combined with a transformer architecture that performs joint spatiotemporal reasoning across all tracks, our design establishes long-range correspondences without computing feature correlations. Our model is simple and achieves state-of-the-art performance on standard dense point tracking benchmarks, including TAP-Vid-DAVIS, TAP-Vid-Kinetics, and Robo-TAP. Remarkably, the model also excels at optical flow, sometimes outperforming specialized methods on the Sintel, KITTI, and Spring benchmarks. These results suggest that warping-based architectures can unify dense point tracking and optical flow estimation.",
      "published": "2026-02-04",
      "updated": "2026-02-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.04877v1",
        "pdf": "https://arxiv.org/pdf/2602.04877v1"
      },
      "arxiv_id": "2602.04877v1",
      "comment": "Project website: cowtracker.github.io",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.219491525423729,
      "score_breakdown": {
        "total_score": 4.22,
        "field_match": {
          "score": 0.42,
          "matches": [
            "computer vision"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      }
    },
    {
      "id": "2602.04605v1",
      "title": "RexBERT: Context Specialized Bidirectional Encoders for E-commerce",
      "authors": [
        "Rahul Bajaj",
        "Anuj Garg"
      ],
      "abstract": "Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BERT-style encoders designed specifically for e-commerce semantics. We make three contributions. First, we release Ecom-niverse, a 350 billion token corpus curated from diverse retail and shopping sources. We describe a modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present a reproducible pretraining recipe building on ModernBERT's architectural advances. The recipe consists of three phases: general pre-training, context extension, and annealed domain specialization. Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification, semantic similarity, and general natural language understanding tasks using e-commerce datasets. Despite having 2-3x fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long-context models on domain-specific benchmarks. Our results demonstrate that high quality in-domain data combined with a principled training approach provides a stronger foundation for e-commerce applications than indiscriminate scaling alone.",
      "published": "2026-02-04",
      "updated": "2026-02-04",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.04605v1",
        "pdf": "https://arxiv.org/pdf/2602.04605v1"
      },
      "arxiv_id": "2602.04605v1",
      "comment": "Blog: https://huggingface.co/blog/thebajajra/rexbert-encoders Models: https://huggingface.co/collections/thebajajra/rexbert Ecom-niverse Dataset: https://huggingface.co/datasets/thebajajra/Ecom-niverse",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.1000000000000005,
      "score_breakdown": {
        "total_score": 4.1,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      }
    },
    {
      "id": "2602.04883v1",
      "title": "Protein Autoregressive Modeling via Multiscale Structure Generation",
      "authors": [
        "Yanru Qu",
        "Cheng-Yen Hsieh",
        "Zaixiang Zheng",
        "Ge Liu",
        "Quanquan Gu"
      ],
      "abstract": "We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.",
      "published": "2026-02-04",
      "updated": "2026-02-04",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.04883v1",
        "pdf": "https://arxiv.org/pdf/2602.04883v1"
      },
      "arxiv_id": "2602.04883v1",
      "comment": "ByteDance Seed Tech Report; Page: https://par-protein.github.io/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.0,
      "score_breakdown": {
        "total_score": 4.0,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      }
    },
    {
      "id": "2602.04770v1",
      "title": "Generative Modeling via Drifting",
      "authors": [
        "Mingyang Deng",
        "He Li",
        "Tianhong Li",
        "Yilun Du",
        "Kaiming He"
      ],
      "abstract": "Generative modeling can be formulated as learning a mapping f such that its pushforward distribution matches the data distribution. The pushforward behavior can be carried out iteratively at inference time, for example in diffusion and flow-based models. In this paper, we propose a new paradigm called Drifting Models, which evolve the pushforward distribution during training and naturally admit one-step inference. We introduce a drifting field that governs the sample movement and achieves equilibrium when the distributions match. This leads to a training objective that allows the neural network optimizer to evolve the distribution. In experiments, our one-step generator achieves state-of-the-art results on ImageNet at 256 x 256 resolution, with an FID of 1.54 in latent space and 1.61 in pixel space. We hope that our work opens up new opportunities for high-quality one-step generation.",
      "published": "2026-02-04",
      "updated": "2026-02-04",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.04770v1",
        "pdf": "https://arxiv.org/pdf/2602.04770v1"
      },
      "arxiv_id": "2602.04770v1",
      "comment": "Project page: https://lambertae.github.io/projects/drifting/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.95,
      "score_breakdown": {
        "total_score": 3.95,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      }
    },
    {
      "id": "2602.04876v1",
      "title": "PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation",
      "authors": [
        "Jiahao Zhan",
        "Zizhang Li",
        "Hong-Xing Yu",
        "Jiajun Wu"
      ],
      "abstract": "We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.",
      "published": "2026-02-04",
      "updated": "2026-02-04",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.04876v1",
        "pdf": "https://arxiv.org/pdf/2602.04876v1"
      },
      "arxiv_id": "2602.04876v1",
      "comment": "Project website: https://johnzhan2023.github.io/PerpetualWonder/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8,
      "score_breakdown": {
        "total_score": 3.8,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      }
    },
    {
      "id": "2602.04863v1",
      "title": "Subliminal Effects in Your Data: A General Mechanism via Log-Linearity",
      "authors": [
        "Ishaq Aden-Ali",
        "Noah Golowich",
        "Allen Liu",
        "Abhishek Shetty",
        "Ankur Moitra",
        "Nika Haghtalab"
      ],
      "abstract": "Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.\n  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.",
      "published": "2026-02-04",
      "updated": "2026-02-04",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.04863v1",
        "pdf": "https://arxiv.org/pdf/2602.04863v1"
      },
      "arxiv_id": "2602.04863v1",
      "comment": "Code available at https://github.com/ishaqadenali/logit-linear-selection",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8,
      "score_breakdown": {
        "total_score": 3.8,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      }
    },
    {
      "id": "2602.04814v1",
      "title": "X2HDR: HDR Image Generation in a Perceptually Uniform Space",
      "authors": [
        "Ronghuan Wu",
        "Wanchao Su",
        "Kede Ma",
        "Jing Liao",
        "Rafa≈Ç K. Mantiuk"
      ],
      "abstract": "High-dynamic-range (HDR) formats and displays are becoming increasingly prevalent, yet state-of-the-art image generators (e.g., Stable Diffusion and FLUX) typically remain limited to low-dynamic-range (LDR) output due to the lack of large-scale HDR training data. In this work, we show that existing pretrained diffusion models can be easily adapted to HDR generation without retraining from scratch. A key challenge is that HDR images are natively represented in linear RGB, whose intensity and color statistics differ substantially from those of sRGB-encoded LDR images. This gap, however, can be effectively bridged by converting HDR inputs into perceptually uniform encodings (e.g., using PU21 or PQ). Empirically, we find that LDR-pretrained variational autoencoders (VAEs) reconstruct PU21-encoded HDR inputs with fidelity comparable to LDR data, whereas linear RGB inputs cause severe degradations. Motivated by this finding, we describe an efficient adaptation strategy that freezes the VAE and finetunes only the denoiser via low-rank adaptation in a perceptually uniform space. This results in a unified computational method that supports both text-to-HDR synthesis and single-image RAW-to-HDR reconstruction. Experiments demonstrate that our perceptually encoded adaptation consistently improves perceptual fidelity, text-image alignment, and effective dynamic range, relative to previous techniques.",
      "published": "2026-02-04",
      "updated": "2026-02-04",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.04814v1",
        "pdf": "https://arxiv.org/pdf/2602.04814v1"
      },
      "arxiv_id": "2602.04814v1",
      "comment": "Project page: https://x2hdr.github.io/, Code: https://github.com/X2HDR/X2HDR",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8,
      "score_breakdown": {
        "total_score": 3.8,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      }
    },
    {
      "id": "2602.04851v1",
      "title": "PDF-HR: Pose Distance Fields for Humanoid Robots",
      "authors": [
        "Yi Gu",
        "Yukang Gao",
        "Yangchen Zhou",
        "Xingyu Chen",
        "Yixiao Feng",
        "Mingle Zhao",
        "Yunyang Mo",
        "Zhaorui Wang",
        "Lixin Xu",
        "Renjing Xu"
      ],
      "abstract": "Pose and motion priors play a crucial role in humanoid robotics. Although such priors have been widely studied in human motion recovery (HMR) domain with a range of models, their adoption for humanoid robots remains limited, largely due to the scarcity of high-quality humanoid motion data. In this work, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that represents the robot pose distribution as a continuous and differentiable manifold. Given an arbitrary pose, PDF-HR predicts its distance to a large corpus of retargeted robot poses, yielding a smooth measure of pose plausibility that is well suited for optimization and control. PDF-HR can be integrated as a reward shaping term, a regularizer, or a standalone plausibility scorer across diverse pipelines. We evaluate PDF-HR on various humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and general motion retargeting. Experiments show that this plug-and-play prior consistently and substantially strengthens strong baselines. Code and models will be released.",
      "published": "2026-02-04",
      "updated": "2026-02-04",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2602.04851v1",
        "pdf": "https://arxiv.org/pdf/2602.04851v1"
      },
      "arxiv_id": "2602.04851v1",
      "comment": "\\href{https://gaoyukang33.github.io/PDF-HR/}{Project page}",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.65,
      "score_breakdown": {
        "total_score": 3.65,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      }
    },
    {
      "id": "2602.04784v1",
      "title": "From independent patches to coordinated attention: Controlling information flow in vision transformers",
      "authors": [
        "Kieran A. Murphy"
      ],
      "abstract": "We make the information transmitted by attention an explicit, measurable quantity in vision transformers. By inserting variational information bottlenecks on all attention-mediated writes to the residual stream -- without other architectural changes -- we train models with an explicit information cost and obtain a controllable spectrum from independent patch processing to fully expressive global attention. On ImageNet-100, we characterize how classification behavior and information routing evolve across this spectrum, and provide initial insights into how global visual representations emerge from local patch processing by analyzing the first attention heads that transmit information. By biasing learning toward solutions with constrained internal communication, our approach yields models that are more tractable for mechanistic analysis and more amenable to control.",
      "published": "2026-02-04",
      "updated": "2026-02-04",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.04784v1",
        "pdf": "https://arxiv.org/pdf/2602.04784v1"
      },
      "arxiv_id": "2602.04784v1",
      "comment": "Code at https://github.com/murphyka/vit_ib",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.65,
      "score_breakdown": {
        "total_score": 3.65,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      }
    }
  ]
}