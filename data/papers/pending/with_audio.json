{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2512.10945v1",
      "title": "MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation",
      "authors": [
        "Henghui Ding",
        "Chang Liu",
        "Shuting He",
        "Kaining Ying",
        "Xudong Jiang",
        "Chen Change Loy",
        "Yu-Gang Jiang"
      ],
      "abstract": "This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10945v1",
        "pdf": "https://arxiv.org/pdf/2512.10945v1"
      },
      "arxiv_id": "2512.10945v1",
      "comment": "IEEE TPAMI, Project Page: https://henghuiding.com/MeViS/",
      "journal_ref": "H. Ding et al., \"MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 47, no. 12, pp. 11400-11416, 2025",
      "has_code": false,
      "relevance_score": 4.953389830508474,
      "score_breakdown": {
        "total_score": 4.95,
        "field_match": {
          "score": 0.51,
          "matches": [
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "PAMI",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be ide...",
        "key_contributions": [
          "This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions.",
          "Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame.",
          "Such datasets underemphasize the role of motion in both videos and languages."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.10945v1.mp3"
      }
    },
    {
      "id": "2512.10950v1",
      "title": "E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training",
      "authors": [
        "Qitao Zhao",
        "Hao Tan",
        "Qianqian Wang",
        "Sai Bi",
        "Kai Zhang",
        "Kalyan Sunkavalli",
        "Shubham Tulsiani",
        "Hanwen Jiang"
      ],
      "abstract": "Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10950v1",
        "pdf": "https://arxiv.org/pdf/2512.10950v1"
      },
      "arxiv_id": "2512.10950v1",
      "comment": "Project website: https://qitaozhao.github.io/E-RayZer",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.458474576271186,
      "score_breakdown": {
        "total_score": 4.46,
        "field_match": {
          "score": 1.27,
          "matches": [
            "self-supervised",
            "3d reconstruction"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such ...",
        "key_contributions": [
          "Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images.",
          "In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images.",
          "Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.10950v1.mp3"
      }
    },
    {
      "id": "2512.10808v1",
      "title": "Graph Laplacian Transformer with Progressive Sampling for Prostate Cancer Grading",
      "authors": [
        "Masum Shah Junayed",
        "John Derek Van Vessem",
        "Qian Wan",
        "Gahie Nam",
        "Sheida Nabavi"
      ],
      "abstract": "Prostate cancer grading from whole-slide images (WSIs) remains a challenging task due to the large-scale nature of WSIs, the presence of heterogeneous tissue structures, and difficulty of selecting diagnostically relevant regions. Existing approaches often rely on random or static patch selection, leading to the inclusion of redundant or non-informative regions that degrade performance. To address this, we propose a Graph Laplacian Attention-Based Transformer (GLAT) integrated with an Iterative Refinement Module (IRM) to enhance both feature learning and spatial consistency. The IRM iteratively refines patch selection by leveraging a pretrained ResNet50 for local feature extraction and a foundation model in no-gradient mode for importance scoring, ensuring only the most relevant tissue regions are preserved. The GLAT models tissue-level connectivity by constructing a graph where patches serve as nodes, ensuring spatial consistency through graph Laplacian constraints and refining feature representations via a learnable filtering mechanism that enhances discriminative histological structures. Additionally, a convex aggregation mechanism dynamically adjusts patch importance to generate a robust WSI-level representation. Extensive experiments on five public and one private dataset demonstrate that our model outperforms state-of-the-art methods, achieving higher performance and spatial consistency while maintaining computational efficiency.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10808v1",
        "pdf": "https://arxiv.org/pdf/2512.10808v1"
      },
      "arxiv_id": "2512.10808v1",
      "comment": "International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) 2025",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.3999999999999995,
      "score_breakdown": {
        "total_score": 4.4,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "MICCAI",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Prostate cancer grading from whole-slide images (WSIs) remains a challenging task due to the large-scale nature of WSIs, the presence of heterogeneous tissue structures, and difficulty of selecting diagnostically relevant regions. Existing approaches often rely on random or static patch selection, leading to the inclusion of redundant or non-informative regions that degrade performance. To address...",
        "key_contributions": [
          "Prostate cancer grading from whole-slide images (WSIs) remains a challenging task due to the large-scale nature of WSIs, the presence of heterogeneous tissue structures, and difficulty of selecting diagnostically relevant regions.",
          "Existing approaches often rely on random or static patch selection, leading to the inclusion of redundant or non-informative regions that degrade performance.",
          "To address this, we propose a Graph Laplacian Attention-Based Transformer (GLAT) integrated with an Iterative Refinement Module (IRM) to enhance both feature learning and spatial consistency."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.10808v1.mp3"
      }
    },
    {
      "id": "2512.10943v1",
      "title": "AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation",
      "authors": [
        "Sharath Girish",
        "Viacheslav Ivanov",
        "Tsai-Shien Chen",
        "Hao Chen",
        "Aliaksandr Siarohin",
        "Sergey Tulyakov"
      ],
      "abstract": "Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10943v1",
        "pdf": "https://arxiv.org/pdf/2512.10943v1"
      },
      "arxiv_id": "2512.10943v1",
      "comment": "Project page: https://snap-research.github.io/Video-AlcheMinT/snap-research.github.io/Video-AlcheMinT",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.35,
      "score_breakdown": {
        "total_score": 4.35,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose Alch...",
        "key_contributions": [
          "Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects.",
          "However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation.",
          "We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.10943v1.mp3"
      }
    },
    {
      "id": "2512.10878v1",
      "title": "Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes",
      "authors": [
        "Xuan Zhao",
        "Zhuo Cao",
        "Arya Bangun",
        "Hanno Scharr",
        "Ira Assent"
      ],
      "abstract": "Counterfactual explanations provide actionable insights by identifying minimal input changes required to achieve a desired model prediction. Beyond their interpretability benefits, counterfactuals can also be leveraged for model reconstruction, where a surrogate model is trained to replicate the behavior of a target model. In this work, we demonstrate that model reconstruction can be significantly improved by recognizing that counterfactuals, which typically lie close to the decision boundary, can serve as informative though less representative samples for both classes. This is particularly beneficial in settings with limited access to labeled data. We propose a method that integrates original data samples with counterfactuals to approximate class prototypes using the Wasserstein barycenter, thereby preserving the underlying distributional structure of each class. This approach enhances the quality of the surrogate model and mitigates the issue of decision boundary shift, which commonly arises when counterfactuals are naively treated as ordinary training instances. Empirical results across multiple datasets show that our method improves fidelity between the surrogate and target models, validating its effectiveness.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10878v1",
        "pdf": "https://arxiv.org/pdf/2512.10878v1"
      },
      "arxiv_id": "2512.10878v1",
      "comment": "Accepted by Actionable Interpretability Workshop at ICML 2025",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.25,
      "score_breakdown": {
        "total_score": 4.25,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICML",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Counterfactual explanations provide actionable insights by identifying minimal input changes required to achieve a desired model prediction. Beyond their interpretability benefits, counterfactuals can also be leveraged for model reconstruction, where a surrogate model is trained to replicate the behavior of a target model. In this work, we demonstrate that model reconstruction can be significantly...",
        "key_contributions": [
          "Counterfactual explanations provide actionable insights by identifying minimal input changes required to achieve a desired model prediction.",
          "Beyond their interpretability benefits, counterfactuals can also be leveraged for model reconstruction, where a surrogate model is trained to replicate the behavior of a target model.",
          "In this work, we demonstrate that model reconstruction can be significantly improved by recognizing that counterfactuals, which typically lie close to the decision boundary, can serve as informative though less representative samples for both classes."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.10878v1.mp3"
      }
    },
    {
      "id": "2512.10927v1",
      "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
      "authors": [
        "Yulu Gan",
        "Ligeng Zhu",
        "Dandan Shan",
        "Baifeng Shi",
        "Hongxu Yin",
        "Boris Ivanovic",
        "Song Han",
        "Trevor Darrell",
        "Jitendra Malik",
        "Marco Pavone",
        "Boyi Li"
      ],
      "abstract": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10927v1",
        "pdf": "https://arxiv.org/pdf/2512.10927v1"
      },
      "arxiv_id": "2512.10927v1",
      "comment": "Code is available at https://github.com/Wolfv0/FoundationMotion/tree/main",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.199999999999999,
      "score_breakdown": {
        "total_score": 4.2,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this ch...",
        "key_contributions": [
          "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states.",
          "However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets.",
          "Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.10927v1.mp3"
      }
    },
    {
      "id": "2512.10685v1",
      "title": "Sharp Monocular View Synthesis in Less Than a Second",
      "authors": [
        "Lars Mescheder",
        "Wei Dong",
        "Shiwei Li",
        "Xuyang Bai",
        "Marcel Santos",
        "Peiyun Hu",
        "Bruno Lecouat",
        "Mingmin Zhen",
        "AmaÃ«l Delaunoy",
        "Tian Fang",
        "Yanghai Tsin",
        "Stephan R. Richter",
        "Vladlen Koltun"
      ],
      "abstract": "We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10685v1",
        "pdf": "https://arxiv.org/pdf/2512.10685v1"
      },
      "arxiv_id": "2512.10685v1",
      "comment": "Code and weights available at https://github.com/apple/ml-sharp",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.188983050847457,
      "score_breakdown": {
        "total_score": 4.19,
        "field_match": {
          "score": 0.85,
          "matches": [
            "3d gaussian"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding h...",
        "key_contributions": [
          "We present SHARP, an approach to photorealistic view synthesis from a single image.",
          "Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene.",
          "This is done in less than a second on a standard GPU via a single feedforward pass through a neural network."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.10685v1.mp3"
      }
    },
    {
      "id": "2512.10955v1",
      "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
      "authors": [
        "Tsai-Shien Chen",
        "Aliaksandr Siarohin",
        "Guocheng Gordon Qian",
        "Kuan-Chieh Jackson Wang",
        "Egor Nemchinov",
        "Moayed Haji-Ali",
        "Riza Alp Guler",
        "Willi Menapace",
        "Ivan Skorokhodov",
        "Anil Kag",
        "Jun-Yan Zhu",
        "Sergey Tulyakov"
      ],
      "abstract": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10955v1",
        "pdf": "https://arxiv.org/pdf/2512.10955v1"
      },
      "arxiv_id": "2512.10955v1",
      "comment": "Project page: https://snap-research.github.io/omni-attribute",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.1499999999999995,
      "score_breakdown": {
        "total_score": 4.15,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis...",
        "key_contributions": [
          "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts.",
          "However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute.",
          "This often leads to information leakage and incoherent synthesis."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.10955v1.mp3"
      }
    },
    {
      "id": "2512.10940v1",
      "title": "OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis",
      "authors": [
        "Xiang Fan",
        "Sharath Girish",
        "Vivek Ramanujan",
        "Chaoyang Wang",
        "Ashkan Mirzaei",
        "Petr Sushko",
        "Aliaksandr Siarohin",
        "Sergey Tulyakov",
        "Ranjay Krishna"
      ],
      "abstract": "Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\\% in multiview NVS LLFF dataset, 60\\% in dynamic NVS Neural 3D Video benchmark, 20\\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10940v1",
        "pdf": "https://arxiv.org/pdf/2512.10940v1"
      },
      "arxiv_id": "2512.10940v1",
      "comment": "Project page: https://snap-research.github.io/OmniView/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.050000000000001,
      "score_breakdown": {
        "total_score": 4.05,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consiste...",
        "key_contributions": [
          "Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others.",
          "Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data.",
          "We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.10940v1.mp3"
      }
    },
    {
      "id": "2512.10949v1",
      "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
      "authors": [
        "Yiwen Tang",
        "Zoey Guo",
        "Kaixin Zhu",
        "Ray Zhang",
        "Qizhi Chen",
        "Dongzhi Jiang",
        "Junli Liu",
        "Bohan Zeng",
        "Haoming Song",
        "Delin Qu",
        "Tianyi Bai",
        "Dan Xu",
        "Wentao Zhang",
        "Bin Zhao"
      ],
      "abstract": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
      "published": "2025-12-11",
      "updated": "2025-12-11",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.10949v1",
        "pdf": "https://arxiv.org/pdf/2512.10949v1"
      },
      "arxiv_id": "2512.10949v1",
      "comment": "Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.0,
      "score_breakdown": {
        "total_score": 4.0,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation signific...",
        "key_contributions": [
          "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently.",
          "However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures.",
          "This makes 3D generation significantly sensitive to reward designs and RL algorithms."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.10949v1.mp3"
      }
    }
  ]
}