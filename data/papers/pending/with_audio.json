{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2601.19753v1",
      "title": "WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration",
      "authors": [
        "Xinrui Zhang",
        "Yufeng Wang",
        "Shuangkang Fang",
        "Zesheng Wang",
        "Dacheng Qi",
        "Wenrui Ding"
      ],
      "abstract": "Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering. Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects. To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network. Our method employs a dual-branch optimization strategy to ensure underwater photometric consistency while naturally recovering water-free appearances. This strategy is enhanced by depth-guided geometry regularization and perception-driven image loss, together with exposure constraints, spatially-adaptive regularization, and physically guided spectral regularization, which collectively enforce local 3D coherence and maintain natural visual perception. Experiments on standard benchmarks and our newly collected dataset demonstrate that WaterClear-GS achieves outstanding performance on both novel view synthesis (NVS) and underwater image restoration (UIR) tasks, while maintaining real-time rendering. The code will be available at https://buaaxrzhang.github.io/WaterClear-GS/.",
      "published": "2026-01-27",
      "updated": "2026-01-27",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.19753v1",
        "pdf": "https://arxiv.org/pdf/2601.19753v1"
      },
      "arxiv_id": "2601.19753v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 5.277118644067796,
      "score_breakdown": {
        "total_score": 5.28,
        "field_match": {
          "score": 4.07,
          "matches": [
            "3d gaussian",
            "gaussian splatting",
            "3d reconstruction",
            "volumetric",
            "neural radiance",
            "nerf"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering. Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering e...",
        "key_contributions": [
          "Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering.",
          "Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects.",
          "To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.19753v1.mp3"
      }
    },
    {
      "id": "2601.19850v1",
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "authors": [
        "Binzhu Xie",
        "Shi Qiu",
        "Sicheng Zhang",
        "Yinqiao Wang",
        "Hao Xu",
        "Muzammal Naseer",
        "Chi-Wing Fu",
        "Pheng-Ann Heng"
      ],
      "abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: https://github.com/Nicous20/EgoHandICL",
      "published": "2026-01-27",
      "updated": "2026-01-27",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.19850v1",
        "pdf": "https://arxiv.org/pdf/2601.19850v1"
      },
      "arxiv_id": "2601.19850v1",
      "comment": "Accepted in ICLR 2026, Codebase: https://github.com/Nicous20/EgoHandICL",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 5.25,
      "score_breakdown": {
        "total_score": 5.25,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignm...",
        "key_contributions": [
          "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions.",
          "Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts.",
          "We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.19850v1.mp3"
      }
    },
    {
      "id": "2601.19580v1",
      "title": "QuaMo: Quaternion Motions for Vision-based 3D Human Kinematics Capture",
      "authors": [
        "Cuong Le",
        "Pavlo Melnyk",
        "Urs Waldmann",
        "Mårten Wadenbäck",
        "Bastian Wandt"
      ],
      "abstract": "Vision-based 3D human motion capture from videos remains a challenge in computer vision. Traditional 3D pose estimation approaches often ignore the temporal consistency between frames, causing implausible and jittery motion. The emerging field of kinematics-based 3D motion capture addresses these issues by estimating the temporal transitioning between poses instead. A major drawback in current kinematics approaches is their reliance on Euler angles. Despite their simplicity, Euler angles suffer from discontinuity that leads to unstable motion reconstructions, especially in online settings where trajectory refinement is unavailable. Contrarily, quaternions have no discontinuity and can produce continuous transitions between poses. In this paper, we propose QuaMo, a novel Quaternion Motions method using quaternion differential equations (QDE) for human kinematics capture. We utilize the state-space model, an effective system for describing real-time kinematics estimations, with quaternion state and the QDE describing quaternion velocity. The corresponding angular acceleration is computed from a meta-PD controller with a novel acceleration enhancement that adaptively regulates the control signals as the human quickly changes to a new pose. Unlike previous work, our QDE is solved under the quaternion unit-sphere constraint that results in more accurate estimations. Experimental results show that our novel formulation of the QDE with acceleration enhancement accurately estimates 3D human kinematics with no discontinuity and minimal implausibilities. QuaMo outperforms comparable state-of-the-art methods on multiple datasets, namely Human3.6M, Fit3D, SportsPose and AIST. The code is available at https://github.com/cuongle1206/QuaMo",
      "published": "2026-01-27",
      "updated": "2026-01-27",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.19580v1",
        "pdf": "https://arxiv.org/pdf/2601.19580v1"
      },
      "arxiv_id": "2601.19580v1",
      "comment": "10 pages, 4 figures, accepted to ICLR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.8694915254237285,
      "score_breakdown": {
        "total_score": 4.87,
        "field_match": {
          "score": 0.42,
          "matches": [
            "computer vision"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Vision-based 3D human motion capture from videos remains a challenge in computer vision. Traditional 3D pose estimation approaches often ignore the temporal consistency between frames, causing implausible and jittery motion. The emerging field of kinematics-based 3D motion capture addresses these issues by estimating the temporal transitioning between poses instead. A major drawback in current kin...",
        "key_contributions": [
          "Vision-based 3D human motion capture from videos remains a challenge in computer vision.",
          "Traditional 3D pose estimation approaches often ignore the temporal consistency between frames, causing implausible and jittery motion.",
          "The emerging field of kinematics-based 3D motion capture addresses these issues by estimating the temporal transitioning between poses instead."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.19580v1.mp3"
      }
    },
    {
      "id": "2601.19659v1",
      "title": "KeepLoRA: Continual Learning with Residual Gradient Adaptation",
      "authors": [
        "Mao-Lin Luo",
        "Zi-Hao Zhou",
        "Yi-Lin Zhang",
        "Yuanyu Wan",
        "Tong Wei",
        "Min-Ling Zhang"
      ],
      "abstract": "Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at https://github.com/MaolinLuo/KeepLoRA.",
      "published": "2026-01-27",
      "updated": "2026-01-27",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.19659v1",
        "pdf": "https://arxiv.org/pdf/2601.19659v1"
      },
      "arxiv_id": "2601.19659v1",
      "comment": "Accepted at ICLR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.699999999999999,
      "score_breakdown": {
        "total_score": 4.7,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention ...",
        "key_contributions": [
          "Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge.",
          "This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives.",
          "We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.19659v1.mp3"
      }
    },
    {
      "id": "2601.19686v1",
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "authors": [
        "Ziyue Wang",
        "Sheng Jin",
        "Zhongrong Zuo",
        "Jiawei Wu",
        "Han Qiu",
        "Qi She",
        "Hao Zhang",
        "Xudong Jiang"
      ],
      "abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.",
      "published": "2026-01-27",
      "updated": "2026-01-27",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.19686v1",
        "pdf": "https://arxiv.org/pdf/2601.19686v1"
      },
      "arxiv_id": "2601.19686v1",
      "comment": "Accepted to ICLR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.55,
      "score_breakdown": {
        "total_score": 4.55,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modali...",
        "key_contributions": [
          "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability.",
          "We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty.",
          "By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.19686v1.mp3"
      }
    },
    {
      "id": "2601.19768v1",
      "title": "GAVEL: Towards rule-based safety through activation monitoring",
      "authors": [
        "Shir Rozenfeld",
        "Rahul Pankajakshan",
        "Itay Zloczower",
        "Eyal Lenga",
        "Gilad Gressel",
        "Yisroel Mirsky"
      ],
      "abstract": "Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.",
      "published": "2026-01-27",
      "updated": "2026-01-27",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.19768v1",
        "pdf": "https://arxiv.org/pdf/2601.19768v1"
      },
      "arxiv_id": "2601.19768v1",
      "comment": "Accepted to ICLR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.449999999999999,
      "score_breakdown": {
        "total_score": 4.45,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation...",
        "key_contributions": [
          "Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level.",
          "However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability.",
          "This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.19768v1.mp3"
      }
    },
    {
      "id": "2601.19541v1",
      "title": "GenCP: Towards Generative Modeling Paradigm of Coupled Physics",
      "authors": [
        "Tianrun Gao",
        "Haoren Zheng",
        "Wenhao Deng",
        "Haodong Feng",
        "Tao Zhang",
        "Ruiqi Feng",
        "Qianyi Chen",
        "Tailin Wu"
      ],
      "abstract": "Real-world physical systems are inherently complex, often involving the coupling of multiple physics, making their simulation both highly valuable and challenging. Many mainstream approaches face challenges when dealing with decoupled data. Besides, they also suffer from low efficiency and fidelity in strongly coupled spatio-temporal physical systems. Here we propose GenCP, a novel and elegant generative paradigm for coupled multiphysics simulation. By formulating coupled-physics modeling as a probability modeling problem, our key innovation is to integrate probability density evolution in generative modeling with iterative multiphysics coupling, thereby enabling training on data from decoupled simulation and inferring coupled physics during sampling. We also utilize operator-splitting theory in the space of probability evolution to establish error controllability guarantees for this \"conditional-to-joint\" sampling scheme. We evaluate our paradigm on a synthetic setting and three challenging multi-physics scenarios to demonstrate both principled insight and superior application performance of GenCP. Code is available at this repo: github.com/AI4Science-WestlakeU/GenCP.",
      "published": "2026-01-27",
      "updated": "2026-01-27",
      "categories": [
        "cs.LG",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.19541v1",
        "pdf": "https://arxiv.org/pdf/2601.19541v1"
      },
      "arxiv_id": "2601.19541v1",
      "comment": "ICLR 2026 Accpeted",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.449999999999999,
      "score_breakdown": {
        "total_score": 4.45,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Real-world physical systems are inherently complex, often involving the coupling of multiple physics, making their simulation both highly valuable and challenging. Many mainstream approaches face challenges when dealing with decoupled data. Besides, they also suffer from low efficiency and fidelity in strongly coupled spatio-temporal physical systems. Here we propose GenCP, a novel and elegant gen...",
        "key_contributions": [
          "Real-world physical systems are inherently complex, often involving the coupling of multiple physics, making their simulation both highly valuable and challenging.",
          "Many mainstream approaches face challenges when dealing with decoupled data.",
          "Besides, they also suffer from low efficiency and fidelity in strongly coupled spatio-temporal physical systems."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.19541v1.mp3"
      }
    },
    {
      "id": "2601.19618v1",
      "title": "The role of self-supervised pretraining in differentially private medical image analysis",
      "authors": [
        "Soroosh Tayebi Arasteh",
        "Mina Farajiamiri",
        "Mahshad Lotfinia",
        "Behrus Hinrichs-Puladi",
        "Jonas Bienzeisler",
        "Mohamed Alhaskir",
        "Mirabela Rusu",
        "Christiane Kuhl",
        "Sven Nebelung",
        "Daniel Truhn"
      ],
      "abstract": "Differential privacy (DP) provides formal protection for sensitive data but typically incurs substantial losses in diagnostic performance. Model initialization has emerged as a critical factor in mitigating this degradation, yet the role of modern self-supervised learning under full-model DP remains poorly understood. Here, we present a large-scale evaluation of initialization strategies for differentially private medical image analysis, using chest radiograph classification as a representative benchmark with more than 800,000 images. Using state-of-the-art ConvNeXt models trained with DP-SGD across realistic privacy regimes, we compare non-domain-specific supervised ImageNet initialization, non-domain-specific self-supervised DINOv3 initialization, and domain-specific supervised pretraining on MIMIC-CXR, the largest publicly available chest radiograph dataset. Evaluations are conducted across five external datasets spanning diverse institutions and acquisition settings. We show that DINOv3 initialization consistently improves diagnostic utility relative to ImageNet initialization under DP, but remains inferior to domain-specific supervised pretraining, which achieves performance closest to non-private baselines. We further demonstrate that initialization choice strongly influences demographic fairness, cross-dataset generalization, and robustness to data scale and model capacity under privacy constraints. The results establish initialization strategy as a central determinant of utility, fairness, and generalization in differentially private medical imaging.",
      "published": "2026-01-27",
      "updated": "2026-01-27",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.19618v1",
        "pdf": "https://arxiv.org/pdf/2601.19618v1"
      },
      "arxiv_id": "2601.19618v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.400847457627119,
      "score_breakdown": {
        "total_score": 4.4,
        "field_match": {
          "score": 2.63,
          "matches": [
            "medical image",
            "medical imaging",
            "self-supervised",
            "image analysis"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Differential privacy (DP) provides formal protection for sensitive data but typically incurs substantial losses in diagnostic performance. Model initialization has emerged as a critical factor in mitigating this degradation, yet the role of modern self-supervised learning under full-model DP remains poorly understood. Here, we present a large-scale evaluation of initialization strategies for diffe...",
        "key_contributions": [
          "Differential privacy (DP) provides formal protection for sensitive data but typically incurs substantial losses in diagnostic performance.",
          "Model initialization has emerged as a critical factor in mitigating this degradation, yet the role of modern self-supervised learning under full-model DP remains poorly understood.",
          "Here, we present a large-scale evaluation of initialization strategies for differentially private medical image analysis, using chest radiograph classification as a representative benchmark with more than 800,000 images."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.19618v1.mp3"
      }
    },
    {
      "id": "2601.19810v1",
      "title": "Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals",
      "authors": [
        "Octavio Pappalardo"
      ],
      "abstract": "Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent's post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent's capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula.",
      "published": "2026-01-27",
      "updated": "2026-01-27",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.19810v1",
        "pdf": "https://arxiv.org/pdf/2601.19810v1"
      },
      "arxiv_id": "2601.19810v1",
      "comment": "To appear at ICLR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.3999999999999995,
      "score_breakdown": {
        "total_score": 4.4,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream t...",
        "key_contributions": [
          "Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks.",
          "A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals.",
          "The core challenge lies in how to effectively generate, select, and learn from such goals."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.19810v1.mp3"
      }
    },
    {
      "id": "2601.19707v1",
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "authors": [
        "Yunyue Wei",
        "Chenhui Zuo",
        "Yanan Sui"
      ],
      "abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale.",
      "published": "2026-01-27",
      "updated": "2026-01-27",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.19707v1",
        "pdf": "https://arxiv.org/pdf/2601.19707v1"
      },
      "arxiv_id": "2601.19707v1",
      "comment": "Accepted by ICLR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.3500000000000005,
      "score_breakdown": {
        "total_score": 4.35,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy express...",
        "key_contributions": [
          "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical.",
          "Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows.",
          "Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.19707v1.mp3"
      }
    }
  ]
}