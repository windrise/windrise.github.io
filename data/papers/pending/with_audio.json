{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2512.01913v1",
      "title": "Disentangling Progress in Medical Image Registration: Beyond Trend-Driven Architectures towards Domain-Specific Strategies",
      "authors": [
        "Bailiang Jian",
        "Jiazhen Pan",
        "Rohit Jena",
        "Morteza Ghahremani",
        "Hongwei Bran Li",
        "Daniel Rueckert",
        "Christian Wachinger",
        "Benedikt Wiestler"
      ],
      "abstract": "Medical image registration drives quantitative analysis across organs, modalities, and patient populations. Recent deep learning methods often combine low-level \"trend-driven\" computational blocks from computer vision, such as large-kernel CNNs, Transformers, and state-space models, with high-level registration-specific designs like motion pyramids, correlation layers, and iterative refinement. Yet, their relative contributions remain unclear and entangled. This raises a central question: should future advances in registration focus on importing generic architectural trends or on refining domain-specific design principles? Through a modular framework spanning brain, lung, cardiac, and abdominal registration, we systematically disentangle the influence of these two paradigms. Our evaluation reveals that low-level \"trend-driven\" computational blocks offer only marginal or inconsistent gains, while high-level registration-specific designs consistently deliver more accurate, smoother, and more robust deformations. These domain priors significantly elevate the performance of a standard U-Net baseline, far more than variants incorporating \"trend-driven\" blocks, achieving an average relative improvement of $\\sim3\\%$. All models and experiments are released within a transparent, modular benchmark that enables plug-and-play comparison for new architectures and registration tasks (https://github.com/BailiangJ/rethink-reg). This dynamic and extensible platform establishes a common ground for reproducible and fair evaluation, inviting the community to isolate genuine methodological contributions from domain priors. Our findings advocate a shift in research emphasis: from following architectural trends to embracing domain-specific design principles as the true drivers of progress in learning-based medical image registration.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01913v1",
        "pdf": "https://arxiv.org/pdf/2512.01913v1"
      },
      "arxiv_id": "2512.01913v1",
      "comment": "Submitted to Medical Image Analysis. Journal Extension of arXiv:2407.19274",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.452542372881356,
      "score_breakdown": {
        "total_score": 4.45,
        "field_match": {
          "score": 2.88,
          "matches": [
            "medical image",
            "cardiac",
            "registration",
            "deep learning",
            "computer vision"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Medical image registration drives quantitative analysis across organs, modalities, and patient populations. Recent deep learning methods often combine low-level \"trend-driven\" computational blocks from computer vision, such as large-kernel CNNs, Transformers, and state-space models, with high-level registration-specific designs like motion pyramids, correlation layers, and iterative refinement. Ye...",
        "key_contributions": [
          "Medical image registration drives quantitative analysis across organs, modalities, and patient populations.",
          "Recent deep learning methods often combine low-level \"trend-driven\" computational blocks from computer vision, such as large-kernel CNNs, Transformers, and state-space models, with high-level registration-specific designs like motion pyramids, correlation layers, and iterative refinement.",
          "Yet, their relative contributions remain unclear and entangled."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.01913v1.mp3"
      }
    },
    {
      "id": "2512.02006v1",
      "title": "MV-TAP: Tracking Any Point in Multi-View Videos",
      "authors": [
        "Jahyeok Koo",
        "In√®s Hyeonsu Kim",
        "Mungyeom Kim",
        "Junghyun Park",
        "Seohyun Park",
        "Jaeyeong Kim",
        "Jung Yi",
        "Seokju Cho",
        "Seungryong Kim"
      ],
      "abstract": "Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications. In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information. MV-TAP utilizes camera geometry and a cross-view attention mechanism to aggregate spatio-temporal information across views, enabling more complete and reliable trajectory estimation in multi-view videos. To support this task, we construct a large-scale synthetic training dataset and real-world evaluation sets tailored for multi-view tracking. Extensive experiments demonstrate that MV-TAP outperforms existing point-tracking methods on challenging benchmarks, establishing an effective baseline for advancing research in multi-view point tracking.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02006v1",
        "pdf": "https://arxiv.org/pdf/2512.02006v1"
      },
      "arxiv_id": "2512.02006v1",
      "comment": "Project Page: https://cvlab-kaist.github.io/MV-TAP/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.15,
      "score_breakdown": {
        "total_score": 4.15,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 7.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications. In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information. MV-TAP utilizes camera geometry and a cross-view attention mech...",
        "key_contributions": [
          "Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications.",
          "In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information.",
          "MV-TAP utilizes camera geometry and a cross-view attention mechanism to aggregate spatio-temporal information across views, enabling more complete and reliable trajectory estimation in multi-view videos."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.02006v1.mp3"
      }
    },
    {
      "id": "2512.01970v1",
      "title": "From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning",
      "authors": [
        "Sitao Cheng",
        "Xunjian Yin",
        "Ruiwen Zhou",
        "Yuxuan Li",
        "Xinyi Wang",
        "Liangming Pan",
        "William Yang Wang",
        "Victor Zhong"
      ],
      "abstract": "The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01970v1",
        "pdf": "https://arxiv.org/pdf/2512.01970v1"
      },
      "arxiv_id": "2512.01970v1",
      "comment": "Work in Progress. Code and data will be available at https://github.com/sitaocheng/from_atomic_to_composite",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.1499999999999995,
      "score_breakdown": {
        "total_score": 4.15,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a con...",
        "key_contributions": [
          "The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate.",
          "In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information.",
          "Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information)."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.01970v1.mp3"
      }
    },
    {
      "id": "2512.01771v1",
      "title": "Robust Rigid and Non-Rigid Medical Image Registration Using Learnable Edge Kernels",
      "authors": [
        "Ahsan Raza Siyal",
        "Markus Haltmeier",
        "Ruth Steiger",
        "Malik Galijasevic",
        "Elke Ruth Gizewski",
        "Astrid Ellen Grams"
      ],
      "abstract": "Medical image registration is crucial for various clinical and research applications including disease diagnosis or treatment planning which require alignment of images from different modalities, time points, or subjects. Traditional registration techniques often struggle with challenges such as contrast differences, spatial distortions, and modality-specific variations. To address these limitations, we propose a method that integrates learnable edge kernels with learning-based rigid and non-rigid registration techniques. Unlike conventional layers that learn all features without specific bias, our approach begins with a predefined edge detection kernel, which is then perturbed with random noise. These kernels are learned during training to extract optimal edge features tailored to the task. This adaptive edge detection enhances the registration process by capturing diverse structural features critical in medical imaging. To provide clearer insight into the contribution of each component in our design, we introduce four variant models for rigid registration and four variant models for non-rigid registration. We evaluated our approach using a dataset provided by the Medical University across three setups: rigid registration without skull removal, with skull removal, and non-rigid registration. Additionally, we assessed performance on two publicly available datasets. Across all experiments, our method consistently outperformed state-of-the-art techniques, demonstrating its potential to improve multi-modal image alignment and anatomical structure analysis.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01771v1",
        "pdf": "https://arxiv.org/pdf/2512.01771v1"
      },
      "arxiv_id": "2512.01771v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.063559322033899,
      "score_breakdown": {
        "total_score": 4.06,
        "field_match": {
          "score": 2.03,
          "matches": [
            "medical image",
            "medical imaging",
            "registration"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Medical image registration is crucial for various clinical and research applications including disease diagnosis or treatment planning which require alignment of images from different modalities, time points, or subjects. Traditional registration techniques often struggle with challenges such as contrast differences, spatial distortions, and modality-specific variations. To address these limitatio...",
        "key_contributions": [
          "Medical image registration is crucial for various clinical and research applications including disease diagnosis or treatment planning which require alignment of images from different modalities, time points, or subjects.",
          "Traditional registration techniques often struggle with challenges such as contrast differences, spatial distortions, and modality-specific variations.",
          "To address these limitations, we propose a method that integrates learnable edge kernels with learning-based rigid and non-rigid registration techniques."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.01771v1.mp3"
      }
    },
    {
      "id": "2512.01827v1",
      "title": "CauSight: Learning to Supersense for Visual Causal Discovery",
      "authors": [
        "Yize Zhang",
        "Meiqi Chen",
        "Sirui Chen",
        "Bo Peng",
        "Yanxi Zhang",
        "Tianyu Li",
        "Chaochao Lu"
      ],
      "abstract": "Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), a large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, a novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with a designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over a threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01827v1",
        "pdf": "https://arxiv.org/pdf/2512.01827v1"
      },
      "arxiv_id": "2512.01827v1",
      "comment": "project page: https://github.com/OpenCausaLab/CauSight",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.0,
      "score_breakdown": {
        "total_score": 4.0,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset ...",
        "key_contributions": [
          "Causal thinking enables humans to understand not just what is seen, but why it happens.",
          "To replicate this capability in modern AI systems, we introduce the task of visual causal discovery.",
          "It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.01827v1.mp3"
      }
    },
    {
      "id": "2512.02020v1",
      "title": "EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI",
      "authors": [
        "Jianlei Chang",
        "Ruofeng Mei",
        "Wei Ke",
        "Xiangyu Xu"
      ],
      "abstract": "Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02020v1",
        "pdf": "https://arxiv.org/pdf/2512.02020v1"
      },
      "arxiv_id": "2512.02020v1",
      "comment": "Accepted by AAAI 2026. Project Page: https://efficientflow.github.io/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8499999999999996,
      "score_breakdown": {
        "total_score": 3.85,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified fram...",
        "key_contributions": [
          "Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks.",
          "However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference.",
          "We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.02020v1.mp3"
      }
    },
    {
      "id": "2512.02009v1",
      "title": "AirSim360: A Panoramic Simulation Platform within Drone View",
      "authors": [
        "Xian Ge",
        "Yuling Pan",
        "Yuhang Zhang",
        "Xiang Li",
        "Weijun Zhang",
        "Dizhe Zhang",
        "Zhaoliang Wan",
        "Xin Lin",
        "Xiangkai Zhang",
        "Juntao Liang",
        "Jason Li",
        "Wenjie Jiang",
        "Bo Du",
        "Ming-Hsuan Yang",
        "Lu Qi"
      ],
      "abstract": "The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02009v1",
        "pdf": "https://arxiv.org/pdf/2512.02009v1"
      },
      "arxiv_id": "2512.02009v1",
      "comment": "Project Website: https://insta360-research-team.github.io/AirSim360-website/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8499999999999996,
      "score_breakdown": {
        "total_score": 3.85,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on ...",
        "key_contributions": [
          "The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence.",
          "However, the lack of large-scale and diverse data remains a major limitation.",
          "In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.02009v1.mp3"
      }
    },
    {
      "id": "2512.02015v1",
      "title": "Generative Video Motion Editing with 3D Point Tracks",
      "authors": [
        "Yao-Chih Lee",
        "Zhoutong Zhang",
        "Jiahui Huang",
        "Jui-Hsien Wang",
        "Joon-Young Lee",
        "Jia-Bin Huang",
        "Eli Shechtman",
        "Zhengqi Li"
      ],
      "abstract": "Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02015v1",
        "pdf": "https://arxiv.org/pdf/2512.02015v1"
      },
      "arxiv_id": "2512.02015v1",
      "comment": "Project page: https://edit-by-track.github.io",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8,
      "score_breakdown": {
        "total_score": 3.8,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but ...",
        "key_contributions": [
          "Camera and object motions are central to a video's narrative.",
          "However, precisely editing these captured motions remains a significant challenge, especially under complex object movements.",
          "Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.02015v1.mp3"
      }
    },
    {
      "id": "2512.01850v1",
      "title": "Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching",
      "authors": [
        "Yue Pan",
        "Tao Sun",
        "Liyuan Zhu",
        "Lucas Nunes",
        "Iro Armeni",
        "Jens Behley",
        "Cyrill Stachniss"
      ],
      "abstract": "Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.01850v1",
        "pdf": "https://arxiv.org/pdf/2512.01850v1"
      },
      "arxiv_id": "2512.01850v1",
      "comment": "22 pages",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.7906779661016947,
      "score_breakdown": {
        "total_score": 3.79,
        "field_match": {
          "score": 1.1,
          "matches": [
            "3d reconstruction",
            "registration"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct corresponde...",
        "key_contributions": [
          "Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization.",
          "In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered.",
          "Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.01850v1.mp3"
      }
    },
    {
      "id": "2512.02017v1",
      "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion",
      "authors": [
        "Shaowei Liu",
        "David Yifan Yao",
        "Saurabh Gupta",
        "Shenlong Wang"
      ],
      "abstract": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",
      "published": "2025-12-01",
      "updated": "2025-12-01",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.02017v1",
        "pdf": "https://arxiv.org/pdf/2512.02017v1"
      },
      "arxiv_id": "2512.02017v1",
      "comment": "Accepted to NeurIPS 2025. Project page: https://stevenlsw.github.io/visualsync/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.7872881355932204,
      "score_breakdown": {
        "total_score": 3.79,
        "field_match": {
          "score": 0.59,
          "matches": [
            "3d reconstruction"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 5.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based o...",
        "key_contributions": [
          "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras.",
          "However, synchronizing these cross-camera streams remains challenging.",
          "Existing methods assume controlled settings, specific targets, manual correction, or costly hardware."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.02017v1.mp3"
      }
    }
  ]
}