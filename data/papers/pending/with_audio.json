{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2512.24763v1",
      "title": "UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning",
      "authors": [
        "Ankit Dhiman",
        "Srinath R",
        "Jaswanth Reddy",
        "Lokesh R Boregowda",
        "Venkatesh Babu Radhakrishnan"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have advanced novel-view synthesis. Recent methods extend multi-view 2D segmentation to 3D, enabling instance/semantic segmentation for better scene understanding. A key challenge is the inconsistency of 2D instance labels across views, leading to poor 3D predictions. Existing methods use a two-stage approach in which some rely on contrastive learning with hyperparameter-sensitive clustering, while others preprocess labels for consistency. We propose a unified framework that merges these steps, reducing training time and improving performance by introducing a learnable feature embedding for segmentation in Gaussian primitives. This embedding is then efficiently decoded into instance labels through a novel \"Embedding-to-Label\" process, effectively integrating the optimization. While this unified framework offers substantial benefits, we observed artifacts at the object boundaries. To address the object boundary issues, we propose hard-mining samples along these boundaries. However, directly applying hard mining to the feature embeddings proved unstable. Therefore, we apply a linear layer to the rasterized feature embeddings before calculating the triplet loss, which stabilizes training and significantly improves performance. Our method outperforms baselines qualitatively and quantitatively on the ScanNet, Replica3D, and Messy-Rooms datasets.",
      "published": "2025-12-31",
      "updated": "2025-12-31",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.24763v1",
        "pdf": "https://arxiv.org/pdf/2512.24763v1"
      },
      "arxiv_id": "2512.24763v1",
      "comment": "Accepted to AAAI 2026. Project Page: https://unic-lift.github.io/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 5.20593220338983,
      "score_breakdown": {
        "total_score": 5.21,
        "field_match": {
          "score": 3.39,
          "matches": [
            "3d gaussian",
            "gaussian splatting",
            "neural radiance",
            "nerf",
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have advanced novel-view synthesis. Recent methods extend multi-view 2D segmentation to 3D, enabling instance/semantic segmentation for better scene understanding. A key challenge is the inconsistency of 2D instance labels across views, leading to poor 3D predictions. Existing methods use a two-stage approach in which some rely on cont...",
        "key_contributions": [
          "3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have advanced novel-view synthesis.",
          "Recent methods extend multi-view 2D segmentation to 3D, enabling instance/semantic segmentation for better scene understanding.",
          "A key challenge is the inconsistency of 2D instance labels across views, leading to poor 3D predictions."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.24763v1.mp3"
      }
    },
    {
      "id": "2512.25034v1",
      "title": "Generative Classifiers Avoid Shortcut Solutions",
      "authors": [
        "Alexander C. Li",
        "Ananya Kumar",
        "Deepak Pathak"
      ],
      "abstract": "Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones.",
      "published": "2025-12-31",
      "updated": "2025-12-31",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.25034v1",
        "pdf": "https://arxiv.org/pdf/2512.25034v1"
      },
      "arxiv_id": "2512.25034v1",
      "comment": "ICLR 2025. Code: https://github.com/alexlioralexli/generative-classifiers",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 5.2,
      "score_breakdown": {
        "total_score": 5.2,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, ins...",
        "key_contributions": [
          "Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift.",
          "This failure mode stems from an overreliance on features that are spuriously correlated with the label.",
          "We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.25034v1.mp3"
      }
    },
    {
      "id": "2512.24971v1",
      "title": "Evaluating the Impact of Compression Techniques on the Robustness of CNNs under Natural Corruptions",
      "authors": [
        "Itallo Patrick Castro Alves Da Silva",
        "Emanuel Adler Medeiros Pereira",
        "Erick de Andrade Barboza",
        "Baldoino Fonseca dos Santos Neto",
        "Marcio de Medeiros Ribeiro"
      ],
      "abstract": "Compressed deep learning models are crucial for deploying computer vision systems on resource-constrained devices. However, model compression may affect robustness, especially under natural corruption. Therefore, it is important to consider robustness evaluation while validating computer vision systems. This paper presents a comprehensive evaluation of compression techniques - quantization, pruning, and weight clustering applied individually and in combination to convolutional neural networks (ResNet-50, VGG-19, and MobileNetV2). Using the CIFAR-10-C and CIFAR 100-C datasets, we analyze the trade-offs between robustness, accuracy, and compression ratio. Our results show that certain compression strategies not only preserve but can also improve robustness, particularly on networks with more complex architectures. Utilizing multiobjective assessment, we determine the best configurations, showing that customized technique combinations produce beneficial multi-objective results. This study provides insights into selecting compression methods for robust and efficient deployment of models in corrupted real-world environments.",
      "published": "2025-12-31",
      "updated": "2025-12-31",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.24971v1",
        "pdf": "https://arxiv.org/pdf/2512.24971v1"
      },
      "arxiv_id": "2512.24971v1",
      "comment": "Accepted for publication at the 2025 International Conference on Machine Learning and Applications (ICMLA). IEEE Catalog Number: CFP25592-ART",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.838983050847458,
      "score_breakdown": {
        "total_score": 4.84,
        "field_match": {
          "score": 0.85,
          "matches": [
            "deep learning",
            "computer vision"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICML",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Compressed deep learning models are crucial for deploying computer vision systems on resource-constrained devices. However, model compression may affect robustness, especially under natural corruption. Therefore, it is important to consider robustness evaluation while validating computer vision systems. This paper presents a comprehensive evaluation of compression techniques - quantization, prunin...",
        "key_contributions": [
          "Compressed deep learning models are crucial for deploying computer vision systems on resource-constrained devices.",
          "However, model compression may affect robustness, especially under natural corruption.",
          "Therefore, it is important to consider robustness evaluation while validating computer vision systems."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.24971v1.mp3"
      }
    },
    {
      "id": "2512.25073v1",
      "title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction",
      "authors": [
        "Yi-Chuan Huang",
        "Hao-Jen Chien",
        "Chin-Yang Lin",
        "Ying-Huan Chen",
        "Yu-Lun Liu"
      ],
      "abstract": "Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a $25\\times$ speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/",
      "published": "2025-12-31",
      "updated": "2025-12-31",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.25073v1",
        "pdf": "https://arxiv.org/pdf/2512.25073v1"
      },
      "arxiv_id": "2512.25073v1",
      "comment": "Project page: https://yichuanh.github.io/GaMO/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.487288135593221,
      "score_breakdown": {
        "total_score": 4.49,
        "field_match": {
          "score": 0.59,
          "matches": [
            "3d reconstruction"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements b...",
        "key_contributions": [
          "Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited.",
          "Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge.",
          "Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.25073v1.mp3"
      }
    },
    {
      "id": "2512.25066v1",
      "title": "From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing",
      "authors": [
        "Xu He",
        "Haoxian Zhang",
        "Hejia Chen",
        "Changyuan Zheng",
        "Liyang Chen",
        "Songlin Tang",
        "Jiehui Huang",
        "Xiaoqiang Liu",
        "Pengfei Wan",
        "Zhiyong Wu"
      ],
      "abstract": "Audio-driven visual dubbing aims to synchronize a video's lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject's lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simultaneously hallucinate missing content and sync lips, leading to visual artifacts, identity drift, and poor synchronization. In this work, we propose a novel self-bootstrapping framework that reframes visual dubbing from an ill-posed inpainting task into a well-conditioned video-to-video editing problem. Our approach employs a Diffusion Transformer, first as a data generator, to synthesize ideal training data: a lip-altered companion video for each real sample, forming visually aligned video pairs. A DiT-based audio-driven editor is then trained on these pairs end-to-end, leveraging the complete and aligned input video frames to focus solely on precise, audio-driven lip modifications. This complete, frame-aligned input conditioning forms a rich visual context for the editor, providing it with complete identity cues, scene interactions, and continuous spatiotemporal dynamics. Leveraging this rich context fundamentally enables our method to achieve highly accurate lip sync, faithful identity preservation, and exceptional robustness against challenging in-the-wild scenarios. We further introduce a timestep-adaptive multi-phase learning strategy as a necessary component to disentangle conflicting editing objectives across diffusion timesteps, thereby facilitating stable training and yielding enhanced lip synchronization and visual fidelity. Additionally, we propose ContextDubBench, a comprehensive benchmark dataset for robust evaluation in diverse and challenging practical application scenarios.",
      "published": "2025-12-31",
      "updated": "2025-12-31",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.25066v1",
        "pdf": "https://arxiv.org/pdf/2512.25066v1"
      },
      "arxiv_id": "2512.25066v1",
      "comment": "Project Page https://hjrphoebus.github.io/X-Dub",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.45,
      "score_breakdown": {
        "total_score": 4.45,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 7.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Audio-driven visual dubbing aims to synchronize a video's lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject's lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simulta...",
        "key_contributions": [
          "Audio-driven visual dubbing aims to synchronize a video's lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject's lip movements differ while all other visual conditions are identical.",
          "Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simultaneously hallucinate missing content and sync lips, leading to visual artifacts, identity drift, and poor synchronization.",
          "In this work, we propose a novel self-bootstrapping framework that reframes visual dubbing from an ill-posed inpainting task into a well-conditioned video-to-video editing problem."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.25066v1.mp3"
      }
    },
    {
      "id": "2512.24742v1",
      "title": "Splatwizard: A Benchmark Toolkit for 3D Gaussian Splatting Compression",
      "authors": [
        "Xiang Liu",
        "Yimin Zhou",
        "Jinxiang Wang",
        "Yujun Huang",
        "Shuzhao Xie",
        "Shiyu Qin",
        "Mingyao Hong",
        "Jiawei Li",
        "Yaowei Wang",
        "Zhi Wang",
        "Shu-Tao Xia",
        "Bin Chen"
      ],
      "abstract": "The recent advent of 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in real-time novel view synthesis. However, the rapid proliferation of 3DGS-based algorithms has created a pressing need for standardized and comprehensive evaluation tools, especially for compression task. Existing benchmarks often lack the specific metrics necessary to holistically assess the unique characteristics of different methods, such as rendering speed, rate distortion trade-offs memory efficiency, and geometric accuracy. To address this gap, we introduce Splatwizard, a unified benchmark toolkit designed specifically for benchmarking 3DGS compression models. Splatwizard provides an easy-to-use framework to implement new 3DGS compression model and utilize state-of-the-art techniques proposed by previous work. Besides, an integrated pipeline that automates the calculation of key performance indicators, including image-based quality metrics, chamfer distance of reconstruct mesh, rendering frame rates, and computational resource consumption is included in the framework as well. Code is available at https://github.com/splatwizard/splatwizard",
      "published": "2025-12-31",
      "updated": "2025-12-31",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.24742v1",
        "pdf": "https://arxiv.org/pdf/2512.24742v1"
      },
      "arxiv_id": "2512.24742v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.277966101694915,
      "score_breakdown": {
        "total_score": 4.28,
        "field_match": {
          "score": 1.69,
          "matches": [
            "3d gaussian",
            "gaussian splatting"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "The recent advent of 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in real-time novel view synthesis. However, the rapid proliferation of 3DGS-based algorithms has created a pressing need for standardized and comprehensive evaluation tools, especially for compression task. Existing benchmarks often lack the specific metrics necessary to holistically assess the unique character...",
        "key_contributions": [
          "The recent advent of 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in real-time novel view synthesis.",
          "However, the rapid proliferation of 3DGS-based algorithms has created a pressing need for standardized and comprehensive evaluation tools, especially for compression task.",
          "Existing benchmarks often lack the specific metrics necessary to holistically assess the unique characteristics of different methods, such as rendering speed, rate distortion trade-offs memory efficiency, and geometric accuracy."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.24742v1.mp3"
      }
    },
    {
      "id": "2512.24794v1",
      "title": "Nonlinear Noise2Noise for Efficient Monte Carlo Denoiser Training",
      "authors": [
        "Andrew Tinits",
        "Stephen Mann"
      ],
      "abstract": "The Noise2Noise method allows for training machine learning-based denoisers with pairs of input and target images where both the input and target can be noisy. This removes the need for training with clean target images, which can be difficult to obtain. However, Noise2Noise training has a major limitation: nonlinear functions applied to the noisy targets will skew the results. This bias occurs because the nonlinearity makes the expected value of the noisy targets different from the clean target image. Since nonlinear functions are common in image processing, avoiding them limits the types of preprocessing that can be performed on the noisy targets. Our main insight is that certain nonlinear functions can be applied to the noisy targets without adding significant bias to the results. We develop a theoretical framework for analyzing the effects of these nonlinearities, and describe a class of nonlinear functions with minimal bias.\n  We demonstrate our method on the denoising of high dynamic range (HDR) images produced by Monte Carlo rendering. Noise2Noise training can have trouble with HDR images, where the training process is overwhelmed by outliers and performs poorly. We consider a commonly used method of addressing these training issues: applying a nonlinear tone mapping function to the model output and target images to reduce their dynamic range. This method was previously thought to be incompatible with Noise2Noise training because of the nonlinearities involved. We show that certain combinations of loss functions and tone mapping functions can reduce the effect of outliers while introducing minimal bias. We apply our method to an existing machine learning-based Monte Carlo denoiser, where the original implementation was trained with high-sample count reference images. Our results approach those of the original implementation, but are produced using only noisy training data.",
      "published": "2025-12-31",
      "updated": "2025-12-31",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.24794v1",
        "pdf": "https://arxiv.org/pdf/2512.24794v1"
      },
      "arxiv_id": "2512.24794v1",
      "comment": "15 pages, 7 figures, 2 tables",
      "journal_ref": "SIGGRAPH Asia 2025 Conference Papers, Article 49, 1-11",
      "has_code": false,
      "relevance_score": 4.1,
      "score_breakdown": {
        "total_score": 4.1,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "SIGGRAPH",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 5.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "The Noise2Noise method allows for training machine learning-based denoisers with pairs of input and target images where both the input and target can be noisy. This removes the need for training with clean target images, which can be difficult to obtain. However, Noise2Noise training has a major limitation: nonlinear functions applied to the noisy targets will skew the results. This bias occurs be...",
        "key_contributions": [
          "The Noise2Noise method allows for training machine learning-based denoisers with pairs of input and target images where both the input and target can be noisy.",
          "This removes the need for training with clean target images, which can be difficult to obtain.",
          "However, Noise2Noise training has a major limitation: nonlinear functions applied to the noisy targets will skew the results."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.24794v1.mp3"
      }
    },
    {
      "id": "2512.25075v1",
      "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
      "authors": [
        "Zhening Huang",
        "Hyeonho Jeong",
        "Xuelin Chen",
        "Yulia Gryaditskaya",
        "Tuanfeng Y. Wang",
        "Joan Lasenby",
        "Chun-Hao Huang"
      ],
      "abstract": "We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot",
      "published": "2025-12-31",
      "updated": "2025-12-31",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.25075v1",
        "pdf": "https://arxiv.org/pdf/2512.25075v1"
      },
      "arxiv_id": "2512.25075v1",
      "comment": "Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.9,
      "score_breakdown": {
        "total_score": 3.9,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animati...",
        "key_contributions": [
          "We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering.",
          "Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time.",
          "To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.25075v1.mp3"
      }
    },
    {
      "id": "2512.25071v1",
      "title": "Edit3r: Instant 3D Scene Editing from Sparse Unposed Images",
      "authors": [
        "Jiageng Liu",
        "Weijie Lyu",
        "Xueting Li",
        "Yejie Guo",
        "Ming-Hsuan Yang"
      ],
      "abstract": "We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images. Unlike prior methods requiring per-scene optimization, Edit3r directly predicts instruction-aligned 3D edits, enabling fast and photorealistic rendering without optimization or pose estimation. A key challenge in training such a model lies in the absence of multi-view consistent edited images for supervision. We address this with (i) a SAM2-based recoloring strategy that generates reliable, cross-view-consistent supervision, and (ii) an asymmetric input strategy that pairs a recolored reference view with raw auxiliary views, encouraging the network to fuse and align disparate observations. At inference, our model effectively handles images edited by 2D methods such as InstructPix2Pix, despite not being exposed to such edits during training. For large-scale quantitative evaluation, we introduce DL3DV-Edit-Bench, a benchmark built on the DL3DV test split, featuring 20 diverse scenes, 4 edit types and 100 edits in total. Comprehensive quantitative and qualitative results show that Edit3r achieves superior semantic alignment and enhanced 3D consistency compared to recent baselines, while operating at significantly higher inference speed, making it promising for real-time 3D editing applications.",
      "published": "2025-12-31",
      "updated": "2025-12-31",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.25071v1",
        "pdf": "https://arxiv.org/pdf/2512.25071v1"
      },
      "arxiv_id": "2512.25071v1",
      "comment": "Project page: https://edit3r.github.io/edit3r/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.9,
      "score_breakdown": {
        "total_score": 3.9,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images. Unlike prior methods requiring per-scene optimization, Edit3r directly predicts instruction-aligned 3D edits, enabling fast and photorealistic rendering without optimization or pose estimation. A key challenge in training such a model lies i...",
        "key_contributions": [
          "We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images.",
          "Unlike prior methods requiring per-scene optimization, Edit3r directly predicts instruction-aligned 3D edits, enabling fast and photorealistic rendering without optimization or pose estimation.",
          "A key challenge in training such a model lies in the absence of multi-view consistent edited images for supervision."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.25071v1.mp3"
      }
    },
    {
      "id": "2512.24986v1",
      "title": "PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes",
      "authors": [
        "Luca Collorone",
        "Mert Kiray",
        "Indro Spinelli",
        "Fabio Galasso",
        "Benjamin Busam"
      ],
      "abstract": "Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a \"render and wait\" paradigm toward an interactive dialogue with a modern, physics-informed pipeline.",
      "published": "2025-12-31",
      "updated": "2025-12-31",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "links": {
        "paper": "http://arxiv.org/abs/2512.24986v1",
        "pdf": "https://arxiv.org/pdf/2512.24986v1"
      },
      "arxiv_id": "2512.24986v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.777966101694915,
      "score_breakdown": {
        "total_score": 3.78,
        "field_match": {
          "score": 1.69,
          "matches": [
            "3d gaussian",
            "gaussian splatting"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In con...",
        "key_contributions": [
          "Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge.",
          "Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential.",
          "However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.24986v1.mp3"
      }
    }
  ]
}