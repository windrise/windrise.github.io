{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2512.17547v1",
      "title": "G3Splat: Geometrically Consistent Generalizable Gaussian Splatting",
      "authors": [
        "Mehdi Hosseinzadeh",
        "Shin-Fang Chng",
        "Yi Xu",
        "Simon Lucey",
        "Ian Reid",
        "Ravi Garg"
      ],
      "abstract": "3D Gaussians have recently emerged as an effective scene representation for real-time splatting and accurate novel-view synthesis, motivating several works to adapt multi-view structure prediction networks to regress per-pixel 3D Gaussians from images. However, most prior work extends these networks to predict additional Gaussian parameters -- orientation, scale, opacity, and appearance -- while relying almost exclusively on view-synthesis supervision. We show that a view-synthesis loss alone is insufficient to recover geometrically meaningful splats in this setting. We analyze and address the ambiguities of learning 3D Gaussian splats under self-supervision for pose-free generalizable splatting, and introduce G3Splat, which enforces geometric priors to obtain geometrically consistent 3D scene representations. Trained on RE10K, our approach achieves state-of-the-art performance in (i) geometrically consistent reconstruction, (ii) relative pose estimation, and (iii) novel-view synthesis. We further demonstrate strong zero-shot generalization on ScanNet, substantially outperforming prior work in both geometry recovery and relative pose estimation. Code and pretrained models are released on our project page (https://m80hz.github.io/g3splat/).",
      "published": "2025-12-19",
      "updated": "2025-12-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.17547v1",
        "pdf": "https://arxiv.org/pdf/2512.17547v1"
      },
      "arxiv_id": "2512.17547v1",
      "comment": "Project page: https://m80hz.github.io/g3splat/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.627966101694915,
      "score_breakdown": {
        "total_score": 4.63,
        "field_match": {
          "score": 1.69,
          "matches": [
            "3d gaussian",
            "gaussian splatting"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "3D Gaussians have recently emerged as an effective scene representation for real-time splatting and accurate novel-view synthesis, motivating several works to adapt multi-view structure prediction networks to regress per-pixel 3D Gaussians from images. However, most prior work extends these networks to predict additional Gaussian parameters -- orientation, scale, opacity, and appearance -- while r...",
        "key_contributions": [
          "3D Gaussians have recently emerged as an effective scene representation for real-time splatting and accurate novel-view synthesis, motivating several works to adapt multi-view structure prediction networks to regress per-pixel 3D Gaussians from images.",
          "However, most prior work extends these networks to predict additional Gaussian parameters -- orientation, scale, opacity, and appearance -- while relying almost exclusively on view-synthesis supervision.",
          "We show that a view-synthesis loss alone is insufficient to recover geometrically meaningful splats in this setting."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.17547v1.mp3"
      }
    },
    {
      "id": "2512.17612v1",
      "title": "Self-Supervised Weighted Image Guided Quantitative MRI Super-Resolution",
      "authors": [
        "Alireza Samadifardheris",
        "Dirk H. J. Poot",
        "Florian Wiesinger",
        "Stefan Klein",
        "Juan A. Hernandez-Tamames"
      ],
      "abstract": "High-resolution (HR) quantitative MRI (qMRI) relaxometry provides objective tissue characterization but remains clinically underutilized due to lengthy acquisition times. We propose a physics-informed, self-supervised framework for qMRI super-resolution that uses routinely acquired HR weighted MRI (wMRI) scans as guidance, thus, removing the necessity for HR qMRI ground truth during training. We formulate super-resolution as Bayesian maximum a posteriori inference, minimizing two discrepancies: (1) between HR images synthesized from super-resolved qMRI maps and acquired wMRI guides via forward signal models, and (2) between acquired LR qMRI and downsampled predictions. This physics-informed objective allows the models to learn from clinical wMRI without HR qMRI supervision. To validate the concept, we generate training data by synthesizing wMRI guides from HR qMRI using signal equations, then degrading qMRI resolution via k-space truncation. A deep neural network learns the super-resolution mapping. Ablation experiments demonstrate that T1-weighted images primarily enhance T1 maps, T2-weighted images improve T2 maps, and combined guidance optimally enhances all parameters simultaneously. Validation on independently acquired in-vivo data from a different qMRI sequence confirms cross-qMRI sequence generalizability. Models trained on synthetic data can produce super-resolved maps from a 1-minute acquisition with quality comparable to a 5-minute reference scan, leveraging the scanner-independent nature of relaxometry parameters. By decoupling training from HR qMRI requirement, our framework enables fast qMRI acquisitions enhanced via routine clinical images, offering a practical pathway for integrating quantitative relaxometry into clinical workflows with acceptable additional scan time.",
      "published": "2025-12-19",
      "updated": "2025-12-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.17612v1",
        "pdf": "https://arxiv.org/pdf/2512.17612v1"
      },
      "arxiv_id": "2512.17612v1",
      "comment": "This work has been submitted to IEEE TMI for possible publication",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.571186440677966,
      "score_breakdown": {
        "total_score": 4.57,
        "field_match": {
          "score": 0.68,
          "matches": [
            "self-supervised"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "TMI",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "High-resolution (HR) quantitative MRI (qMRI) relaxometry provides objective tissue characterization but remains clinically underutilized due to lengthy acquisition times. We propose a physics-informed, self-supervised framework for qMRI super-resolution that uses routinely acquired HR weighted MRI (wMRI) scans as guidance, thus, removing the necessity for HR qMRI ground truth during training. We f...",
        "key_contributions": [
          "High-resolution (HR) quantitative MRI (qMRI) relaxometry provides objective tissue characterization but remains clinically underutilized due to lengthy acquisition times.",
          "We propose a physics-informed, self-supervised framework for qMRI super-resolution that uses routinely acquired HR weighted MRI (wMRI) scans as guidance, thus, removing the necessity for HR qMRI ground truth during training.",
          "We formulate super-resolution as Bayesian maximum a posteriori inference, minimizing two discrepancies: (1) between HR images synthesized from super-resolved qMRI maps and acquired wMRI guides via forward signal models, and (2) between acquired LR qMRI and downsampled predictions."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.17612v1.mp3"
      }
    },
    {
      "id": "2512.17605v1",
      "title": "MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration",
      "authors": [
        "Svetlana Krasnova",
        "Emiliya Starikova",
        "Ilia Naletov",
        "Andrey Krylov",
        "Dmitry Sorokin"
      ],
      "abstract": "Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue. However, progress has been limited by the absence of public datasets and standardized benchmarks. Existing studies are often not directly comparable, as they use private data and inconsistent evaluation frameworks. To address this, we present MGRegBench, a public benchmark dataset for mammogram registration. It comprises over 5,000 image pairs, with 100 containing manual anatomical landmarks and segmentation masks for rigorous evaluation. This makes MGRegBench one of the largest public 2D registration datasets with manual annotations. Using this resource, we benchmarked diverse registration methods including classical (ANTs), learning-based (VoxelMorph, TransMorph), implicit neural representation (IDIR), a classic mammography-specific approach, and a recent state-of-the-art deep learning method MammoRegNet. The implementations were adapted to this modality from the authors' implementations or re-implemented from scratch. Our contributions are: (1) the first public dataset of this scale with manual landmarks and masks for mammography registration; (2) the first like-for-like comparison of diverse methods on this modality; and (3) an extensive analysis of deep learning-based registration. We publicly release our code and data to establish a foundational resource for fair comparisons and catalyze future research. The source code and data are at https://github.com/KourtKardash/MGRegBench.",
      "published": "2025-12-19",
      "updated": "2025-12-19",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.17605v1",
        "pdf": "https://arxiv.org/pdf/2512.17605v1"
      },
      "arxiv_id": "2512.17605v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.376271186440678,
      "score_breakdown": {
        "total_score": 4.38,
        "field_match": {
          "score": 1.44,
          "matches": [
            "segmentation",
            "registration",
            "deep learning"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 7.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue. However, progress has been limited by the absence of public datasets and standardized benchmarks. Existing studies are often not directly comparable, as they use private data and inconsistent evaluation frameworks. To address this, we presen...",
        "key_contributions": [
          "Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue.",
          "However, progress has been limited by the absence of public datasets and standardized benchmarks.",
          "Existing studies are often not directly comparable, as they use private data and inconsistent evaluation frameworks."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.17605v1.mp3"
      }
    },
    {
      "id": "2512.17541v1",
      "title": "FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views",
      "authors": [
        "Qijian Tian",
        "Xin Tan",
        "Jiayu Ying",
        "Xuhong Wang",
        "Yuan Xie",
        "Lizhuang Ma"
      ],
      "abstract": "We present FLEG, a feed-forward network that reconstructs language-embedded 3D Gaussians from any views. Previous straightforward solutions combine feed-forward reconstruction with Gaussian heads but suffer from fixed input views and insufficient 3D training data. In contrast, we propose a 3D-annotation-free training framework for 2D-to-3D lifting from arbitrary uncalibrated and unposed multi-view images. Since the framework does not require 3D annotations, we can leverage large-scale video data with easily obtained 2D instance information to enrich semantic embedding. We also propose an instance-guided contrastive learning to align 2D semantics with the 3D representations. In addition, to mitigate the high memory and computational cost of dense views, we further propose a geometry-semantic hierarchical sparsification strategy. Our FLEG efficiently reconstructs language-embedded 3D Gaussian representation in a feed-forward manner from arbitrary sparse or dense views, jointly producing accurate geometry, high-fidelity appearance, and language-aligned semantics. Extensive experiments show that it outperforms existing methods on various related tasks. Project page: https://fangzhou2000.github.io/projects/fleg.",
      "published": "2025-12-19",
      "updated": "2025-12-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.17541v1",
        "pdf": "https://arxiv.org/pdf/2512.17541v1"
      },
      "arxiv_id": "2512.17541v1",
      "comment": "Project page: https://fangzhou2000.github.io/projects/fleg",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.327966101694916,
      "score_breakdown": {
        "total_score": 4.33,
        "field_match": {
          "score": 1.69,
          "matches": [
            "3d gaussian",
            "gaussian splatting"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "We present FLEG, a feed-forward network that reconstructs language-embedded 3D Gaussians from any views. Previous straightforward solutions combine feed-forward reconstruction with Gaussian heads but suffer from fixed input views and insufficient 3D training data. In contrast, we propose a 3D-annotation-free training framework for 2D-to-3D lifting from arbitrary uncalibrated and unposed multi-view...",
        "key_contributions": [
          "We present FLEG, a feed-forward network that reconstructs language-embedded 3D Gaussians from any views.",
          "Previous straightforward solutions combine feed-forward reconstruction with Gaussian heads but suffer from fixed input views and insufficient 3D training data.",
          "In contrast, we propose a 3D-annotation-free training framework for 2D-to-3D lifting from arbitrary uncalibrated and unposed multi-view images."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.17541v1.mp3"
      }
    },
    {
      "id": "2512.17774v1",
      "title": "MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation",
      "authors": [
        "Saikat Roy",
        "Yannick Kirchhoff",
        "Constantin Ulrich",
        "Maximillian Rokuss",
        "Tassilo Wald",
        "Fabian Isensee",
        "Klaus Maier-Hein"
      ],
      "abstract": "Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet",
      "published": "2025-12-19",
      "updated": "2025-12-19",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.17774v1",
        "pdf": "https://arxiv.org/pdf/2512.17774v1"
      },
      "arxiv_id": "2512.17774v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.245762711864407,
      "score_breakdown": {
        "total_score": 4.25,
        "field_match": {
          "score": 1.86,
          "matches": [
            "medical image",
            "volumetric",
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a co...",
        "key_contributions": [
          "Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation.",
          "However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale.",
          "In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.17774v1.mp3"
      }
    },
    {
      "id": "2512.17838v1",
      "title": "ReX-MLE: The Autonomous Agent Benchmark for Medical Imaging Challenges",
      "authors": [
        "Roshan Kenia",
        "Xiaoman Zhang",
        "Pranav Rajpurkar"
      ],
      "abstract": "Autonomous coding agents built on large language models (LLMs) can now solve many general software and machine learning tasks, but they remain ineffective on complex, domain-specific scientific problems. Medical imaging is a particularly demanding domain, requiring long training cycles, high-dimensional data handling, and specialized preprocessing and validation pipelines, capabilities not fully measured in existing agent benchmarks. To address this gap, we introduce ReX-MLE, a benchmark of 20 challenges derived from high-impact medical imaging competitions spanning diverse modalities and task types. Unlike prior ML-agent benchmarks, ReX-MLE evaluates full end-to-end workflows, requiring agents to independently manage data preprocessing, model training, and submission under realistic compute and time constraints. Evaluating state-of-the-art agents (AIDE, ML-Master, R&D-Agent) with different LLM backends (GPT-5, Gemini, Claude), we observe a severe performance gap: most submissions rank in the 0th percentile compared to human experts. Failures stem from domain-knowledge and engineering limitations. ReX-MLE exposes these bottlenecks and provides a foundation for developing domain-aware autonomous AI systems.",
      "published": "2025-12-19",
      "updated": "2025-12-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.17838v1",
        "pdf": "https://arxiv.org/pdf/2512.17838v1"
      },
      "arxiv_id": "2512.17838v1",
      "comment": "https://github.com/rajpurkarlab/ReX-MLE",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.155084745762712,
      "score_breakdown": {
        "total_score": 4.16,
        "field_match": {
          "score": 0.76,
          "matches": [
            "medical imaging"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Autonomous coding agents built on large language models (LLMs) can now solve many general software and machine learning tasks, but they remain ineffective on complex, domain-specific scientific problems. Medical imaging is a particularly demanding domain, requiring long training cycles, high-dimensional data handling, and specialized preprocessing and validation pipelines, capabilities not fully m...",
        "key_contributions": [
          "Autonomous coding agents built on large language models (LLMs) can now solve many general software and machine learning tasks, but they remain ineffective on complex, domain-specific scientific problems.",
          "Medical imaging is a particularly demanding domain, requiring long training cycles, high-dimensional data handling, and specialized preprocessing and validation pipelines, capabilities not fully measured in existing agent benchmarks.",
          "To address this gap, we introduce ReX-MLE, a benchmark of 20 challenges derived from high-impact medical imaging competitions spanning diverse modalities and task types."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.17838v1.mp3"
      }
    },
    {
      "id": "2512.17773v1",
      "title": "Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image",
      "authors": [
        "Simon Giebenhain",
        "Tobias Kirschstein",
        "Liam Schoneveld",
        "Davide Davoli",
        "Zhe Chen",
        "Matthias Nießner"
      ],
      "abstract": "Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail. However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space. To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a single image as input. Compared to existing approaches, the neural parametric space allows our method to reconstruct more recognizable facial geometry and accurate facial expressions. For broad generalization, we exploit domain-specific ViTs as backbones, which are pretrained on geometric prediction tasks. We train Pix2NPHM on a mixture of 3D data, including a total of over 100K NPHM registrations that enable direct supervision in SDF space, and large-scale 2D video datasets, for which normal estimates serve as pseudo ground truth geometry. Pix2NPHM not only allows for 3D reconstructions at interactive frame rates, it is also possible to improve geometric fidelity by a subsequent inference-time optimization against estimated surface normals and canonical point maps. As a result, we achieve unprecedented face reconstruction quality that can run at scale on in-the-wild data.",
      "published": "2025-12-19",
      "updated": "2025-12-19",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.17773v1",
        "pdf": "https://arxiv.org/pdf/2512.17773v1"
      },
      "arxiv_id": "2512.17773v1",
      "comment": "Project website: https://simongiebenhain.github.io/Pix2NPHM/ , Video: https://www.youtube.com/watch?v=MgpEJC5p1Ts",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.140677966101695,
      "score_breakdown": {
        "total_score": 4.14,
        "field_match": {
          "score": 1.1,
          "matches": [
            "3d reconstruction",
            "registration"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail. However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space. To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a...",
        "key_contributions": [
          "Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail.",
          "However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space.",
          "To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a single image as input."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.17773v1.mp3"
      }
    },
    {
      "id": "2512.17581v1",
      "title": "Medical Imaging AI Competitions Lack Fairness",
      "authors": [
        "Annika Reinke",
        "Evangelia Christodoulou",
        "Sthuthi Sadananda",
        "A. Emre Kavur",
        "Khrystyna Faryna",
        "Daan Schouten",
        "Bennett A. Landman",
        "Carole Sudre",
        "Olivier Colliot",
        "Nick Heller",
        "Sophie Loizillon",
        "Martin Maška",
        "Maëlys Solal",
        "Arya Yazdan-Panah",
        "Vilma Bozgo",
        "Ömer Sümer",
        "Siem de Jong",
        "Sophie Fischer",
        "Michal Kozubek",
        "Tim Rädsch",
        "Nadim Hammoud",
        "Fruzsina Molnár-Gábor",
        "Steven Hicks",
        "Michael A. Riegler",
        "Anindo Saha",
        "Vajira Thambawita",
        "Pal Halvorsen",
        "Amelia Jiménez-Sánchez",
        "Qingyang Yang",
        "Veronika Cheplygina",
        "Sabrina Bottazzi",
        "Alexander Seitel",
        "Spyridon Bakas",
        "Alexandros Karargyris",
        "Kiran Vaidhya Venkadesh",
        "Bram van Ginneken",
        "Lena Maier-Hein"
      ],
      "abstract": "Benchmarking competitions are central to the development of artificial intelligence (AI) in medical imaging, defining performance standards and shaping methodological progress. However, it remains unclear whether these benchmarks provide data that are sufficiently representative, accessible, and reusable to support clinically meaningful AI. In this work, we assess fairness along two complementary dimensions: (1) whether challenge datasets are representative of real-world clinical diversity, and (2) whether they are accessible and legally reusable in line with the FAIR principles. To address this question, we conducted a large-scale systematic study of 241 biomedical image analysis challenges comprising 458 tasks across 19 imaging modalities. Our findings show substantial biases in dataset composition, including geographic location, modality-, and problem type-related biases, indicating that current benchmarks do not adequately reflect real-world clinical diversity. Despite their widespread influence, challenge datasets were frequently constrained by restrictive or ambiguous access conditions, inconsistent or non-compliant licensing practices, and incomplete documentation, limiting reproducibility and long-term reuse. Together, these shortcomings expose foundational fairness limitations in our benchmarking ecosystem and highlight a disconnect between leaderboard success and clinical relevance.",
      "published": "2025-12-19",
      "updated": "2025-12-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.17581v1",
        "pdf": "https://arxiv.org/pdf/2512.17581v1"
      },
      "arxiv_id": "2512.17581v1",
      "comment": "Submitted to Nature BME",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.079661016949153,
      "score_breakdown": {
        "total_score": 4.08,
        "field_match": {
          "score": 1.95,
          "matches": [
            "medical image",
            "medical imaging",
            "image analysis"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 7.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Benchmarking competitions are central to the development of artificial intelligence (AI) in medical imaging, defining performance standards and shaping methodological progress. However, it remains unclear whether these benchmarks provide data that are sufficiently representative, accessible, and reusable to support clinically meaningful AI. In this work, we assess fairness along two complementary ...",
        "key_contributions": [
          "Benchmarking competitions are central to the development of artificial intelligence (AI) in medical imaging, defining performance standards and shaping methodological progress.",
          "However, it remains unclear whether these benchmarks provide data that are sufficiently representative, accessible, and reusable to support clinically meaningful AI.",
          "In this work, we assess fairness along two complementary dimensions: (1) whether challenge datasets are representative of real-world clinical diversity, and (2) whether they are accessible and legally reusable in line with the FAIR principles."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.17581v1.mp3"
      }
    },
    {
      "id": "2512.17817v1",
      "title": "Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding",
      "authors": [
        "Yue Li",
        "Qi Ma",
        "Runyi Yang",
        "Mengjiao Ma",
        "Bin Ren",
        "Nikola Popovic",
        "Nicu Sebe",
        "Theo Gevers",
        "Luc Van Gool",
        "Danda Pani Paudel",
        "Martin R. Oswald"
      ],
      "abstract": "While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.\n  We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians' centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.",
      "published": "2025-12-19",
      "updated": "2025-12-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.17817v1",
        "pdf": "https://arxiv.org/pdf/2512.17817v1"
      },
      "arxiv_id": "2512.17817v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.03135593220339,
      "score_breakdown": {
        "total_score": 4.03,
        "field_match": {
          "score": 2.2,
          "matches": [
            "3d gaussian",
            "gaussian splatting",
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a sha...",
        "key_contributions": [
          "While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored.",
          "We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models.",
          "Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.17817v1.mp3"
      }
    },
    {
      "id": "2512.17909v1",
      "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
      "authors": [
        "Shilong Zhang",
        "He Zhang",
        "Zhifei Zhang",
        "Chongjian Ge",
        "Shuchen Xue",
        "Shaoteng Liu",
        "Mengwei Ren",
        "Soo Ye Kim",
        "Yuqian Zhou",
        "Qing Liu",
        "Daniil Pakhomov",
        "Kai Zhang",
        "Zhe Lin",
        "Ping Luo"
      ],
      "abstract": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.",
      "published": "2025-12-19",
      "updated": "2025-12-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.17909v1",
        "pdf": "https://arxiv.org/pdf/2512.17909v1"
      },
      "arxiv_id": "2512.17909v1",
      "comment": "Project Page: https://jshilong.github.io/PS-VAE-PAGE/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.0,
      "score_breakdown": {
        "total_score": 4.0,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this para...",
        "key_contributions": [
          "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction.",
          "To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents.",
          "However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2512.17909v1.mp3"
      }
    }
  ]
}