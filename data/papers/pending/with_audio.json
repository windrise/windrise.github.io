{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2601.15283v1",
      "title": "LuxRemix: Lighting Decomposition and Remixing for Indoor Scenes",
      "authors": [
        "Ruofan Liang",
        "Norman Müller",
        "Ethan Weber",
        "Duncan Zauss",
        "Nandita Vijaykumar",
        "Peter Kontschieder",
        "Christian Richardt"
      ],
      "abstract": "We present a novel approach for interactive light editing in indoor scenes from a single multi-view scene capture. Our method leverages a generative image-based light decomposition model that factorizes complex indoor scene illumination into its constituent light sources. This factorization enables independent manipulation of individual light sources, specifically allowing control over their state (on/off), chromaticity, and intensity. We further introduce multi-view lighting harmonization to ensure consistent propagation of the lighting decomposition across all scene views. This is integrated into a relightable 3D Gaussian splatting representation, providing real-time interactive control over the individual light sources. Our results demonstrate highly photorealistic lighting decomposition and relighting outcomes across diverse indoor scenes. We evaluate our method on both synthetic and real-world datasets and provide a quantitative and qualitative comparison to state-of-the-art techniques. For video results and interactive demos, see https://luxremix.github.io.",
      "published": "2026-01-21",
      "updated": "2026-01-21",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.15283v1",
        "pdf": "https://arxiv.org/pdf/2601.15283v1"
      },
      "arxiv_id": "2601.15283v1",
      "comment": "Project page: https://luxremix.github.io",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.727966101694916,
      "score_breakdown": {
        "total_score": 4.73,
        "field_match": {
          "score": 1.69,
          "matches": [
            "3d gaussian",
            "gaussian splatting"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "We present a novel approach for interactive light editing in indoor scenes from a single multi-view scene capture. Our method leverages a generative image-based light decomposition model that factorizes complex indoor scene illumination into its constituent light sources. This factorization enables independent manipulation of individual light sources, specifically allowing control over their state...",
        "key_contributions": [
          "We present a novel approach for interactive light editing in indoor scenes from a single multi-view scene capture.",
          "Our method leverages a generative image-based light decomposition model that factorizes complex indoor scene illumination into its constituent light sources.",
          "This factorization enables independent manipulation of individual light sources, specifically allowing control over their state (on/off), chromaticity, and intensity."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.15283v1.mp3"
      }
    },
    {
      "id": "2601.15200v1",
      "title": "BBoxMaskPose v2: Expanding Mutual Conditioning to 3D",
      "authors": [
        "Miroslav Purkrabek",
        "Constantin Kolomiiets",
        "Jiri Matas"
      ],
      "abstract": "Most 2D human pose estimation benchmarks are nearly saturated, with the exception of crowded scenes. We introduce PMPose, a top-down 2D pose estimator that incorporates the probabilistic formulation and the mask-conditioning. PMPose improves crowded pose estimation without sacrificing performance on standard scenes. Building on this, we present BBoxMaskPose v2 (BMPv2) integrating PMPose and an enhanced SAM-based mask refinement module. BMPv2 surpasses state-of-the-art by 1.5 average precision (AP) points on COCO and 6 AP points on OCHuman, becoming the first method to exceed 50 AP on OCHuman. We demonstrate that BMP's 2D prompting of 3D model improves 3D pose estimation in crowded scenes and that advances in 2D pose quality directly benefit 3D estimation. Results on the new OCHuman-Pose dataset show that multi-person performance is more affected by pose prediction accuracy than by detection. The code, models, and data are available on https://MiraPurkrabek.github.io/BBox-Mask-Pose/.",
      "published": "2026-01-21",
      "updated": "2026-01-21",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.15200v1",
        "pdf": "https://arxiv.org/pdf/2601.15200v1"
      },
      "arxiv_id": "2601.15200v1",
      "comment": "GitHub repository: https://github.com/MiraPurkrabek/BBoxMaskPose/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.199999999999999,
      "score_breakdown": {
        "total_score": 4.2,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Most 2D human pose estimation benchmarks are nearly saturated, with the exception of crowded scenes. We introduce PMPose, a top-down 2D pose estimator that incorporates the probabilistic formulation and the mask-conditioning. PMPose improves crowded pose estimation without sacrificing performance on standard scenes. Building on this, we present BBoxMaskPose v2 (BMPv2) integrating PMPose and an enh...",
        "key_contributions": [
          "Most 2D human pose estimation benchmarks are nearly saturated, with the exception of crowded scenes.",
          "We introduce PMPose, a top-down 2D pose estimator that incorporates the probabilistic formulation and the mask-conditioning.",
          "PMPose improves crowded pose estimation without sacrificing performance on standard scenes."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.15200v1.mp3"
      }
    },
    {
      "id": "2601.15282v1",
      "title": "Rethinking Video Generation Model for the Embodied World",
      "authors": [
        "Yufan Deng",
        "Zilin Pan",
        "Hongyu Zhang",
        "Xiaojie Li",
        "Ruoqing Hu",
        "Yufei Ding",
        "Yiming Zou",
        "Yan Zeng",
        "Daquan Zhou"
      ],
      "abstract": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.",
      "published": "2026-01-21",
      "updated": "2026-01-21",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.15282v1",
        "pdf": "https://arxiv.org/pdf/2601.15282v1"
      },
      "arxiv_id": "2601.15282v1",
      "comment": "Github: https://github.com/DAGroup-PKU/ReVidgen/ Project website: https://dagroup-pku.github.io/ReVidgen.github.io/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.1000000000000005,
      "score_breakdown": {
        "total_score": 4.1,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progr...",
        "key_contributions": [
          "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world.",
          "However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress.",
          "To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.15282v1.mp3"
      }
    },
    {
      "id": "2601.15123v1",
      "title": "BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation",
      "authors": [
        "Andrey Moskalenko",
        "Danil Kuznetsov",
        "Irina Dudko",
        "Anastasiia Iasakova",
        "Nikita Boldyrev",
        "Denis Shepelev",
        "Andrei Spiridonov",
        "Andrey Kuznetsov",
        "Vlad Shakhuro"
      ],
      "abstract": "Promptable segmentation models such as SAM have established a powerful paradigm, enabling strong generalization to unseen objects and domains with minimal user input, including points, bounding boxes, and text prompts. Among these, bounding boxes stand out as particularly effective, often outperforming points while significantly reducing annotation costs. However, current training and evaluation protocols typically rely on synthetic prompts generated through simple heuristics, offering limited insight into real-world robustness. In this paper, we investigate the robustness of promptable segmentation models to natural variations in bounding box prompts. First, we conduct a controlled user study and collect thousands of real bounding box annotations. Our analysis reveals substantial variability in segmentation quality across users for the same model and instance, indicating that SAM-like models are highly sensitive to natural prompt noise. Then, since exhaustive testing of all possible user inputs is computationally prohibitive, we reformulate robustness evaluation as a white-box optimization problem over the bounding box prompt space. We introduce BREPS, a method for generating adversarial bounding boxes that minimize or maximize segmentation error while adhering to naturalness constraints. Finally, we benchmark state-of-the-art models across 10 datasets, spanning everyday scenes to medical imaging. Code - https://github.com/emb-ai/BREPS.",
      "published": "2026-01-21",
      "updated": "2026-01-21",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.15123v1",
        "pdf": "https://arxiv.org/pdf/2601.15123v1"
      },
      "arxiv_id": "2601.15123v1",
      "comment": "Accepted by AAAI2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.058474576271187,
      "score_breakdown": {
        "total_score": 4.06,
        "field_match": {
          "score": 1.27,
          "matches": [
            "medical imaging",
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Promptable segmentation models such as SAM have established a powerful paradigm, enabling strong generalization to unseen objects and domains with minimal user input, including points, bounding boxes, and text prompts. Among these, bounding boxes stand out as particularly effective, often outperforming points while significantly reducing annotation costs. However, current training and evaluation p...",
        "key_contributions": [
          "Promptable segmentation models such as SAM have established a powerful paradigm, enabling strong generalization to unseen objects and domains with minimal user input, including points, bounding boxes, and text prompts.",
          "Among these, bounding boxes stand out as particularly effective, often outperforming points while significantly reducing annotation costs.",
          "However, current training and evaluation protocols typically rely on synthetic prompts generated through simple heuristics, offering limited insight into real-world robustness."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.15123v1.mp3"
      }
    },
    {
      "id": "2601.15260v1",
      "title": "DrivIng: A Large-Scale Multimodal Driving Dataset with Full Digital Twin Integration",
      "authors": [
        "Dominik Rößle",
        "Xujun Xie",
        "Adithya Mohan",
        "Venkatesh Thirugnana Sambandham",
        "Daniel Cremers",
        "Torsten Schön"
      ],
      "abstract": "Perception is a cornerstone of autonomous driving, enabling vehicles to understand their surroundings and make safe, reliable decisions. Developing robust perception algorithms requires large-scale, high-quality datasets that cover diverse driving conditions and support thorough evaluation. Existing datasets often lack a high-fidelity digital twin, limiting systematic testing, edge-case simulation, sensor modification, and sim-to-real evaluations. To address this gap, we present DrivIng, a large-scale multimodal dataset with a complete geo-referenced digital twin of a ~18 km route spanning urban, suburban, and highway segments. Our dataset provides continuous recordings from six RGB cameras, one LiDAR, and high-precision ADMA-based localization, captured across day, dusk, and night. All sequences are annotated at 10 Hz with 3D bounding boxes and track IDs across 12 classes, yielding ~1.2 million annotated instances. Alongside the benefits of a digital twin, DrivIng enables a 1-to-1 transfer of real traffic into simulation, preserving agent interactions while enabling realistic and flexible scenario testing. To support reproducible research and robust validation, we benchmark DrivIng with state-of-the-art perception models and publicly release the dataset, digital twin, HD map, and codebase.",
      "published": "2026-01-21",
      "updated": "2026-01-21",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.15260v1",
        "pdf": "https://arxiv.org/pdf/2601.15260v1"
      },
      "arxiv_id": "2601.15260v1",
      "comment": "Accepted to the IEEE Intelligent Vehicles Symposium 2026. For code and dataset, see https://github.com/cvims/DrivIng",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.050000000000001,
      "score_breakdown": {
        "total_score": 4.05,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Perception is a cornerstone of autonomous driving, enabling vehicles to understand their surroundings and make safe, reliable decisions. Developing robust perception algorithms requires large-scale, high-quality datasets that cover diverse driving conditions and support thorough evaluation. Existing datasets often lack a high-fidelity digital twin, limiting systematic testing, edge-case simulation...",
        "key_contributions": [
          "Perception is a cornerstone of autonomous driving, enabling vehicles to understand their surroundings and make safe, reliable decisions.",
          "Developing robust perception algorithms requires large-scale, high-quality datasets that cover diverse driving conditions and support thorough evaluation.",
          "Existing datasets often lack a high-fidelity digital twin, limiting systematic testing, edge-case simulation, sensor modification, and sim-to-real evaluations."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.15260v1.mp3"
      }
    },
    {
      "id": "2601.15119v1",
      "title": "Vision Models for Medical Imaging: A Hybrid Approach for PCOS Detection from Ultrasound Scans",
      "authors": [
        "Md Mahmudul Hoque",
        "Md Mehedi Hassain",
        "Muntakimur Rahaman",
        "Md. Towhidul Islam",
        "Shaista Rani",
        "Md Sharif Mollah"
      ],
      "abstract": "Polycystic Ovary Syndrome (PCOS) is the most familiar endocrine illness in women of reproductive age. Many Bangladeshi women suffer from PCOS disease in their older age. The aim of our research is to identify effective vision-based medical image analysis techniques and evaluate hybrid models for the accurate detection of PCOS. We introduced two novel hybrid models combining convolutional and transformer-based approaches. The training and testing data were organized into two categories: \"infected\" (PCOS-positive) and \"noninfected\" (healthy ovaries). In the initial stage, our first hybrid model, 'DenConST' (integrating DenseNet121, Swin Transformer, and ConvNeXt), achieved 85.69% accuracy. The final optimized model, 'DenConREST' (incorporating Swin Transformer, ConvNeXt, DenseNet121, ResNet18, and EfficientNetV2), demonstrated superior performance with 98.23% accuracy. Among all evaluated models, DenConREST showed the best performance. This research highlights an efficient solution for PCOS detection from ultrasound images, significantly improving diagnostic accuracy while reducing detection errors.",
      "published": "2026-01-21",
      "updated": "2026-01-21",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.15119v1",
        "pdf": "https://arxiv.org/pdf/2601.15119v1"
      },
      "arxiv_id": "2601.15119v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.029661016949152,
      "score_breakdown": {
        "total_score": 4.03,
        "field_match": {
          "score": 1.95,
          "matches": [
            "medical image",
            "medical imaging",
            "image analysis"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Polycystic Ovary Syndrome (PCOS) is the most familiar endocrine illness in women of reproductive age. Many Bangladeshi women suffer from PCOS disease in their older age. The aim of our research is to identify effective vision-based medical image analysis techniques and evaluate hybrid models for the accurate detection of PCOS. We introduced two novel hybrid models combining convolutional and trans...",
        "key_contributions": [
          "Polycystic Ovary Syndrome (PCOS) is the most familiar endocrine illness in women of reproductive age.",
          "Many Bangladeshi women suffer from PCOS disease in their older age.",
          "The aim of our research is to identify effective vision-based medical image analysis techniques and evaluate hybrid models for the accurate detection of PCOS."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.15119v1.mp3"
      }
    },
    {
      "id": "2601.15015v1",
      "title": "Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control",
      "authors": [
        "Jannis Becktepe",
        "Aleksandra Franz",
        "Nils Thuerey",
        "Sebastian Peitz"
      ],
      "abstract": "Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym.",
      "published": "2026-01-21",
      "updated": "2026-01-21",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2601.15015v1",
        "pdf": "https://arxiv.org/pdf/2601.15015v1"
      },
      "arxiv_id": "2601.15015v1",
      "comment": "Code available at https://github.com/safe-autonomous-systems/fluidgym",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.9499999999999997,
      "score_breakdown": {
        "total_score": 3.95,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not ful...",
        "key_contributions": [
          "Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols.",
          "Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support.",
          "To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.15015v1.mp3"
      }
    },
    {
      "id": "2601.15224v1",
      "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models",
      "authors": [
        "Jianshu Zhang",
        "Chengxuan Qian",
        "Haosen Sun",
        "Haoran Lu",
        "Dingcheng Wang",
        "Letian Xue",
        "Han Liu"
      ],
      "abstract": "Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.",
      "published": "2026-01-21",
      "updated": "2026-01-21",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.15224v1",
        "pdf": "https://arxiv.org/pdf/2601.15224v1"
      },
      "arxiv_id": "2601.15224v1",
      "comment": "Website: https://progresslm.github.io/ProgressLM/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.9,
      "score_breakdown": {
        "total_score": 3.9,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning ...",
        "key_contributions": [
          "Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content.",
          "While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations.",
          "To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.15224v1.mp3"
      }
    },
    {
      "id": "2601.15098v1",
      "title": "Three-dimensional visualization of X-ray micro-CT with large-scale datasets: Efficiency and accuracy for real-time interaction",
      "authors": [
        "Yipeng Yin",
        "Rao Yao",
        "Qingying Li",
        "Dazhong Wang",
        "Hong Zhou",
        "Zhijun Fang",
        "Jianing Chen",
        "Longjie Qian",
        "Mingyue Wu"
      ],
      "abstract": "As Micro-CT technology continues to refine its characterization of material microstructures, industrial CT ultra-precision inspection is generating increasingly large datasets, necessitating solutions to the trade-off between accuracy and efficiency in the 3D characterization of defects during ultra-precise detection. This article provides a unique perspective on recent advances in accurate and efficient 3D visualization using Micro-CT, tracing its evolution from medical imaging to industrial non-destructive testing (NDT). Among the numerous CT reconstruction and volume rendering methods, this article selectively reviews and analyzes approaches that balance accuracy and efficiency, offering a comprehensive analysis to help researchers quickly grasp highly efficient and accurate 3D reconstruction methods for microscopic features. By comparing the principles of computed tomography with advancements in microstructural technology, this article examines the evolution of CT reconstruction algorithms from analytical methods to deep learning techniques, as well as improvements in volume rendering algorithms, acceleration, and data reduction. Additionally, it explores advanced lighting models for high-accuracy, photorealistic, and efficient volume rendering. Furthermore, this article envisions potential directions in CT reconstruction and volume rendering. It aims to guide future research in quickly selecting efficient and precise methods and developing new ideas and approaches for real-time online monitoring of internal material defects through virtual-physical interaction, for applying digital twin model to structural health monitoring (SHM).",
      "published": "2026-01-21",
      "updated": "2026-01-21",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.15098v1",
        "pdf": "https://arxiv.org/pdf/2601.15098v1"
      },
      "arxiv_id": "2601.15098v1",
      "comment": "Page1-37",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.8618644067796604,
      "score_breakdown": {
        "total_score": 3.86,
        "field_match": {
          "score": 1.78,
          "matches": [
            "medical imaging",
            "3d reconstruction",
            "deep learning"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "As Micro-CT technology continues to refine its characterization of material microstructures, industrial CT ultra-precision inspection is generating increasingly large datasets, necessitating solutions to the trade-off between accuracy and efficiency in the 3D characterization of defects during ultra-precise detection. This article provides a unique perspective on recent advances in accurate and ef...",
        "key_contributions": [
          "As Micro-CT technology continues to refine its characterization of material microstructures, industrial CT ultra-precision inspection is generating increasingly large datasets, necessitating solutions to the trade-off between accuracy and efficiency in the 3D characterization of defects during ultra-precise detection.",
          "This article provides a unique perspective on recent advances in accurate and efficient 3D visualization using Micro-CT, tracing its evolution from medical imaging to industrial non-destructive testing (NDT).",
          "Among the numerous CT reconstruction and volume rendering methods, this article selectively reviews and analyzes approaches that balance accuracy and efficiency, offering a comprehensive analysis to help researchers quickly grasp highly efficient and accurate 3D reconstruction methods for microscopic features."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.15098v1.mp3"
      }
    },
    {
      "id": "2601.15286v1",
      "title": "Iterative Refinement Improves Compositional Image Generation",
      "authors": [
        "Shantanu Jaiswal",
        "Mihir Prabhudesai",
        "Nikash Bhardwaj",
        "Zheyang Qin",
        "Amir Zadeh",
        "Chuan Li",
        "Katerina Fragkiadaki",
        "Deepak Pathak"
      ],
      "abstract": "Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/",
      "published": "2026-01-21",
      "updated": "2026-01-21",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.15286v1",
        "pdf": "https://arxiv.org/pdf/2601.15286v1"
      },
      "arxiv_id": "2601.15286v1",
      "comment": "Project webpage: https://iterative-img-gen.github.io/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8499999999999996,
      "score_breakdown": {
        "total_score": 3.85,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where...",
        "key_contributions": [
          "Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes.",
          "Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied.",
          "Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2601.15286v1.mp3"
      }
    }
  ]
}