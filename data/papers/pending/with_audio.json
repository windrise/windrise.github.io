{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2602.17395v1",
      "title": "SpectralGCD: Spectral Concept Selection and Cross-modal Representation Learning for Generalized Category Discovery",
      "authors": [
        "Lorenzo Caselli",
        "Marco Mistretta",
        "Simone Magistri",
        "Andrew D. Bagdanov"
      ],
      "abstract": "Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high computational cost. We propose SpectralGCD, an efficient and effective multimodal approach to GCD that uses CLIP cross-modal image-concept similarities as a unified cross-modal representation. Each image is expressed as a mixture over semantic concepts from a large task-agnostic dictionary, which anchors learning to explicit semantics and reduces reliance on spurious visual cues. To maintain the semantic quality of representations learned by an efficient student, we introduce Spectral Filtering which exploits a cross-modal covariance matrix over the softmaxed similarities measured by a strong teacher model to automatically retain only relevant concepts from the dictionary. Forward and reverse knowledge distillation from the same teacher ensures that the cross-modal representations of the student remain both semantically sufficient and well-aligned. Across six benchmarks, SpectralGCD delivers accuracy comparable to or significantly superior to state-of-the-art methods at a fraction of the computational cost. The code is publicly available at: https://github.com/miccunifi/SpectralGCD.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.17395v1",
        "pdf": "https://arxiv.org/pdf/2602.17395v1"
      },
      "arxiv_id": "2602.17395v1",
      "comment": "Accepted at ICLR 2026. Code available at https://github.com/miccunifi/SpectralGCD",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 5.300000000000001,
      "score_breakdown": {
        "total_score": 5.3,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high c...",
        "key_contributions": [
          "Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes.",
          "Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information.",
          "However, they treat modalities independently and incur high computational cost."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2602.17395v1.mp3"
      }
    },
    {
      "id": "2602.17560v1",
      "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
      "authors": [
        "Hongjue Zhao",
        "Haosen Sun",
        "Jiangtao Kong",
        "Xiaochang Li",
        "Qineng Wang",
        "Liwei Jiang",
        "Qi Zhu",
        "Tarek Abdelzaher",
        "Yejin Choi",
        "Manling Li",
        "Huajie Shao"
      ],
      "abstract": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \\textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \\textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \\textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \\textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\\%$ improvement over TruthfulQA, $2.5\\%$ over UltraFeedback, and $2.4\\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.17560v1",
        "pdf": "https://arxiv.org/pdf/2602.17560v1"
      },
      "arxiv_id": "2602.17560v1",
      "comment": "Accepted by ICLR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.85,
      "score_breakdown": {
        "total_score": 4.85,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step s...",
        "key_contributions": [
          "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time.",
          "However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions.",
          "In this work, we propose a unified ordinary differential equations (ODEs)-based \\textit{theoretical} framework for activation steering in LLM alignment."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2602.17560v1.mp3"
      }
    },
    {
      "id": "2602.17337v1",
      "title": "Polaffini: A feature-based approach for robust affine and polyaffine image registration",
      "authors": [
        "Antoine Legouhy",
        "Cosimo Campo",
        "Ross Callaghan",
        "Hojjat Azadbakht",
        "Hui Zhang"
      ],
      "abstract": "In this work we present Polaffini, a robust and versatile framework for anatomically grounded registration. Medical image registration is dominated by intensity-based registration methods that rely on surrogate measures of alignment quality. In contrast, feature-based approaches that operate by identifying explicit anatomical correspondences, while more desirable in theory, have largely fallen out of favor due to the challenges of reliably extracting features. However, such challenges are now significantly overcome thanks to recent advances in deep learning, which provide pre-trained segmentation models capable of instantly delivering reliable, fine-grained anatomical delineations. We aim to demonstrate that these advances can be leveraged to create new anatomically-grounded image registration algorithms. To this end, we propose Polaffini, which obtains, from these segmented regions, anatomically grounded feature points with 1-to-1 correspondence in a particularly simple way: extracting their centroids. These enable efficient global and local affine matching via closed-form solutions. Those are used to produce an overall transformation ranging from affine to polyaffine with tunable smoothness. Polyaffine transformations can have many more degrees of freedom than affine ones allowing for finer alignment, and their embedding in the log-Euclidean framework ensures diffeomorphic properties. Polaffini has applications both for standalone registration and as pre-alignment for subsequent non-linear registration, and we evaluate it against popular intensity-based registration techniques. Results demonstrate that Polaffini outperforms competing methods in terms of structural alignment and provides improved initialisation for downstream non-linear registration. Polaffini is fast, robust, and accurate, making it particularly well-suited for integration into medical image processing pipelines.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.17337v1",
        "pdf": "https://arxiv.org/pdf/2602.17337v1"
      },
      "arxiv_id": "2602.17337v1",
      "comment": "associated github repo: https://github.com/CIG-UCL/polaffini",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.73135593220339,
      "score_breakdown": {
        "total_score": 4.73,
        "field_match": {
          "score": 2.2,
          "matches": [
            "medical image",
            "segmentation",
            "registration",
            "deep learning"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "In this work we present Polaffini, a robust and versatile framework for anatomically grounded registration. Medical image registration is dominated by intensity-based registration methods that rely on surrogate measures of alignment quality. In contrast, feature-based approaches that operate by identifying explicit anatomical correspondences, while more desirable in theory, have largely fallen out...",
        "key_contributions": [
          "In this work we present Polaffini, a robust and versatile framework for anatomically grounded registration.",
          "Medical image registration is dominated by intensity-based registration methods that rely on surrogate measures of alignment quality.",
          "In contrast, feature-based approaches that operate by identifying explicit anatomical correspondences, while more desirable in theory, have largely fallen out of favor due to the challenges of reliably extracting features."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2602.17337v1.mp3"
      }
    },
    {
      "id": "2602.17559v1",
      "title": "Revisiting Weight Regularization for Low-Rank Continual Learning",
      "authors": [
        "Yaoyue Zheng",
        "Yin Zhang",
        "Joost van de Weijer",
        "Gido M van de Ven",
        "Shaoyi Du",
        "Xuetao Zhang",
        "Zhiqiang Tian"
      ],
      "abstract": "Continual Learning (CL) with large-scale pre-trained models (PTMs) has recently gained wide attention, shifting the focus from training from scratch to continually adapting PTMs. This has given rise to a promising paradigm: parameter-efficient continual learning (PECL), where task interference is typically mitigated by assigning a task-specific module during training, such as low-rank adapters. However, weight regularization techniques, such as Elastic Weight Consolidation (EWC)-a key strategy in CL-remain underexplored in this new paradigm. In this paper, we revisit weight regularization in low-rank CL as a new perspective for mitigating task interference in PECL. Unlike existing low-rank CL methods, we mitigate task interference by regularizing a shared low-rank update through EWC, thereby keeping the storage requirement and inference costs constant regardless of the number of tasks. Our proposed method EWC-LoRA leverages a low-rank representation to estimate parameter importance over the full-dimensional space. This design offers a practical, computational- and memory-efficient solution for CL with PTMs, and provides insights that may inform the broader application of regularization techniques within PECL. Extensive experiments on various benchmarks demonstrate the effectiveness of EWC-LoRA, achieving a stability-plasticity trade-off superior to existing low-rank CL approaches. These results indicate that, even under low-rank parameterizations, weight regularization remains an effective mechanism for mitigating task interference. Code is available at: https://github.com/yaoyz96/low-rank-cl.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.17559v1",
        "pdf": "https://arxiv.org/pdf/2602.17559v1"
      },
      "arxiv_id": "2602.17559v1",
      "comment": "Accepted by ICLR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.65,
      "score_breakdown": {
        "total_score": 4.65,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Continual Learning (CL) with large-scale pre-trained models (PTMs) has recently gained wide attention, shifting the focus from training from scratch to continually adapting PTMs. This has given rise to a promising paradigm: parameter-efficient continual learning (PECL), where task interference is typically mitigated by assigning a task-specific module during training, such as low-rank adapters. Ho...",
        "key_contributions": [
          "Continual Learning (CL) with large-scale pre-trained models (PTMs) has recently gained wide attention, shifting the focus from training from scratch to continually adapting PTMs.",
          "This has given rise to a promising paradigm: parameter-efficient continual learning (PECL), where task interference is typically mitigated by assigning a task-specific module during training, such as low-rank adapters.",
          "However, weight regularization techniques, such as Elastic Weight Consolidation (EWC)-a key strategy in CL-remain underexplored in this new paradigm."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2602.17559v1.mp3"
      }
    },
    {
      "id": "2602.17385v1",
      "title": "Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature",
      "authors": [
        "Angelo Porrello",
        "Pietro Buzzega",
        "Felix Dangel",
        "Thomas Sommariva",
        "Riccardo Salami",
        "Lorenzo Bonicelli",
        "Simone Calderara"
      ],
      "abstract": "Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.17385v1",
        "pdf": "https://arxiv.org/pdf/2602.17385v1"
      },
      "arxiv_id": "2602.17385v1",
      "comment": "Accepted to ICLR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.3999999999999995,
      "score_breakdown": {
        "total_score": 4.4,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and d...",
        "key_contributions": [
          "Task Arithmetic yields a modular, scalable way to adapt foundation models.",
          "Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance.",
          "Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2602.17385v1.mp3"
      }
    },
    {
      "id": "2602.17530v1",
      "title": "Provably Explaining Neural Additive Models",
      "authors": [
        "Shahaf Bassan",
        "Yizhak Yisrael Elboher",
        "Tobias Ladner",
        "Volkan Şahin",
        "Jan Kretinsky",
        "Matthias Althoff",
        "Guy Katz"
      ],
      "abstract": "Despite significant progress in post-hoc explanation methods for neural networks, many remain heuristic and lack provable guarantees. A key approach for obtaining explanations with provable guarantees is by identifying a cardinally-minimal subset of input features which by itself is provably sufficient to determine the prediction. However, for standard neural networks, this task is often computationally infeasible, as it demands a worst-case exponential number of verification queries in the number of input features, each of which is NP-hard.\n  In this work, we show that for Neural Additive Models (NAMs), a recent and more interpretable neural network family, we can efficiently generate explanations with such guarantees. We present a new model-specific algorithm for NAMs that generates provably cardinally-minimal explanations using only a logarithmic number of verification queries\n  in the number of input features, after a parallelized preprocessing step with logarithmic runtime in the required precision is applied to each small univariate NAM component.\n  Our algorithm not only makes the task of obtaining cardinally-minimal explanations feasible, but even outperforms existing algorithms designed to find the relaxed variant of subset-minimal explanations - which may be larger and less informative but easier to compute - despite our algorithm solving a much more difficult task.\n  Our experiments demonstrate that, compared to previous algorithms, our approach provides provably smaller explanations than existing works and substantially reduces the computation time. Moreover, we show that our generated provable explanations offer benefits that are unattainable by standard sampling-based techniques typically used to interpret NAMs.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cs.LG",
        "cs.CC",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.17530v1",
        "pdf": "https://arxiv.org/pdf/2602.17530v1"
      },
      "arxiv_id": "2602.17530v1",
      "comment": "To appear in ICLR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.35,
      "score_breakdown": {
        "total_score": 4.35,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Despite significant progress in post-hoc explanation methods for neural networks, many remain heuristic and lack provable guarantees. A key approach for obtaining explanations with provable guarantees is by identifying a cardinally-minimal subset of input features which by itself is provably sufficient to determine the prediction. However, for standard neural networks, this task is often computati...",
        "key_contributions": [
          "Despite significant progress in post-hoc explanation methods for neural networks, many remain heuristic and lack provable guarantees.",
          "A key approach for obtaining explanations with provable guarantees is by identifying a cardinally-minimal subset of input features which by itself is provably sufficient to determine the prediction.",
          "However, for standard neural networks, this task is often computationally infeasible, as it demands a worst-case exponential number of verification queries in the number of input features, each of which is NP-hard."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2602.17530v1.mp3"
      }
    },
    {
      "id": "2602.17659v1",
      "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs",
      "authors": [
        "Yu Fang",
        "Yuchun Feng",
        "Dong Jing",
        "Jiaqi Liu",
        "Yue Yang",
        "Zhenyu Wei",
        "Daniel Szafir",
        "Mingyu Ding"
      ],
      "abstract": "Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.17659v1",
        "pdf": "https://arxiv.org/pdf/2602.17659v1"
      },
      "arxiv_id": "2602.17659v1",
      "comment": "Website: https://vla-va.github.io/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.25,
      "score_breakdown": {
        "total_score": 4.25,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects...",
        "key_contributions": [
          "Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language.",
          "When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent.",
          "To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2602.17659v1.mp3"
      }
    },
    {
      "id": "2602.17478v1",
      "title": "QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery",
      "authors": [
        "Xuan-Bac Nguyen",
        "Hoang-Quan Nguyen",
        "Sankalp Pandey",
        "Tim Faltermeier",
        "Nicholas Borys",
        "Hugh Churchill",
        "Khoa Luu"
      ],
      "abstract": "Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offering standardized protocols for fair and reproducible evaluation.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.17478v1",
        "pdf": "https://arxiv.org/pdf/2602.17478v1"
      },
      "arxiv_id": "2602.17478v1",
      "comment": "Project page: https://uark-cviu.github.io/projects/qupaint/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.199999999999999,
      "score_breakdown": {
        "total_score": 4.2,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physic...",
        "key_contributions": [
          "Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups.",
          "Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions.",
          "This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2602.17478v1.mp3"
      }
    },
    {
      "id": "2602.17473v1",
      "title": "4D Monocular Surgical Reconstruction under Arbitrary Camera Motions",
      "authors": [
        "Jiwei Shan",
        "Zeyu Cai",
        "Cheng-Tai Hsieh",
        "Yirui Li",
        "Hao Liu",
        "Lijun Han",
        "Hesheng Wang",
        "Shing Shin Cheng"
      ],
      "abstract": "Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in real clinical settings. To address this, we propose Local-EndoGS, a high-quality 4D reconstruction framework for monocular endoscopic sequences with arbitrary camera motion. Local-EndoGS introduces a progressive, window-based global representation that allocates local deformable scene models to each observed window, enabling scalability to long sequences with substantial motion. To overcome unreliable initialization without stereo depth or accurate structure-from-motion, we design a coarse-to-fine strategy integrating multi-view geometry, cross-window information, and monocular depth priors, providing a robust foundation for optimization. We further incorporate long-range 2D pixel trajectory constraints and physical motion priors to improve deformation plausibility. Experiments on three public endoscopic datasets with deformable scenes and varying camera motions show that Local-EndoGS consistently outperforms state-of-the-art methods in appearance quality and geometry. Ablation studies validate the effectiveness of our key designs. Code will be released upon acceptance at: https://github.com/IRMVLab/Local-EndoGS.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.17473v1",
        "pdf": "https://arxiv.org/pdf/2602.17473v1"
      },
      "arxiv_id": "2602.17473v1",
      "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920 characters\", the abstract here is shorter than that in the PDF file Subjects",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.877966101694915,
      "score_breakdown": {
        "total_score": 3.88,
        "field_match": {
          "score": 1.69,
          "matches": [
            "3d gaussian",
            "gaussian splatting"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization an...",
        "key_contributions": [
          "Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important.",
          "Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress.",
          "However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in real clinical settings."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2602.17473v1.mp3"
      }
    },
    {
      "id": "2602.17663v1",
      "title": "CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts",
      "authors": [
        "Juri Opitz",
        "Corina Raclé",
        "Emanuela Boros",
        "Andrianos Michail",
        "Matteo Romanello",
        "Maud Ehrmann",
        "Simon Clematide"
      ],
      "abstract": "HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ (\"Has the person ever been at this place?\") and $isAt$ (\"Is the person located at this place around publication time?\") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.",
      "published": "2026-02-19",
      "updated": "2026-02-19",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.17663v1",
        "pdf": "https://arxiv.org/pdf/2602.17663v1"
      },
      "arxiv_id": "2602.17663v1",
      "comment": "ECIR 2026. CLEF Evaluation Lab. Registration DL: 2026/04/23. Task Homepage at https://hipe-eval.github.io/HIPE-2026/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8499999999999996,
      "score_breakdown": {
        "total_score": 3.85,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ (\"H...",
        "key_contributions": [
          "HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts.",
          "Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods.",
          "Systems are asked to classify relations of two types - $at$ (\"Has the person ever been at this place?\") and $isAt$ (\"Is the person located at this place around publication time?\") - requiring reasoning over temporal and geographical cues."
        ],
        "provider": "gemini",
        "audio_url": "/audio/2602.17663v1.mp3"
      }
    }
  ]
}