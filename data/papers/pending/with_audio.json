{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2511.04665v1",
      "title": "Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions",
      "authors": [
        "Kaifeng Zhang",
        "Shuo Sha",
        "Hanxiao Jiang",
        "Matthew Loper",
        "Hyunjong Song",
        "Guangyan Cai",
        "Zhuo Xu",
        "Xiaochen Hu",
        "Changxi Zheng",
        "Yunzhu Li"
      ],
      "abstract": "Robotic manipulation policies are advancing rapidly, but their direct\nevaluation in the real world remains costly, time-consuming, and difficult to\nreproduce, particularly for tasks involving deformable objects. Simulation\nprovides a scalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of soft-body\ninteractions. We present a real-to-sim policy evaluation framework that\nconstructs soft-body digital twins from real-world videos and renders robots,\nobjects, and environments with photorealistic fidelity using 3D Gaussian\nSplatting. We validate our approach on representative deformable manipulation\ntasks, including plush toy packing, rope routing, and T-block pushing,\ndemonstrating that simulated rollouts correlate strongly with real-world\nexecution performance and reveal key behavioral patterns of learned policies.\nOur results suggest that combining physics-informed reconstruction with\nhigh-quality rendering enables reproducible, scalable, and accurate evaluation\nof robotic manipulation policies. Website: https://real2sim-eval.github.io/",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.04665v1",
        "pdf": "http://arxiv.org/pdf/2511.04665v1"
      },
      "arxiv_id": "2511.04665v1",
      "comment": "Website: https://real2sim-eval.github.io/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.527966101694915,
      "score_breakdown": {
        "total_score": 4.53,
        "field_match": {
          "score": 1.69,
          "matches": [
            "3d gaussian",
            "gaussian splatting"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "tldr": "Summary generation failed.",
        "short": "Summary generation failed.",
        "detailed": "Summary generation failed.",
        "key_contributions": [
          "Contribution extraction failed."
        ],
        "chinese": "摘要生成失败。",
        "audio_url": "/audio/2511.04665v1.mp3"
      }
    },
    {
      "id": "2511.04670v1",
      "title": "Cambrian-S: Towards Spatial Supersensing in Video",
      "authors": [
        "Shusheng Yang",
        "Jihan Yang",
        "Pinzhi Huang",
        "Ellis Brown",
        "Zihao Yang",
        "Yue Yu",
        "Shengbang Tong",
        "Zihan Zheng",
        "Yifan Xu",
        "Muhan Wang",
        "Daohan Lu",
        "Rob Fergus",
        "Yann LeCun",
        "Li Fei-Fei",
        "Saining Xie"
      ],
      "abstract": "We argue that progress in true multimodal intelligence calls for a shift from\nreactive, task-driven systems and brute-force long context towards a broader\nparadigm of supersensing. We frame spatial supersensing as four stages beyond\nlinguistic-only understanding: semantic perception (naming what is seen),\nstreaming event cognition (maintaining memory across continuous experiences),\nimplicit 3D spatial cognition (inferring the world behind pixels), and\npredictive world modeling (creating internal models that filter and organize\ninformation). Current benchmarks largely test only the early stages, offering\nnarrow coverage of spatial cognition and rarely challenging models in ways that\nrequire true world modeling. To drive progress in spatial supersensing, we\npresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial\nrecall) and VSC (continual visual spatial counting). These tasks require\narbitrarily long video inputs yet are resistant to brute-force context\nexpansion. We then test data scaling limits by curating VSI-590K and training\nCambrian-S, achieving +30% absolute improvement on VSI-Bench without\nsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,\nindicating that scale alone is insufficient for spatial supersensing. We\npropose predictive sensing as a path forward, presenting a proof-of-concept in\nwhich a self-supervised next-latent-frame predictor leverages surprise\n(prediction error) to drive memory and event segmentation. On VSI-SUPER, this\napproach substantially outperforms leading proprietary baselines, showing that\nspatial supersensing requires models that not only see but also anticipate,\nselect, and organize experience.",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.04670v1",
        "pdf": "http://arxiv.org/pdf/2511.04670v1"
      },
      "arxiv_id": "2511.04670v1",
      "comment": "Website: https://cambrian-mllm.github.io/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.324576271186441,
      "score_breakdown": {
        "total_score": 4.32,
        "field_match": {
          "score": 1.19,
          "matches": [
            "self-supervised",
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "tldr": "Summary generation failed.",
        "short": "Summary generation failed.",
        "detailed": "Summary generation failed.",
        "key_contributions": [
          "Contribution extraction failed."
        ],
        "chinese": "摘要生成失败。",
        "audio_url": "/audio/2511.04670v1.mp3"
      }
    },
    {
      "id": "2511.04465v1",
      "title": "Fraud-Proof Revenue Division on Subscription Platforms",
      "authors": [
        "Abheek Ghosh",
        "Tzeh Yuan Neoh",
        "Nicholas Teh",
        "Giannis Tyrovolas"
      ],
      "abstract": "We study a model of subscription-based platforms where users pay a fixed fee\nfor unlimited access to content, and creators receive a share of the revenue.\nExisting approaches to detecting fraud predominantly rely on machine learning\nmethods, engaging in an ongoing arms race with bad actors. We explore revenue\ndivision mechanisms that inherently disincentivize manipulation. We formalize\nthree types of manipulation-resistance axioms and examine which existing rules\nsatisfy these. We show that a mechanism widely used by streaming platforms, not\nonly fails to prevent fraud, but also makes detecting manipulation\ncomputationally intractable. We also introduce a novel rule, ScaledUserProp,\nthat satisfies all three manipulation-resistance axioms. Finally, experiments\nwith both real-world and synthetic streaming data support ScaledUserProp as a\nfairer alternative compared to existing rules.",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "econ.TH"
      ],
      "primary_category": "cs.GT",
      "links": {
        "paper": "http://arxiv.org/abs/2511.04465v1",
        "pdf": "http://arxiv.org/pdf/2511.04465v1"
      },
      "arxiv_id": "2511.04465v1",
      "comment": "Appears in the 42nd International Conference on Machine Learning\n  (ICML), 2025",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.25,
      "score_breakdown": {
        "total_score": 4.25,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICML",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "tldr": "Summary generation failed.",
        "short": "Summary generation failed.",
        "detailed": "Summary generation failed.",
        "key_contributions": [
          "Contribution extraction failed."
        ],
        "chinese": "摘要生成失败。",
        "audio_url": "/audio/2511.04465v1.mp3"
      }
    },
    {
      "id": "2511.04583v1",
      "title": "Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper",
      "authors": [
        "Atsuyuki Miyai",
        "Mashiro Toyooka",
        "Takashi Otonari",
        "Zaiying Zhao",
        "Kiyoharu Aizawa"
      ],
      "abstract": "Understanding the current capabilities and risks of AI Scientist systems is\nessential for ensuring trustworthy and sustainable AI-driven scientific\nprogress while preserving the integrity of the academic ecosystem. To this end,\nwe develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system\nthat mimics the core research workflow of a novice student researcher: Given\nthe baseline paper from the human mentor, it analyzes its limitations,\nformulates novel hypotheses for improvement, validates them through rigorous\nexperimentation, and writes a paper with the results. Unlike previous\napproaches that assume full automation or operate on small-scale code, Jr. AI\nScientist follows a well-defined research workflow and leverages modern coding\nagents to handle complex, multi-file implementations, leading to scientifically\nvaluable contributions. For evaluation, we conducted automated assessments\nusing AI Reviewers, author-led evaluations, and submissions to Agents4Science,\na venue dedicated to AI-driven scientific contributions. The findings\ndemonstrate that Jr. AI Scientist generates papers receiving higher review\nscores than existing fully automated systems. Nevertheless, we identify\nimportant limitations from both the author evaluation and the Agents4Science\nreviews, indicating the potential risks of directly applying current AI\nScientist systems and key challenges for future research. Finally, we\ncomprehensively report various risks identified during development. We hope\nthese insights will deepen understanding of current progress and risks in AI\nScientist development.",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.04583v1",
        "pdf": "http://arxiv.org/pdf/2511.04583v1"
      },
      "arxiv_id": "2511.04583v1",
      "comment": "Issues, comments, and questions are all welcome in\n  https://github.com/Agent4Science-UTokyo/Jr.AI-Scientist",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.1499999999999995,
      "score_breakdown": {
        "total_score": 4.15,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "tldr": "Summary generation failed.",
        "short": "Summary generation failed.",
        "detailed": "Summary generation failed.",
        "key_contributions": [
          "Contribution extraction failed."
        ],
        "chinese": "摘要生成失败。",
        "audio_url": "/audio/2511.04583v1.mp3"
      }
    },
    {
      "id": "2511.04555v1",
      "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment",
      "authors": [
        "Tao Lin",
        "Yilei Zhong",
        "Yuxin Du",
        "Jingjing Zhang",
        "Jiting Liu",
        "Yinxinyu Chen",
        "Encheng Gu",
        "Ziyan Liu",
        "Hongyi Cai",
        "Yanwen Zou",
        "Lixing Zou",
        "Zhaoye Zhou",
        "Gen Li",
        "Bo Zhao"
      ],
      "abstract": "Vision-Language-Action (VLA) models have emerged as a powerful framework that\nunifies perception, language, and control, enabling robots to perform diverse\ntasks through multimodal understanding. However, current VLA models typically\ncontain massive parameters and rely heavily on large-scale robot data\npretraining, leading to high computational costs during training, as well as\nlimited deployability for real-time inference. Moreover, most training\nparadigms often degrade the perceptual representations of the vision-language\nbackbone, resulting in overfitting and poor generalization to downstream tasks.\nIn this work, we present Evo-1, a lightweight VLA model that reduces\ncomputation and improves deployment efficiency, while maintaining strong\nperformance without pretraining on robot data. Evo-1 builds on a native\nmultimodal Vision-Language model (VLM), incorporating a novel cross-modulated\ndiffusion transformer along with an optimized integration module, together\nforming an effective architecture. We further introduce a two-stage training\nparadigm that progressively aligns action with perception, preserving the\nrepresentations of the VLM. Notably, with only 0.77 billion parameters, Evo-1\nachieves state-of-the-art results on the Meta-World and RoboTwin suite,\nsurpassing the previous best models by 12.4% and 6.9%, respectively, and also\nattains a competitive result of 94.8% on LIBERO. In real-world evaluations,\nEvo-1 attains a 78% success rate with high inference frequency and low memory\noverhead, outperforming all baseline methods. We release code, data, and model\nweights to facilitate future research on lightweight and efficient VLA models.",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2511.04555v1",
        "pdf": "http://arxiv.org/pdf/2511.04555v1"
      },
      "arxiv_id": "2511.04555v1",
      "comment": "Github: https://github.com/MINT-SJTU/Evo-1",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.1499999999999995,
      "score_breakdown": {
        "total_score": 4.15,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "tldr": "Summary generation failed.",
        "short": "Summary generation failed.",
        "detailed": "Summary generation failed.",
        "key_contributions": [
          "Contribution extraction failed."
        ],
        "chinese": "摘要生成失败。",
        "audio_url": "/audio/2511.04555v1.mp3"
      }
    },
    {
      "id": "2511.04590v1",
      "title": "Complexity as Advantage: A Regret-Based Perspective on Emergent Structure",
      "authors": [
        "Oshri Naparstek"
      ],
      "abstract": "We introduce Complexity as Advantage (CAA), a framework that defines the\ncomplexity of a system relative to a family of observers. Instead of measuring\ncomplexity as an intrinsic property, we evaluate how much predictive regret a\nsystem induces for different observers attempting to model it. A system is\ncomplex when it is easy for some observers and hard for others, creating an\ninformation advantage. We show that this formulation unifies several notions of\nemergent behavior, including multiscale entropy, predictive information, and\nobserver-dependent structure. The framework suggests that \"interesting\" systems\nare those positioned to create differentiated regret across observers,\nproviding a quantitative grounding for why complexity can be functionally\nvaluable. We demonstrate the idea through simple dynamical models and discuss\nimplications for learning, evolution, and artificial agents.",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "categories": [
        "cs.LG",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.04590v1",
        "pdf": "http://arxiv.org/pdf/2511.04590v1"
      },
      "arxiv_id": "2511.04590v1",
      "comment": "15 pages. Under preparation for submission to ICML 2026. Feedback\n  welcome",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.05,
      "score_breakdown": {
        "total_score": 4.05,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICML",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 5.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "tldr": "Summary generation failed.",
        "short": "Summary generation failed.",
        "detailed": "Summary generation failed.",
        "key_contributions": [
          "Contribution extraction failed."
        ],
        "chinese": "摘要生成失败。",
        "audio_url": "/audio/2511.04590v1.mp3"
      }
    },
    {
      "id": "2511.04655v1",
      "title": "Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable Non-Visual Shortcuts",
      "authors": [
        "Ellis Brown",
        "Jihan Yang",
        "Shusheng Yang",
        "Rob Fergus",
        "Saining Xie"
      ],
      "abstract": "Robust benchmarks are crucial for evaluating Multimodal Large Language Models\n(MLLMs). Yet we find that models can ace many multimodal benchmarks without\nstrong visual understanding, instead exploiting biases, linguistic priors, and\nsuperficial patterns. This is especially problematic for vision-centric\nbenchmarks that are meant to require visual inputs. We adopt a diagnostic\nprinciple for benchmark design: if a benchmark can be gamed, it will be.\nDesigners should therefore try to ``game'' their own benchmarks first, using\ndiagnostic and debiasing procedures to systematically identify and mitigate\nnon-visual biases. Effective diagnosis requires directly ``training on the test\nset'' -- probing the released test set for its intrinsic, exploitable patterns.\n  We operationalize this standard with two components. First, we diagnose\nbenchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.\nOur primary diagnostic tool involves fine-tuning a powerful Large Language\nModel via $k$-fold cross-validation on exclusively the non-visual, textual\ninputs of the test set to reveal shortcut performance and assign each sample a\nbias score $s(x)$. We complement this with a lightweight Random Forest-based\ndiagnostic operating on hand-crafted features for fast, interpretable auditing.\nSecond, we debias benchmarks by filtering high-bias samples using an\n``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four\nbenchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive\nnon-visual biases. As a case study, we apply our full framework to create\nVSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider\nvision-blind performance gap than the original.",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.04655v1",
        "pdf": "http://arxiv.org/pdf/2511.04655v1"
      },
      "arxiv_id": "2511.04655v1",
      "comment": "Project page: https://cambrian-mllm.github.io",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.0,
      "score_breakdown": {
        "total_score": 4.0,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "tldr": "Summary generation failed.",
        "short": "Summary generation failed.",
        "detailed": "Summary generation failed.",
        "key_contributions": [
          "Contribution extraction failed."
        ],
        "chinese": "摘要生成失败。",
        "audio_url": "/audio/2511.04655v1.mp3"
      }
    },
    {
      "id": "2511.04668v1",
      "title": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding",
      "authors": [
        "Ellis Brown",
        "Arijit Ray",
        "Ranjay Krishna",
        "Ross Girshick",
        "Rob Fergus",
        "Saining Xie"
      ],
      "abstract": "Despite impressive high-level video comprehension, multimodal language models\nstruggle with spatial reasoning across time and space. While current spatial\ntraining approaches rely on real-world video data, obtaining diverse footage\nwith precise spatial annotations remains a bottleneck. To alleviate this\nbottleneck, we present SIMS-V -- a systematic data-generation framework that\nleverages the privileged information of 3D simulators to create spatially-rich\nvideo training data for multimodal language models. Using this framework, we\ninvestigate which properties of simulated data drive effective real-world\ntransfer through systematic ablations of question types, mixes, and scales. We\nidentify a minimal set of three question categories (metric measurement,\nperspective-dependent reasoning, and temporal tracking) that prove most\neffective for developing transferable spatial intelligence, outperforming\ncomprehensive coverage despite using fewer question types. These insights\nenable highly efficient training: our 7B-parameter video LLM fine-tuned on just\n25K simulated examples outperforms the larger 72B baseline and achieves\ncompetitive performance with proprietary models on rigorous real-world spatial\nreasoning benchmarks. Our approach demonstrates robust generalization,\nmaintaining performance on general video understanding while showing\nsubstantial improvements on embodied and real-world spatial tasks.",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.04668v1",
        "pdf": "http://arxiv.org/pdf/2511.04668v1"
      },
      "arxiv_id": "2511.04668v1",
      "comment": "Project page: https://ellisbrown.github.io/sims-v",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.9,
      "score_breakdown": {
        "total_score": 3.9,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "tldr": "Summary generation failed.",
        "short": "Summary generation failed.",
        "detailed": "Summary generation failed.",
        "key_contributions": [
          "Contribution extraction failed."
        ],
        "chinese": "摘要生成失败。",
        "audio_url": "/audio/2511.04668v1.mp3"
      }
    },
    {
      "id": "2511.04666v1",
      "title": "Forgetting is Everywhere",
      "authors": [
        "Ben Sanati",
        "Thomas L. Lee",
        "Trevor McInroe",
        "Aidan Scannell",
        "Nikolay Malkin",
        "David Abel",
        "Amos Storkey"
      ],
      "abstract": "A fundamental challenge in developing general learning algorithms is their\ntendency to forget past knowledge when adapting to new data. Addressing this\nproblem requires a principled understanding of forgetting; yet, despite decades\nof study, no unified definition has emerged that provides insights into the\nunderlying dynamics of learning. We propose an algorithm- and task-agnostic\ntheory that characterises forgetting as a lack of self-consistency in a\nlearner's predictive distribution over future experiences, manifesting as a\nloss of predictive information. Our theory naturally yields a general measure\nof an algorithm's propensity to forget. To validate the theory, we design a\ncomprehensive set of experiments that span classification, regression,\ngenerative modelling, and reinforcement learning. We empirically demonstrate\nhow forgetting is present across all learning settings and plays a significant\nrole in determining learning efficiency. Together, these results establish a\nprincipled understanding of forgetting and lay the foundation for analysing and\nimproving the information retention capabilities of general learning\nalgorithms.",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.04666v1",
        "pdf": "http://arxiv.org/pdf/2511.04666v1"
      },
      "arxiv_id": "2511.04666v1",
      "comment": "Project page:\n  https://ben-sanati.github.io/forgetting-is-everywhere-project/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8,
      "score_breakdown": {
        "total_score": 3.8,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "tldr": "Summary generation failed.",
        "short": "Summary generation failed.",
        "detailed": "Summary generation failed.",
        "key_contributions": [
          "Contribution extraction failed."
        ],
        "chinese": "摘要生成失败。",
        "audio_url": "/audio/2511.04666v1.mp3"
      }
    },
    {
      "id": "2511.04394v1",
      "title": "DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale",
      "authors": [
        "Ke Du",
        "Yimin Peng",
        "Chao Gao",
        "Fan Zhou",
        "Siqiao Xue"
      ],
      "abstract": "DORAEMON is an open-source PyTorch library that unifies visual object\nmodeling and representation learning across diverse scales. A single\nYAML-driven workflow covers classification, retrieval and metric learning; more\nthan 1000 pretrained backbones are exposed through a timm-compatible interface,\ntogether with modular losses, augmentations and distributed-training utilities.\nReproducible recipes match or exceed reference results on ImageNet-1K,\nMS-Celeb-1M and Stanford online products, while one-command export to ONNX or\nHuggingFace bridges research and deployment. By consolidating datasets, models,\nand training techniques into one platform, DORAEMON offers a scalable\nfoundation for rapid experimentation in visual recognition and representation\nlearning, enabling efficient transfer of research advances to real-world\napplications. The repository is available at https://github.com/wuji3/DORAEMON.",
      "published": "2025-11-06",
      "updated": "2025-11-06",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.04394v1",
        "pdf": "http://arxiv.org/pdf/2511.04394v1"
      },
      "arxiv_id": "2511.04394v1",
      "comment": "code: https://github.com/wuji3/DORAEMON",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8,
      "score_breakdown": {
        "total_score": 3.8,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "tldr": "Summary generation failed.",
        "short": "Summary generation failed.",
        "detailed": "Summary generation failed.",
        "key_contributions": [
          "Contribution extraction failed."
        ],
        "chinese": "摘要生成失败。",
        "audio_url": "/audio/2511.04394v1.mp3"
      }
    }
  ]
}