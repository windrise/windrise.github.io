{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2602.21105v1",
      "title": "BrepGaussian: CAD reconstruction from Multi-View Images with Gaussian Splatting",
      "authors": [
        "Jiaxing Yu",
        "Dongyang Ren",
        "Hangyu Xu",
        "Zhouyuxiao Yang",
        "Yuanqi Li",
        "Jie Guo",
        "Zhengkang Zhou",
        "Yanwen Guo"
      ],
      "abstract": "The boundary representation (B-rep) models a 3D solid as its explicit boundaries: trimmed corners, edges, and faces. Recovering B-rep representation from unstructured data is a challenging and valuable task of computer vision and graphics. Recent advances in deep learning have greatly improved the recovery of 3D shape geometry, but still depend on dense and clean point clouds and struggle to generalize to novel shapes. We propose B-rep Gaussian Splatting (BrepGaussian), a novel framework that learns 3D parametric representations from 2D images. We employ a Gaussian Splatting renderer with learnable features, followed by a specific fitting strategy. To disentangle geometry reconstruction and feature learning, we introduce a two-stage learning framework that first captures geometry and edges and then refines patch features to achieve clean geometry and coherent instance representations. Extensive experiments demonstrate the superior performance of our approach to state-of-the-art methods. We will release our code and datasets upon acceptance.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.21105v1",
        "pdf": "https://arxiv.org/pdf/2602.21105v1"
      },
      "arxiv_id": "2602.21105v1",
      "comment": "Accepted to CVPR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 5.3779661016949145,
      "score_breakdown": {
        "total_score": 5.38,
        "field_match": {
          "score": 1.69,
          "matches": [
            "gaussian splatting",
            "deep learning",
            "computer vision"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "CVPR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "The boundary representation (B-rep) models a 3D solid as its explicit boundaries: trimmed corners, edges, and faces. Recovering B-rep representation from unstructured data is a challenging and valuable task of computer vision and graphics. Recent advances in deep learning have greatly improved the recovery of 3D shape geometry, but still depend on dense and clean point clouds and struggle to gener...",
        "key_contributions": [
          "The boundary representation (B-rep) models a 3D solid as its explicit boundaries: trimmed corners, edges, and faces.",
          "Recovering B-rep representation from unstructured data is a challenging and valuable task of computer vision and graphics.",
          "Recent advances in deep learning have greatly improved the recovery of 3D shape geometry, but still depend on dense and clean point clouds and struggle to generalize to novel shapes."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.20880v1",
      "title": "When Safety Collides: Resolving Multi-Category Harmful Conflicts in Text-to-Image Diffusion via Adaptive Safety Guidance",
      "authors": [
        "Yongli Xiang",
        "Ziming Hong",
        "Zhaoqing Wang",
        "Xiangyu Zhao",
        "Bo Han",
        "Tongliang Liu"
      ],
      "abstract": "Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords. However, these approaches fail to capture the complex interplay among different harm categories, leading to \"harmful conflicts\" where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate. To address this issue, we propose Conflict-aware Adaptive Safety Guidance (CASG), a training-free framework that dynamically identifies and applies the category-aligned safety direction during generation. CASG is composed of two components: (i) Conflict-aware Category Identification (CaCI), which identifies the harmful category most aligned with the model's evolving generative state, and (ii) Conflict-resolving Guidance Application (CrGA), which applies safety steering solely along the identified category to avoid multi-category interference. CASG can be applied to both latent-space and text-space safeguards. Experiments on T2I safety benchmarks demonstrate CASG's state-of-the-art performance, reducing the harmful rate by up to 15.4% compared to existing methods.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.20880v1",
        "pdf": "https://arxiv.org/pdf/2602.20880v1"
      },
      "arxiv_id": "2602.20880v1",
      "comment": "CVPR 2026; Code is released at https://github.com/tmllab/2026_CVPR_CASG",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 5.300000000000001,
      "score_breakdown": {
        "total_score": 5.3,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "CVPR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefine...",
        "key_contributions": [
          "Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation.",
          "Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords.",
          "However, these approaches fail to capture the complex interplay among different harm categories, leading to \"harmful conflicts\" where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.20933v1",
      "title": "Dropping Anchor and Spherical Harmonics for Sparse-view Gaussian Splatting",
      "authors": [
        "Shuangkang Fang",
        "I-Chao Shen",
        "Xuanyang Zhang",
        "Zesheng Wang",
        "Yufeng Wang",
        "Wenrui Ding",
        "Gang Yu",
        "Takeo Igarashi"
      ],
      "abstract": "Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities. However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization. Moreover, these methods overlook the contribution of high-degree spherical harmonic coefficients (SH) to overfitting. To address these issues, we propose DropAnSH-GS, a novel anchor-based Dropout strategy. Rather than dropping Gaussians independently, our method randomly selects certain Gaussians as anchors and simultaneously removes their spatial neighbors. This effectively disrupts local redundancies near anchors and encourages the model to learn more robust, globally informed representations. Furthermore, we extend the Dropout to color attributes by randomly dropping higher-degree SH to concentrate appearance information in lower-degree SH. This strategy further mitigates overfitting and enables flexible post-training model compression via SH truncation. Experimental results demonstrate that DropAnSH-GS substantially outperforms existing Dropout methods with negligible computational overhead, and can be readily integrated into various 3DGS variants to enhance their performances. Project Website: https://sk-fun.fun/DropAnSH-GS",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.20933v1",
        "pdf": "https://arxiv.org/pdf/2602.20933v1"
      },
      "arxiv_id": "2602.20933v1",
      "comment": "Accepted by CVPR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 5.027966101694915,
      "score_breakdown": {
        "total_score": 5.03,
        "field_match": {
          "score": 1.69,
          "matches": [
            "3d gaussian",
            "gaussian splatting"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "CVPR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities. However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization. Moreover, these methods overlook the contribution of high-degree spherical harmonic ...",
        "key_contributions": [
          "Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities.",
          "However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization.",
          "Moreover, these methods overlook the contribution of high-degree spherical harmonic coefficients (SH) to overfitting."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.21010v1",
      "title": "Le-DETR: Revisiting Real-Time Detection Transformer with Efficient Encoder Design",
      "authors": [
        "Jiannan Huang",
        "Aditya Kane",
        "Fengzhe Zhou",
        "Yunchao Wei",
        "Humphrey Shi"
      ],
      "abstract": "Real-time object detection is crucial for real-world applications as it requires high accuracy with low latency. While Detection Transformers (DETR) have demonstrated significant performance improvements, current real-time DETR models are challenging to reproduce from scratch due to excessive pre-training overheads on the backbone, constraining research advancements by hindering the exploration of novel backbone architectures. In this paper, we want to show that by using general good design, it is possible to have \\textbf{high performance} with \\textbf{low pre-training cost}. After a thorough study of the backbone architecture, we propose EfficientNAT at various scales, which incorporates modern efficient convolution and local attention mechanisms. Moreover, we re-design the hybrid encoder with local attention, significantly enhancing both performance and inference speed. Based on these advancements, we present Le-DETR (\\textbf{L}ow-cost and \\textbf{E}fficient \\textbf{DE}tection \\textbf{TR}ansformer), which achieves a new \\textbf{SOTA} in real-time detection using only ImageNet1K and COCO2017 training datasets, saving about 80\\% images in pre-training stage compared with previous methods. We demonstrate that with well-designed, real-time DETR models can achieve strong performance without the need for complex and computationally expensive pretraining. Extensive experiments show that Le-DETR-M/L/X achieves \\textbf{52.9/54.3/55.1 mAP} on COCO Val2017 with \\textbf{4.45/5.01/6.68 ms} on an RTX4090. It surpasses YOLOv12-L/X by \\textbf{+0.6/-0.1 mAP} while achieving similar speed and \\textbf{+20\\%} speedup. Compared with DEIM-D-FINE, Le-DETR-M achieves \\textbf{+0.2 mAP} with slightly faster inference, and surpasses DEIM-D-FINE-L by \\textbf{+0.4 mAP} with only \\textbf{0.4 ms} additional latency. Code and weights will be open-sourced.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.21010v1",
        "pdf": "https://arxiv.org/pdf/2602.21010v1"
      },
      "arxiv_id": "2602.21010v1",
      "comment": "CVPR Findings",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.8,
      "score_breakdown": {
        "total_score": 4.8,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "CVPR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Real-time object detection is crucial for real-world applications as it requires high accuracy with low latency. While Detection Transformers (DETR) have demonstrated significant performance improvements, current real-time DETR models are challenging to reproduce from scratch due to excessive pre-training overheads on the backbone, constraining research advancements by hindering the exploration of...",
        "key_contributions": [
          "Real-time object detection is crucial for real-world applications as it requires high accuracy with low latency.",
          "While Detection Transformers (DETR) have demonstrated significant performance improvements, current real-time DETR models are challenging to reproduce from scratch due to excessive pre-training overheads on the backbone, constraining research advancements by hindering the exploration of novel backbone architectures.",
          "In this paper, we want to show that by using general good design, it is possible to have \\textbf{high performance} with \\textbf{low pre-training cost}."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.21143v1",
      "title": "A Benchmark for Deep Information Synthesis",
      "authors": [
        "Debjit Paul",
        "Daniel Murphy",
        "Milan Gritta",
        "Ronald Cardenas",
        "Victor Prokhorov",
        "Lena Sophia Bolliger",
        "Aysim Toker",
        "Roy Miles",
        "Andreea-Maria Oncescu",
        "Jasivan Alex Sivakumar",
        "Philipp Borchert",
        "Ismail Elezi",
        "Meiru Zhang",
        "Ka Yiu Lee",
        "Guchun Zhang",
        "Jun Wang",
        "Gerasimos Lampouras"
      ],
      "abstract": "Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2602.21143v1",
        "pdf": "https://arxiv.org/pdf/2602.21143v1"
      },
      "arxiv_id": "2602.21143v1",
      "comment": "Accepted at ICLR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.75,
      "score_breakdown": {
        "total_score": 4.75,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we i...",
        "key_contributions": [
          "Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis.",
          "However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval.",
          "To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.21078v1",
      "title": "ProxyFL: A Proxy-Guided Framework for Federated Semi-Supervised Learning",
      "authors": [
        "Duowen Chen",
        "Yan Wang"
      ],
      "abstract": "Federated Semi-Supervised Learning (FSSL) aims to collaboratively train a global model across clients by leveraging partially-annotated local data in a privacy-preserving manner. In FSSL, data heterogeneity is a challenging issue, which exists both across clients and within clients. External heterogeneity refers to the data distribution discrepancy across different clients, while internal heterogeneity represents the mismatch between labeled and unlabeled data within clients. Most FSSL methods typically design fixed or dynamic parameter aggregation strategies to collect client knowledge on the server (external) and / or filter out low-confidence unlabeled samples to reduce mistakes in local client (internal). But, the former is hard to precisely fit the ideal global distribution via direct weights, and the latter results in fewer data participation into FL training. To this end, we propose a proxy-guided framework called ProxyFL that focuses on simultaneously mitigating external and internal heterogeneity via a unified proxy. I.e., we consider the learnable weights of classifier as proxy to simulate the category distribution both locally and globally. For external, we explicitly optimize global proxy against outliers instead of direct weights; for internal, we re-include the discarded samples into training by a positive-negative proxy pool to mitigate the impact of potentially-incorrect pseudo-labels. Insight experiments & theoretical analysis show our significant performance and convergence in FSSL.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.21078v1",
        "pdf": "https://arxiv.org/pdf/2602.21078v1"
      },
      "arxiv_id": "2602.21078v1",
      "comment": "CVPR 2026. code: https://github.com/DuowenC/FSSLlib",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.75,
      "score_breakdown": {
        "total_score": 4.75,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "CVPR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 5.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Federated Semi-Supervised Learning (FSSL) aims to collaboratively train a global model across clients by leveraging partially-annotated local data in a privacy-preserving manner. In FSSL, data heterogeneity is a challenging issue, which exists both across clients and within clients. External heterogeneity refers to the data distribution discrepancy across different clients, while internal heteroge...",
        "key_contributions": [
          "Federated Semi-Supervised Learning (FSSL) aims to collaboratively train a global model across clients by leveraging partially-annotated local data in a privacy-preserving manner.",
          "In FSSL, data heterogeneity is a challenging issue, which exists both across clients and within clients.",
          "External heterogeneity refers to the data distribution discrepancy across different clients, while internal heterogeneity represents the mismatch between labeled and unlabeled data within clients."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.21043v1",
      "title": "T1: One-to-One Channel-Head Binding for Multivariate Time-Series Imputation",
      "authors": [
        "Dongik Park",
        "Hyunwoo Ryu",
        "Suahn Bae",
        "Keondo Park",
        "Hyung-Sin Kim"
      ],
      "abstract": "Imputing missing values in multivariate time series remains challenging, especially under diverse missing patterns and heavy missingness. Existing methods suffer from suboptimal performance as corrupted temporal features hinder effective cross-variable information transfer, amplifying reconstruction errors. Robust imputation requires both extracting temporal patterns from sparse observations within each variable and selectively transferring information across variables--yet current approaches excel at one while compromising the other. We introduce T1 (Time series imputation with 1-to-1 channel-head binding), a CNN-Transformer hybrid architecture that achieves robust imputation through Channel-Head Binding--a mechanism creating one-to-one correspondence between CNN channels and attention heads. This design enables selective information transfer: when missingness corrupts certain temporal patterns, their corresponding attention pathways adaptively down-weight based on remaining observable patterns while preserving reliable cross-variable connections through unaffected channels. Experiments on 11 benchmark datasets demonstrate that T1 achieves state-of-the-art performance, reducing MSE by 46% on average compared to the second-best baseline, with particularly strong gains under extreme sparsity (70% missing ratio). The model generalizes to unseen missing patterns without retraining and uses a consistent hyperparameter configuration across all datasets. The code is available at https://github.com/Oppenheimerdinger/T1.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.21043v1",
        "pdf": "https://arxiv.org/pdf/2602.21043v1"
      },
      "arxiv_id": "2602.21043v1",
      "comment": "Accepted at ICLR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.6,
      "score_breakdown": {
        "total_score": 4.6,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Imputing missing values in multivariate time series remains challenging, especially under diverse missing patterns and heavy missingness. Existing methods suffer from suboptimal performance as corrupted temporal features hinder effective cross-variable information transfer, amplifying reconstruction errors. Robust imputation requires both extracting temporal patterns from sparse observations withi...",
        "key_contributions": [
          "Imputing missing values in multivariate time series remains challenging, especially under diverse missing patterns and heavy missingness.",
          "Existing methods suffer from suboptimal performance as corrupted temporal features hinder effective cross-variable information transfer, amplifying reconstruction errors.",
          "Robust imputation requires both extracting temporal patterns from sparse observations within each variable and selectively transferring information across variables--yet current approaches excel at one while compromising the other."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.20985v1",
      "title": "EW-DETR: Evolving World Object Detection via Incremental Low-Rank DEtection TRansformer",
      "authors": [
        "Munish Monga",
        "Vishal Chudasama",
        "Pankaj Wasnik",
        "C. V. Jawahar"
      ],
      "abstract": "Real-world object detection must operate in evolving environments where new classes emerge, domains shift, and unseen objects must be identified as \"unknown\": all without accessing prior data. We introduce Evolving World Object Detection (EWOD), a paradigm coupling incremental learning, domain adaptation, and unknown detection under exemplar-free constraints. To tackle EWOD, we propose EW-DETR framework that augments DETR-based detectors with three synergistic modules: Incremental LoRA Adapters for exemplar-free incremental learning under evolving domains; a Query-Norm Objectness Adapter that decouples objectness-aware features from DETR decoder queries; and Entropy-Aware Unknown Mixing for calibrated unknown detection. This framework generalises across DETR-based detectors, enabling state-of-the-art RF-DETR to operate effectively in evolving-world settings. We also introduce FOGS (Forgetting, Openness, Generalisation Score) to holistically evaluate performance across these dimensions. Extensive experiments on Pascal Series and Diverse Weather benchmarks show EW-DETR outperforms other methods, improving FOGS by 57.24%.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.20985v1",
        "pdf": "https://arxiv.org/pdf/2602.20985v1"
      },
      "arxiv_id": "2602.20985v1",
      "comment": "Accepted at CVPR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.6,
      "score_breakdown": {
        "total_score": 4.6,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "CVPR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Real-world object detection must operate in evolving environments where new classes emerge, domains shift, and unseen objects must be identified as \"unknown\": all without accessing prior data. We introduce Evolving World Object Detection (EWOD), a paradigm coupling incremental learning, domain adaptation, and unknown detection under exemplar-free constraints. To tackle EWOD, we propose EW-DETR fra...",
        "key_contributions": [
          "Real-world object detection must operate in evolving environments where new classes emerge, domains shift, and unseen objects must be identified as \"unknown\": all without accessing prior data.",
          "We introduce Evolving World Object Detection (EWOD), a paradigm coupling incremental learning, domain adaptation, and unknown detection under exemplar-free constraints.",
          "To tackle EWOD, we propose EW-DETR framework that augments DETR-based detectors with three synergistic modules: Incremental LoRA Adapters for exemplar-free incremental learning under evolving domains; a Query-Norm Objectness Adapter that decouples objectness-aware features from DETR decoder queries; and Entropy-Aware Unknown Mixing for calibrated unknown detection."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.20873v1",
      "title": "MUSE: Harnessing Precise and Diverse Semantics for Few-Shot Whole Slide Image Classification",
      "authors": [
        "Jiahao Xu",
        "Sheng Huang",
        "Xin Zhang",
        "Zhixiong Nan",
        "Jiajun Dong",
        "Nankun Mu"
      ],
      "abstract": "In computational pathology, few-shot whole slide image classification is primarily driven by the extreme scarcity of expert-labeled slides. Recent vision-language methods incorporate textual semantics generated by large language models, but treat these descriptions as static class-level priors that are shared across all samples and lack sample-wise refinement. This limits both the diversity and precision of visual-semantic alignment, hindering generalization under limited supervision. To overcome this, we propose the stochastic MUlti-view Semantic Enhancement (MUSE), a framework that first refines semantic precision via sample-wise adaptation and then enhances semantic richness through retrieval-augmented multi-view generation. Specifically, MUSE introduces Sample-wise Fine-grained Semantic Enhancement (SFSE), which yields a fine-grained semantic prior for each sample through MoE-based adaptive visual-semantic interaction. Guided by this prior, Stochastic Multi-view Model Optimization (SMMO) constructs an LLM-generated knowledge base of diverse pathological descriptions per class, then retrieves and stochastically integrates multiple matched textual views during training. These dynamically selected texts serve as enriched semantic supervisions to stochastically optimize the vision-language model, promoting robustness and mitigating overfitting. Experiments on three benchmark WSI datasets show that MUSE consistently outperforms existing vision-language baselines in few-shot settings, demonstrating that effective few-shot pathology learning requires not only richer semantic sources but also their active and sample-aware semantic optimization. Our code is available at: https://github.com/JiahaoXu-god/CVPR2026_MUSE.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.20873v1",
        "pdf": "https://arxiv.org/pdf/2602.20873v1"
      },
      "arxiv_id": "2602.20873v1",
      "comment": "Accepted by CVPR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.6,
      "score_breakdown": {
        "total_score": 4.6,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "CVPR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "In computational pathology, few-shot whole slide image classification is primarily driven by the extreme scarcity of expert-labeled slides. Recent vision-language methods incorporate textual semantics generated by large language models, but treat these descriptions as static class-level priors that are shared across all samples and lack sample-wise refinement. This limits both the diversity and pr...",
        "key_contributions": [
          "In computational pathology, few-shot whole slide image classification is primarily driven by the extreme scarcity of expert-labeled slides.",
          "Recent vision-language methods incorporate textual semantics generated by large language models, but treat these descriptions as static class-level priors that are shared across all samples and lack sample-wise refinement.",
          "This limits both the diversity and precision of visual-semantic alignment, hindering generalization under limited supervision."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.20981v1",
      "title": "Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models",
      "authors": [
        "Christian Simon",
        "MAsato Ishii",
        "Wei-Yao Wang",
        "Koichi Saito",
        "Akio Hayakawa",
        "Dongseok Shim",
        "Zhi Zhong",
        "Shuyang Cui",
        "Shusuke Takahashi",
        "Takashi Shibuya",
        "Yuki Mitsufuji"
      ],
      "abstract": "Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations.",
      "published": "2026-02-24",
      "updated": "2026-02-24",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.20981v1",
        "pdf": "https://arxiv.org/pdf/2602.20981v1"
      },
      "arxiv_id": "2602.20981v1",
      "comment": "Accepted to CVPR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.55,
      "score_breakdown": {
        "total_score": 4.55,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "CVPR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present mult...",
        "key_contributions": [
          "Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information.",
          "In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing.",
          "To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models."
        ],
        "provider": "gemini"
      }
    }
  ]
}