{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2601.16672v1",
      "title": "ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction",
      "authors": [
        "Ming Li",
        "Hui Shan",
        "Kai Zheng",
        "Chentao Shen",
        "Siyu Liu",
        "Yanwei Fu",
        "Zhen Chen",
        "Xiangru Huang"
      ],
      "abstract": "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency.",
      "published": "2026-01-23",
      "updated": "2026-01-23",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.16672v1",
        "pdf": "https://arxiv.org/pdf/2601.16672v1"
      },
      "arxiv_id": "2601.16672v1",
      "comment": "15 pages, 8 figures, Submitted to CVPR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.788983050847458,
      "score_breakdown": {
        "total_score": 4.79,
        "field_match": {
          "score": 0.85,
          "matches": [
            "3d gaussian"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "CVPR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a resu...",
        "key_contributions": [
          "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation.",
          "However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures.",
          "As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2601.16788v1",
      "title": "REL-SF4PASS: Panoramic Semantic Segmentation with REL Depth Representation and Spherical Fusion",
      "authors": [
        "Xuewei Li",
        "Xinghan Bao",
        "Zhimin Chen",
        "Xi Li"
      ],
      "abstract": "As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view. Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry. To address these shortcomings, we propose REL-SF4PASS with our REL depth representation based on cylindrical coordinate and Spherical-dynamic Multi-Modal Fusion SMMF. REL is made up of Rectified Depth, Elevation-Gained Vertical Inclination Angle, and Lateral Orientation Angle, which fully represents 3D space in cylindrical coordinate style and the surface normal direction. SMMF aims to ensure the diversity of fusion for different panoramic image regions and reduce the breakage of cylinder side surface expansion in ERP projection, which uses different fusion strategies to match the different regions in panoramic images. Experimental results show that REL-SF4PASS considerably improves performance and robustness on popular benchmark, Stanford2D3D Panoramic datasets. It gains 2.35% average mIoU improvement on all 3 folds and reduces the performance variance by approximately 70% when facing 3D disturbance.",
      "published": "2026-01-23",
      "updated": "2026-01-23",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.16788v1",
        "pdf": "https://arxiv.org/pdf/2601.16788v1"
      },
      "arxiv_id": "2601.16788v1",
      "comment": "submitted to CVPR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.672881355932203,
      "score_breakdown": {
        "total_score": 4.67,
        "field_match": {
          "score": 0.93,
          "matches": [
            "segmentation",
            "computer vision"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "CVPR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view. Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry. To address these shortcomings, we propose...",
        "key_contributions": [
          "As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view.",
          "Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry.",
          "To address these shortcomings, we propose REL-SF4PASS with our REL depth representation based on cylindrical coordinate and Spherical-dynamic Multi-Modal Fusion SMMF."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2601.16982v1",
      "title": "AnyView: Synthesizing Any Novel View in Dynamic Scenes",
      "authors": [
        "Basile Van Hoorick",
        "Dian Chen",
        "Shun Iwase",
        "Pavel Tokmakov",
        "Muhammad Zubair Irshad",
        "Igor Vasiljevic",
        "Swati Gupta",
        "Fangzhou Cheng",
        "Sergey Zakharov",
        "Vitor Campagnolo Guizilini"
      ],
      "abstract": "Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \\textbf{AnyView}, a diffusion-based video generation framework for \\emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \\textbf{AnyViewBench}, a challenging new benchmark tailored towards \\emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \\emph{any} viewpoint. Results, data, code, and models can be viewed at: https://tri-ml.github.io/AnyView/",
      "published": "2026-01-23",
      "updated": "2026-01-23",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.16982v1",
        "pdf": "https://arxiv.org/pdf/2601.16982v1"
      },
      "arxiv_id": "2601.16982v1",
      "comment": "Project webpage: https://tri-ml.github.io/AnyView/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.25,
      "score_breakdown": {
        "total_score": 4.25,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \\textbf{AnyView}, a diffusion-based video generation framework for \\emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data ...",
        "key_contributions": [
          "Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments.",
          "In this work, we introduce \\textbf{AnyView}, a diffusion-based video generation framework for \\emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions.",
          "We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2601.16967v1",
      "title": "Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians",
      "authors": [
        "Bernes Lorier Atabonfack",
        "Ahmed Tahiru Issah",
        "Mohammed Hardi Abdul Baaki",
        "Clemence Ingabire",
        "Tolulope Olusuyi",
        "Maruf Adewole",
        "Udunna C. Anazodo",
        "Timothy X Brown"
      ],
      "abstract": "In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.",
      "published": "2026-01-23",
      "updated": "2026-01-23",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2601.16967v1",
        "pdf": "https://arxiv.org/pdf/2601.16967v1"
      },
      "arxiv_id": "2601.16967v1",
      "comment": "Accepted at the MIRASOL Workshop at MICCAI 2025. To appear in Lecture Notes in Computer Science (LNCS)",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.2,
      "score_breakdown": {
        "total_score": 4.2,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "MICCAI",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delaye...",
        "key_contributions": [
          "In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations.",
          "This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care.",
          "This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2601.16981v1",
      "title": "SyncLight: Controllable and Consistent Multi-View Relighting",
      "authors": [
        "David Serrano-Lozano",
        "Anand Bhattad",
        "Luis Herranz",
        "Jean-François Lalonde",
        "Javier Vazquez-Corral"
      ],
      "abstract": "We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit. Our method leverages a multi-view diffusion transformer trained using a latent bridge matching formulation, achieving high-fidelity relighting of the entire image set in a single inference step. To facilitate training, we introduce a large-scale hybrid dataset comprising diverse synthetic environments -- curated from existing sources and newly designed scenes -- alongside high-fidelity, real-world multi-view captures under calibrated illumination. Surprisingly, though trained only on image pairs, SyncLight generalizes zero-shot to an arbitrary number of viewpoints, effectively propagating lighting changes across all views, without requiring camera pose information. SyncLight enables practical relighting workflows for multi-view capture systems.",
      "published": "2026-01-23",
      "updated": "2026-01-23",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.16981v1",
        "pdf": "https://arxiv.org/pdf/2601.16981v1"
      },
      "arxiv_id": "2601.16981v1",
      "comment": "Project page: http://sync-light.github.io",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.1000000000000005,
      "score_breakdown": {
        "total_score": 4.1,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling...",
        "key_contributions": [
          "We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene.",
          "While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production.",
          "SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2601.16954v1",
      "title": "Domain-invariant Mixed-domain Semi-supervised Medical Image Segmentation with Clustered Maximum Mean Discrepancy Alignment",
      "authors": [
        "Ba-Thinh Lam",
        "Thanh-Huy Nguyen",
        "Hoang-Thien Nguyen",
        "Quang-Khai Bui-Tran",
        "Nguyen Lan Vi Vu",
        "Phat K. Huynh",
        "Ulas Bagci",
        "Min Xu"
      ],
      "abstract": "Deep learning has shown remarkable progress in medical image semantic segmentation, yet its success heavily depends on large-scale expert annotations and consistent data distributions. In practice, annotations are scarce, and images are collected from multiple scanners or centers, leading to mixed-domain settings with unknown domain labels and severe domain gaps. Existing semi-supervised or domain adaptation approaches typically assume either a single domain shift or access to explicit domain indices, which rarely hold in real-world deployment. In this paper, we propose a domain-invariant mixed-domain semi-supervised segmentation framework that jointly enhances data diversity and mitigates domain bias. A Copy-Paste Mechanism (CPM) augments the training set by transferring informative regions across domains, while a Cluster Maximum Mean Discrepancy (CMMD) block clusters unlabeled features and aligns them with labeled anchors via an MMD objective, encouraging domain-invariant representations. Integrated within a teacher-student framework, our method achieves robust and precise segmentation even with very few labeled examples and multiple unknown domain discrepancies. Experiments on Fundus and M&Ms benchmarks demonstrate that our approach consistently surpasses semi-supervised and domain adaptation methods, establishing a potential solution for mixed-domain semi-supervised medical image segmentation.",
      "published": "2026-01-23",
      "updated": "2026-01-23",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.16954v1",
        "pdf": "https://arxiv.org/pdf/2601.16954v1"
      },
      "arxiv_id": "2601.16954v1",
      "comment": "accepted in ICASSP 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.877966101694915,
      "score_breakdown": {
        "total_score": 3.88,
        "field_match": {
          "score": 1.69,
          "matches": [
            "medical image",
            "segmentation",
            "deep learning"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Deep learning has shown remarkable progress in medical image semantic segmentation, yet its success heavily depends on large-scale expert annotations and consistent data distributions. In practice, annotations are scarce, and images are collected from multiple scanners or centers, leading to mixed-domain settings with unknown domain labels and severe domain gaps. Existing semi-supervised or domain...",
        "key_contributions": [
          "Deep learning has shown remarkable progress in medical image semantic segmentation, yet its success heavily depends on large-scale expert annotations and consistent data distributions.",
          "In practice, annotations are scarce, and images are collected from multiple scanners or centers, leading to mixed-domain settings with unknown domain labels and severe domain gaps.",
          "Existing semi-supervised or domain adaptation approaches typically assume either a single domain shift or access to explicit domain indices, which rarely hold in real-world deployment."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2601.16631v1",
      "title": "PanopMamba: Vision State Space Modeling for Nuclei Panoptic Segmentation",
      "authors": [
        "Ming Kang",
        "Fung Fung Ting",
        "Raphaël C. -W. Phan",
        "Zongyuan Ge",
        "Chee-Ming Ting"
      ],
      "abstract": "Nuclei panoptic segmentation supports cancer diagnostics by integrating both semantic and instance segmentation of different cell types to analyze overall tissue structure and individual nuclei in histopathology images. Major challenges include detecting small objects, handling ambiguous boundaries, and addressing class imbalance. To address these issues, we propose PanopMamba, a novel hybrid encoder-decoder architecture that integrates Mamba and Transformer with additional feature-enhanced fusion via state space modeling. We design a multiscale Mamba backbone and a State Space Model (SSM)-based fusion network to enable efficient long-range perception in pyramid features, thereby extending the pure encoder-decoder framework while facilitating information sharing across multiscale features of nuclei. The proposed SSM-based feature-enhanced fusion integrates pyramid feature networks and dynamic feature enhancement across different spatial scales, enhancing the feature representation of densely overlapping nuclei in both semantic and spatial dimensions. To the best of our knowledge, this is the first Mamba-based approach for panoptic segmentation. Additionally, we introduce alternative evaluation metrics, including image-level Panoptic Quality ($i$PQ), boundary-weighted PQ ($w$PQ), and frequency-weighted PQ ($fw$PQ), which are specifically designed to address the unique challenges of nuclei segmentation and thereby mitigate the potential bias inherent in vanilla PQ. Experimental evaluations on two multiclass nuclei segmentation benchmark datasets, MoNuSAC2020 and NuInsSeg, demonstrate the superiority of PanopMamba for nuclei panoptic segmentation over state-of-the-art methods. Consequently, the robustness of PanopMamba is validated across various metrics, while the distinctiveness of PQ variants is also demonstrated. Code is available at https://github.com/mkang315/PanopMamba.",
      "published": "2026-01-23",
      "updated": "2026-01-23",
      "categories": [
        "eess.IV",
        "cs.CV",
        "eess.SP",
        "stat.AP"
      ],
      "primary_category": "eess.IV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.16631v1",
        "pdf": "https://arxiv.org/pdf/2601.16631v1"
      },
      "arxiv_id": "2601.16631v1",
      "comment": "10 pages, 3 figures",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.853389830508475,
      "score_breakdown": {
        "total_score": 3.85,
        "field_match": {
          "score": 0.51,
          "matches": [
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Nuclei panoptic segmentation supports cancer diagnostics by integrating both semantic and instance segmentation of different cell types to analyze overall tissue structure and individual nuclei in histopathology images. Major challenges include detecting small objects, handling ambiguous boundaries, and addressing class imbalance. To address these issues, we propose PanopMamba, a novel hybrid enco...",
        "key_contributions": [
          "Nuclei panoptic segmentation supports cancer diagnostics by integrating both semantic and instance segmentation of different cell types to analyze overall tissue structure and individual nuclei in histopathology images.",
          "Major challenges include detecting small objects, handling ambiguous boundaries, and addressing class imbalance.",
          "To address these issues, we propose PanopMamba, a novel hybrid encoder-decoder architecture that integrates Mamba and Transformer with additional feature-enhanced fusion via state space modeling."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2601.16973v1",
      "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents",
      "authors": [
        "Zirui Wang",
        "Junyi Zhang",
        "Jiaxin Ge",
        "Long Lian",
        "Letian Fu",
        "Lisa Dunlap",
        "Ken Goldberg",
        "XuDong Wang",
        "Ion Stoica",
        "David M. Chan",
        "Sewon Min",
        "Joseph E. Gonzalez"
      ],
      "abstract": "Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.",
      "published": "2026-01-23",
      "updated": "2026-01-23",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.16973v1",
        "pdf": "https://arxiv.org/pdf/2601.16973v1"
      },
      "arxiv_id": "2601.16973v1",
      "comment": "Project page: https://visgym.github.io/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8,
      "score_breakdown": {
        "total_score": 3.8,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over di...",
        "key_contributions": [
          "Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons.",
          "We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs.",
          "The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2601.16736v1",
      "title": "A Step to Decouple Optimization in 3DGS",
      "authors": [
        "Renjie Ding",
        "Yaonan Wang",
        "Min Liu",
        "Jialin Zhu",
        "Jiazheng Wang",
        "Jiahao Zhao",
        "Wenting Shen",
        "Feixiang He",
        "Xiang Che"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time novel view synthesis. As an explicit representation optimized through gradient propagation among primitives, optimization widely accepted in deep neural networks (DNNs) is actually adopted in 3DGS, such as synchronous weight updating and Adam with the adaptive gradient. However, considering the physical significance and specific design in 3DGS, there are two overlooked details in the optimization of 3DGS: (i) update step coupling, which induces optimizer state rescaling and costly attribute updates outside the viewpoints, and (ii) gradient coupling in the moment, which may lead to under- or over-effective regularization. Nevertheless, such a complex coupling is under-explored. After revisiting the optimization of 3DGS, we take a step to decouple it and recompose the process into: Sparse Adam, Re-State Regularization and Decoupled Attribute Regularization. Taking a large number of experiments under the 3DGS and 3DGS-MCMC frameworks, our work provides a deeper understanding of these components. Finally, based on the empirical analysis, we re-design the optimization and propose AdamW-GS by re-coupling the beneficial components, under which better optimization efficiency and representation effectiveness are achieved simultaneously.",
      "published": "2026-01-23",
      "updated": "2026-01-23",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.16736v1",
        "pdf": "https://arxiv.org/pdf/2601.16736v1"
      },
      "arxiv_id": "2601.16736v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.777966101694915,
      "score_breakdown": {
        "total_score": 3.78,
        "field_match": {
          "score": 1.69,
          "matches": [
            "3d gaussian",
            "gaussian splatting"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time novel view synthesis. As an explicit representation optimized through gradient propagation among primitives, optimization widely accepted in deep neural networks (DNNs) is actually adopted in 3DGS, such as synchronous weight updating and Adam with the adaptive gradient. However, considering the physical significance and...",
        "key_contributions": [
          "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time novel view synthesis.",
          "As an explicit representation optimized through gradient propagation among primitives, optimization widely accepted in deep neural networks (DNNs) is actually adopted in 3DGS, such as synchronous weight updating and Adam with the adaptive gradient.",
          "However, considering the physical significance and specific design in 3DGS, there are two overlooked details in the optimization of 3DGS: (i) update step coupling, which induces optimizer state rescaling and costly attribute updates outside the viewpoints, and (ii) gradient coupling in the moment, which may lead to under- or over-effective regularization."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2601.16652v1",
      "title": "Reliable Brain Tumor Segmentation Based on Spiking Neural Networks with Efficient Training",
      "authors": [
        "Aurora Pia Ghiardelli",
        "Guangzhi Tang",
        "Tao Sun"
      ],
      "abstract": "We propose a reliable and energy-efficient framework for 3D brain tumor segmentation using spiking neural networks (SNNs). A multi-view ensemble of sagittal, coronal, and axial SNN models provides voxel-wise uncertainty estimation and enhances segmentation robustness. To address the high computational cost in training SNN models for semantic image segmentation, we employ Forward Propagation Through Time (FPTT), which maintains temporal learning efficiency with significantly reduced computational cost. Experiments on the Multimodal Brain Tumor Segmentation Challenges (BraTS 2017 and BraTS 2023) demonstrate competitive accuracy, well-calibrated uncertainty, and an 87% reduction in FLOPs, underscoring the potential of SNNs for reliable, low-power medical IoT and Point-of-Care systems.",
      "published": "2026-01-23",
      "updated": "2026-01-23",
      "categories": [
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2601.16652v1",
        "pdf": "https://arxiv.org/pdf/2601.16652v1"
      },
      "arxiv_id": "2601.16652v1",
      "comment": "Accepted at ISBI 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.7533898305084747,
      "score_breakdown": {
        "total_score": 3.75,
        "field_match": {
          "score": 0.51,
          "matches": [
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 8,
          "venue": "ISBI",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 5.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "We propose a reliable and energy-efficient framework for 3D brain tumor segmentation using spiking neural networks (SNNs). A multi-view ensemble of sagittal, coronal, and axial SNN models provides voxel-wise uncertainty estimation and enhances segmentation robustness. To address the high computational cost in training SNN models for semantic image segmentation, we employ Forward Propagation Throug...",
        "key_contributions": [
          "We propose a reliable and energy-efficient framework for 3D brain tumor segmentation using spiking neural networks (SNNs).",
          "A multi-view ensemble of sagittal, coronal, and axial SNN models provides voxel-wise uncertainty estimation and enhances segmentation robustness.",
          "To address the high computational cost in training SNN models for semantic image segmentation, we employ Forward Propagation Through Time (FPTT), which maintains temporal learning efficiency with significantly reduced computational cost."
        ],
        "provider": "gemini"
      }
    }
  ]
}