{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2511.17354v1",
      "title": "DSeq-JEPA: Discriminative Sequential Joint-Embedding Predictive Architecture",
      "authors": [
        "Xiangteng He",
        "Shunsuke Sakai",
        "Kun Yuan",
        "Nicolas Padoy",
        "Tatsuhito Hasegawa",
        "Leonid Sigal"
      ],
      "abstract": "Image-based Joint-Embedding Predictive Architecture (I-JEPA) learns visual representations by predicting latent embeddings of masked regions from visible context. However, it treats all regions uniformly and independently, lacking an explicit notion of where or in what order predictions should be made. Inspired by human visual perception, which deploys attention selectively and sequentially from the most informative to secondary regions, we propose DSeq-JEPA, a Discriminative Sequential Joint-Embedding Predictive Architecture that bridges predictive and autoregressive self-supervised learning, integrating JEPA-style latent prediction with GPT-style sequential reasoning. Specifically, DSeq-JEPA (i) first identifies primary discriminative regions based on a transformer-derived saliency map, emphasizing the distribution of visual importance, and then (ii) predicts subsequent regions in this discriminative order, progressively forming a curriculum-like semantic progression from primary to secondary cues -- a form of GPT-style pre-training. Extensive experiments across diverse tasks, including image classification (ImageNet), fine-grained visual categorization (iNaturalist21, CUB-200-2011, Stanford-Cars), detection and segmentation (MS-COCO, ADE20K), and low-level reasoning tasks (Clevr/Count, Clevr/Dist), demonstrate that DSeq-JEPA consistently focuses on more discriminative and generalizable representations than I-JEPA variants. Project page: https://github.com/SkyShunsuke/DSeq-JEPA.",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.17354v1",
        "pdf": "https://arxiv.org/pdf/2511.17354v1"
      },
      "arxiv_id": "2511.17354v1",
      "comment": "Project page: https://github.com/SkyShunsuke/DSeq-JEPA",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.274576271186441,
      "score_breakdown": {
        "total_score": 4.27,
        "field_match": {
          "score": 1.19,
          "matches": [
            "self-supervised",
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Image-based Joint-Embedding Predictive Architecture (I-JEPA) learns visual representations by predicting latent embeddings of masked regions from visible context. However, it treats all regions uniformly and independently, lacking an explicit notion of where or in what order predictions should be made. Inspired by human visual perception, which deploys attention selectively and sequentially from t...",
        "key_contributions": [
          "Image-based Joint-Embedding Predictive Architecture (I-JEPA) learns visual representations by predicting latent embeddings of masked regions from visible context.",
          "However, it treats all regions uniformly and independently, lacking an explicit notion of where or in what order predictions should be made.",
          "Inspired by human visual perception, which deploys attention selectively and sequentially from the most informative to secondary regions, we propose DSeq-JEPA, a Discriminative Sequential Joint-Embedding Predictive Architecture that bridges predictive and autoregressive self-supervised learning, integrating JEPA-style latent prediction with GPT-style sequential reasoning."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.17442v1",
      "title": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing",
      "authors": [
        "Binger Chen",
        "Tacettin Emre Bök",
        "Behnood Rasti",
        "Volker Markl",
        "Begüm Demir"
      ],
      "abstract": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.17442v1",
        "pdf": "https://arxiv.org/pdf/2511.17442v1"
      },
      "arxiv_id": "2511.17442v1",
      "comment": "Code and data available at https://github.com/be-chen/REMSA",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.203389830508474,
      "score_breakdown": {
        "total_score": 4.2,
        "field_match": {
          "score": 0.51,
          "matches": [
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic seg...",
        "key_contributions": [
          "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping.",
          "These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data.",
          "They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.17421v1",
      "title": "Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers",
      "authors": [
        "Christopher Boland",
        "Sotirios Tsaftaris",
        "Sonia Dahdouh"
      ],
      "abstract": "Deep learning models are prone to learning shortcut solutions to problems using spuriously correlated yet irrelevant features of their training data. In high-risk applications such as medical image analysis, this phenomenon may prevent models from using clinically meaningful features when making predictions, potentially leading to poor robustness and harm to patients. We demonstrate that different types of shortcuts (those that are diffuse and spread throughout the image, as well as those that are localized to specific areas) manifest distinctly across network layers and can, therefore, be more effectively targeted through mitigation strategies that target the intermediate layers. We propose a novel knowledge distillation framework that leverages a teacher network fine-tuned on a small subset of task-relevant data to mitigate shortcut learning in a student network trained on a large dataset corrupted with a bias feature. Through extensive experiments on CheXpert, ISIC 2017, and SimBA datasets using various architectures (ResNet-18, AlexNet, DenseNet-121, and 3D CNNs), we demonstrate consistent improvements over traditional Empirical Risk Minimization, augmentation-based bias-mitigation, and group-based bias-mitigation approaches. In many cases, we achieve comparable performance with a baseline model trained on bias-free data, even on out-of-distribution test data. Our results demonstrate the practical applicability of our approach to real-world medical imaging scenarios where bias annotations are limited and shortcut features are difficult to identify a priori.",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.17421v1",
        "pdf": "https://arxiv.org/pdf/2511.17421v1"
      },
      "arxiv_id": "2511.17421v1",
      "comment": "Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA) https://melba-journal.org/2025:020",
      "journal_ref": "Machine.Learning.for.Biomedical.Imaging. 3 (2025)",
      "has_code": false,
      "relevance_score": 4.149152542372882,
      "score_breakdown": {
        "total_score": 4.15,
        "field_match": {
          "score": 2.37,
          "matches": [
            "medical image",
            "medical imaging",
            "deep learning",
            "image analysis"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 7.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Deep learning models are prone to learning shortcut solutions to problems using spuriously correlated yet irrelevant features of their training data. In high-risk applications such as medical image analysis, this phenomenon may prevent models from using clinically meaningful features when making predictions, potentially leading to poor robustness and harm to patients. We demonstrate that different...",
        "key_contributions": [
          "Deep learning models are prone to learning shortcut solutions to problems using spuriously correlated yet irrelevant features of their training data.",
          "In high-risk applications such as medical image analysis, this phenomenon may prevent models from using clinically meaningful features when making predictions, potentially leading to poor robustness and harm to patients.",
          "We demonstrate that different types of shortcuts (those that are diffuse and spread throughout the image, as well as those that are localized to specific areas) manifest distinctly across network layers and can, therefore, be more effectively targeted through mitigation strategies that target the intermediate layers."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.17322v1",
      "title": "NoPe-NeRF++: Local-to-Global Optimization of NeRF with No Pose Prior",
      "authors": [
        "Dongbo Shi",
        "Shen Cao",
        "Bojian Wu",
        "Jinhui Guo",
        "Lubin Fan",
        "Renjie Chen",
        "Ligang Liu",
        "Jieping Ye"
      ],
      "abstract": "In this paper, we introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors. Existing methods, particularly NoPe-NeRF, which focus solely on the local relationships within images, often struggle to recover accurate camera poses in complex scenarios. To overcome the challenges, our approach begins with a relative pose initialization with explicit feature matching, followed by a local joint optimization to enhance the pose estimation for training a more robust NeRF representation. This method significantly improves the quality of initial poses. Additionally, we introduce global optimization phase that incorporates geometric consistency constraints through bundle adjustment, which integrates feature trajectories to further refine poses and collectively boost the quality of NeRF. Notably, our method is the first work that seamlessly combines the local and global cues with NeRF, and outperforms state-of-the-art methods in both pose estimation accuracy and novel view synthesis. Extensive evaluations on benchmark datasets demonstrate our superior performance and robustness, even in challenging scenes, thus validating our design choices.",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.17322v1",
        "pdf": "https://arxiv.org/pdf/2511.17322v1"
      },
      "arxiv_id": "2511.17322v1",
      "comment": "",
      "journal_ref": "Eurographics 2025",
      "has_code": false,
      "relevance_score": 4.1245762711864415,
      "score_breakdown": {
        "total_score": 4.12,
        "field_match": {
          "score": 1.19,
          "matches": [
            "neural radiance",
            "nerf"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "In this paper, we introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors. Existing methods, particularly NoPe-NeRF, which focus solely on the local relationships within images, often struggle to recover accurate camera poses in complex scenarios. To overcome the challenges, our approach begins with a relative p...",
        "key_contributions": [
          "In this paper, we introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors.",
          "Existing methods, particularly NoPe-NeRF, which focus solely on the local relationships within images, often struggle to recover accurate camera poses in complex scenarios.",
          "To overcome the challenges, our approach begins with a relative pose initialization with explicit feature matching, followed by a local joint optimization to enhance the pose estimation for training a more robust NeRF representation."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.17209v1",
      "title": "Scaling Self-Supervised and Cross-Modal Pretraining for Volumetric CT Transformers",
      "authors": [
        "Cris Claessens",
        "Christiaan Viviers",
        "Giacomo D'Amicantonio",
        "Egor Bondarev",
        "Fons van der Sommen"
      ],
      "abstract": "We introduce SPECTRE, a fully transformer-based foundation model for volumetric computed tomography (CT). Our Self-Supervised & Cross-Modal Pretraining for CT Representation Extraction (SPECTRE) approach utilizes scalable 3D Vision Transformer architectures and modern self-supervised and vision-language pretraining strategies to learn general-purpose CT representations. Volumetric CT poses unique challenges, such as extreme token scaling, geometric anisotropy, and weak or noisy clinical supervision, that make standard transformer and contrastive learning recipes ineffective out of the box. The framework jointly optimizes a local transformer for high-resolution volumetric feature extraction and a global transformer for whole-scan context modeling, making large-scale 3D attention computationally tractable. Notably, SPECTRE is trained exclusively on openly available CT datasets, demonstrating that high-performing, generalizable representations can be achieved without relying on private data. Pretraining combines DINO-style self-distillation with SigLIP-based vision-language alignment using paired radiology reports, yielding features that are both geometrically consistent and clinically meaningful. Across multiple CT benchmarks, SPECTRE consistently outperforms prior CT foundation models in both zero-shot and fine-tuned settings, establishing SPECTRE as a scalable, open, and fully transformer-based foundation model for 3D medical imaging.",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.17209v1",
        "pdf": "https://arxiv.org/pdf/2511.17209v1"
      },
      "arxiv_id": "2511.17209v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.063559322033899,
      "score_breakdown": {
        "total_score": 4.06,
        "field_match": {
          "score": 2.03,
          "matches": [
            "medical imaging",
            "self-supervised",
            "volumetric"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "We introduce SPECTRE, a fully transformer-based foundation model for volumetric computed tomography (CT). Our Self-Supervised & Cross-Modal Pretraining for CT Representation Extraction (SPECTRE) approach utilizes scalable 3D Vision Transformer architectures and modern self-supervised and vision-language pretraining strategies to learn general-purpose CT representations. Volumetric CT poses unique ...",
        "key_contributions": [
          "We introduce SPECTRE, a fully transformer-based foundation model for volumetric computed tomography (CT).",
          "Our Self-Supervised & Cross-Modal Pretraining for CT Representation Extraction (SPECTRE) approach utilizes scalable 3D Vision Transformer architectures and modern self-supervised and vision-language pretraining strategies to learn general-purpose CT representations.",
          "Volumetric CT poses unique challenges, such as extreme token scaling, geometric anisotropy, and weak or noisy clinical supervision, that make standard transformer and contrastive learning recipes ineffective out of the box."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.17217v1",
      "title": "Dual-domain Adaptation Networks for Realistic Image Super-resolution",
      "authors": [
        "Chaowei Fang",
        "Bolin Fu",
        "De Cheng",
        "Lechao Cheng",
        "Guanbin Li"
      ],
      "abstract": "Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks. This is critical for applications like surveillance, medical imaging, and consumer electronics. However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic image features. Pre-trained SR models from large-scale synthetic datasets offer valuable prior knowledge, which can improve generalization, speed up training, and reduce the need for extensive real-world data in realistic SR tasks. In this paper, we introduce a novel approach, Dual-domain Adaptation Networks, which is able to efficiently adapt pre-trained image SR models from simulated to real-world datasets. To achieve this target, we first set up a spatial-domain adaptation strategy through selectively updating parameters of pre-trained models and employing the low-rank adaptation technique to adjust frozen parameters. Recognizing that image super-resolution involves recovering high-frequency components, we further integrate a frequency domain adaptation branch into the adapted model, which combines the spectral data of the input and the spatial-domain backbone's intermediate features to infer HR frequency maps, enhancing the SR result. Experimental evaluations on public realistic image SR benchmarks, including RealSR, D2CRealSR, and DRealSR, demonstrate the superiority of our proposed method over existing state-of-the-art models. Codes are available at: https://github.com/dummerchen/DAN.",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.17217v1",
        "pdf": "https://arxiv.org/pdf/2511.17217v1"
      },
      "arxiv_id": "2511.17217v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.055084745762712,
      "score_breakdown": {
        "total_score": 4.06,
        "field_match": {
          "score": 0.76,
          "matches": [
            "medical imaging"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 7.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks. This is critical for applications like surveillance, medical imaging, and consumer electronics. However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic im...",
        "key_contributions": [
          "Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks.",
          "This is critical for applications like surveillance, medical imaging, and consumer electronics.",
          "However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic image features."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.17249v1",
      "title": "FlexiFlow: decomposable flow matching for generation of flexible molecular ensemble",
      "authors": [
        "Riccardo Tedoldi",
        "Ola Engkvist",
        "Patrick Bryant",
        "Hossein Azizpour",
        "Jon Paul Janet",
        "Alessandro Tibo"
      ],
      "abstract": "Sampling useful three-dimensional molecular structures along with their most favorable conformations is a key challenge in drug discovery. Current state-of-the-art 3D de-novo design flow matching or diffusion-based models are limited to generating a single conformation. However, the conformational landscape of a molecule determines its observable properties and how tightly it is able to bind to a given protein target. By generating a representative set of low-energy conformers, we can more directly assess these properties and potentially improve the ability to generate molecules with desired thermodynamic observables. Towards this aim, we propose FlexiFlow, a novel architecture that extends flow-matching models, allowing for the joint sampling of molecules along with multiple conformations while preserving both equivariance and permutation invariance. We demonstrate the effectiveness of our approach on the QM9 and GEOM Drugs datasets, achieving state-of-the-art results in molecular generation tasks. Our results show that FlexiFlow can generate valid, unstrained, unique, and novel molecules with high fidelity to the training data distribution, while also capturing the conformational diversity of molecules. Moreover, we show that our model can generate conformational ensembles that provide similar coverage to state-of-the-art physics-based methods at a fraction of the inference time. Finally, FlexiFlow can be successfully transferred to the protein-conditioned ligand generation task, even when the dataset contains only static pockets without accompanying conformations.",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2511.17249v1",
        "pdf": "https://arxiv.org/pdf/2511.17249v1"
      },
      "arxiv_id": "2511.17249v1",
      "comment": "Preprint. Code to be released upon full publication",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.0,
      "score_breakdown": {
        "total_score": 4.0,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Sampling useful three-dimensional molecular structures along with their most favorable conformations is a key challenge in drug discovery. Current state-of-the-art 3D de-novo design flow matching or diffusion-based models are limited to generating a single conformation. However, the conformational landscape of a molecule determines its observable properties and how tightly it is able to bind to a ...",
        "key_contributions": [
          "Sampling useful three-dimensional molecular structures along with their most favorable conformations is a key challenge in drug discovery.",
          "Current state-of-the-art 3D de-novo design flow matching or diffusion-based models are limited to generating a single conformation.",
          "However, the conformational landscape of a molecule determines its observable properties and how tightly it is able to bind to a given protein target."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.17198v1",
      "title": "Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism",
      "authors": [
        "Kaiyu Li",
        "Jiayu Wang",
        "Zhi Wang",
        "Hui Qiao",
        "Weizhan Zhang",
        "Deyu Meng",
        "Xiangyong Cao"
      ],
      "abstract": "LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems.",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2511.17198v1",
        "pdf": "https://arxiv.org/pdf/2511.17198v1"
      },
      "arxiv_id": "2511.17198v1",
      "comment": "Page: https://earth-insights.github.io/EarthAgent",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.0,
      "score_breakdown": {
        "total_score": 4.0,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantl...",
        "key_contributions": [
          "LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows.",
          "Fields such as remote sensing, requiring specialized tools (e.",
          "g."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.17484v1",
      "title": "Radar2Shape: 3D Shape Reconstruction from High-Frequency Radar using Multiresolution Signed Distance Functions",
      "authors": [
        "Neel Sortur",
        "Justin Goodwin",
        "Purvik Patel",
        "Luis Enrique Martinez",
        "Tzofi Klinghoffer",
        "Rajmonda S. Caceres",
        "Robin Walters"
      ],
      "abstract": "Determining the shape of 3D objects from high-frequency radar signals is analytically complex but critical for commercial and aerospace applications. Previous deep learning methods have been applied to radar modeling; however, they often fail to represent arbitrary shapes or have difficulty with real-world radar signals which are collected over limited viewing angles. Existing methods in optical 3D reconstruction can generate arbitrary shapes from limited camera views, but struggle when they naively treat the radar signal as a camera view. In this work, we present Radar2Shape, a denoising diffusion model that handles a partially observable radar signal for 3D reconstruction by correlating its frequencies with multiresolution shape features. Our method consists of a two-stage approach: first, Radar2Shape learns a regularized latent space with hierarchical resolutions of shape features, and second, it diffuses into this latent space by conditioning on the frequencies of the radar signal in an analogous coarse-to-fine manner. We demonstrate that Radar2Shape can successfully reconstruct arbitrary 3D shapes even from partially-observed radar signals, and we show robust generalization to two different simulation methods and real-world data. Additionally, we release two synthetic benchmark datasets to encourage future research in the high-frequency radar domain so that models like Radar2Shape can safely be adapted into real-world radar systems.",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.17484v1",
        "pdf": "https://arxiv.org/pdf/2511.17484v1"
      },
      "arxiv_id": "2511.17484v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.856779661016949,
      "score_breakdown": {
        "total_score": 3.86,
        "field_match": {
          "score": 1.02,
          "matches": [
            "3d reconstruction",
            "deep learning"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 7.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Determining the shape of 3D objects from high-frequency radar signals is analytically complex but critical for commercial and aerospace applications. Previous deep learning methods have been applied to radar modeling; however, they often fail to represent arbitrary shapes or have difficulty with real-world radar signals which are collected over limited viewing angles. Existing methods in optical 3...",
        "key_contributions": [
          "Determining the shape of 3D objects from high-frequency radar signals is analytically complex but critical for commercial and aerospace applications.",
          "Previous deep learning methods have been applied to radar modeling; however, they often fail to represent arbitrary shapes or have difficulty with real-world radar signals which are collected over limited viewing angles.",
          "Existing methods in optical 3D reconstruction can generate arbitrary shapes from limited camera views, but struggle when they naively treat the radar signal as a camera view."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.17405v1",
      "title": "Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training",
      "authors": [
        "Yesheng Liu",
        "Hao Li",
        "Haiyu Xu",
        "Baoqi Pei",
        "Jiahao Wang",
        "Mingxuan Zhao",
        "Jingshu Zheng",
        "Zheqi He",
        "JG Yao",
        "Bowen Qin",
        "Xi Yang",
        "Jiajun Zhang"
      ],
      "abstract": "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.",
      "published": "2025-11-21",
      "updated": "2025-11-21",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2511.17405v1",
        "pdf": "https://arxiv.org/pdf/2511.17405v1"
      },
      "arxiv_id": "2511.17405v1",
      "comment": "Project url: https://flageval-baai.github.io/ReVeL/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8499999999999996,
      "score_breakdown": {
        "total_score": 3.85,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encoura...",
        "key_contributions": [
          "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models.",
          "Its constrained output format allows for simplified, deterministic automatic verification.",
          "However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT."
        ],
        "provider": "gemini"
      }
    }
  ]
}