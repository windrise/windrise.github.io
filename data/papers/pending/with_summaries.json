{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2512.15508v1",
      "title": "Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting",
      "authors": [
        "Arthur Moreau",
        "Richard Shaw",
        "Michal Nazarczuk",
        "Jisu Shin",
        "Thomas Tanay",
        "Zhensong Zhang",
        "Songcen Xu",
        "Eduardo Pérez-Pellitero"
      ],
      "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, \"Off The Grid\" distribution. Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches. This module is trained end-to-end with a 3D reconstruction backbone using self-supervised learning. Our resulting pose-free model generates photorealistic scenes in seconds, achieving state-of-the-art novel view synthesis for feed-forward models. It outperforms competitors while using far fewer primitives, demonstrating a more accurate and efficient allocation that captures fine details and reduces artifacts. Moreover, we observe that by learning to render 3D Gaussians, our 3D reconstruction backbone improves camera pose estimation, suggesting opportunities to train these foundational models without labels.",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.15508v1",
        "pdf": "https://arxiv.org/pdf/2512.15508v1"
      },
      "arxiv_id": "2512.15508v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.5864406779661016,
      "score_breakdown": {
        "total_score": 4.59,
        "field_match": {
          "score": 2.97,
          "matches": [
            "3d gaussian",
            "gaussian splatting",
            "self-supervised",
            "3d reconstruction"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, \"Off The Grid\" distribution. ...",
        "key_contributions": [
          "Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency.",
          "We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, \"Off The Grid\" distribution.",
          "Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.15715v1",
      "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
      "authors": [
        "Lihe Yang",
        "Shang-Wen Li",
        "Yang Li",
        "Xinjie Lei",
        "Dong Wang",
        "Abdelrahman Mohamed",
        "Hengshuang Zhao",
        "Hu Xu"
      ],
      "abstract": "At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed \"Pixio\", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.15715v1",
        "pdf": "https://arxiv.org/pdf/2512.15715v1"
      },
      "arxiv_id": "2512.15715v1",
      "comment": "Project page: https://github.com/facebookresearch/pixio",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.361864406779661,
      "score_breakdown": {
        "total_score": 4.36,
        "field_match": {
          "score": 1.78,
          "matches": [
            "self-supervised",
            "3d reconstruction",
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervise...",
        "key_contributions": [
          "At the most basic level, pixels are the source of the visual information through which we perceive the world.",
          "Pixels contain information at all levels, ranging from low-level attributes to high-level concepts.",
          "Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.15693v1",
      "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
      "authors": [
        "Yifei Li",
        "Wenzhao Zheng",
        "Yanran Zhang",
        "Runze Sun",
        "Yu Zheng",
        "Lei Chen",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.15693v1",
        "pdf": "https://arxiv.org/pdf/2512.15693v1"
      },
      "arxiv_id": "2512.15693v1",
      "comment": "Project Page: https://github.com/JoeLeelyf/Skyra",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.199999999999999,
      "score_breakdown": {
        "total_score": 4.2,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-...",
        "key_contributions": [
          "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors.",
          "However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation.",
          "In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.15647v1",
      "title": "Hard Labels In! Rethinking the Role of Hard Labels in Mitigating Local Semantic Drift",
      "authors": [
        "Jiacheng Cui",
        "Bingkui Tong",
        "Xinyue Bi",
        "Xiaohan Zhao",
        "Jiacheng Liu",
        "Zhiqiang shen"
      ],
      "abstract": "Soft labels generated by teacher models have become a dominant paradigm for knowledge transfer and recent large-scale dataset distillation such as SRe2L, RDED, LPLD, offering richer supervision than conventional hard labels. However, we observe that when only a limited number of crops per image are used, soft labels are prone to local semantic drift: a crop may visually resemble another class, causing its soft embedding to deviate from the ground-truth semantics of the original image. This mismatch between local visual content and global semantic meaning introduces systematic errors and distribution misalignment between training and testing. In this work, we revisit the overlooked role of hard labels and show that, when appropriately integrated, they provide a powerful content-agnostic anchor to calibrate semantic drift. We theoretically characterize the emergence of drift under few soft-label supervision and demonstrate that hybridizing soft and hard labels restores alignment between visual content and semantic supervision. Building on this insight, we propose a new training paradigm, Hard Label for Alleviating Local Semantic Drift (HALD), which leverages hard labels as intermediate corrective signals while retaining the fine-grained advantages of soft labels. Extensive experiments on dataset distillation and large-scale conventional classification benchmarks validate our approach, showing consistent improvements in generalization. On ImageNet-1K, we achieve 42.7% with only 285M storage for soft labels, outperforming prior state-of-the-art LPLD by 9.0%. Our findings re-establish the importance of hard labels as a complementary tool, and call for a rethinking of their role in soft-label-dominated training.",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.15647v1",
        "pdf": "https://arxiv.org/pdf/2512.15647v1"
      },
      "arxiv_id": "2512.15647v1",
      "comment": "Code at: https://github.com/Jiacheng8/HALD",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.199999999999999,
      "score_breakdown": {
        "total_score": 4.2,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Soft labels generated by teacher models have become a dominant paradigm for knowledge transfer and recent large-scale dataset distillation such as SRe2L, RDED, LPLD, offering richer supervision than conventional hard labels. However, we observe that when only a limited number of crops per image are used, soft labels are prone to local semantic drift: a crop may visually resemble another class, cau...",
        "key_contributions": [
          "Soft labels generated by teacher models have become a dominant paradigm for knowledge transfer and recent large-scale dataset distillation such as SRe2L, RDED, LPLD, offering richer supervision than conventional hard labels.",
          "However, we observe that when only a limited number of crops per image are used, soft labels are prone to local semantic drift: a crop may visually resemble another class, causing its soft embedding to deviate from the ground-truth semantics of the original image.",
          "This mismatch between local visual content and global semantic meaning introduces systematic errors and distribution misalignment between training and testing."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.15524v1",
      "title": "DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations",
      "authors": [
        "Yuxiang Shi",
        "Zhe Li",
        "Yanwen Wang",
        "Hao Zhu",
        "Xun Cao",
        "Ligang Liu"
      ],
      "abstract": "Portrait animation from a single source image and a driving video is a long-standing problem. Recent approaches tend to adopt diffusion-based image/video generation models for realistic and expressive animation. However, none of these diffusion models realizes high-fidelity disentangled control between the head pose and facial expression, hindering applications like expression-only or pose-only editing and animation. To address this, we propose DeX-Portrait, a novel approach capable of generating expressive portrait animation driven by disentangled pose and expression signals. Specifically, we represent the pose as an explicit global transformation and the expression as an implicit latent code. First, we design a powerful motion trainer to learn both pose and expression encoders for extracting precise and decomposed driving signals. Then we propose to inject the pose transformation into the diffusion model through a dual-branch conditioning mechanism, and the expression latent through cross attention. Finally, we design a progressive hybrid classifier-free guidance for more faithful identity consistency. Experiments show that our method outperforms state-of-the-art baselines on both animation quality and disentangled controllability.",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.15524v1",
        "pdf": "https://arxiv.org/pdf/2512.15524v1"
      },
      "arxiv_id": "2512.15524v1",
      "comment": "Projectpage: https://syx132.github.io/DeX-Portrait/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.1499999999999995,
      "score_breakdown": {
        "total_score": 4.15,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Portrait animation from a single source image and a driving video is a long-standing problem. Recent approaches tend to adopt diffusion-based image/video generation models for realistic and expressive animation. However, none of these diffusion models realizes high-fidelity disentangled control between the head pose and facial expression, hindering applications like expression-only or pose-only ed...",
        "key_contributions": [
          "Portrait animation from a single source image and a driving video is a long-standing problem.",
          "Recent approaches tend to adopt diffusion-based image/video generation models for realistic and expressive animation.",
          "However, none of these diffusion models realizes high-fidelity disentangled control between the head pose and facial expression, hindering applications like expression-only or pose-only editing and animation."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.15466v1",
      "title": "On Assessing the Relevance of Code Reviews Authored by Generative Models",
      "authors": [
        "Robert Heumüller",
        "Frank Ortmeier"
      ],
      "abstract": "The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of \"usefulness\", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2512.15466v1",
        "pdf": "https://arxiv.org/pdf/2512.15466v1"
      },
      "arxiv_id": "2512.15466v1",
      "comment": "Replication Package: https://github.com/robert-heumueller-ovgu/repl-generative-review-relevance",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.1499999999999995,
      "score_breakdown": {
        "total_score": 4.15,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of \"usefulness\", a highly ambiguous conce...",
        "key_contributions": [
          "The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety.",
          "Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of \"usefulness\", a highly ambiguous concept.",
          "We propose a novel evaluation approach based on what we call multi-subjective ranking."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.15542v1",
      "title": "BLANKET: Anonymizing Faces in Infant Video Recordings",
      "authors": [
        "Ditmar Hadera",
        "Jan Cech",
        "Miroslav Purkrabek",
        "Matej Hoffmann"
      ],
      "abstract": "Ensuring the ethical use of video data involving human subjects, particularly infants, requires robust anonymization methods. We propose BLANKET (Baby-face Landmark-preserving ANonymization with Keypoint dEtection consisTency), a novel approach designed to anonymize infant faces in video recordings while preserving essential facial attributes. Our method comprises two stages. First, a new random face, compatible with the original identity, is generated via inpainting using a diffusion model. Second, the new identity is seamlessly incorporated into each video frame through temporally consistent face swapping with authentic expression transfer. The method is evaluated on a dataset of short video recordings of babies and is compared to the popular anonymization method, DeepPrivacy2. Key metrics assessed include the level of de-identification, preservation of facial attributes, impact on human pose estimation (as an example of a downstream task), and presence of artifacts. Both methods alter the identity, and our method outperforms DeepPrivacy2 in all other respects. The code is available as an easy-to-use anonymization demo at https://github.com/ctu-vras/blanket-infant-face-anonym.",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.15542v1",
        "pdf": "https://arxiv.org/pdf/2512.15542v1"
      },
      "arxiv_id": "2512.15542v1",
      "comment": "Project website: https://github.com/ctu-vras/blanket-infant-face-anonym",
      "journal_ref": "2025 IEEE International Conference on Development and Learning (ICDL)",
      "has_code": true,
      "relevance_score": 4.0,
      "score_breakdown": {
        "total_score": 4.0,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Ensuring the ethical use of video data involving human subjects, particularly infants, requires robust anonymization methods. We propose BLANKET (Baby-face Landmark-preserving ANonymization with Keypoint dEtection consisTency), a novel approach designed to anonymize infant faces in video recordings while preserving essential facial attributes. Our method comprises two stages. First, a new random f...",
        "key_contributions": [
          "Ensuring the ethical use of video data involving human subjects, particularly infants, requires robust anonymization methods.",
          "We propose BLANKET (Baby-face Landmark-preserving ANonymization with Keypoint dEtection consisTency), a novel approach designed to anonymize infant faces in video recordings while preserving essential facial attributes.",
          "Our method comprises two stages."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.15386v1",
      "title": "See It Before You Grab It: Deep Learning-based Action Anticipation in Basketball",
      "authors": [
        "Arnau Barrera Roy",
        "Albert Clapés Sintes"
      ],
      "abstract": "Computer vision and video understanding have transformed sports analytics by enabling large-scale, automated analysis of game dynamics from broadcast footage. Despite significant advances in player and ball tracking, pose estimation, action localization, and automatic foul recognition, anticipating actions before they occur in sports videos has received comparatively little attention. This work introduces the task of action anticipation in basketball broadcast videos, focusing on predicting which team will gain possession of the ball following a shot attempt. To benchmark this task, a new self-curated dataset comprising 100,000 basketball video clips, over 300 hours of footage, and more than 2,000 manually annotated rebound events is presented. Comprehensive baseline results are reported using state-of-the-art action anticipation methods, representing the first application of deep learning techniques to basketball rebound prediction. Additionally, two complementary tasks, rebound classification and rebound spotting, are explored, demonstrating that this dataset supports a wide range of video understanding applications in basketball, for which no comparable datasets currently exist. Experimental results highlight both the feasibility and inherent challenges of anticipating rebounds, providing valuable insights into predictive modeling for dynamic multi-agent sports scenarios. By forecasting team possession before rebounds occur, this work enables applications in real-time automated broadcasting and post-game analysis tools to support decision-making.",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.15386v1",
        "pdf": "https://arxiv.org/pdf/2512.15386v1"
      },
      "arxiv_id": "2512.15386v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.888983050847457,
      "score_breakdown": {
        "total_score": 3.89,
        "field_match": {
          "score": 0.85,
          "matches": [
            "deep learning",
            "computer vision"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Computer vision and video understanding have transformed sports analytics by enabling large-scale, automated analysis of game dynamics from broadcast footage. Despite significant advances in player and ball tracking, pose estimation, action localization, and automatic foul recognition, anticipating actions before they occur in sports videos has received comparatively little attention. This work in...",
        "key_contributions": [
          "Computer vision and video understanding have transformed sports analytics by enabling large-scale, automated analysis of game dynamics from broadcast footage.",
          "Despite significant advances in player and ball tracking, pose estimation, action localization, and automatic foul recognition, anticipating actions before they occur in sports videos has received comparatively little attention.",
          "This work introduces the task of action anticipation in basketball broadcast videos, focusing on predicting which team will gain possession of the ball following a shot attempt."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.15711v1",
      "title": "Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering",
      "authors": [
        "Divam Gupta",
        "Anuj Pahuja",
        "Nemanja Bartolovic",
        "Tomas Simon",
        "Forrest Iandola",
        "Giljoo Nam"
      ],
      "abstract": "We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is highly efficient in representing surface areas like facial skin, while the 3D Gaussians effectively handle non-surface areas such as hair and beard. To this end, we develop a unified differentiable rendering pipeline that treats the mesh as a semi-transparent layer within the volumetric rendering paradigm of 3D Gaussian Splatting. We train neural networks to decode a facial expression code into three components: a 3D face mesh, an RGBA texture, and a set of 3D Gaussians. These components are rendered simultaneously in a unified rendering engine. The networks are trained using multi-view image supervision. Our results demonstrate that GPiCA achieves the realism of purely Gaussian-based avatars while matching the rendering performance of mesh-based avatars.",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.15711v1",
        "pdf": "https://arxiv.org/pdf/2512.15711v1"
      },
      "arxiv_id": "2512.15711v1",
      "comment": "Tech report",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.865254237288136,
      "score_breakdown": {
        "total_score": 3.87,
        "field_match": {
          "score": 2.29,
          "matches": [
            "3d gaussian",
            "gaussian splatting",
            "volumetric"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is h...",
        "key_contributions": [
          "We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices.",
          "GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians.",
          "This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.15699v1",
      "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
      "authors": [
        "Qiuyang Mang",
        "Wenhao Chai",
        "Zhifei Li",
        "Huanzhi Mao",
        "Shang Zhou",
        "Alexander Du",
        "Hanchen Li",
        "Shu Liu",
        "Edwin Chen",
        "Yichuan Wang",
        "Xieting Chu",
        "Zerui Cheng",
        "Yuan Xu",
        "Tian Xia",
        "Zirui Wang",
        "Tianneng Shi",
        "Jianzhu Yao",
        "Yilong Zhao",
        "Qizheng Zhang",
        "Charlie Ruan",
        "Zeyu Shen",
        "Kaiyuan Liu",
        "Runyuan He",
        "Dong Xing",
        "Zerui Li",
        "Zirong Zeng",
        "Yige Jiang",
        "Lufeng Cheng",
        "Ziyi Zhao",
        "Youran Sun",
        "Wesley Zheng",
        "Meiyuwang Zhang",
        "Ruyi Ji",
        "Xuechang Tu",
        "Zihan Zheng",
        "Zexing Chen",
        "Kangyang Zhou",
        "Zhaozi Wang",
        "Jingbang Chen",
        "Aleksandra Korolova",
        "Peter Henderson",
        "Pramod Viswanath",
        "Vijay Ganesh",
        "Saining Xie",
        "Zhuang Liu",
        "Dawn Song",
        "Sewon Min",
        "Ion Stoica",
        "Joseph E. Gonzalez",
        "Jingbo Shang",
        "Alvin Cheung"
      ],
      "abstract": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.",
      "published": "2025-12-17",
      "updated": "2025-12-17",
      "categories": [
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.15699v1",
        "pdf": "https://arxiv.org/pdf/2512.15699v1"
      },
      "arxiv_id": "2512.15699v1",
      "comment": "Code with instruction: https://github.com/FrontierCS/Frontier-CS",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8499999999999996,
      "score_breakdown": {
        "total_score": 3.85,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a soluti...",
        "key_contributions": [
          "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters.",
          "Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated.",
          "Models solve these tasks by implementing executable programs rather than outputting a direct answer."
        ],
        "provider": "gemini"
      }
    }
  ]
}