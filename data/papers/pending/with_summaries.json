{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2602.02381v1",
      "title": "Self-Supervised Learning from Structural Invariance",
      "authors": [
        "Yipeng Zhang",
        "Hafez Ghaemi",
        "Jungyoon Lee",
        "Shahab Bakhtiari",
        "Eilif B. Muller",
        "Laurent Charlin"
      ],
      "abstract": "Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02381v1",
        "pdf": "https://arxiv.org/pdf/2602.02381v1"
      },
      "arxiv_id": "2602.02381v1",
      "comment": "ICLR 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.4711864406779664,
      "score_breakdown": {
        "total_score": 4.47,
        "field_match": {
          "score": 0.68,
          "matches": [
            "self-supervised"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 10,
          "venue": "ICLR",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video f...",
        "key_contributions": [
          "Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs.",
          "We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets.",
          "This arises when data pairs come from naturally occurring generative processes, e."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.02370v1",
      "title": "Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes",
      "authors": [
        "Uma Meleti",
        "Jeffrey J. Nirschl"
      ],
      "abstract": "Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02370v1",
        "pdf": "https://arxiv.org/pdf/2602.02370v1"
      },
      "arxiv_id": "2602.02370v1",
      "comment": "Accepted for publication at the IEEE International Symposium on Biomedical Imaging (ISBI) 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.1245762711864415,
      "score_breakdown": {
        "total_score": 4.12,
        "field_match": {
          "score": 1.19,
          "matches": [
            "medical imaging",
            "deep learning"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 8,
          "venue": "ISBI",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 5.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We im...",
        "key_contributions": [
          "Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption.",
          "Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input.",
          "We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.02402v1",
      "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation",
      "authors": [
        "Mu Huang",
        "Hui Wang",
        "Kerui Ren",
        "Linning Xu",
        "Yunsong Zhou",
        "Mulin Yu",
        "Bo Dai",
        "Jiangmiao Pang"
      ],
      "abstract": "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "physics.app-ph"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02402v1",
        "pdf": "https://arxiv.org/pdf/2602.02402v1"
      },
      "arxiv_id": "2602.02402v1",
      "comment": "Project page: https://city-super.github.io/SoMA/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.038983050847458,
      "score_breakdown": {
        "total_score": 4.04,
        "field_match": {
          "score": 0.85,
          "matches": [
            "3d gaussian"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat sim...",
        "key_contributions": [
          "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions.",
          "Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization.",
          "This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.02331v1",
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "authors": [
        "Shaoting Zhu",
        "Baijun Ye",
        "Jiaxuan Wang",
        "Jiakang Chen",
        "Ziwen Zhuang",
        "Linzhan Mou",
        "Runhan Huang",
        "Hang Zhao"
      ],
      "abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02331v1",
        "pdf": "https://arxiv.org/pdf/2602.02331v1"
      },
      "arxiv_id": "2602.02331v1",
      "comment": "Project Page: https://ttt-parkour.github.io/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.0,
      "score_breakdown": {
        "total_score": 4.0,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel ...",
        "key_contributions": [
          "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics.",
          "Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments.",
          "To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.02380v1",
      "title": "Unified Personalized Reward Model for Vision Generation",
      "authors": [
        "Yibin Wang",
        "Yuhang Zang",
        "Feng Han",
        "Jiazi Bu",
        "Yujie Zhou",
        "Cheng Jin",
        "Jiaqi Wang"
      ],
      "abstract": "Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02380v1",
        "pdf": "https://arxiv.org/pdf/2602.02380v1"
      },
      "arxiv_id": "2602.02380v1",
      "comment": "Website: https://codegoat24.github.io/UnifiedReward/flex",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.95,
      "score_breakdown": {
        "total_score": 3.95,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-f...",
        "key_contributions": [
          "Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation.",
          "Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning.",
          "However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.02454v1",
      "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model",
      "authors": [
        "Ansh Kumar Sharma",
        "Yixiang Sun",
        "Ninghao Lu",
        "Yunzhe Zhang",
        "Jiarao Liu",
        "Sherry Yang"
      ],
      "abstract": "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02454v1",
        "pdf": "https://arxiv.org/pdf/2602.02454v1"
      },
      "arxiv_id": "2602.02454v1",
      "comment": "https://world-gymnast.github.io/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8499999999999996,
      "score_breakdown": {
        "total_score": 3.85,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models le...",
        "key_contributions": [
          "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction.",
          "The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation.",
          "With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.02474v1",
      "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents",
      "authors": [
        "Haozhen Zhang",
        "Quanyu Long",
        "Jianzhu Bao",
        "Tao Feng",
        "Weizhi Zhang",
        "Haodong Yue",
        "Wenya Wang"
      ],
      "abstract": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \\emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \\emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \\emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02474v1",
        "pdf": "https://arxiv.org/pdf/2602.02474v1"
      },
      "arxiv_id": "2602.02474v1",
      "comment": "Code is available at https://github.com/ViktorAxelsen/MemSkill",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8,
      "score_breakdown": {
        "total_score": 3.8,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable ...",
        "key_contributions": [
          "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory.",
          "These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories.",
          "To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.02280v1",
      "title": "RACA: Representation-Aware Coverage Criteria for LLM Safety Testing",
      "authors": [
        "Zeming Wei",
        "Zhixin Zhang",
        "Chengcan Wu",
        "Yihao Zhang",
        "Xiaokun Luan",
        "Meng Sun"
      ],
      "abstract": "Recent advancements in LLMs have led to significant breakthroughs in various AI applications. However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks. Current safety testing for LLMs often relies on static datasets and lacks systematic criteria to evaluate the quality and adequacy of these tests. While coverage criteria have been effective for smaller neural networks, they are not directly applicable to LLMs due to scalability issues and differing objectives. To address these challenges, this paper introduces RACA, a novel set of coverage criteria specifically designed for LLM safety testing. RACA leverages representation engineering to focus on safety-critical concepts within LLMs, thereby reducing dimensionality and filtering out irrelevant information. The framework operates in three stages: first, it identifies safety-critical representations using a small, expert-curated calibration set of jailbreak prompts. Second, it calculates conceptual activation scores for a given test suite based on these representations. Finally, it computes coverage results using six sub-criteria that assess both individual and compositional safety concepts. We conduct comprehensive experiments to validate RACA's effectiveness, applicability, and generalization, where the results demonstrate that RACA successfully identifies high-quality jailbreak prompts and is superior to traditional neuron-level criteria. We also showcase its practical application in real-world scenarios, such as test set prioritization and attack prompt sampling. Furthermore, our findings confirm RACA's generalization to various scenarios and its robustness across various configurations. Overall, RACA provides a new framework for evaluating the safety of LLMs, contributing a valuable technique to the field of testing for AI.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02280v1",
        "pdf": "https://arxiv.org/pdf/2602.02280v1"
      },
      "arxiv_id": "2602.02280v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.75,
      "score_breakdown": {
        "total_score": 3.75,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 7.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Recent advancements in LLMs have led to significant breakthroughs in various AI applications. However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks. Current safety testing for LLMs often relies on static datasets and lacks systematic criteria to evaluate the quality and adequacy of these tests. Whil...",
        "key_contributions": [
          "Recent advancements in LLMs have led to significant breakthroughs in various AI applications.",
          "However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks.",
          "Current safety testing for LLMs often relies on static datasets and lacks systematic criteria to evaluate the quality and adequacy of these tests."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.02426v1",
      "title": "SelvaMask: Segmenting Trees in Tropical Forests and Beyond",
      "authors": [
        "Simon-Olivier Duguay",
        "Hugo Baudchon",
        "Etienne Lalibert√©",
        "Helene Muller-Landau",
        "Gonzalo Rivas-Torres",
        "Arthur Ouaknine"
      ],
      "abstract": "Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02426v1",
        "pdf": "https://arxiv.org/pdf/2602.02426v1"
      },
      "arxiv_id": "2602.02426v1",
      "comment": "22 pages, 8 figures",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.7033898305084745,
      "score_breakdown": {
        "total_score": 3.7,
        "field_match": {
          "score": 0.51,
          "matches": [
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-b...",
        "key_contributions": [
          "Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance.",
          "Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems.",
          "Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2602.02488v1",
      "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",
      "authors": [
        "Yinjie Wang",
        "Tianbao Xie",
        "Ke Shen",
        "Mengdi Wang",
        "Ling Yang"
      ],
      "abstract": "We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL",
      "published": "2026-02-02",
      "updated": "2026-02-02",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2602.02488v1",
        "pdf": "https://arxiv.org/pdf/2602.02488v1"
      },
      "arxiv_id": "2602.02488v1",
      "comment": "Code: https://github.com/Gen-Verse/Open-AgentRL",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.65,
      "score_breakdown": {
        "total_score": 3.65,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized vi...",
        "key_contributions": [
          "We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios.",
          "Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training.",
          "Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience."
        ],
        "provider": "gemini"
      }
    }
  ]
}