{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2512.21135v1",
      "title": "TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation",
      "authors": [
        "Gaoren Lin",
        "Huangxuan Zhao",
        "Yuan Xiong",
        "Lefei Zhang",
        "Bo Du",
        "Wentao Zhu"
      ],
      "abstract": "Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.",
      "published": "2025-12-24",
      "updated": "2025-12-24",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.21135v1",
        "pdf": "https://arxiv.org/pdf/2512.21135v1"
      },
      "arxiv_id": "2512.21135v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 4.263559322033898,
      "score_breakdown": {
        "total_score": 4.26,
        "field_match": {
          "score": 2.03,
          "matches": [
            "medical image",
            "medical imaging",
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 7.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issue...",
        "key_contributions": [
          "Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information.",
          "However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion.",
          "While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.21337v1",
      "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
      "authors": [
        "Li-Zhong Szu-Tu",
        "Ting-Lin Wu",
        "Chia-Jui Chang",
        "He Syu",
        "Yu-Lun Liu"
      ],
      "abstract": "We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/",
      "published": "2025-12-24",
      "updated": "2025-12-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.21337v1",
        "pdf": "https://arxiv.org/pdf/2512.21337v1"
      },
      "arxiv_id": "2512.21337v1",
      "comment": "Project page: https://sytwu.github.io/BeyondMemo/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.050000000000001,
      "score_breakdown": {
        "total_score": 4.05,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building ima...",
        "key_contributions": [
          "We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding.",
          "To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity.",
          "Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.21311v1",
      "title": "Learning to Solve PDEs on Neural Shape Representations",
      "authors": [
        "Lilian Welschinger",
        "Yilin Liu",
        "Zican Wang",
        "Niloy Mitra"
      ],
      "abstract": "Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations. This mismatch leaves no suitable method to solve surface PDEs directly within the neural domain, forcing explicit mesh extraction or per-instance residual training, preventing end-to-end workflows. We present a novel, mesh-free formulation that learns a local update operator conditioned on neural (local) shape attributes, enabling surface PDEs to be solved directly where the (neural) data lives. The operator integrates naturally with prevalent neural surface representations, is trained once on a single representative shape, and generalizes across shape and topology variations, enabling accurate, fast inference without explicit meshing or per-instance optimization while preserving differentiability. Across analytic benchmarks (heat equation and Poisson solve on sphere) and real neural assets across different representations, our method slightly outperforms CPM while remaining reasonably close to FEM, and, to our knowledge, delivers the first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations. Code will be released on acceptance.",
      "published": "2025-12-24",
      "updated": "2025-12-24",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "links": {
        "paper": "http://arxiv.org/abs/2512.21311v1",
        "pdf": "https://arxiv.org/pdf/2512.21311v1"
      },
      "arxiv_id": "2512.21311v1",
      "comment": "Article webpage link: https://welschinger.github.io/Learning-to-Solve-PDEs-on-Neural-Shape-Representations/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.0,
      "score_breakdown": {
        "total_score": 4.0,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations. This mismatch leaves no suitable method to solve surface PDEs directly within the neural domain, forcing explicit mesh extraction or per-instance residual tra...",
        "key_contributions": [
          "Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations.",
          "This mismatch leaves no suitable method to solve surface PDEs directly within the neural domain, forcing explicit mesh extraction or per-instance residual training, preventing end-to-end workflows.",
          "We present a novel, mesh-free formulation that learns a local update operator conditioned on neural (local) shape attributes, enabling surface PDEs to be solved directly where the (neural) data lives."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.21099v1",
      "title": "TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars",
      "authors": [
        "Jaeseong Lee",
        "Junyeong Ahn",
        "Taewoong Kang",
        "Jaegul Choo"
      ],
      "abstract": "Constructing drivable and photorealistic 3D head avatars has become a central task in AR/XR, enabling immersive and expressive user experiences. With the emergence of high-fidelity and efficient representations such as 3D Gaussians, recent works have pushed toward ultra-detailed head avatars. Existing approaches typically fall into two categories: rule-based analytic rigging or neural network-based deformation fields. While effective in constrained settings, both approaches often fail to generalize to unseen expressions and poses, particularly in extreme reenactment scenarios. Other methods constrain Gaussians to the global texel space of 3DMMs to reduce rendering complexity. However, these texel-based avatars tend to underutilize the underlying mesh structure. They apply minimal analytic deformation and rely heavily on neural regressors and heuristic regularization in UV space, which weakens geometric consistency and limits extrapolation to complex, out-of-distribution deformations. To address these limitations, we introduce TexAvatars, a hybrid avatar representation that combines the explicit geometric grounding of analytic rigging with the spatial continuity of texel space. Our approach predicts local geometric attributes in UV space via CNNs, but drives 3D deformation through mesh-aware Jacobians, enabling smooth and semantically meaningful transitions across triangle boundaries. This hybrid design separates semantic modeling from geometric control, resulting in improved generalization, interpretability, and stability. Furthermore, TexAvatars captures fine-grained expression effects, including muscle-induced wrinkles, glabellar lines, and realistic mouth cavity geometry, with high fidelity. Our method achieves state-of-the-art performance under extreme pose and expression variations, demonstrating strong generalization in challenging head reenactment settings.",
      "published": "2025-12-24",
      "updated": "2025-12-24",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "links": {
        "paper": "http://arxiv.org/abs/2512.21099v1",
        "pdf": "https://arxiv.org/pdf/2512.21099v1"
      },
      "arxiv_id": "2512.21099v1",
      "comment": "3DV 2026, Project page with videos: https://summertight.github.io/TexAvatars/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.9889830508474575,
      "score_breakdown": {
        "total_score": 3.99,
        "field_match": {
          "score": 0.85,
          "matches": [
            "3d gaussian"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Constructing drivable and photorealistic 3D head avatars has become a central task in AR/XR, enabling immersive and expressive user experiences. With the emergence of high-fidelity and efficient representations such as 3D Gaussians, recent works have pushed toward ultra-detailed head avatars. Existing approaches typically fall into two categories: rule-based analytic rigging or neural network-base...",
        "key_contributions": [
          "Constructing drivable and photorealistic 3D head avatars has become a central task in AR/XR, enabling immersive and expressive user experiences.",
          "With the emergence of high-fidelity and efficient representations such as 3D Gaussians, recent works have pushed toward ultra-detailed head avatars.",
          "Existing approaches typically fall into two categories: rule-based analytic rigging or neural network-based deformation fields."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.21252v1",
      "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation",
      "authors": [
        "Jiawei Liu",
        "Junqiao Li",
        "Jiangfan Deng",
        "Gen Li",
        "Siyu Zhou",
        "Zetao Fang",
        "Shanshan Lao",
        "Zengde Deng",
        "Jianing Zhu",
        "Tingting Ma",
        "Jiayi Li",
        "Yunqiu Wang",
        "Qian He",
        "Xinglong Wu"
      ],
      "abstract": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.",
      "published": "2025-12-24",
      "updated": "2025-12-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.21252v1",
        "pdf": "https://arxiv.org/pdf/2512.21252v1"
      },
      "arxiv_id": "2512.21252v1",
      "comment": "Project Page: https://dreamontage.github.io/DreaMontage/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8,
      "score_breakdown": {
        "total_score": 3.8,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal...",
        "key_contributions": [
          "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking.",
          "However, its practical realization is often hindered by prohibitive costs and complex real-world constraints.",
          "Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.20996v1",
      "title": "TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control",
      "authors": [
        "Yuwei Du",
        "Jun Zhang",
        "Jie Feng",
        "Zhicheng Liu",
        "Jian Yuan",
        "Yong Li"
      ],
      "abstract": "Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.",
      "published": "2025-12-24",
      "updated": "2025-12-24",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "links": {
        "paper": "http://arxiv.org/abs/2512.20996v1",
        "pdf": "https://arxiv.org/pdf/2512.20996v1"
      },
      "arxiv_id": "2512.20996v1",
      "comment": "The code will be available at: https://github.com/tsinghua-fib-lab/TrafficSimAgent",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.8,
      "score_breakdown": {
        "total_score": 3.8,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 7.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent,...",
        "key_contributions": [
          "Traffic simulation is important for transportation optimization and policy making.",
          "While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work.",
          "To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.21038v1",
      "title": "Next-Scale Prediction: A Self-Supervised Approach for Real-World Image Denoising",
      "authors": [
        "Yiwen Shan",
        "Haiyu Zhao",
        "Peng Hu",
        "Xi Peng",
        "Yuanbiao Gou"
      ],
      "abstract": "Self-supervised real-world image denoising remains a fundamental challenge, arising from the antagonistic trade-off between decorrelating spatially structured noise and preserving high-frequency details. Existing blind-spot network (BSN) methods rely on pixel-shuffle downsampling (PD) to decorrelate noise, but aggressive downsampling fragments fine structures, while milder downsampling fails to remove correlated noise. To address this, we introduce Next-Scale Prediction (NSP), a novel self-supervised paradigm that decouples noise decorrelation from detail preservation. NSP constructs cross-scale training pairs, where BSN takes low-resolution, fully decorrelated sub-images as input to predict high-resolution targets that retain fine details. As a by-product, NSP naturally supports super-resolution of noisy images without retraining or modification. Extensive experiments demonstrate that NSP achieves state-of-the-art self-supervised denoising performance on real-world benchmarks, significantly alleviating the long-standing conflict between noise decorrelation and detail preservation.",
      "published": "2025-12-24",
      "updated": "2025-12-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.21038v1",
        "pdf": "https://arxiv.org/pdf/2512.21038v1"
      },
      "arxiv_id": "2512.21038v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.771186440677966,
      "score_breakdown": {
        "total_score": 3.77,
        "field_match": {
          "score": 0.68,
          "matches": [
            "self-supervised"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Self-supervised real-world image denoising remains a fundamental challenge, arising from the antagonistic trade-off between decorrelating spatially structured noise and preserving high-frequency details. Existing blind-spot network (BSN) methods rely on pixel-shuffle downsampling (PD) to decorrelate noise, but aggressive downsampling fragments fine structures, while milder downsampling fails to re...",
        "key_contributions": [
          "Self-supervised real-world image denoising remains a fundamental challenge, arising from the antagonistic trade-off between decorrelating spatially structured noise and preserving high-frequency details.",
          "Existing blind-spot network (BSN) methods rely on pixel-shuffle downsampling (PD) to decorrelate noise, but aggressive downsampling fragments fine structures, while milder downsampling fails to remove correlated noise.",
          "To address this, we introduce Next-Scale Prediction (NSP), a novel self-supervised paradigm that decouples noise decorrelation from detail preservation."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.21204v1",
      "title": "SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation",
      "authors": [
        "Mahi Luthra",
        "Jiayi Shen",
        "Maxime Poli",
        "Angelo Ortiz",
        "Yosuke Higuchi",
        "Youssef Benchekroun",
        "Martin Gleize",
        "Charles-Eric Saint-James",
        "Dongyan Lin",
        "Phillip Rust",
        "Angel Villar",
        "Surya Parimi",
        "Vanessa Stark",
        "Rashel Moritz",
        "Juan Pino",
        "Yann LeCun",
        "Emmanuel Dupoux"
      ],
      "abstract": "Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.",
      "published": "2025-12-24",
      "updated": "2025-12-24",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "links": {
        "paper": "http://arxiv.org/abs/2512.21204v1",
        "pdf": "https://arxiv.org/pdf/2512.21204v1"
      },
      "arxiv_id": "2512.21204v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.7211864406779656,
      "score_breakdown": {
        "total_score": 3.72,
        "field_match": {
          "score": 0.68,
          "matches": [
            "self-supervised"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning ...",
        "key_contributions": [
          "Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models.",
          "To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data.",
          "We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.21284v1",
      "title": "Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential",
      "authors": [
        "Shihao Zou",
        "Jingjing Li",
        "Wei Ji",
        "Jincai Huang",
        "Kai Wang",
        "Guo Dan",
        "Weixin Si",
        "Yi Pan"
      ],
      "abstract": "Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \\textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\\times$. Notably, it delivers over $20\\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.",
      "published": "2025-12-24",
      "updated": "2025-12-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.21284v1",
        "pdf": "https://arxiv.org/pdf/2512.21284v1"
      },
      "arxiv_id": "2512.21284v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.672881355932203,
      "score_breakdown": {
        "total_score": 3.67,
        "field_match": {
          "score": 0.93,
          "matches": [
            "segmentation",
            "deep learning"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their su...",
        "key_contributions": [
          "Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety.",
          "Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events.",
          "Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2512.21331v1",
      "title": "TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning",
      "authors": [
        "Varun Belagali",
        "Saarthak Kapse",
        "Pierre Marza",
        "Srijan Das",
        "Zilinghan Li",
        "Sofi√®ne Boutaj",
        "Pushpak Pati",
        "Srikar Yellapragada",
        "Tarak Nath Nandi",
        "Ravi K Madduri",
        "Joel Saltz",
        "Prateek Prasanna",
        "Stergios Christodoulidis Maria Vakalopoulou",
        "Dimitris Samaras"
      ],
      "abstract": "The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from ''any'' tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.",
      "published": "2025-12-24",
      "updated": "2025-12-24",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2512.21331v1",
        "pdf": "https://arxiv.org/pdf/2512.21331v1"
      },
      "arxiv_id": "2512.21331v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.65,
      "score_breakdown": {
        "total_score": 3.65,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the ric...",
        "key_contributions": [
          "The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context.",
          "We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology.",
          "Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks."
        ],
        "provider": "gemini"
      }
    }
  ]
}