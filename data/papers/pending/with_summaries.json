{
  "filtered_at": "{\"timestamp\": \"now\"}",
  "total_papers": 10,
  "papers": [
    {
      "id": "2511.10518v1",
      "title": "SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation",
      "authors": [
        "Wei Li",
        "Renshan Zhang",
        "Rui Shao",
        "Zhijian Fang",
        "Kaiwen Zhou",
        "Zhuotao Tian",
        "Liqiang Nie"
      ],
      "abstract": "Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10518v1",
        "pdf": "https://arxiv.org/pdf/2511.10518v1"
      },
      "arxiv_id": "2511.10518v1",
      "comment": "Accepted to AAAI 2026 (Oral), Project Page: https://github.com/JiuTian-VL/SemanticVLA",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.4,
      "score_breakdown": {
        "total_score": 4.4,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Sema...",
        "key_contributions": [
          "Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions.",
          "In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation.",
          "Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.10647v1",
      "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
      "authors": [
        "Haotong Lin",
        "Sili Chen",
        "Junhao Liew",
        "Donny Y. Chen",
        "Zhenyu Li",
        "Guang Shi",
        "Jiashi Feng",
        "Bingyi Kang"
      ],
      "abstract": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10647v1",
        "pdf": "https://arxiv.org/pdf/2511.10647v1"
      },
      "arxiv_id": "2511.10647v1",
      "comment": "https://depth-anything-3.github.io/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.35,
      "score_breakdown": {
        "total_score": 4.35,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates...",
        "key_contributions": [
          "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses.",
          "In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.",
          "g."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.10334v1",
      "title": "Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment",
      "authors": [
        "Wenti Yin",
        "Huaxin Zhang",
        "Xiang Wang",
        "Yuqing Lu",
        "Yicheng Zhang",
        "Bingquan Gong",
        "Jialong Zuo",
        "Li Yu",
        "Changxin Gao",
        "Nong Sang"
      ],
      "abstract": "Recent advancements in weakly-supervised video anomaly detection have achieved remarkable performance by applying the multiple instance learning paradigm based on multimodal foundation models such as CLIP to highlight anomalous instances and classify categories. However, their objectives may tend to detect the most salient response segments, while neglecting to mine diverse normal patterns separated from anomalies, and are prone to category confusion due to similar appearance, leading to unsatisfactory fine-grained classification results. Therefore, we propose a novel Disentangled Semantic Alignment Network (DSANet) to explicitly separate abnormal and normal features from coarse-grained and fine-grained aspects, enhancing the distinguishability. Specifically, at the coarse-grained level, we introduce a self-guided normality modeling branch that reconstructs input video features under the guidance of learned normal prototypes, encouraging the model to exploit normality cues inherent in the video, thereby improving the temporal separation of normal patterns and anomalous events. At the fine-grained level, we present a decoupled contrastive semantic alignment mechanism, which first temporally decomposes each video into event-centric and background-centric components using frame-level anomaly scores and then applies visual-language contrastive learning to enhance class-discriminative representations. Comprehensive experiments on two standard benchmarks, namely XD-Violence and UCF-Crime, demonstrate that DSANet outperforms existing state-of-the-art methods.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10334v1",
        "pdf": "https://arxiv.org/pdf/2511.10334v1"
      },
      "arxiv_id": "2511.10334v1",
      "comment": "Accepted to AAAI 2026. Code is available at https://github.com/lessiYin/DSANet",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.3,
      "score_breakdown": {
        "total_score": 4.3,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 10.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Recent advancements in weakly-supervised video anomaly detection have achieved remarkable performance by applying the multiple instance learning paradigm based on multimodal foundation models such as CLIP to highlight anomalous instances and classify categories. However, their objectives may tend to detect the most salient response segments, while neglecting to mine diverse normal patterns separat...",
        "key_contributions": [
          "Recent advancements in weakly-supervised video anomaly detection have achieved remarkable performance by applying the multiple instance learning paradigm based on multimodal foundation models such as CLIP to highlight anomalous instances and classify categories.",
          "However, their objectives may tend to detect the most salient response segments, while neglecting to mine diverse normal patterns separated from anomalies, and are prone to category confusion due to similar appearance, leading to unsatisfactory fine-grained classification results.",
          "Therefore, we propose a novel Disentangled Semantic Alignment Network (DSANet) to explicitly separate abnormal and normal features from coarse-grained and fine-grained aspects, enhancing the distinguishability."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.10560v1",
      "title": "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded",
      "authors": [
        "Haosong Peng",
        "Hao Li",
        "Yalun Dai",
        "Yushi Lan",
        "Yihang Luo",
        "Tianyu Qi",
        "Zhengshen Zhang",
        "Yufeng Zhan",
        "Junfei Zhang",
        "Wenchao Xu",
        "Ziwei Liu"
      ],
      "abstract": "General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10560v1",
        "pdf": "https://arxiv.org/pdf/2511.10560v1"
      },
      "arxiv_id": "2511.10560v1",
      "comment": "Project Page: https://livioni.github.io/OmniVGGT-offcial/",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 4.199999999999999,
      "score_breakdown": {
        "total_score": 4.2,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inf...",
        "key_contributions": [
          "General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.",
          "g.",
          ", camera intrinsics, poses, and depth maps)."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.10316v1",
      "title": "Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision",
      "authors": [
        "Yu Deng",
        "Baozhu Zhao",
        "Junyan Su",
        "Xiaohan Zhang",
        "Qi Liu"
      ],
      "abstract": "Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions. Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions. This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting. Our approach comprises two core components: (1) Depth-of-field Supervision employs a scale-recovered monocular depth estimator (e.g., Metric3D) to generate depth priors, leverages defocus convolution to synthesize physically accurate defocused images, and enforces geometric consistency through a novel depth-of-field loss, thereby enhancing depth fidelity in both far-field and near-field regions; (2) Multi-View Consistency Supervision employing LoFTR-based semi-dense feature matching to minimize cross-view geometric errors and enforce depth consistency via least squares optimization of reliable matched points. By unifying defocus physics with multi-view geometric constraints, our method achieves superior depth fidelity, demonstrating a 0.8 dB PSNR improvement over the state-of-the-art method on the Waymo Open Dataset. This framework bridges physical imaging principles and learning-based depth regularization, offering a scalable solution for complex depth stratification in urban environments.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10316v1",
        "pdf": "https://arxiv.org/pdf/2511.10316v1"
      },
      "arxiv_id": "2511.10316v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.977966101694915,
      "score_breakdown": {
        "total_score": 3.98,
        "field_match": {
          "score": 1.69,
          "matches": [
            "3d gaussian",
            "gaussian splatting"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 8.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions. Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions. This paper proposes a novel computational framework that integrates depth-of-fiel...",
        "key_contributions": [
          "Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions.",
          "Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions.",
          "This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.10484v1",
      "title": "Utility of Pancreas Surface Lobularity as a CT Biomarker for Opportunistic Screening of Type 2 Diabetes",
      "authors": [
        "Tejas Sudharshan Mathai",
        "Anisa V. Prasad",
        "Xinya Wang",
        "Praveen T. S. Balamuralikrishna",
        "Yan Zhuang",
        "Abhinav Suri",
        "Jianfei Liu",
        "Perry J. Pickhardt",
        "Ronald M. Summers"
      ],
      "abstract": "Type 2 Diabetes Mellitus (T2DM) is a chronic metabolic disease that affects millions of people worldwide. Early detection is crucial as it can alter pancreas function through morphological changes and increased deposition of ectopic fat, eventually leading to organ damage. While studies have shown an association between T2DM and pancreas volume and fat content, the role of increased pancreatic surface lobularity (PSL) in patients with T2DM has not been fully investigated. In this pilot work, we propose a fully automated approach to delineate the pancreas and other abdominal structures, derive CT imaging biomarkers, and opportunistically screen for T2DM. Four deep learning-based models were used to segment the pancreas in an internal dataset of 584 patients (297 males, 437 non-diabetic, age: 45$\\pm$15 years). PSL was automatically detected and it was higher for diabetic patients (p=0.01) at 4.26 $\\pm$ 8.32 compared to 3.19 $\\pm$ 3.62 for non-diabetic patients. The PancAP model achieved the highest Dice score of 0.79 $\\pm$ 0.17 and lowest ASSD error of 1.94 $\\pm$ 2.63 mm (p$<$0.05). For predicting T2DM, a multivariate model trained with CT biomarkers attained 0.90 AUC, 66.7\\% sensitivity, and 91.9\\% specificity. Our results suggest that PSL is useful for T2DM screening and could potentially help predict the early onset of T2DM.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10484v1",
        "pdf": "https://arxiv.org/pdf/2511.10484v1"
      },
      "arxiv_id": "2511.10484v1",
      "comment": "Submitted to IEEE ISBI 2026",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.9194915254237284,
      "score_breakdown": {
        "total_score": 3.92,
        "field_match": {
          "score": 0.42,
          "matches": [
            "deep learning"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 8,
          "venue": "ISBI",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Type 2 Diabetes Mellitus (T2DM) is a chronic metabolic disease that affects millions of people worldwide. Early detection is crucial as it can alter pancreas function through morphological changes and increased deposition of ectopic fat, eventually leading to organ damage. While studies have shown an association between T2DM and pancreas volume and fat content, the role of increased pancreatic sur...",
        "key_contributions": [
          "Type 2 Diabetes Mellitus (T2DM) is a chronic metabolic disease that affects millions of people worldwide.",
          "Early detection is crucial as it can alter pancreas function through morphological changes and increased deposition of ectopic fat, eventually leading to organ damage.",
          "While studies have shown an association between T2DM and pancreas volume and fat content, the role of increased pancreatic surface lobularity (PSL) in patients with T2DM has not been fully investigated."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.10367v1",
      "title": "DermAI: Clinical dermatology acquisition through quality-driven image collection for AI classification in mobile",
      "authors": [
        "Thales Bezerra",
        "Emanoel Thyago",
        "Kelvin Cunha",
        "Rodrigo Abreu",
        "Fábio Papais",
        "Francisco Mauro",
        "Natália Lopes",
        "Érico Medeiros",
        "Jéssica Guido",
        "Shirley Cruz",
        "Paulo Borba",
        "Tsang Ing Ren"
      ],
      "abstract": "AI-based dermatology adoption remains limited by biased datasets, variable image quality, and limited validation. We introduce DermAI, a lightweight, smartphone-based application that enables real-time capture, annotation, and classification of skin lesions during routine consultations. Unlike prior dermoscopy-focused tools, DermAI performs on-device quality checks, and local model adaptation. The DermAI clinical dataset, encompasses a wide range of skin tones, ethinicity and source devices. In preliminary experiments, models trained on public datasets failed to generalize to our samples, while fine-tuning with local data improved performance. These results highlight the importance of standardized, diverse data collection aligned with healthcare needs and oriented to machine learning development.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10367v1",
        "pdf": "https://arxiv.org/pdf/2511.10367v1"
      },
      "arxiv_id": "2511.10367v1",
      "comment": "4 pages, 2 figures, 1 table, submitted on ISBI",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.85,
      "score_breakdown": {
        "total_score": 3.85,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 8,
          "venue": "ISBI",
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "AI-based dermatology adoption remains limited by biased datasets, variable image quality, and limited validation. We introduce DermAI, a lightweight, smartphone-based application that enables real-time capture, annotation, and classification of skin lesions during routine consultations. Unlike prior dermoscopy-focused tools, DermAI performs on-device quality checks, and local model adaptation. The...",
        "key_contributions": [
          "AI-based dermatology adoption remains limited by biased datasets, variable image quality, and limited validation.",
          "We introduce DermAI, a lightweight, smartphone-based application that enables real-time capture, annotation, and classification of skin lesions during routine consultations.",
          "Unlike prior dermoscopy-focused tools, DermAI performs on-device quality checks, and local model adaptation."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.10308v1",
      "title": "Revisiting Evaluation of Deep Neural Networks for Pedestrian Detection",
      "authors": [
        "Patrick Feifel",
        "Benedikt Franke",
        "Frank Bonarens",
        "Frank Köster",
        "Arne Raulf",
        "Friedhelm Schwenker"
      ],
      "abstract": "Reliable pedestrian detection represents a crucial step towards automated driving systems. However, the current performance benchmarks exhibit weaknesses. The currently applied metrics for various subsets of a validation dataset prohibit a realistic performance evaluation of a DNN for pedestrian detection. As image segmentation supplies fine-grained information about a street scene, it can serve as a starting point to automatically distinguish between different types of errors during the evaluation of a pedestrian detector. In this work, eight different error categories for pedestrian detection are proposed and new metrics are proposed for performance comparison along these error categories. We use the new metrics to compare various backbones for a simplified version of the APD, and show a more fine-grained and robust way to compare models with each other especially in terms of safety-critical performance. We achieve SOTA on CityPersons-reasonable (without extra training data) by using a rather simple architecture.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10308v1",
        "pdf": "https://arxiv.org/pdf/2511.10308v1"
      },
      "arxiv_id": "2511.10308v1",
      "comment": "",
      "journal_ref": "2022 Workshop on Artificial Intelligence Safety, AISafety 2022",
      "has_code": false,
      "relevance_score": 3.7033898305084745,
      "score_breakdown": {
        "total_score": 3.7,
        "field_match": {
          "score": 0.51,
          "matches": [
            "segmentation"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 6.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Reliable pedestrian detection represents a crucial step towards automated driving systems. However, the current performance benchmarks exhibit weaknesses. The currently applied metrics for various subsets of a validation dataset prohibit a realistic performance evaluation of a DNN for pedestrian detection. As image segmentation supplies fine-grained information about a street scene, it can serve a...",
        "key_contributions": [
          "Reliable pedestrian detection represents a crucial step towards automated driving systems.",
          "However, the current performance benchmarks exhibit weaknesses.",
          "The currently applied metrics for various subsets of a validation dataset prohibit a realistic performance evaluation of a DNN for pedestrian detection."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.10488v1",
      "title": "SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers",
      "authors": [
        "Oded Schlesinger",
        "Amirhossein Farzam",
        "J. Matias Di Martino",
        "Guillermo Sapiro"
      ],
      "abstract": "While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process. SPOT informs token sparsification and facilitates the elimination of such tokens, improving computational efficiency without sacrificing performance. SPOT employs computationally lightweight predictors that can be plugged into various ViT architectures and learn to derive effective input-specific token prioritization across layers. Its versatile design supports a range of performance levels adaptable to varying resource constraints. Empirical evaluations demonstrate significant efficiency gains of up to 40% compared to standard ViTs, while maintaining or even improving accuracy. Code and models are available at https://github.com/odedsc/SPOT .",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10488v1",
        "pdf": "https://arxiv.org/pdf/2511.10488v1"
      },
      "arxiv_id": "2511.10488v1",
      "comment": "Project repository: https://github.com/odedsc/SPOT",
      "journal_ref": "",
      "has_code": true,
      "relevance_score": 3.65,
      "score_breakdown": {
        "total_score": 3.65,
        "field_match": {
          "score": 0.0,
          "matches": [],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 6.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 10.0,
          "has_code": true,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.0,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we prese...",
        "key_contributions": [
          "While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens.",
          "Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation.",
          "Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process."
        ],
        "provider": "gemini"
      }
    },
    {
      "id": "2511.10604v1",
      "title": "Multitask GLocal OBIA-Mamba for Sentinel-2 Landcover Mapping",
      "authors": [
        "Zack Dewis",
        "Yimin Zhu",
        "Zhengsen Xu",
        "Mabel Heffring",
        "Saeid Taleghanidoozdoozan",
        "Kaylee Xiao",
        "Motasem Alkayid",
        "Lincoln Linlin Xu"
      ],
      "abstract": "Although Sentinel-2 based land use and land cover (LULC) classification is critical for various environmental monitoring applications, it is a very difficult task due to some key data challenges (e.g., spatial heterogeneity, context information, signature ambiguity). This paper presents a novel Multitask Glocal OBIA-Mamba (MSOM) for enhanced Sentinel-2 classification with the following contributions. First, an object-based image analysis (OBIA) Mamba model (OBIA-Mamba) is designed to reduce redundant computation without compromising fine-grained details by using superpixels as Mamba tokens. Second, a global-local (GLocal) dual-branch convolutional neural network (CNN)-mamba architecture is designed to jointly model local spatial detail and global contextual information. Third, a multitask optimization framework is designed to employ dual loss functions to balance local precision with global consistency. The proposed approach is tested on Sentinel-2 imagery in Alberta, Canada, in comparison with several advanced classification approaches, and the results demonstrate that the proposed approach achieves higher classification accuracy and finer details that the other state-of-the-art methods.",
      "published": "2025-11-13",
      "updated": "2025-11-13",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "links": {
        "paper": "http://arxiv.org/abs/2511.10604v1",
        "pdf": "https://arxiv.org/pdf/2511.10604v1"
      },
      "arxiv_id": "2511.10604v1",
      "comment": "",
      "journal_ref": "",
      "has_code": false,
      "relevance_score": 3.6194915254237285,
      "score_breakdown": {
        "total_score": 3.62,
        "field_match": {
          "score": 0.42,
          "matches": [
            "image analysis"
          ],
          "weight": 0.4
        },
        "venue_quality": {
          "score": 5.0,
          "venue": null,
          "weight": 0.25
        },
        "citation_potential": {
          "score": 9.0,
          "weight": 0.15
        },
        "code_availability": {
          "score": 3.0,
          "has_code": false,
          "weight": 0.1
        },
        "practicality": {
          "score": 5.5,
          "weight": 0.1
        }
      },
      "ai_summaries": {
        "short": "Although Sentinel-2 based land use and land cover (LULC) classification is critical for various environmental monitoring applications, it is a very difficult task due to some key data challenges (e.g., spatial heterogeneity, context information, signature ambiguity). This paper presents a novel Multitask Glocal OBIA-Mamba (MSOM) for enhanced Sentinel-2 classification with the following contributio...",
        "key_contributions": [
          "Although Sentinel-2 based land use and land cover (LULC) classification is critical for various environmental monitoring applications, it is a very difficult task due to some key data challenges (e.",
          "g.",
          ", spatial heterogeneity, context information, signature ambiguity)."
        ],
        "provider": "gemini"
      }
    }
  ]
}